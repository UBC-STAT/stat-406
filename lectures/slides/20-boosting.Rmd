---
title: "20 Boosting"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

## Last time



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```


```{r css-extras, file="css-extras.R", echo=FALSE}
```


We learned about bagging, for averaging .secondary[low-bias] / .primary[high-variance] estimators.

Today, we examine it's opposite: __Boosting__.

__Boosting__ also combines estimators, but it combines __high-bias__ / low-variance estimators.

Boosting has a number of flavours. And if you Google descriptions, most are wrong.

For a deep (and accurate) treatment, see [ESL] Chapter 10


--

We'll discuss 2 flavours: AdaBoost and Gradient Boosting

Neither requires a tree, but that's the typical usage.

Boosting needs a "weak learner", so small trees (called stumps) are natural.

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$


---


## AdaBoost intuition

At each iteration, we weight the __observations__.

Observations that are currently misclassified, get __higher__ weights.

So on the next iteration, we'll try harder to correctly classify our mistakes.

The number of iterations must be chosen.

---

## AdaBoost (Freund and Schapire)

Let $G(x, \theta)$ be a weak learner (say a tree with one split)

.emphasis[

__Algorithm (AdaBoost):__

1. Set observation weights $w_i=1/n$.

2. Until we quit ( $m<M$ iterations )
    
    a. Estimate the classifier $G(x,\theta_m)$ using weights $w_i$
    
    b. Calculate it's weighted error $\textrm{err}_m = \sum_{i=1}^n w_i I(y_i \neq G(x_i, \theta_m)) / \sum w_i$
    
    c. Set $\alpha_m = \log((1-\textrm{err}_m)/\text{err}_m)$
    
    d. Update $w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G(x_i,\theta_m)))$

3. Final classifier is $G(x) = \textrm{sign}\left( \sum_{m=1}^M \alpha_m G(x, \theta_m)\right)$
]

---

## Using mobility data again

```{r echo=FALSE}
library(kableExtra)
library(randomForest)
mob <- readRDS("data/mobility.RDS") %>%
  mutate(mobile = as.factor(Mobility > .1)) %>%
  dplyr::select(-ID, -Name, -Mobility, -State) %>% 
  drop_na()
n <- nrow(mob)
trainidx <- sample.int(n, floor(n * .75))
testidx <- setdiff(1:n, trainidx)
train <- mob[trainidx, ]
test <- mob[testidx,]
rf <- randomForest(mobile~., data = train)
bag <- randomForest(mobile~., data = train, 
                    mtry = ncol(mob) - 1)
preds <-  tibble(
  truth = test$mobile,
  rf = predict(rf, test),
  bag = predict(bag, test))
```

```{r, fig.align='center', fig.height=5, fig.width=10}
library(gbm)
train_boost <- train %>% mutate(mobile = as.integer(mobile) - 1) # needs {0, 1} responses
test_boost <- test %>% mutate(mobile = as.integer(mobile) - 1)
adab <- gbm(mobile ~ ., data = train_boost, n.trees = 500, distribution = "adaboost")
preds$adab = as.numeric(predict(adab, test_boost) > 0)
par(mar = c(5,15,0,1))
summary(adab, las = 1)
```

---

## Forward stagewise additive modeling

Generic for regression or classification, any weak learner $G(x,\ \theta)$

.emphasis[

__Algorithm:__

1. Set initial predictor $f_0(x)=0$

2. Until we quit ( $m<M$ iterations )
    
    a. Compute $$(\beta_m, \theta_m) = \arg\min_{\beta, \theta} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i,\ \theta)\right)$$
    
    b. Set $f_m(x) = f_{m-1}(x) + \beta_m G(x,\ \theta_m)$
    
3. Final classifier is $G(x, \theta_M) = \textrm{sign}\left( f_M(x) \right)$
]

Here, $L$ is a loss function that measures prediction accuracy

If $L(y,\ f(x))= \exp(-y f(x))$, $G$ is a classifier, and $y \in \{-1, 1\}$ then this is equivalent to AdaBoost. Proven 5 years later (Friedman, Hastie, and Tibshirani 2000).

---

## So what?

It turns out that "exponential loss" $L(y,\ f(x))= \exp(-y f(x))$ is not very robust.

Here are some other loss functions for 2-class classification

```{r loss-funs, fig.align="center", echo=FALSE, fig.height=4, fig.width=8}
losses = tibble(x = seq(-2,2,length.out = 100),
       `Misclassification (0-1)` = as.numeric(x<0),
       Exponential = exp(-x),
       Binomial_deviance = log2(1 + exp(-x)),
       Squared_error = (x-1)^2,
       Support_vector = pmax((1-x),0))
losses %>% pivot_longer(-x) %>%
  ggplot(aes(x, y=value, color=name)) + 
  geom_line(size = 1.5) +
  coord_cartesian(ylim=c(0,3)) + 
  theme(legend.title = element_blank()) + 
  scale_color_viridis_d() +
  ylab("Loss") + 
  xlab(bquote(y~f(x)~(Margin)))
```

--

Want losses which penalize negative margin, but not positive margins.

Robust means .hand[don't over-penalize large negatives]

---

## Gradient boosting


In the forward stagewise algorithm, we solved a minimization and then made an update 

$f_m(x) = f_{m-1}(x) + \beta_m G(x, \theta_m)$.

For most loss functions, $\arg\min_{\beta, \theta} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i, \theta)\right)$ cannot be solved

Instead, if we take one gradient step toward the minimum, we get

$f_m(x) = f_{m-1}(x) -\gamma_m \nabla L(y,f_{m-1}(x)) = f_{m-1}(x) +\gamma_m \left(-\nabla L(y,f_{m-1}(x))\right)$

This is called __Gradient boosting__

Notice how similar the update steps look.

Gradient boosting goes only part of the way toward the minimum at each $m$. This has two implications:

1. Since we're not fitting $\beta, \theta$ to the data as "hard", the learner is weaker.

2. This procedure is computationally much simpler.


---

## Gradient boosting

.emphasis[

__Algorithm:__

1. Set initial predictor $f_0(x)=\overline{\y}$

2. Until we quit ( $m<M$ iterations )
    
    a. Compute pseudo-residuals (what is the gradient of $L(y,f)=(y-f(x))^2$?)
    $$r_i = -\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{m-1}(x_i)}$$
    
    b. Estimate weak learner, $G(x, \theta_m)$, with the training set $\{r_i, x_i\}$.
    
    c. Find the step size $\gamma_m = \arg\min_\gamma \sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \gamma G(x_i, \theta_m))$
    
    b. Set $f_m(x) = f_{m-1}(x) + \gamma_m G(x, \theta_m)$
    
3. Final predictor is $f_M(x)$.
]

```{r gbm}
grad_boost <- gbm(mobile ~ ., data = train_boost, n.trees = 500, distribution = "bernoulli")
```

---

## Gradient boosting modifications

* Typically done with "small" trees, not stumps because of the gradient. You can specify the size. Usually 4-8 terminal nodes is recommended (more gives more interactions between predictors)

* Usually modify the gradient step to $f_m(x) = f_{m-1}(x) + \gamma_m \alpha G(x,\theta_m)$ with $0<\alpha<1$. Helps to keep from fitting too hard.

* Often combined with Bagging so that each step is fit using a bootstrap resample of the data. Gives us out-of-bag options.

* There are many other extensions, notably XGBoost.

```{r, fig.height=3.5, fig.width=10, fig.align='center', echo=FALSE}
library(cowplot)
boost_preds = tibble(
  adaboost = predict(adab, test_boost),
  gbm = predict(grad_boost, test_boost),
  truth = test$mobile)
g1 = ggplot(boost_preds, aes(adaboost, gbm, color=as.factor(truth))) +
  geom_text(aes(label=as.integer(truth) - 1)) + 
  geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) +
  theme(legend.position = 'none') +
  scale_color_manual(values=c('orange','blue')) +
  annotate("text", x=-4,y=5, color=red,
           label=paste("gbm error\n",
                       round(with(boost_preds,mean((gbm>0)!=truth)),2))) +
  annotate("text", x=4,y=-5, color=red, 
           label=paste("adaboost error\n", round(with(boost_preds,mean((adaboost>0)!=truth)),2)))
boost_oob = tibble(adaboost = adab$oobag.improve, gbm = grad_boost$oobag.improve,
                   ntrees=1:500)
g2 = boost_oob %>% pivot_longer(-ntrees, values_to = "OOB_Error") %>% 
  ggplot(aes(x=ntrees, y=OOB_Error, color=name)) + geom_line() + 
  scale_color_manual(values=c(orange,blue)) +
  theme(legend.title = element_blank())
plot_grid(g1,g2, rel_widths = c(.4,.6))
```

---

## Major takeaways

* Two flavours of Boosting 
    1. AdaBoost (the original) and 
    2. gradient boosting (easier and more computationally friendly)

* The connection is "Forward stagewise additive modelling" (AdaBoost is a special case)

* The connection reveals that AdaBoost "isn't robust because it uses exponential loss" (squared error is even worse)

* Gradient boosting is a computationally easier version of FSAM

* All use **weak learners** (compare to Bagging)

* Think about the Bias-Variance implications

* You can use these for regression or classification

* You can do this with other weak learners besides trees.





---
class: middle, inverse, center

# Next time...

Neural networks and deep learning, the beginning