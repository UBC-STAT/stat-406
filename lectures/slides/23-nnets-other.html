<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>23 Neural nets - other considerations</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="materials/libs/fabric/fabric.min.js"></script>
    <link href="materials/libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#e98a15"],"pen_size":3,"eraser_size":30,"palette":["#2c365e","#e98a15","#0a8754","#a8201a","#E41A1C","#377EB8","#4DAF4A","#984EA3","#FF7F00","#FFFF33"]}) })</script>
    <link href="materials/libs/panelset/panelset.css" rel="stylesheet" />
    <script src="materials/libs/panelset/panelset.js"></script>
    <script src="materials/libs/clipboard/clipboard.min.js"></script>
    <link href="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="materials/libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="materials/libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="materials/libs/twitter-widget/widgets.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 23 Neural nets - other considerations
]
.author[
### STAT 406
]
.author[
### Daniel J. McDonald
]
.date[
### Last modified - 2022-11-02
]

---





<style>.panelset{--panel-tab-active-foreground: #2c365e;--panel-tab-hover-foreground: #e98a15;}</style>


## Estimation procedures (training)

`$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfx}{\mathbf{x}}$$`

Back-propagation

__Advantages:__ 

-   It's updates only depend on local
    information in the sense that if objects in the hierarchical model
    are unrelated to each other, the updates aren't affected

    (This helps in many ways, most notably in parallel architectures)

-   It doesn't require second-derivative information

-   As the updates are only in terms of `\(\hat{R}_i\)`, the algorithm can
    be run in either batch  or online  mode

__Down sides:__ 

-   It can be very slow

-   Need to choose the learning rate
    `\(\gamma_t\)`
---

## Other algorithms

There are many  variations on the fitting algorithm

__Stochastic gradient descent__: (SGD) discussed in the optimization lecture

The rest are variations that use lots of tricks

* RMSprop
* Adam
* Adadelta
* Adagrad
* Adamax
* Nadam
* Ftrl

---

## Regularizing neural networks

NNets can almost always achieve 0 training error. Even with regularization. Because they have so many parameters.

Flavors:

-   a complexity penalization term

-   early stopping on the back propagation algorithm used for fitting


Explicit regularization `\(\longrightarrow\)` solve `\(\min \hat{R} + \rho(\alpha,\beta)\)`

-   __Weight decay:__  This is like
    ridge regression in that we penalize the squared Euclidean norm of
    the weights `\(\rho(\mathbf{W},\mathbf{B}) = \sum w_i^2 + \sum b_i^2\)`

-   __Weight elimination:__  This
    encourages more shrinking of small weights
    `\(\rho(\mathbf{W},\mathbf{B}) =  \sum \frac{w_i^2}{1+w_i^2} + \sum \frac{b_i^2}{1 + b_i^2}\)` or Lasso-type


__Dropout:__

In each epoch, randomly choose `\(z\%\)` of the nodes and set those weights to zero.

---

## Other common pitfalls

There are three areas to watch out for

- __Nonconvexity:__  The neural network optimization problem is non-convex. This makes any numerical solution highly dependent on the initial values. These should be

  - chosen carefully, typically random near 0. .hand[DON'T] use all 0.

  - regenerated several times to check sensitivity

-   __Scaling:__  Be sure to
    standardize the covariates before training

-   __Number of hidden units:__  It is generally
    better to have too many hidden units than too few (regularization
    can eliminate some).


- __Sifting the output:__ 
    
    -   Choose the solution that minimizes training
    error

    -   Choose the solution that minimizes the
    penalized  training error

    -   Average the solutions across runs

---

## Tuning parameters

There are many.

* Regularization
* Stopping criterion
* learning rate
* Architecture
* Dropout %
* others...

These are hard to tune.

In practice, people might choose "some" with a validation set, and fix the rest largely arbitrarily

--

More often, people set them all arbitrarily

---

## Thoughts on NNets

Off the top of my head, without lots of justification

.pull-left[

__Why don't statisticians like them?__

.center[.larger[<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#a8201a;overflow:visible;position:relative;"><path d="M328.4 393.5C318.7 402.6 303.5 402.1 294.5 392.4C287.1 384.5 274.4 376 256 376C237.6 376 224.9 384.5 217.5 392.4C208.5 402.1 193.3 402.6 183.6 393.5C173.9 384.5 173.4 369.3 182.5 359.6C196.7 344.3 221.4 328 256 328C290.6 328 315.3 344.3 329.5 359.6C338.6 369.3 338.1 384.5 328.4 393.5zM144.4 240C144.4 231.2 147.9 223.2 153.7 217.4L122.9 207.2C114.6 204.4 110 195.3 112.8 186.9C115.6 178.6 124.7 174 133.1 176.8L229.1 208.8C237.4 211.6 241.1 220.7 239.2 229.1C236.4 237.4 227.3 241.1 218.9 239.2L208.1 235.6C208.3 237 208.4 238.5 208.4 240C208.4 257.7 194 272 176.4 272C158.7 272 144.4 257.7 144.4 240V240zM368.4 240C368.4 257.7 354 272 336.4 272C318.7 272 304.4 257.7 304.4 240C304.4 238.4 304.5 236.8 304.7 235.3L293.1 239.2C284.7 241.1 275.6 237.4 272.8 229.1C270 220.7 274.6 211.6 282.9 208.8L378.9 176.8C387.3 174 396.4 178.6 399.2 186.9C401.1 195.3 397.4 204.4 389.1 207.2L358.9 217.2C364.7 223 368.4 231.1 368.4 240H368.4zM0 256C0 114.6 114.6 0 256 0C397.4 0 512 114.6 512 256C512 397.4 397.4 512 256 512C114.6 512 0 397.4 0 256zM256 464C370.9 464 464 370.9 464 256C464 141.1 370.9 48 256 48C141.1 48 48 141.1 48 256C48 370.9 141.1 464 256 464z"/></svg>]]

- There is little theory (though this is increasing)
- Statistical theory applies to global minima, here, only local determined by the optimizer
- Little understanding of when they work
- In large part, NNets look like logistic regression + feature creation. We understand that well, and in many applications, it performs as well
- Explosion of tuning parameters without a way to decide
- Require massive datasets to work
]

.pull-right[

__Why are they hot?__

.center[.larger[<svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#e98a15;overflow:visible;position:relative;"><path d="M159.3 5.4c7.8-7.3 19.9-7.2 27.7 .1c27.6 25.9 53.5 53.8 77.7 84c11-14.4 23.5-30.1 37-42.9c7.9-7.4 20.1-7.4 28 .1c34.6 33 63.9 76.6 84.5 118c20.3 40.8 33.8 82.5 33.8 111.9C448 404.2 348.2 512 224 512C98.4 512 0 404.1 0 276.5c0-38.4 17.8-85.3 45.4-131.7C73.3 97.7 112.7 48.6 159.3 5.4zM225.7 416c25.3 0 47.7-7 68.8-21c42.1-29.4 53.4-88.2 28.1-134.4c-2.8-5.6-5.6-11.2-9.8-16.8l-50.6 58.8s-81.4-103.6-87.1-110.6C133.1 243.8 112 273.2 112 306.8C112 375.4 162.6 416 225.7 416z"/></svg>]]

- Perform exceptionally well on typical CS tasks (images, translation)
- Take advantage of SOTA computing (parallel, GPUs)
- Very good for multinomial logistic regression
- An excellent example of "transfer learning"
- They generate pretty pictures (the nets, pseudo-responses at hidden units)

]

---

## Keras

Most people who do deep learning use Python `\(+\)` Keras `\(+\)` Tensorflow

It takes some work to get all this software up and running.

It is possible to do in with R using an [interface to Keras](https://keras.rstudio.com/index.html).

--

I used to try to do a walk-through, but the interface is quite brittle

If you want to explore, see the handout:
* Knitted: https://ubc-stat.github.io/stat-406-lectures/handouts/keras-nnet.html
* Rmd: https://ubc-stat.github.io/stat-406-lectures/handouts/keras-nnet.Rmd



---
class: middle, inverse, center

# Double descent and model complexity

---


<blockquote class="twitter-tweet" data-width="550" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">The Bias-Variance Trade-Off &amp; &quot;DOUBLE DESCENT&quot; ðŸ§µ<br><br>Remember the bias-variance trade-off? It says that models  perform well for an &quot;intermediate level of flexibility&quot;.  You&#39;ve seen the picture of the U-shape test error curve.<br><br>We try to hit the &quot;sweet spot&quot; of flexibility.<br><br>1/ðŸ§µ <a href="https://t.co/HPk05izkZh">pic.twitter.com/HPk05izkZh</a></p>&mdash; Daniela Witten (@daniela_witten) <a href="https://twitter.com/daniela_witten/status/1292293102103748609?ref_src=twsrc%5Etfw">August 9, 2020</a></blockquote>


---

## Where does this U shape come from?


.center.large[
MSE = Squared Bias + Variance + Irreducible Noise
]

As we increase flexibility:
* Squared bias goes down
* Variance goes up
* Eventually, | `\(\partial\)` Variance | `\(&gt;\)` | `\(\partial\)` Squared Bias |.


**Goal:** Choose amount of flexibility to balance these and minimize MSE.

--

.hand[Use CV or something to estimate MSE and decide how much flexibility.]


---

<blockquote class="twitter-tweet" data-conversation="none" data-width="550" data-lang="en" data-dnt="true" data-theme="light"><p lang="en" dir="ltr">In the past few yrs, (and particularly in the context of deep learning) ppl have noticed &quot;double descent&quot; -- when you continue to fit increasingly flexible models that interpolate the training data, then the test error can start to DECREASE again!! <br><br>Check it out:Â <br>3/ <a href="https://t.co/Vo54tRVRNG">pic.twitter.com/Vo54tRVRNG</a></p>&mdash; Daniela Witten (@daniela_witten) <a href="https://twitter.com/daniela_witten/status/1292293104855158784?ref_src=twsrc%5Etfw">August 9, 2020</a></blockquote>


---

## Zero training error and model saturation

* In Deep Learning, the recommendation is to "fit until you get zero training error"

* This somehow magically, leads to a continued decrease in test error.

* So, who cares about the Bias-Variance Trade off!!

--

**Lesson:**

BV Trade off is not wrong. <svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#e98a15;overflow:visible;position:relative;"><path d="M169.6 291.3C172.8 286.9 179.2 286.9 182.4 291.3C195.6 308.6 223.1 349 223.1 369C223.1 395 202.5 416 175.1 416C149.5 416 127.1 395 127.1 369C127.1 349 156.6 308.6 169.6 291.3H169.6zM368 346.8C377.9 355.6 378.7 370.8 369.9 380.7C361 390.6 345.9 391.4 335.1 382.6C314.7 363.5 286.7 352 256 352C242.7 352 232 341.3 232 328C232 314.7 242.7 304 256 304C299 304 338.3 320.2 368 346.8L368 346.8zM335.6 176C353.3 176 367.6 190.3 367.6 208C367.6 225.7 353.3 240 335.6 240C317.1 240 303.6 225.7 303.6 208C303.6 190.3 317.1 176 335.6 176zM175.6 240C157.1 240 143.6 225.7 143.6 208C143.6 190.3 157.1 176 175.6 176C193.3 176 207.6 190.3 207.6 208C207.6 225.7 193.3 240 175.6 240zM256 0C397.4 0 512 114.6 512 256C512 397.4 397.4 512 256 512C114.6 512 0 397.4 0 256C0 114.6 114.6 0 256 0zM175.9 448C200.5 458.3 227.6 464 256 464C370.9 464 464 370.9 464 256C464 141.1 370.9 48 256 48C141.1 48 48 141.1 48 256C48 308.7 67.59 356.8 99.88 393.4C110.4 425.4 140.9 447.9 175.9 448V448z"/></svg>

This is a misunderstanding of black box algorithms and flexibility.

We don't even need deep learning to illustrate. 


---


```r
library(splines)
set.seed(20221102)
n &lt;- 20
df &lt;- tibble(
  x = seq(-1.5 * pi, 1.5 * pi, length.out = n),
  y = sin(x) + runif(n, -0.5, 0.5)
)
g &lt;- ggplot(df, aes(x, y)) + geom_point() + stat_function(fun = sin) + ylim(c(-2, 2))
g + stat_smooth(method = lm, formula = y ~ bs(x, df = 4), se = FALSE, color = green) + # too smooth
  stat_smooth(method = lm, formula = y ~ bs(x, df = 8), se = FALSE, color = orange) # looks good
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;


---


```r
xn &lt;- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)
# Spline by hand
X &lt;- bs(df$x, df = 20, intercept = TRUE)
Xn &lt;- bs(xn, df = 20, intercept = TRUE)
S &lt;- svd(X)
yhat &lt;- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)
g + geom_line(data = tibble(x=xn, y=yhat), colour = orange) +
  ggtitle("20 degrees of freedom")
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;

---


```r
xn &lt;- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)
# Spline by hand
X &lt;- bs(df$x, df = 40, intercept = TRUE)
Xn &lt;- bs(xn, df = 40, intercept = TRUE)
S &lt;- svd(X)
yhat &lt;- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)
g + geom_line(data = tibble(x = xn, y = yhat), colour = orange) +
  ggtitle("40 degrees of freedom")
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: true

## What happened?!

---


```r
doffs &lt;- 4:50
mse &lt;- function(x, y) mean((x - y)^2)
get_errs &lt;- function(doff) {
  X &lt;- bs(df$x, df = doff, intercept = TRUE)
  Xn &lt;- bs(xn, df = doff, intercept = TRUE)
  S &lt;- svd(X)
  yh &lt;- S$u %*% crossprod(S$u, df$y)
  bhat &lt;- S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)
  yhat &lt;- Xn %*% S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)
  nb &lt;- sqrt(sum(bhat^2))
  tibble(train = mse(df$y, yh), test = mse(yhat, sin(xn)), norm = nb)
}
errs &lt;- map_dfr(doffs, get_errs, .id = "degrees of freedom") %&gt;%
  mutate(`degrees of freedom` = doffs) %&gt;%
  pivot_longer(train:test, values_to = "error")
```

---



```r
ggplot(errs, aes(`degrees of freedom`, error, color = name)) +
  geom_line(size = 2) + 
  coord_cartesian(ylim = c(0, .12)) +
  scale_colour_manual(values = c(blue, orange)) +
  scale_x_log10() + geom_vline(xintercept = 20)
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-7-1.svg" style="display: block; margin: auto;" /&gt;

---


```r
ggplot(errs %&gt;% filter(name == "test"), aes(norm, error)) +
  geom_line(colour = blue, size = 2) + 
  scale_y_log10() + scale_x_log10() + geom_vline(xintercept = 20)
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: false

## Degrees of freedom and complexity

* In low dimensions (where `\(n \gg p\)`), with linear smoothers, edf and model complexity are roughly the same.

* But this relationship breaks down in more complicated settings

* We've already seen this:


```r
library(glmnet)
out &lt;- cv.glmnet(X, df$y, nfolds = n) # leave one out
par(mfrow=c(1, 2), mar = c(5, 3, 0, 0))
plot(out$lambda, out$cvm, type = "b", log = "xy")
plot(out$nzero, out$cvm, type = "b", log = "xy")
```

&lt;img src="rmd_gfx/23-nnets-other/unnamed-chunk-9-1.svg" style="display: block; margin: auto;" /&gt;

---

## Infinite solutions

* In Lasso, edf is not really the right measure of complexity

* Better is `\(\lambda\)` or the norm of the coefficients (these are basically the same)

* So what happened with the Splines?

--

* When df `\(= 20\)`, there's a unique solution that interpolates the data

* When df `\(&gt; 20\)`, there are infinitely many solutions that interpolate the data.

Because we used the SVD to solve the system, we happened to pick one: the one that has the smallest `\(||\hat\beta||_2\)`

Recent work in Deep Learning shows that SGD has the same property: it returns the local optima with the smallest norm.

If we measure complexity in terms of the norm of the weights, rather than by counting parameters, we don't see double descent anymore.

---

## The lesson

Deep learning isn't magic.

Zero training error with lots of parameters doesn't mean good test error.

We still need the bias variance tradeoff

It's intuition still applies: more flexibility eventually leads to increased MSE

But we need to be careful how we measure complexity.


---

class: middle, center
background-image: url(rmd_gfx/23-nnets-other/embeddings-1.svg)
background-size: cover



.primary[.larger[Next time...]]

.primary[.larger[Module]] .huge-blue-number[5]

.primary[.larger[unsupervised learning]]




    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
