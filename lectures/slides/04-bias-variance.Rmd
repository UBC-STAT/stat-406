---
title: "04 Bias and variance"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---
class: middle



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

```{r css-extras, file="css-extras.R", echo=FALSE}
```

$$
\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
$$



.alert[
We just talked about 

* Variance of an estimator.

* Irriducible error when making predictions.

* These are 2 of the 3 components of the "Prediction Risk" $R_n$
]



---

## Component 3, the Bias




We need to be more specific about what we mean when we say __bias__.

Bias is neither good nor bad in and of itself.

A very simple example: let $Z_1,\ldots,Z_n \sim N(\mu, 1)$.
  - We don't know $\mu$, so we try to use the data (the $Z_i$'s) to estimate it.
  
  - I propose 3 estimators: 
      1. $\widehat{\mu}_1 = 12$, 
  
      2. $\widehat{\mu}_2=Z_6$, 
  
      3. $\widehat{\mu}_3=\overline{Z}$.
  
  - The __bias__ (by definition) of my estimator is $E\left[\widehat{\mu_i}\right]-\mu$.
  
--

Calculate the bias and variance of each estimator.

  
---


## Regression in general

If I want to predict $Y$ from $X$, it is almost always the case that

$$
\mu(x) = \Expect{Y\given X=x} \neq x^{\top}\beta
$$

So the __bias__ of using a linear model is __not__ zero.

 ---

Why? Because

$$
\Expect{Y\given X=x}-x^\top\beta \neq \Expect{Y\given X=x} - \mu(x) = 0.
$$

We can include as many predictors as we like, 

but this doesn't change the fact that the world is __non-linear__.

---

## Predicting new Y's

.emphasis[
Suppose we know that we want to predict a quantity $Y$, 

where $E[Y]= \mu \in \mathbb{R}$ and $\textrm{Var}[Y] = 1$.  


Our data is $\{y_1,\ldots,y_n\}$

We want to estimate $\mu$ 
]

--

Let's try one more: $\widehat Y_a = a\overline{Y}_n$ for some $a \in (0,1]$.
  
$$
  R_n(\widehat Y_a) = \Expect{(\widehat Y_a-Y)^2} = (1 - a)^2\mu^2 +
  \frac{a^2}{n} +1 
$$
  
We can minimize this in $a$ to get the best possible prediction risk for an estimator of the form $\widehat Y_a$: 
  
$$
\arg\min_{a} R_n(\widehat Y_a) = \left(\frac{\mu^2}{\mu^2 + 1/n} \right)
$$

--

What happens if $\mu \ll 1$?
  
---
class: middle
  
.alert[
Wait a minute! You're saying there is a __better__ estimator than $\overline{Y}_n$?
]



---

## Bias-variance tradeoff: Estimating the mean

$$
R_n(\widehat Y_a) = (a - 1)^2\mu^2 +  \frac{a^2}{n} + \sigma^2
$$

```{r}
mu = 1; n = 5; sig2 = 1
```

```{r, fig.align='center',echo=FALSE,dev="svg",fig.width=8,fig.height=4}
biasSqA <- function(a, mu=1) (a-1)^2 * mu
varA <- function(a, n=1) a^2/n
risk <- function(a, mu=1, n=1, sig2=1) biasSqA(a, mu) + varA(a, n) +  sig2
meanrisk <- function(a, n=1, sig2=1) sig2+1/n
aopt <- function(mu=1, n=1) mu^2/(mu^2+1/n)
ggplot(data.frame(x=c(0,1),y=c(0,2)), aes(x,y)) + 
  stat_function(fun=biasSqA, aes(color="squared bias"))+
  stat_function(fun=varA, aes(color="variance"), args = list(n=n))+
  stat_function(fun=risk, aes(color="risk"), args = list(mu=mu,n=n,sig2=sig2)) + 
  ylab(bquote(R[n](a))) + xlab("a") +
  stat_function(fun=meanrisk, aes(color="risk of mean"), args=list(n=n,sig2=sig2)) + 
  geom_vline(xintercept = aopt(mu,n),color="black") +
  scale_color_manual(guide="legend",
    values = c("squared bias"=red, "variance"=blue,
               "risk"=green, "risk of mean"= orange,
               "best a"="black")
    ) +
  theme(legend.title = element_blank())
```

---

## To restate

If $\mu=$ `r mu` and $n=$ `r n` 

then it is better to predict with `r round(aopt(mu,n),2)` $\overline{Y}_5$ 

than with $\overline{Y}_5$ itself.  

--

For this $a =$ `r round(aopt(mu,n),2)` and $n=5$

1. $R_5(\widehat{Y}_a) =$ `r round(risk(aopt(mu, n), mu, n), 2)`

2. $R_5(\overline{Y}_5)=$ `r 1 / n + sig2`

---

## Prediction risk


$$
R_n(f) = \Expect{(Y - f(X))^2}
$$
  
Why care about $R_n(f)$? 


`r fa("plus", fill=green)` Measures predictive accuracy on average.

`r fa("plus", fill=green)` How much confidence should you have in $f$'s predictions.

`r fa("plus", fill=green)` Compare with other models.

`r fa("bomb", fill=red)` __This is hard:__  Don't know the distribution of the data (if I knew the truth, this would be easy)

  
---

## Bias-variance decomposition



$$R_n(\widehat{Y}_a)=(a - 1)^2\mu^2 + \frac{a^2}{n} + 1$$


1. prediction risk  =  $\textrm{bias}^2$  +  variance  +  irreducible error 

2. estimation risk  =  $\textrm{bias}^2$  +  variance
    

What is $R_n(\widehat{Y}_a)$ for our estimator $\widehat{Y}_a=a\overline{Y}_n$?


\begin{aligned}
\textrm{bias}(\widehat{Y}_a) &= \Expect{a\overline{Y}_n} - \mu=(a-1)\mu\\
\textrm{var}(\widehat f(x)) &= \Expect{ \left(a\overline{Y}_n - \Expect{a\overline{Y}_n}\right)^2}
=a^2\Expect{\left(\overline{Y}_n-\mu\right)^2}=\frac{a^2}{n} \\
\sigma^2 &= \Expect{(Y-\mu)^2}=1
\end{aligned}

---
class: inverse

## This decomposition holds generally

\begin{aligned}
R_n(\hat{Y}) 
&= \Expect{(Y-\hat{Y})^2} \\
&= \Expect{(Y-\mu + \mu - \hat{Y})^2} \\
&= \Expect{(Y-\mu)^2} + \Expect{(\mu - \hat{Y})^2} + 
2\Expect{(Y-\mu)(\mu-\hat{Y})}\\
&= \Expect{(Y-\mu)^2} + \Expect{(\mu - \hat{Y})^2} + 0\\
&= \text{irr. error} + \text{estimation risk}\\
&= \sigma^2 + \Expect{(\mu - E[\hat{Y}] + E[\hat{Y}] - \hat{Y})^2}\\
&= \sigma^2 + \Expect{(\mu - E[\hat{Y}])^2} + \Expect{(E[\hat{Y}] - \hat{Y})^2} + 2\Expect{(\mu-E[\hat{Y}])(E[\hat{Y}] - \hat{Y})}\\
&= \sigma^2 + \Expect{(\mu - E[\hat{Y}])^2} + \Expect{(E[\hat{Y}] - \hat{Y})^2} + 0\\
&= \text{irr. error} + \text{squared bias} + \text{variance}
\end{aligned}


---

## Bias-variance decomposition

\begin{aligned}
R_n(\hat{Y}) 
&= \Expect{(Y-\hat{Y})^2} \\
&= \text{irr. error} + \text{estimation risk}\\
&= \text{irr. error} + \text{squared bias} + \text{variance}
\end{aligned}



.alert[  
__Important implication:__ prediction risk is proportional to estimation risk.  However, defining estimation risk requires stronger assumptions.
]

--

.emphasis[
In order to make good predictions, we want our prediction risk to be small.  This means that we want to "balance" the bias and variance.
]

---
  


```{r,fig.align='center',fig.height=6, fig.width=8, dpi=300, echo=FALSE, message=FALSE}
cols = c(blue, red, green, orange)

par(mfrow=c(2,2),bty='n',ann=FALSE,xaxt='n',yaxt='n',family='serif',mar=c(0,0,0,0),oma=c(0,2,2,0))
require(mvtnorm)
mv = matrix(c(0,0,0,0,-.5,-.5,-.5,-.5),4,byrow=T)
va = matrix(c(.02,.02,.1,.1,.02,.02,.1,.1),4,byrow=T)

for(i in 1:4){
  plot(0,0,ylim=c(-2,2),xlim=c(-2,2),pch=19,cex=42,col=blue,ann=FALSE,pty='s')
  points(0,0,pch=19,cex=30,col='white')
  points(0,0,pch=19,cex=18,col=green)
  points(0,0,pch=19,cex=6,col=orange)
  points(rmvnorm(20,mean=mv[i,],sigma=diag(va[i,])), cex=1, pch=19)
  switch(i, 
         '1'= {
           mtext('low variance',3,cex=2)
           mtext('low bias',2,cex=2)
         },
         '2'= mtext('high variance',3,cex=2),
         '3' = mtext('high bias',2,cex=2)
  )
}
```

---

## Bias-variance tradeoff: Overview

 __bias:__ how well does $\widehat{f}(x)$ approximate the truth $\Expect{Y\given X=x}$

* If we allow more complicated possible $f$, lower bias. Flexibility $\Rightarrow$ Expressivity

* But, more flexibility $\Rightarrow$ larger variance

* Complicated models are hard to estimate precisely for fixed $n$

* Irreducible error

--


.emphasis[
Since we can't evaluate any expectations with real data...
]


---
class: inverse, center, middle

# Next time...


Estimating risk