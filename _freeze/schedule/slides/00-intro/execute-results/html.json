{
  "hash": "a1a70984465b34da4a1ae2d73da13862",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"Methods for Statistical Learning\"\nformat: revealjs\nmetadata-files:\n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 04 September 2025\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Who am I?\n\n::: flex\n\n::: w-50\n**Geoff Pleiss**\n\n- [geoff.pleiss\\@stat.ubc.ca](mailto:geoff.pleiss@stat.ubc.ca){.email}\n- <http://geoffpleiss.com/>\n- Assistant Professor, Department of Statistics\n- Research interests: machine learning\n  - Uncertainty quantification\n  - Sequential decision making\n  - Deep learning\n  - ML for science\n:::\n\n::: w-50\n<img src=\"https://geoffpleiss.com/static/media/me.0c3be0b6.jpg\" alt=\"Geoff Pleiss\" width=\"350px\" style=\"border-radius: 50%\" />\n:::\n:::\n\n\n## TAs\n\n::: {.columns}\n::: {.column width=\"33%\"}\n\n![](https://i1.rgstatic.net/ii/profile.image/962305089085441-1606442820507_Q512/Atabak-Eghbal.jpg){width=\"300px\"}\n\n**Atabak Eghbal**\n\n:::\n::: {.column width=\"33%\"}\n\n![](gfx/junsong.png){width=\"300px\"}\n\n**Junsong Tang**\n\n:::\n::: {.column width=\"33%\"}\n\n![](https://vision.ubc.ca/sites/default/files/styles/square_600/public/profile-images/2022-10/Parsa%20Delavari.jpg.webp?itok=PsOkXQTu){width=\"300px\"}\n\n**Parsa Delivary**\n\n:::\n:::\n\n## Who are You?\n\n::: {.incremental}\n- Stats major?\n- Took STAT 306?\n- Took CPSC 340?\n- Feel \"knowledgable\" about ML?\n- Need this course to graduate?\n:::\n\n## This Course\n\n**Goal**:\n\n- Develop deep statistical intuitions about prediction and learning\n- Draw connections between modelling/learning paradigms\n\n**Assumptions**:\n\n- You have familiarity with linear models (STAT 306) or ML basics (CPSC 340)\n- You are willing to put in the work!\n\n## Differences from Prior Courses\n\n<br/>\n\n::: {.columns}\n::: {.column}\n### If You've Taken STAT306\n\n- Risks analyses\n- High dimensional learning methods\n- Non-linear learning methods\n- Non-parametric learning methods\n- Unsupervised learning methods\n- \"Modern\" methods (deep learning/ensembles)\n:::\n\n::: {.column}\n### If You've Taken CPSC340\n\n- Surface level: mostly same content\n- Under the hood: more depth/stats\n  - Statistical modelling/model selection\n  - Bias-variance tradeoff\n  - Curse of dimensionality\n  - Black-box computational methods\n  - Generative versus discrminative modelling\n:::\n:::\n\n# Course Content and Learning Outcomes\n\n## What is Statistical Learning?\n\n*A history lesson*\n\n\n::: {.columns .fragment}\n::: {.column .m-5 width=\"60%\"}\n### Early AI, Summers, and Winters\n- 1950s: \"intelligent machines\", Turing test\n- 1960s-80s: Early perceptrons, rule-based systems, hype cycles\n- 1970s-80s: AI winter(s)\n:::\n\n::: {.column .ml-5 width=\"40%\"}\n![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Alan_turing_header.jpg/250px-Alan_turing_header.jpg){width=\"40%\"}\n![](https://upload.wikimedia.org/wikipedia/commons/3/3b/Frank_Rosenblatt.jpg){width=\"40%\"}\n:::\n:::\n\n## What is Statistical Learning?\n\n### Meanwhile, in Statistics Land...\n\nStatisticians are developing frameworks for reasoning, predicting, and making decision from data.\n\n- 1800s (Legendre and Gauss): predicting form data (least squares)\n- 1910s (Fisher): estimating unknown parameters (MLE)\n- 1940s (Shannon): quantifying information in data (information theory)\n- 1950s (Wald): making decisions from data (decision theory)\n- 1970s-80s: blackbox computer-based algorithms (bootstrap, MCMC)\n\n::: {.fragment}\nüßê Aren't these the same goals that the AI community has? üßê\n:::\n\n## What is Statistical Learning?\n\n\n::: {.columns}\n::: {.column width=\"70%\"}\n### Statistians Guide AI\n- 1980s (Vapnik, Chervonenkis): statistical learning theory\n  - Study learning general knowledge from data\n  - Mathematical formalism for when learning is possible\n- 1990s-2000s: new algorithms based on stats\\\n    - Support vector machines\n    - Bagging/random forests\n    - Topic modelling\n- Applications in the new internet economy (search, ads, product recommendations)\n- AI $\\rightarrow$ machine learning\n:::\n::: {.column width=\"30%\"}\n![](https://m.media-amazon.com/images/I/61-TrX6+yKL._UF1000,1000_QL80_.jpg)\n:::\n:::\n\n## What is Statistical Learning?\n\n::: {.columns}\n::: {.column width=\"60%\"}\n### Statisticians Play Catch-Up\n- 2010s: deep learning revolution\n    - Stats: ideas as \"unprincipled\"\n    - CS: see lots of empirical success\n- 2020s: generative AI\n    - Breakthroughs come from scale (more data, more compute)\n    - Big gap between theory and practice\n    - Core ideas are stat. learning principles\n:::\n::: {.column width=\"40%\"}\n![](https://miro.medium.com/v2/resize:fit:380/1*2vyKzFlzIHfSmOU_lnQE4A.png)\n:::\n:::\n\n## Course Learning Outcomes\n\n1. **Formulate and analyze** machine learning algorithms from a statistical perspective\n2. **Categorize** learning methods through multiple criteria\n3. **Draw connections** between any two learning algorithms and paradigms\n4. **Reason** about which methods are most appropriate for a given situation\n5. **Apply** these methods correctly, troubleshoot common pitfalls, and identify probable steps for improving\n6. **Implement** and utilize models in R using best coding practices\n\n## 6 Modules\n\nEach module has a **technical content theme** (*and a statistical principles theme*)\n\n1.  **Basic regression/classification** (*statistical modelling/selection*)\n2.  **Linear methods, regularization, featurization** (*bias-variance tradeoff*)\n3.  **Nonparametric methods** (*curse of dimensionality*)\n5.  **Unsupervised learning** (*generative vs. discriminative modelling*)\n4.  **Ensembles** (*black-box methods*)\n6.  **Deep learning**\n\n\n# Logistics / Syllabus\n\n\n## Course Components\n\n1. **In-Class** *(discussion, mathematical derivations)*\n2. **Labs** *(coding, tools, how-tos)*\n3. **Homework Assignments** *(synthesis, debugging, analysis)*\n4. **Final Exam** *(all of the above)*\n\n## Grading Structure\n\n::: {.columns}\n::: {.column}\n### Knowledge Based (40 points)\n\n| Component   | Points        |\n|-------------|---------------|\n| Midterm     | 10 points     |\n| Final       | 30 points     |\n| **Total**   | **40 points** |\n:::\n\n::: {.column}\n### Effort Based (60 points)\n\n| Component   | Points        |\n|-------------|---------------|\n| In-Class    | 15 points     |\n| Labs        | 20 points     |\n| Homework    | 40 points     |\n| **Total**   | **60 points** |\n:::\n:::\n\n## üßê 15 + 20 + 40 > 60??? üßê\n\n:::{.columns}\n:::{.column}\n\n| Component   | Points        |\n|-------------|---------------|\n| In-Class    | 15 points     |\n| Labs        | 20 points     |\n| Homework    | 40 points     |\n| **Total**   | **60 points** |\n\n:::\n:::{.column}\n\n![](https://upload.wikimedia.org/wikipedia/en/6/64/Math_Lady_meme.jpg)\n\n:::\n:::\n\n. . .\n\n$$\\text{effort} = \\min\\left\\{60, \\text{class} + \\text{lab} + \\text{hw}\\right\\}$$\n\n### Get 60 of the 75 total points any way your want\n- Skip class + perfect on labs/homework = 60 points\n- Miss 1 homework + get in-class points = 60 points\n- (More on this later)\n\n# In-Class\n\n## Class Structure\n\n- **Lecture**: I'll go over content\n- **Derivations**: You'll do math in groups\n- **Discussions**: We'll analyze content together\n- **Clickers**: You'll apply your knowledge (in pairs)\n\n. . .\n\n<br />\n\n::: {.callout-important}\n8am is hard.\n\n**Attendance is optional.**\n:::\n\n## Reasons to Skip Class\n\n1. **If you're sleep deprived**\\\n*Getting more sleep may be beneficial to your learning in the long run*\n\n2. **If you're not going to engage/participate**\\\n*Lectures are recorded, watch on your own time*\n\n3. **If you're going to have your laptop open**\\\n*Do your homework/social media browsing/k-pop video watching in the comfort of your own home; then revisit the course material on your own time*\n\n\n## Reasons to Attend Class (without Distractions)\n\n### 1. Time to Think/Digest/Struggle with the Material\n- Developing deep intuitions takes time/effort.\n- Class gives you 3 hours/week to chew on the material.\n- I'll pace the material accordingly\n\n\n## Reasons to Attend Class (without Distractions)\n\n### 2. In-Class is Where We Wrestle with Math\n- There is lots of math in the course\n- We will spend time in class working through derivations together\n- Working through derivations builds intuitions and deep knowledge\n- Few opportunities on labs/homeworks for this kind of work\n\n\n## Reasons to Attend Class (without Distractions)\n\n### 3. Questions and Discussions\n- There's plenty of online materials that teach you this content (books, videos, etc)\n- There's few opportunities to discuss content with peers/experts\n- *If you're graduating this year, this course may be one of your last opportunities for this type of learning!*\n\n## Reasons to Attend Class (without Distractions)\n\n### 4. Effort-Based Points\n- If you skip class, you'll need to get a perfect grade on labs/homeworks to get a maximum effort-based grade\n- If you attend class, you can skip a homework, mess up on all of them, and still get a perfect grade!\n\n## Reasons to Attend Class (without Distractions)\n### 5. Recommendation Letters\n\n:::{.columns .fragment}\n:::{.column}\n**What I Write for Students who Attend/Participate/Ask Questions**\n![](gfx/good_rec.png)\n:::\n\n:::{.column .fragment}\n**What I Write for Students who Just \"Show Up\"**\n\n*I don't get to know you, so all I can talk about is your grade.*\n\n![](gfx/okay_rec.png)\n:::\n:::\n\n\n\n\n## TLDR\n\n- Waking up early, avoiding distractions, and participating is hard\n- If you don't have the energy or mental bandwidth on any given day, give yourself the gift of staying home.\n- If you come in, be prepared to engage and put in effort.\n- You'll get a lot out of attending/engaging. I'll make sure of it!\n\n## Earning Points\n\n::: {.columns}\n::: {.column .fragment}\n### Participation (5 points)\n\n- We will (try to) use [Agora](https://www.cs.ubc.ca/~hzarkoob/participation_app.html) for virtual \"hand-raising\"\n- 1 point for \"raising\" your hand\n- 1 point if you are called on (assigned randomly by Agora)\n- Grade: $\\min\\left\\{ 5, 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{classes\\_w/\\_agora}} \\right\\}$\n:::\n::: {.column .fragment}\n### Clickers (10 points)\n-   We will use [iClicker](https://lthub.ubc.ca/guides/iclicker-cloud-student-guide/) for questions that are similar to the midterm/final\n-   0 points for skipping, 2 points for trying, 4 points for correct\n    -   Average of 3 = 10 points (the max)\n    -   Average of 2 = 5 points\n    -   Average of 1 = 0 points\n- Grade: $\\max\\left\\{ 0, \\min\\left\\{ 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{questions}} - 5, 10 \\right\\} \\right\\}$\n-   Be sure to sync your device in Canvas.\n:::\n:::\n\n# Labs / Assignments\n\n## Mechanics and Grading\n\nThe goal is to \"Do the work\"\n\n::: {.columns}\n::: {.column}\n### Labs\n\n* Labs should give you practice, allow for questions with the TAs.\n\n* They are due at 2300 on the day of your lab, lightly graded.\n\n* You may do them at home, but you must submit individually (in lab, you may share submission)\n\n* Labs are lightly graded\n:::\n\n::: {.column}\n### Assignments\n\n* Not easy, especially the first 2, especially if you are unfamiliar with R / Rmarkdown / ggplot\n\n* You may revise to raise your score to 7/10, see [Syllabus](https://ubc-stat.github.io/stat-406/syllabus/). Only if you lose 3+ for content (penalties can't be redeemed).\n\n* Don't leave these for the last minute\n:::\n:::\n\n## Tools: R and Github\n\n:::{.columns}\n:::{.column}\n- **Language/Libraries:** R + Tidyverse\n\n- **Submission:** via Github\n\n::: {.callout-important .fragment}\nWe assume you're familiar with these tools (R, Tidyverse, Git, Github)\n\nIf you're not, it's your responsibility to get up-to-speed with them.\n\nSee Canvas for tutorials / the website for resources.\n:::\n:::\n\n:::{.column .fragment}\n### Workflow\n\n* You each have your own repo\n\n* You make a branch\n\n* [DO NOT]{.secondary} rename files\n\n* Make enough commits (3 for labs, 5 for HW).\n\n* Push your changes (at anytime) and make a PR against `main` when done.\n\n* TAs review your work.\n\n* If you want to revise HWs, make changes in response to feedback and push\nto the same branch. Then \"re-request review\".\n:::\n:::\n\n## Generative AI Policy\n\n- **AI tools are quite capable** with this material (I use them frequently in my own work!)\n- **Strong recommendation: Use AI as little as possible** to maximize learning\n- If you use AI, use it to **supplement your thinking** (code completion, rewriting) not **replace your thinking** (feeding entire assignments to chatbots)\n\n## Why Minimize AI Use?\n\n1. **Deep intuitions require struggle**\\\n*The only way to develop intuitions about challenging material is to wrestle with content*\n\n2. **Stand out from the crowd**\\\n*Anyone can use Claude/Copilot for simple ML. Demonstrate thinking beyond these tools will make you more hireable/trusted*\n\n3. **Course-specific subtleties**\\\n*Even with good prompting, chatbots likely won't score above 7-8 on assignments*\n\n4. **No AI on exams**\\\n*Don't become too dependent on tools you can't use during midterm/final*\n\n## If you Use Generative AI...\n\n<br />\n\n**Self-reporting required:**\n\n- Describe your AI usage on all assignments and labs.\n- Include all prompts you use.\n- Please be honest (it'll help me improve the course/your learning experience)\n- No grade penalty\n\n## Late policy\n\nIf you have not submitted your lab/assignment by the time grading starts,\nyou will get a 0.\n\n. . .\n\n<br>\n\n| **When you submit**     | **Likelihood that your submission gets a 0** |\n|-------------------------|------------------------------------------------------------|\n| Before 11pm on due date\\ (i.e. on time) | 0%                                        |\n| 11:01pm on due date     | 0.01%                                                       |\n| 9am after due date      | 50%                                                        |\n| 2 weeks after due date  | 99.99999999%                                               |\n\n. . .\n\n<br />\n\n**Exception**: when you have grounds for academic consession.\n(See the [UBC policy](https://science.ubc.ca/students/concession).)\n\n:::callout-tip\n**Remember**: you can still get a \"perfect\" effort grade even if you get a 0 on one assignment.\n:::\n\n# Reading/Textbook\n\n## Textbooks\n\n::: {.columns}\n::: {.column}\n::: callout-important\n## An Introduction to Statistical Learning\n\nJames, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted \\[ISLR\\])\n\nAvailable **free** online: http://statlearning.com/\n:::\n:::\n\n::: {.column}\n::: callout-tip\n## The Elements of Statistical Learning\n\nHastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted \\[ESL\\])\n\nAlso available **free** online: https://web.stanford.edu/\\~hastie/ElemStatLearn/\n:::\n:::\n:::\n\n- Each class has a \"required\" reading from *Introduction to Statistical Learning*\\\n*No quizzes/knowledge demonstration/accountability for not reading, but please do them!*\n- Reading before class will improve your preparation/ability to engage\n\n\n# Knowledge-Based Assessment\n\n## Exams\n\n::: {.columns}\n::: {.column}\n### Midterm Exam\n\n-   In-class, October 23\n-   It is hard\n-   Computer-based\n-   T/F, multiple choice, etc.\n\n:::\n::: {.column}\n### Final Exam\n\n-   Scheduled by the university.\n-   It is hard\n-   The median last year was 50% $\\Rightarrow$ A-\n-   Format: TBD\n\n:::\n:::\n\n. . .\n\n</br>\n\n### Philosophy\n\n- If you put in the effort, you're guaranteed a C+.\n- To get an A+, you need to deeply understand the material.\n- No penalty for skipping the midterm/final.\n\n\n\n\n\n## Time expectations per week:\n\n-   Coming to class -- 3 hours\n\n-   Reading the book -- 1 hour\n\n-   Labs -- 1 hour\n\n-   Homework -- 4 hours\n\n-   Study / thinking / playing -- 1 hour\n\n\n\n# Resources\n\n## Computer\n\n::: {.columns}\n::: {.column}\n![](https://upload.wikimedia.org/wikipedia/commons/0/06/Apple_IIe_running_ProDOS.jpg)\n:::\n\n::: {.column}\n* We will use R and we assume some background knowledge.\n\n* Suggest you use **RStudio** IDE\n\n* See <https://ubc-stat.github.io/stat-406/> for what you need to install for the whole term.\n\n* Links to useful supplementary resources are available on the website.\n\n:::\n:::\n\n\n## Other Resources\n\n1. **Course website**\\\nAll the material (slides, extra worksheets) <https://ubc-stat.github.io/stat-406>\n\n2. **Canvas (minimal)**\\\nQuiz 0, grades, course time/location info, links to videos from class\n\n3. **Slack**\\\nDiscussion board, questions\n\n4. **Github**\\\nHomework / Lab submission\n\n\n## Final Words of Wisdom\n\n::: {.fragment}\n- 8am is hard. But you can do it!\n    - I strongly urge you to get up at the same time everyday.\n:::\n\n::: {.fragment}\n- If you need help, please ask!\n    - I'm here to encourage you, get you un-stuck, and ponder ML mysteries with you\n:::\n\n::: {.fragment}\n- Think of this course as preparation for a race/performance/etc.\n    - You have to work out/practice, and there's no shortcuts.\n    - Training is hard, but you'll be pleased with the outcome if you put in the work.\n    - We're here to help you stay excited and motivated on your journey!\n:::\n\n::: {.fragment}\n- Join me on a fun exploration of cool material!\n:::\n\n# Questions?\n\n## TODOs\n\n::: {.columns}\n::: {.column}\n**By EOD Tomorrow:**\n\n1. Read the syllabus\n2. Install the R package, read docs, check your LaTeX installation\\\n*BE SURE to follow the Computer Setup instructions on the website!*\n3. **Take Quiz 00 on Canvas**\n\n:::\n::: {.column}\n\n**Before Next Tuesday:**\n\n1. Sign up for Slack (see Canvas)\n3. Sign up for Agora (details on Canvas)\n2. Link your iClicker account with Canvas\n4. (If needed) watch Github and R tutorials\n\n![Agora Signup](gfx/agora.png)\n:::\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}