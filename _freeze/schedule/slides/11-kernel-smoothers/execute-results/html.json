{
  "hash": "705133ca6bd22eef7712667d691269e0",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"11 Local methods\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n# Two Great Questions from Last Lecture\n\n## 1) The Bias of OLS\n\nIn last class' clicker question, we were trying to predict income from\na $p=2$, $n=50000$ dataset. I claimed that the *OLS predictor is high bias* in this example.\\\n\n\\\nBut Trevor proved that the *OLS predictor is unbiased* (via the following proof):\n\nA. Assume that $y = x^\\top \\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2).$\n\nB. Then $E[\\hat \\beta_\\mathrm{ols}] = E[ E[\\hat \\beta_\\mathrm{ols} \\mid \\mathbf X] ]$\n\nC. $E[\\hat \\beta_\\mathrm{ols} \\mid \\mathbf X] = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top E[ \\mathbf y \\mid \\mathbf X]$\n\nD. $E[ \\mathbf y \\mid \\mathbf X] = \\mathbf X \\beta$\n\nE. So $E[\\hat \\beta_\\mathrm{ols}] - \\beta = E[(\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf X \\beta] - \\beta = \\beta - \\beta = 0$.\n\n\\\n[Why did this proof not apply to the clicker question?]{.secondary} \\\n[(Which step of this proof breaks down?)]{.small}\n\n\n## 1) The Bias of OLS\n\nWhich step did the proof break down?\n\nA. Assume that $y = x^\\top \\beta + \\epsilon, \\quad \\epsilon \\sim \\mathcal N(0, \\sigma^2).$\n\n\\\nThis assumption does not hold.\n\nIt is (almost certainly) not the case that `Income ~ Age + Education`.\n\n. . .\n\n\\\nIn reality, $y \\sim f(x) + \\epsilon$ where $f(x)$ is some potentially nonlinear function. So\n\n$$\n\\begin{align}\nE[\\hat \\beta_\\mathrm{ols}] = E[E[\\hat \\beta_\\mathrm{ols} \\mid \\mathbf X ]]\n= E[ (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X f(\\mathbf X) ] \\ne \\beta\n\\end{align}\n$$\n\nIn statistics speak, our model is *misspecified*.\\\n[Ridge/lasso will always increase bias and decrease variance, even under misspecification.]{.small}\n\n\n## 2) Why does ridge regression shrink varinace?\n\n### Mathematically\n\n- Variance of OLS: $\\mathrm{Cov}[\\hat \\beta_\\mathrm{ols}] = \\sigma^2 E[ (\\mathbf X^\\top \\mathbf X)^{-1} ]$\n- Variance of ridge regression: $\\mathrm{Cov}[\\hat \\beta_\\mathrm{ols}] = \\sigma^2 E[ (\\mathbf X^\\top \\mathbf X + \\lambda \\mathbf I)^{-1} ]$\n\n::: fragment\n\n### Intuitively...\n\nThink about the constrained optimization problem pictorally. \\\nWhat happens if we had a different dataset?\n\n:::\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 08 October 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Last time...\n\nWe looked at [feature maps]{.secondary} as a way to do nonlinear regression.\n\nWe used new \"features\" $\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)$\n\n\nNow we examine a *nonparametric* alternative\n\nSuppose I just look at the \"neighbours\" of some point (based on the $x$-values)\n\nI just average the $y$'s at those locations together\n\n## Let's use 3 neighbours\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\ndata(arcuate, package = \"Stat406\")\nset.seed(406406)\narcuate_unif <- arcuate |> slice_sample(n = 40) |> arrange(position)\npt <- 15\nnn <-  3\nseq_range <- function(x, n = 101) seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out = n)\nneibs <- sort.int(abs(arcuate_unif$position - arcuate_unif$position[pt]), index.return = TRUE)$ix[1:nn]\narcuate_unif$neighbours = seq_len(40) %in% neibs\ng1 <- ggplot(arcuate_unif, aes(position, fa, colour = neighbours)) + \n  geom_point() +\n  scale_colour_manual(values = c(blue, red)) + \n  geom_vline(xintercept = arcuate_unif$position[pt], colour = red) + \n  annotate(\"rect\", fill = red, alpha = .25, ymin = -Inf, ymax = Inf,\n           xmin = min(arcuate_unif$position[neibs]), \n           xmax = max(arcuate_unif$position[neibs])\n  ) +\n  theme(legend.position = \"none\")\ng2 <- ggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(\n    data = tibble(\n      position = seq_range(arcuate_unif$position),\n      fa = FNN::knn.reg(\n        arcuate_unif$position, matrix(position, ncol = 1),\n        y = arcuate_unif$fa\n      )$pred\n    ),\n    colour = orange, linewidth = 2\n  )\nplot_grid(g1, g2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/load-lidar-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## KNN \n\n::: flex\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/small-lidar-again-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(arcuate, package = \"Stat406\")\nlibrary(FNN)\narcuate_unif <- arcuate |> \n  slice_sample(n = 40) |> \n  arrange(position) \n\nnew_position <- seq(\n  min(arcuate_unif$position), \n  max(arcuate_unif$position),\n  length.out = 101\n)\n\nknn3 <- knn.reg(\n  train = arcuate_unif$position, \n  test = matrix(arcuate_unif$position, ncol = 1), \n  y = arcuate_unif$fa, \n  k = 3\n)\n```\n:::\n\n\n:::\n:::\n\n\n\n## This method is $K$-nearest neighbours.\n\nIt's a [linear smoother]{.secondary} just like in previous lectures: \n$\\widehat{\\mathbf{y}} = \\mathbf{S} \\mathbf{y}$ for some matrix $S$.\n\nYou should imagine what $\\mathbf{S}$ looks like.\n\n1. What is the degrees of freedom of KNN, and how does it depend on $k$?\n2. How does $k$ affect the bias/variance?\n\n. . .\n\n- $\\mathrm{df} = \\tr{\\mathbf S} = n/k$.\n- $k = n$ produces a constant predictor (highest bias, lowest variance).\n- $k = 1$ produces a low bias but extremely high variance predictor.\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(406406)\n\nplot_knn <- function(k) {\n  ggplot(arcuate_unif, aes(position, fa)) +\n    geom_point(colour = blue) +\n    geom_line(\n      data = tibble(\n        position = seq_range(arcuate_unif$position),\n        fa = FNN::knn.reg(\n          arcuate_unif$position, matrix(position, ncol = 1),\n          y = arcuate_unif$fa,\n          k = k\n        )$pred\n      ),\n      colour = orange, linewidth = 2\n    ) + ggtitle(paste(\"k =\", k))\n}\n\ng1 <- plot_knn(1)\ng2 <- plot_knn(5)\ng3 <- plot_knn(length(arcuate_unif$position))\nplot_grid(g1, g2, g3, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Local averages (soft KNN)\n\nKNN averages the neighbours with equal weight.\n\nBut some neighbours are \"closer\" than other neighbours.\n\nInstead of choosing the number of neighbours to average, we can average \nany observations within a certain distance.\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThe boxes have width 30. \n\n\n## What is a \"kernel\" smoother?\n\n* The mathematics:\n\n> A kernel is any function $K$ such that for any $u$,\n>\n> - $K(u) \\geq 0$,\n> - $\\int K(u) du = 1$,\n> - $\\int uK(u) du = 0$.\n\n* The idea: a kernel takes weighted averages. The kernel function gives the weights.\n\n* The previous example is called the [boxcar]{.secondary} kernel. \n\n## Smoothing with the boxcar\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntestpts <- seq(0, 200, length.out = 101)\ndmat <- abs(outer(testpts, arcuate_unif$position, \"-\"))\nS <- (dmat < 15)\nS <- S / rowSums(S)\nboxcar <- tibble(position = testpts, fa = S %*% arcuate_unif$fa)\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(data = boxcar, colour = orange)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/boxcar-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nThis one gives the same non-zero weight to all points within $\\pm 15$ range.\n\n\n\n## Other kernels\n\nMost of the time, we don't use the boxcar because the weights are weird.\\\n[Ideally we would like closer points to have more weight.]{.small}\n\n::: flex\n::: w-60\nA more common kernel: the Gaussian kernel\n\n$$\nK(u) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{u^2}{2\\sigma^2}\\right)\n$$\n\nFor the plot, I made $\\sigma=7.5$. \n\nNow the weights \"die away\" for points farther from where we're predicting.\\\n[(but all nonzero!!)]{.small}\n\n:::\n\n::: w-40\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngaussian_kernel <- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 7.5) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n:::\n:::\n\n## Other kernels\n\nWhat if I made $\\sigma=15$?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngaussian_kernel <- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 15) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nBefore, points far from $x_{15}$ got very small weights, now they have more influence.\n\nFor the Gaussian kernel, $\\sigma$ determines something like the \"range\" of the smoother.\n\n\n\n## Many Gaussians\n\nThe following code creates $\\mathbf{S}$ for Gaussian kernel smoothers with different $\\sigma$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndmat <- as.matrix(dist(x))\nSgauss <- function(sigma) {\n  gg <- dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg), \"/\") # make the rows sum to 1.\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nSgauss <- function(sigma) {\n  gg <-  dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg),'/') # make the rows sum to 1.\n}\nboxcar$S15 = with(arcuate_unif, Sgauss(15) %*% fa)\nboxcar$S08 = with(arcuate_unif, Sgauss(8) %*% fa)\nboxcar$S30 = with(arcuate_unif, Sgauss(30) %*% fa)\nbc = boxcar %>% select(position, S15, S08, S30) %>% \n  pivot_longer(-position, names_to = \"Sigma\")\nggplot(arcuate_unif, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_line(data = bc, aes(position, value, colour = Sigma), linewidth = 1.5) +\n  scale_colour_brewer(palette = \"Set1\")\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## The bandwidth\n\n* Choosing $\\sigma$ is [very]{.secondary} important.\n\n* This \"range\" parameter is called the [bandwidth]{.secondary}.\n\n* It is way more important than which kernel you use.\n\n<!-- * The default kernel in `ksmooth()` is something called 'Epanechnikov': -->\n\n<!-- ```{r} -->\n<!-- epan <- function(x) 3/4 * (1 - x^2) * (abs(x) < 1) -->\n<!-- ggplot(data.frame(x = c(-2, 2)), aes(x)) + stat_function(fun = epan, colour = green, linewidth = 2) -->\n<!-- ``` -->\n\n\n## Choosing the bandwidth\n\nAs we have discussed, kernel smoothing (and KNN) are linear smoothers\n\n$$\\widehat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}$$\n\n\n\nThe [degrees of freedom]{.secondary} is $\\textrm{tr}(\\mathbf{S})$\n\nTherefore we can use our model selection criteria from before \n\n. . .\n\nUnfortunately, these don't satisfy the \"technical condition\", so\n`cv_nice()` doesn't give LOO-CV\n\n\n## Smoothing the full Lidar data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nar <- arcuate |> slice_sample(n = 200)\n\ngcv <- function(y, S) {\n  yhat <- S %*% y\n  mean( (y - yhat)^2 / (1 - mean(diag(S)))^2 )\n}\n\nfake_loocv <- function(y, S) {\n  yhat <- S %*% y\n  mean( (y - yhat)^2 / (1 - diag(S))^2 )\n}\n\ndmat <- as.matrix(dist(ar$position))\nsigmas <- 10^(seq(log10(300), log10(.3), length = 100))\n\ngcvs <- map_dbl(sigmas, ~ gcv(ar$fa, Sgauss(.x)))\nflcvs <- map_dbl(sigmas, ~ fake_loocv(ar$fa, Sgauss(.x)))\nbest_s <- sigmas[which.min(gcvs)]\nother_s <- sigmas[which.min(flcvs)]\n\nar$smoothed <- Sgauss(best_s) %*% ar$fa\nar$other <- Sgauss(other_s) %*% ar$fa\n```\n:::\n\n\n\n## Smoothing the full Lidar data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ng3 <- ggplot(data.frame(sigma = sigmas, gcv = gcvs), aes(sigma, gcv)) +\n  geom_point(colour = blue) +\n  geom_vline(xintercept = best_s, colour = red) +\n  scale_x_log10() +\n  xlab(sprintf(\"Sigma, best is sig = %.2f\", best_s))\ng4 <- ggplot(ar, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(aes(y = smoothed), colour = orange, linewidth = 2)\nplot_grid(g3, g4, nrow = 1)\n```\n\n::: {.cell-output-display}\n![](11-kernel-smoothers_files/figure-revealjs/smoothed-lidar-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nI considered $\\sigma \\in [0.3,\\ 300]$ and used $3.97$. \n\nIt's too wiggly, to my eye. Typical for GCV.\n\n\n## Smoothing manually\n\nI did Kernel Smoothing \"manually\"\n\n1. For a fixed bandwidth\n\n2. Compute the smoothing matrix\n\n3. Make the predictions\n\n4. Repeat and compute GCV\n\nThe point is to \"show how it works\". It's also really easy.\n\n## `R` functions / packages\n\nThere are a number of other ways to do this in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nloess()\nksmooth()\nKernSmooth::locpoly()\nmgcv::gam()\nnp::npreg()\n```\n:::\n\n\n\nThese have tricks and ways of doing CV and other things automatically.\n\nNote\n: All I needed was the distance matrix `dist(x)`. \n: Given ANY distance function \n: say, $d(\\mathbf{x}_i, \\mathbf{x}_j) = \\Vert\\mathbf{x}_i - \\mathbf{x}_j\\Vert_2 + I(x_{i,3} = x_{j,3})$\n: I can use these methods.\n\n\n# Next time...\n\nWhy don't we just smooth everything all the time?\n",
    "supporting": [
      "11-kernel-smoothers_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}