{
  "hash": "76924a8868f28e35fddc5ffb4b57dbd1",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"23 Neural nets - generalization\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 13 November 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n\n## This lecture\n\n1. What factors affect generalization? (The ability to make accurate predictions)\n\n2. Why do NN generalize, despite having lots of parameters?\n\n3. Modern techniques to improve generalization\n\n\n\n\n# What factors affect generalization?\n\n(The ability to make accurate predictions)\n\n\n## Tunable parameters of NN\n\n1. Number of hidden layers ($L$)\n\n2. Width of hidden layers ($D$)\n\n3. Nonlinearity function\n\n4. Loss function\n\n5. Initial SGD step size\n\n6. SGD step size decay rate\n\n7. SGD batch size\n\n8. SGD stopping criterion\n\n9. Amount of regularization (we'll talk about this concept in a bit)\n\n10. Initialization of NN parameters\n\n11. ...\n\n\n## How to tune NN parameters\n\n### ðŸ˜“\n\n* There are exponentially many designs of NNs\n\n* Training a single NN is expensive\n\n* NN training depends on random initialization, so you generally have to do multiple runs\n\n\n### In Practice\n\n* Compare a handful of designs on a single holdout set (no cross val)\n\n* Principled NN architecture search is an active area of research\n\n\n\n## Some Common Patterns to Reduce the Search Space\n\n1. Use ReLU nonlinearities, and nothing else\n\n2. Use the same width for all layers (or grow with with a simple formula)\n\n3. Measure loss on a validation set throughout training, and stop SGD when the validation loss plateaus\n\n4. Ask a grad student for their tricks\n\n\n# Why do NN generalize...\n\n... despite having tons of parameters?\n\n\n\n## Capacity vs Generalization\n\nConsider a NN with ReLU nonlinearities\n$g( \\boldsymbol w^\\top \\boldsymbol z) = \\max\\{\\boldsymbol w^\\top \\boldsymbol z, 0 \\}$\nwith $L$ hidden layers, each with $D$ hidden activations.\n\n### Recall:\n\n* Number of piecewise-linear regions: $O(D^L)$ (exponential!)\n\n* Number of parameters: $O(D^2 L)$\n\n### This implies:\n\n* Our NN is capable of learning complicated functions (many piecewise-linear components)\n\n* But will it learn the *right* function from limited data?\n\n\n## Recall: Bias/Variance Tradeoff For Trees\n\n::: flex\n::: w-58\n\n![](gfx/tree_bias_variance.png){width=600 fig-align=\"center\" fig-caption=\"Bias/variance tradeoff for trees as a function of depth.\"}\n\n:::\n::: w-40\n\n* Neural networks have lots of parameters ( $O(D^2 L)$, which is typically $> n$ )\n\n* In theory, we would expect similar bias/variance curves for neural networks as a function of `# params`\n\n:::\n:::\n\n\n## The Surprising Bias/Var Curves For NN (Double Descent)\n\n![](gfx/double_descent_nn_toy.png){width=1000 fig-align=\"center\" fig-caption=\"Toy depiction of double descent curve for neural networks.\"}\n\n* NN risk (as a function of `# params`) experiences a \"double descent\" shape?!?!?!\n\n* Most modern NN have tons of parameters, and so they're explained by the right side of the graph\n\n\n## The Surprising Bias/Var Curves For NN (Double Descent)\n\n![[Image credit: Belkin et al., (2019)]{.small}](gfx/double_descent_nn.png){width=1000 fig-align=\"center\" fig-caption=\"Double descent curve for neural networks.\"}\n\n* Double descent is a *newly discovered phenomenon* (~2019)\n\n* Statisticians are still trying to understand why it occurs.\\\n  [There has been good progress since ~2020!]{.small}\n\n\n\n## To Understand Double Descent: Study Basis Regression\n\nThe double descent phenomenon is not specific to neural networks.\\\nWe can observe it in basis regression (read: linear models!) as we increase the number of basis functions $> n$:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(splines)\nset.seed(20221102)\nn <- 20\ndf <- tibble(\n  x = seq(-1.5 * pi, 1.5 * pi, length.out = n),\n  y = sin(x) + runif(n, -0.5, 0.5)\n)\ng <- ggplot(df, aes(x, y)) + geom_point() + stat_function(fun = sin) + ylim(c(-2, 2))\ng\n```\n\n::: {.cell-output-display}\n![](23-nnets-other_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxn <- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)\n# Spline by hand\nX <- bs(df$x, df = 20, intercept = TRUE)\nXn <- bs(xn, df = 20, intercept = TRUE)\nS <- svd(X)\nyhat <- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)\ng + geom_line(data = tibble(x = xn, y = yhat), colour = orange) +\n  ggtitle(\"20 basis functions (n=20)\")\n```\n\n::: {.cell-output-display}\n![](23-nnets-other_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n##\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nxn <- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)\n# Spline by hand\nX <- bs(df$x, df = 40, intercept = TRUE)\nXn <- bs(xn, df = 40, intercept = TRUE)\nS <- svd(X)\nyhat <- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)\ng + geom_line(data = tibble(x = xn, y = yhat), colour = orange) +\n  ggtitle(\"40 basis functions (n=20)\")\n```\n\n::: {.cell-output-display}\n![](23-nnets-other_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ndoffs <- 4:50\nmse <- function(x, y) mean((x - y)^2)\nget_errs <- function(doff) {\n  X <- bs(df$x, df = doff, intercept = TRUE)\n  Xn <- bs(xn, df = doff, intercept = TRUE)\n  S <- svd(X)\n  yh <- S$u %*% crossprod(S$u, df$y)\n  bhat <- S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)\n  yhat <- Xn %*% S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)\n  nb <- sqrt(sum(bhat^2))\n  tibble(train = mse(df$y, yh), test = mse(yhat, sin(xn)), norm = nb)\n}\nerrs <- map(doffs, get_errs) |>\n  list_rbind() |> \n  mutate(`degrees of freedom` = doffs) |> \n  pivot_longer(train:test, values_to = \"error\")\nggplot(errs, aes(`degrees of freedom`, error, color = name)) +\n  geom_line(linewidth = 2) + \n  coord_cartesian(ylim = c(0, .12)) +\n  scale_x_log10() + \n  scale_colour_manual(values = c(blue, orange), name = \"\") +\n  geom_vline(xintercept = 20)\n```\n\n::: {.cell-output-display}\n![](23-nnets-other_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n* Inflection point occurs when `# basis functions = n`!\n\n* This is the point at which our basis regressor is able to perfectly fit the training data.\n\n## Understanding Double Descent (Hand-Wavy)\n\nLet $\\boldsymbol Z \\in \\R^{n \\times d}$ be the matrix of basis expansions for our $n$ training points.\n\nBasis regression is just OLS with the basis expansion $\\boldsymbol Z$:\n[$$ \\min_{\\boldsymbol \\beta} \\left\\Vert \\boldsymbol Z \\boldsymbol \\beta - \\boldsymbol y \\right\\Vert_2^2. $$]{.small}\n\n* When $d < n$, the regressor is **underparameterized.**\\\n  [I.e. there is no $\\boldsymbol \\beta$ that perfectly explains our training responses given our basis-expanded training inputs.]{.small}\n  \n* When $d = n$, there is a value of $\\boldsymbol \\beta$ that fits our training data perfectly.\\\n  [I.e. $\\Vert \\boldsymbol Z \\boldsymbol \\beta - \\boldsymbol y \\Vert = 0$.]{.small}\n  \n  * [We are fitting both the *noise* and the *signal* (leading to a high variance predictor).]{.small}\n  \n* When $d > n$, we can also fit the data (noise + signal) perfectly.ðŸ‘‹ However, more features implies that the the noise gets \"spread out\" over all of parameters. ðŸ‘‹ \n  \n  * [ðŸ‘‹ Since each parameter only captures \"some\" of the noise, we are less likely to make predictions based on it. ðŸ‘‹]{.small}\n  \n  * [This explanation is overly simplified, and there is a lot more at play.]{.small}\n\n\n## Do we need to worry about variance?\n\n*Regularizing* a neural network (adding a complexity penalty to the loss) is a common practice to prevent overfitting to the noise.\n\n$$ \\argmin_{\\boldsymbol W^{(t)}, \\boldsymbol \\beta} \\sum_{i=1}^n \\ell(y_i, \\hat f_\\mathrm{NN}(\\boldsymbol x_i)  \\: + \\: \\text{complexity penalty} $$\n\nE.g. *weight decay / L2 regularization*:\n\n$$ \\text{complexity penalty} = \\frac{\\lambda}{2} \\left( \\Vert \\boldsymbol \\beta \\Vert_2^2 + \\sum_{i=1}^L \\Vert \\mathrm{vec} (\\boldsymbol W^{(L)}) \\Vert_2^2 \\right) $$\n\n* $\\lambda$ is a tuning parameter\n\n* [What does weight decay / L2 regularization remind you of? Think about linear models]{.secondary}\n\n\n\n## Do we need to worry about variance?\n\n$$ \\text{complexity penalty} = \\frac{\\lambda}{2} \\left( \\Vert \\boldsymbol \\beta \\Vert_2^2 + \\sum_{i=1}^L \\Vert \\mathrm{vec} (\\boldsymbol W^{(L)}) \\Vert_2^2 \\right) $$\n\n* Before we understood double descent, we used to think you needed high $\\lambda$ (lots of regularization) to combat high variance\n\n  * People invented many other regularizers (e.g. dropout, pruning, mixup, etc.)\n\n* Now that we understand double descent (and we realize we don't have a variance problem), [it's now uncommon to do anything more than light weight decay (small $\\lambda$)]{.secondary}\n\n<!-- ## Degrees of freedom and complexity -->\n\n<!-- * In low dimensions (where $n \\gg p$), with linear smoothers, df and model complexity are roughly the same. -->\n\n<!-- * But this relationship breaks down in more complicated settings -->\n\n<!-- * We've already seen this: -->\n\n<!-- ```{r, message=FALSE} -->\n<!-- library(glmnet) -->\n<!-- out <- cv.glmnet(X, df$y, nfolds = n) # leave one out -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| code-fold: true -->\n<!-- #| fig-width: 9 -->\n<!-- #| fig-height: 2.5 -->\n<!-- with( -->\n<!--   out,  -->\n<!--   tibble(lambda = lambda, df = nzero, cv = cvm, cvup = cvup, cvlo = cvlo ) -->\n<!-- ) |>  -->\n<!--   filter(df > 0) |> -->\n<!--   pivot_longer(lambda:df) |>  -->\n<!--   ggplot(aes(x = value)) + -->\n<!--   geom_errorbar(aes(ymax = cvup, ymin = cvlo)) + -->\n<!--   geom_point(aes(y = cv), colour = orange) + -->\n<!--   facet_wrap(~ name, strip.position = \"bottom\", scales = \"free_x\") + -->\n<!--   scale_y_log10() + -->\n<!--   scale_x_log10() + theme(axis.title.x = element_blank()) -->\n<!-- ``` -->\n\n\n<!-- ## Infinite solutions -->\n\n<!-- * In Lasso, df is not really the right measure of complexity -->\n\n<!-- * Better is $\\lambda$ or the norm of the coefficients (these are basically the same) -->\n\n<!-- * So what happened with the Splines? -->\n\n<!-- . . . -->\n\n<!-- * When df $= 20$, there's a unique solution that interpolates the data -->\n\n<!-- * When df $> 20$, there are infinitely many solutions that interpolate the data. -->\n\n<!-- Because we used the SVD to solve the system, we happened to pick one: the one that has the smallest $\\Vert\\hat\\beta\\Vert_2$ -->\n\n<!-- Recent work in Deep Learning shows that SGD has the same property: it returns the local optima with the smallest norm. -->\n\n<!-- If we measure complexity in terms of the norm of the weights, rather than by counting parameters, we don't see double descent anymore. -->\n\n\n<!-- ## The lesson -->\n\n<!-- * Deep learning isn't magic. -->\n\n<!-- * Zero training error with lots of parameters doesn't mean good test error. -->\n\n<!-- * We still need the bias variance tradeoff -->\n\n<!-- * It's intuition still applies: more flexibility eventually leads to increased MSE -->\n\n<!-- * But we need to be careful how we measure complexity. -->\n\n<!-- ::: aside -->\n\n<!-- There is very interesting recent theory that says  -->\n<!-- when we can expect lower test error to the right of the interpolation threshold -->\n<!-- than to the left.  -->\n\n<!-- ::: -->\n\n\n\n\n\n\n\n<!-- ## Regularizing neural networks -->\n\n<!-- NNets can almost always achieve 0 training error. Even with regularization. Because they have so many parameters. -->\n\n<!-- Flavours: -->\n\n<!-- -   a complexity penalization term $\\longrightarrow$ solve $\\min \\hat{R} + \\rho(\\alpha,\\beta)$ -->\n<!-- -   early stopping on the back propagation algorithm used for fitting -->\n\n\n<!-- Weight decay -->\n<!-- : This is like ridge regression in that we penalize the squared Euclidean norm of the weights $\\rho(\\mathbf{W},\\mathbf{B}) = \\sum w_i^2 + \\sum b_i^2$ -->\n\n<!-- Weight elimination -->\n<!-- : This encourages more shrinking of small weights $\\rho(\\mathbf{W},\\mathbf{B}) =  \\sum \\frac{w_i^2}{1+w_i^2} + \\sum \\frac{b_i^2}{1 + b_i^2}$ or Lasso-type -->\n\n<!-- Dropout -->\n<!-- : In each epoch, randomly choose $z\\%$ of the nodes and set those weights to zero. -->\n\n\n\n<!-- ## Other common pitfalls -->\n\n<!-- [Number of hidden units:]{.tertiary}   -->\n<!-- It is generally -->\n<!-- better to have too many hidden units than too few (regularization -->\n<!-- can eliminate some). -->\n\n\n<!-- [Sifting the output:]{.tertiary} -->\n\n<!-- * Choose the solution that minimizes training error -->\n<!-- * Choose the solution that minimizes the penalized  training error -->\n<!-- * Average the solutions across runs -->\n\n\n\n\n# Modern Techniques to Improve Generalization\n\n## Specialty architectures\n\nSo far we've studied neural networks where we (recursively) construct basis functions from \"building blocks\" of the form:\n$$ \\boldsymbol a^{(t)}_j = g( \\boldsymbol w^{(i)\\top}_j \\boldsymbol a^{(t - 1)}) $$\n\n* These neural networks are known as *multilayer perceptrons* (MLP).\n\n* By using different building blocks, we can make neural networks that are more adept to different types of data. E.g.:\n\n   1. **Convolutional NN** (good for image data)\n   \n   2. **Graph NN** (good for molecules, social networks, etc.)\n   \n   3. **Transformers** (good for language and sequential data)\n\n   \n   \n## Specialty architectures: convolutional neural networks\n\n::: flex\n::: w-60\n\nRather than computing an *inner product* with the hidden layer parameters (i.e. $\\boldsymbol w^{(i)\\top}_j \\boldsymbol a^{(t - 1)}$), we instead perform a *convolution*:\n\n$$ \\boldsymbol a^{(t)}_j = g( \\boldsymbol w^{(i)}_j \\star \\boldsymbol a^{(t - 1)}) $$\n\n* Captures spatial correlations amongst neighbouring pixels\n\n* Predictions remain constant even if we *translate* objects in the image\n\nThe convolutional building blocks are usually combined with other building blocks, like *pooling layers* and *normalization layers*.\n\n   \n:::\n\n::: w-35\n![](https://maucher.home.hdm-stuttgart.de/Pics/gif/same_padding_no_strides.gif){width=100% fig-align=\"center\" fig-caption=\"Animation of convolution operation\"}\n:::\n:::\n\n## Specialty architectures: convolutional neural networks\n\nWhy is a convolutional neural network better for images?\n\n![[Image credit: Varsha Kishore]{.small}](gfx/image_mlp.png){width=800 fig-align=\"center\" fig-caption=\"MLP for image data\"}\n\n\n\n* With an standard MLP, we'd need to \"flatten\" our image into a vector of pixels. This flattening doesn't preseve spatial correlations amongst pixels.\n\n* If the dog in our image shifts, then we are not guaranteed to make the same prediction (we are not translation invariant).\n\n## Transfer Learning\n\nYou want to build an image classifier for CT scans, but you only have $n=1000$ ðŸ˜¢\\\n[Conventional wisdom would tell you that you don't have enough data to train a neural network.]{.small}\n\n### Transfer learning to the rescue!\n\n* Start with an existing neural network trained on a related predictive task\n\n* Train this neural network on your data using gradient descent with a *small step size*\\\n  [Also known as *fine-tuning*]{.secondary}\n  \n![](gfx/transfer_learning.png){width=800 fig-align=\"center\" fig-caption=\"Transfer learning depiction\"}\n\n## Transfer Learning\n\n\n![](gfx/transfer_learning.png){width=800 fig-align=\"center\" fig-caption=\"Transfer learning depiction\"}\n\n### Why Does This Work?\n\n* The original NN has learned basis functions that are REALLY good for image data\n\n* You are now essentially using these good basis functions on your smaller dataset\n\n\n\n## Final Thoughts\n\n* Not much theory for why NN work (though this is increasing)\n\n* NN are best for *unstructured data types* (e.g. images, text, etc.)\n   \n   * [Best when combined with a specialty architecture (e.g. convolutional NN)]{.small}\n   \n   * [If you have \"tabular\" data, use another algorithm (e.g. random forest)]{.small}\n   \n* *Transfer learning* is now the defacto approach\n\n   * [Try not to train NN from scratch]{.small}\n   \n   * [Makes NN work for small datasets]{.small}\n   \n* NN are computational expensive\n\n   * [They won't run on your laptiop]{.small}\n   \n   * [You need a GPU cluster]{.small}\n   \n* NN are amazing, but they're not always the right solution. [What are some other downsides?]{.secondary}\n\n\n## Final Thoughts\n\n* If you want to play around with NN, [learn Python]{.secondary}\n\n   * [There's an example on the website of how to train NN in R. It's gnarly.]{.small}\n   \n   * [Use the PyTorch library]{.small}\n\n* There's a wide world of NN to learn about!\n\n\n\n\n# Next time...\n\n[Module 5]{.secondary}\n\n[unsupervised learning]{.secondary}\n\n\n\n\n",
    "supporting": [
      "23-nnets-other_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}