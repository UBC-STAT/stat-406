{
  "hash": "5b51dbd0cf9e4432982d1b42a1497f31",
  "result": {
    "markdown": "---\nlecture: \"27 K-means clustering\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 30 November 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Clustering\n\nSo far, we've looked at ways of reducing the dimension.\n\nEither linearly or nonlinearly, \n\n> The goal is visualization/exploration or possibly for an input to supervised learning.\n\nNow we try to find groups or clusters in our data.\n\nThink of [clustering]{.secondary} as classification without the labels.\n\n\n\n\n## K-means (ideally) {.smaller}\n\n\n1.  Select a number of clusters $K$.\n\n2.  Let $C_1,\\ldots,C_K$ partition $\\{1,2,3,\\ldots,n\\}$ such that\n    - All observations belong to some set $C_k$.\n    - No observation belongs to more than one set.\n\n3.  Make __within-cluster\n    variation__, $W(C_k)$, as small as\n    possible. $$\\min_{C_1,\\ldots,C_K} \\sum_{k=1}^K W(C_k).$$\n\n4.  Define $W$ as $$W(C_k) =  \\frac{1}{2|C_k|} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2.$$\nThat is, the average (Euclidean) distance between all cluster\nmembers.\n\n. . .\n\nTo work, K-means needs [distance to a center]{.secondary} and [notion of center]{.secondary}\n\n\n## Why this formula? {background-color=\"#97D4E9\" .smaller}\n\nLet $\\overline{x}_k = \\frac{1}{|C_k|} \\sum_{i\\in C_k} x_i$\n\n$$\n\\begin{aligned}\n\\sum_{k=1}^K W(C_k) \n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2\n= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i\\neq i' \\in C_k} \\norm{x_i - x_{i'}}_2^2 \\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i \\neq i' \\in C_k} \\norm{x_i -\\overline{x}_k + \\overline{x}_k - x_{i'}}_2^2\\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\left[\\sum_{i \\neq i' \\in C_k} \\left(\\norm{x_i - \\overline{x}_k}_2^2 + \n\\norm{x_{i'} - \\overline{x}_k}_2^2\\right) + \\sum_{i \\neq i' \\in C_k} 2 (x_i-\\overline{x}_k)^\\top(\\overline{x}_k - x_{i'})\\right]\\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\left[2(|C_k|-1)\\sum_{i \\in C_k} \\norm{x_i - \\overline{x}_k}_2^2  + 2\\sum_{i \\in C_k} \\norm{x_i - \\overline{x}_k}_2^2 \\right]\\\\\n&= \\sum_{k=1}^K \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2\n\\end{aligned}\n$$\n\nIf you wanted (equivalently) to minimize $\\sum_{k=1}^K \\frac{1}{|C_k|} \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2$, then you'd use $\\sum_{k=1}^K \\frac{1}{\\binom{C_k}{2}} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2$\n\n\n## K-means (in reality)\n\nThis is too computationally challenging ( $K^n$ partions! )\n$$\\min_{C_1,\\ldots,C_K} \\sum_{k=1}^K W(C_k).$$\nSo, we make a greedy approximation:\n\n1.  Randomly assign observations to the $K$ clusters\n2.  Iterate the following:\n    -   For each cluster, compute the $p$-length\n        vector of the means in that cluster.\n    -   Assign each observation to the cluster whose centroid is closest\n        (in Euclidean distance).\n\nThis procedure is guaranteed to decrease $\\sum_{k=1}^K W(C_k)$ at each step.\n\n\n## Best practices\n\nTo fit K-means, you need to\n\n1.  Pick $K$ (inherent in the method)\n\n2.  Convince yourself you have found a good solution (due to the\n    randomized / greedy algorithm).\n\nFor 2., run\nK-means many times with different starting points. Pick the solution\nthat has the smallest value for\n$$\\sum_{k=1}^K W(C_k)$$\n\n\nIt turns out that [1.]{.secondary} is difficult to do in a\nprincipled way.\n\n\n## Choosing the Number of Clusters\n\nWhy is it important?\n\n-   It might make a big difference (concluding there are $K = 2$ cancer\n    sub-types versus $K = 3$).\n\n-   One of the major goals of statistical learning is automatic\n    inference. A good way of choosing $K$ is certainly a part of this.\n\n\n## Withinness and betweenness\n\n$$W(K) = \\sum_{k=1}^K W(C_k) = \\sum_{k=1}^K \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2,$$\n\n\n\nWithin-cluster variation measures how [tightly grouped]{.secondary} the clusters are. \n\n. . .\n\nIt's opposite is [Between-cluster variation]{.secondary}: How spread apart are the clusters?\n\n$$B(K) = \\sum_{k=1}^K |C_k| \\norm{\\overline{x}_k - \\overline{x} }_2^2,$$\n\n\nwhere $|C_k|$ is the number of points in $C_k$, and $\\overline{x}$ is\nthe grand mean\n\n::: flex\n::: w-30\n$W$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0a8754;overflow:visible;position:relative;\"><path d=\"M169.4 470.6c12.5 12.5 32.8 12.5 45.3 0l160-160c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L224 370.8 224 64c0-17.7-14.3-32-32-32s-32 14.3-32 32l0 306.7L54.6 265.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l160 160z\"/></svg>`{=html} when $K$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M214.6 41.4c-12.5-12.5-32.8-12.5-45.3 0l-160 160c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L160 141.2V448c0 17.7 14.3 32 32 32s32-14.3 32-32V141.2L329.4 246.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-160-160z\"/></svg>`{=html}\n:::\n::: w-30\n$B$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M214.6 41.4c-12.5-12.5-32.8-12.5-45.3 0l-160 160c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L160 141.2V448c0 17.7 14.3 32 32 32s32-14.3 32-32V141.2L329.4 246.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-160-160z\"/></svg>`{=html} when $K$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M214.6 41.4c-12.5-12.5-32.8-12.5-45.3 0l-160 160c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L160 141.2V448c0 17.7 14.3 32 32 32s32-14.3 32-32V141.2L329.4 246.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-160-160z\"/></svg>`{=html}\n:::\n::: w-30\n$B/K$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M169.4 470.6c12.5 12.5 32.8 12.5 45.3 0l160-160c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L224 370.8 224 64c0-17.7-14.3-32-32-32s-32 14.3-32 32l0 306.7L54.6 265.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l160 160z\"/></svg>`{=html} when $K$ `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M214.6 41.4c-12.5-12.5-32.8-12.5-45.3 0l-160 160c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L160 141.2V448c0 17.7 14.3 32 32 32s32-14.3 32-32V141.2L329.4 246.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-160-160z\"/></svg>`{=html}\n:::\n\n:::\n\n\n\n## CH index  \n\n> Want small $W$, big $B/K$\n\n$$\\textrm{CH}(K) = \\frac{B(K)/(K-1)}{W(K)/(n-K)}$$ \n\nTo choose $K$, pick some\nmaximum number of clusters to be considered, $K_{\\max} = 20$, for\nexample\n\n\n$$\\widehat K = \\argmax_{K \\in \\{ 2,\\ldots, K_{\\max} \\}} CH(K).$$\n\n\n::: callout-note\n* CH is undefined for $K = 1$. \n* The divisors $(K-1)$ and $(n-K)$ scale $B$ and $W$ appropriately.\n:::\n\n\n\n## Dumb example\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(mvtnorm)\nset.seed(406406406)\nX1 <- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))\nX2 <- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))\nX3 <- rmvnorm(40, c(4, 4))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](27-kmeans_files/figure-revealjs/plotting-dumb-clusts-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Dumb example\n\n* We would [maximize]{.secondary} CH\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nK <- 2:40\nN <- nrow(clust_raw)\nall_clusters <- map(K, ~ kmeans(clust_raw, .x, nstart = 20))\nall_assignments <- map_dfc(all_clusters, \"cluster\")\nnames(all_assignments) <- paste0(\"K = \", K)\nsummaries <- map_dfr(all_clusters, `[`, c(\"tot.withinss\", \"betweenss\")) |>\n  rename(W = tot.withinss, B = betweenss) |>\n  mutate(\n    K = K,\n    `W / (N - K)` = W / (N - K),\n    `B / K` = B / (K - 1), \n    `CH index` = `B / K` / `W / (N - K)`\n  )\nsummaries |>\n  pivot_longer(-K) |>\n  ggplot(aes(K, value)) +\n  geom_line(color = blue, linewidth = 2) +\n  ylab(\"\") +\n  coord_cartesian(c(1, 20)) +\n  facet_wrap(~name, ncol = 3, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](27-kmeans_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Dumb example\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](27-kmeans_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Dumb example\n\n* $K = 3$\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkm <- kmeans(clust_raw, 3, nstart = 20)\nnames(km)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n\n```{.r .cell-code}\ncenters <- as_tibble(km$centers, .name_repair = \"unique\")\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](27-kmeans_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Next time...\n\nHierarchical clustering\n",
    "supporting": [
      "27-kmeans_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}