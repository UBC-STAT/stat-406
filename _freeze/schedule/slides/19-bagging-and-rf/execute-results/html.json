{
  "hash": "29f6552f00d740624391a741a4ffc657",
  "result": {
    "markdown": "---\nlecture: \"19 Bagging and random forests\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 11 October 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n\n\n\n## Bagging\n\nMany methods (trees, nonparametric smoothers) tend to have [low bias]{.secondary} but [high variance]{.secondary}.  \n\nEspecially fully grown trees (that's why we prune them)\n\n\nHigh-variance\n: if we split the training data into two parts at random and fit a decision tree to each part, the results will be quite different.\n\n\nIn contrast, a low variance estimator \n: would yield similar results if applied to the two parts (consider $\\widehat{f} = 0$).\n\n\n[Bagging]{.tertiary}, short for [bootstrap aggregation]{.tertiary}, is a general purpose procedure for reducing variance. \n\nWe'll use it specifically in the context of trees, but it can be applied much more broadly.\n\n\n\n## Bagging: The heuristic motivation\n\n\nSuppose we have $n$ uncorrelated observations $Z_1, \\ldots, Z_n$, each with variance $\\sigma^2$.\n\n\nWhat is the variance of\n\n$$\\overline{Z} = \\frac{1}{n} \\sum_{i=1}^n Z_i\\ \\ \\ ?$$\n\n\n. . .\n\nSuppose we had $B$ separate (uncorrelated) training sets, $1, \\ldots, B$, \n\nWe can form $B$ separate model fits, \n$\\widehat{f}^1(x), \\ldots, \\widehat{f}^B(x)$, and then average them:\n\n$$\\widehat{f}_{B}(x) = \\frac{1}{B} \\sum_{b=1}^B \\widehat{f}^b(x)$$\n\n\n\n## Bagging: The bootstrap part\n\nThis isn't practical \n: we don't have many training sets.  \n\nWe therefore turn to the bootstrap to [simulate]{.secondary} having many training sets.\n\nSuppose we have data $Z_1, \\ldots, Z_n$ \n\n1. Choose some large number of samples, $B$. \n2. For each $b = 1,\\ldots,B$, resample from $Z_1, \\ldots, Z_n$, call it \n$\\widetilde{Z}_1, \\ldots, \\widetilde{Z}_n$. \n3. Compute $\\widehat{f}^b = \\widehat{f}(\\widetilde{Z}_1, \\ldots, \\widetilde{Z}_n)$.\n\n$$\\widehat{f}_{\\textrm{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\widehat{f}^b(x)$$\n\nThis process is known as [Bagging]{.secondary}\n\n\n## Bagging trees\n\n::: flex\n::: w-50\n![](gfx/bagtree.jpg)\n:::\n\n\n::: w-50\n\nThe procedure for trees is the following\n\n\n1. Choose a large number $B$.\n2. For each $b = 1,\\ldots, B$, grow an unpruned tree on the $b^{th}$ bootstrap draw from the data.\n3. Average all these trees together.\n\n\n:::\n:::\n\n## Bagging trees\n\n::: flex\n::: w-50\n![](gfx/bagtree.jpg)\n:::\n\n\n::: w-50\n\n\n\nEach tree, since it is unpruned, will have \n\n* [low]{.primary} / [high]{.secondary} variance\n\n* [low]{.primary} / [high]{.secondary} bias\n\n\n\nTherefore averaging many trees results in an estimator that has \n\n* [lower]{.primary} / [higher]{.secondary} variance and \n\n* [low]{.primary} / [high]{.secondary} bias.\n\n:::\n:::\n\n## Bagging trees: Variable importance measures\n\n\nBagging can dramatically improve predictive performance of trees \n\nBut we sacrificed some [interpretability]{.hand}. \n\nWe no longer have that nice diagram that shows the segmentation of the predictor space \n\n(more accurately, we have $B$ of them).  \n\nTo recover some information, we can do the following:\n\n\n1. For each of the $b$ trees and each of the $p$ variables, we record the amount that the Gini index is reduced by the addition of that variable \n2. Report the average reduction over all $B$ trees.\n\n\n\n\n## Random Forest\n\nRandom Forest is an extension of Bagging, in which the bootstrap trees are [decorrelated]{.tertiary}.  \n\n\nRemember: $\\Var{\\overline{Z}} = \\frac{1}{n}\\Var{Z_1}$ _unless the $Z_i$'s are correlated_\n\nSo Bagging may not reduce the variance _that_ much because the training sets are correlated across trees.\n\n. . .\n\nHow do we decorrelate?\n\n\n\nDraw a bootstrap sample and start to build a tree. \n\nBut\n: Before we split, we randomly pick\n: $m$ of the possible $p$ predictors as candidates for the split. \n\n## Decorrelating\n\nA new sample of size $m$ of the predictors is taken [at each split]{.secondary}. \n\nUsually, we use about $m = \\sqrt{p}$ \n\nIn other words, at each split, we [aren't even allowed to consider the majority of possible predictors!]{.hand}\n\n\n## What is going on here?\n\n\nSuppose there is 1 really strong predictor and many mediocre ones. \n\n\n* Then each tree will have this one predictor in it,\n\n* Therefore, each tree will look very similar (i.e. highly correlated). \n\n* Averaging highly correlated things leads to much less variance reduction than if they were uncorrelated.\n\nIf we don't allow some trees/splits to use this important variable, each of the trees will be much less similar and\nhence much less correlated.\n\n\nBagging Trees is Random Forest when $m = p$, that is, when we can consider all the variables at each split.\n\n\n\n## Example with Mobility data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(randomForest)\nlibrary(kableExtra)\nset.seed(406406)\nmob <- Stat406::mobility |>\n  mutate(mobile = as.factor(Mobility > .1)) |>\n  select(-ID, -Name, -Mobility, -State) |>\n  drop_na()\nn <- nrow(mob)\ntrainidx <- sample.int(n, floor(n * .75))\ntestidx <- setdiff(1:n, trainidx)\ntrain <- mob[trainidx, ]\ntest <- mob[testidx, ]\nrf <- randomForest(mobile ~ ., data = train)\nbag <- randomForest(mobile ~ ., data = train, mtry = ncol(mob) - 1)\npreds <-  tibble(truth = test$mobile, rf = predict(rf, test), bag = predict(bag, test))\n\nkbl(cbind(table(preds$truth, preds$rf), table(preds$truth, preds$bag))) |>\n  add_header_above(c(\"Truth\" = 1, \"RF\" = 2, \"Bagging\" = 2))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n<tr>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"1\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Truth</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">RF</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Bagging</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> FALSE </th>\n   <th style=\"text-align:right;\"> TRUE </th>\n   <th style=\"text-align:right;\"> FALSE </th>\n   <th style=\"text-align:right;\"> TRUE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 61 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 60 </td>\n   <td style=\"text-align:right;\"> 11 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 12 </td>\n   <td style=\"text-align:right;\"> 22 </td>\n   <td style=\"text-align:right;\"> 10 </td>\n   <td style=\"text-align:right;\"> 24 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Example with Mobility data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nvarImpPlot(rf, pch = 16, col = orange)\n```\n\n::: {.cell-output-display}\n![](19-bagging-and-rf_files/figure-revealjs/mobility-results-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## One last thing...\n\nOn average\n: drawing $n$ samples from $n$ observations with replacement (bootstrapping) results in [~ 2/3]{.hand} of the observations being selected. (Can you show this?)\n\n\nThe remaining ~ 1/3 of the observations are [not used on that tree]{.tertiary}.\n\nThese are referred to as [out-of-bag (OOB)]{.tertiary}.\n\n\nWe can think of it as a [for-free cross-validation]{.hand}.  \n\n\nEach time a tree is grown, we get its prediction error on the unused observations.  \n\nWe average this over all bootstrap samples.\n\n\n\n## Out-of-bag error estimation for bagging / RF\n\nFor `randomForest()`, `predict()` without passing `newdata = ` gives the OOB prediction\n\n[not]{.secondary} like `lm()` where it gives the fitted values\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntab <- table(predict(bag), train$mobile) \nkbl(tab) |> add_header_above(c(\"Truth\" = 1, \"Bagging\" = 2))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n<tr>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"1\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Truth</div></th>\n<th style=\"border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; \" colspan=\"2\"><div style=\"border-bottom: 1px solid #ddd; padding-bottom: 5px; \">Bagging</div></th>\n</tr>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:right;\"> FALSE </th>\n   <th style=\"text-align:right;\"> TRUE </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> FALSE </td>\n   <td style=\"text-align:right;\"> 182 </td>\n   <td style=\"text-align:right;\"> 28 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> TRUE </td>\n   <td style=\"text-align:right;\"> 21 </td>\n   <td style=\"text-align:right;\"> 82 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n\n```{.r .cell-code}\n1 - sum(diag(tab)) / sum(tab) ## OOB misclassification error, no need for CV\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1565495\n```\n:::\n:::\n\n\n\n# Next time...\n\nBoosting\n",
    "supporting": [
      "19-bagging-and-rf_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../../site_libs/kePrint/kePrint.js\"></script>\n<link href=\"../../site_libs/lightable/lightable.css\" rel=\"stylesheet\" />\n"
      ],
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}