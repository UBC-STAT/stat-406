{
  "hash": "81d2686692c199cb2714ad2bef573cd8",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"16 Logistic regression\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 16 October 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Last time\n\n\n* We showed that with two classes, the [Bayes' classifier]{.secondary} is\n\n$$g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} > \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}$$\n\nwhere\n\n* $p_1(X) = \\P(X \\given Y=1)$\n* $p_0(X) = \\P(X \\given Y=0)$\n* $\\pi = \\P(Y=1)$\n\n. . .\n\n### This lecture\n\nHow do we estimate $p_1(X), p_0(X), \\pi$?\n\n## Warmup: estimating $\\pi = Pr(Y=1)$\n\nA good estimator:\n\n$$\n\\hat \\pi = \\frac{1}{n} \\sum_{i=1}^n 1_{y_i = 1}\n$$\n\n(I.e. count the number of $1$s in the training set)\n\nThis estimator is low bias/variance.\n\n. . .\n\n\\\n*As we will soon see, it turns out we won't have to use this estimator.*\n\n<!-- * We then looked at what happens if we **assume** $Pr(X \\given Y=y)$ is Normally distributed. -->\n\n<!-- We then used this distribution and the class prior $\\pi$ to find the __posterior__ $Pr(Y=1 \\given X=x)$. -->\n\n\n## Estimating $p_0$ and $p_1$ is much harder\n\n$\\P(X=x \\mid Y = 1)$ and $\\P(X=x \\mid Y = 0)$ are $p$-dimensional distributions.\\\n[Remember the *curse of dimensionality*?]{.small}\n\n\\\n[Can we simplify our estimation problem?]{.secondary}\n\n\n## Sidestepping the hard estimation problem\n\nRather than estimating $\\P(X=x \\mid Y = 1)$ and $\\P(X=x \\mid Y = 0)$,\nI claim we can instead estimate the simpler ratio\n\n$$\n\\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)}\n$$\nWhy?\n\n. . .\n\n\\\nRecall that $g_*(X)$ ony depends on the *ratio* $\\P(X \\mid Y = 1) / \\P(X \\mid Y = 0)$.\n\n. . .\n\n$$\n\\begin{align*}\n  \\frac{\\P(X=x \\mid Y = 1)}{\\P(X=x \\mid Y = 0)}\n  &=\n  \\frac{\n    \\tfrac{\\P(Y = 1 \\mid X=x) \\P(X=x)}{\\P(Y = 1)}\n  }{\n    \\tfrac{\\P(Y = 0 \\mid X=x) \\P(X=x)}{\\P(Y = 0)}\n  }\n  =\n  \\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)}\n  \\underbrace{\\left(\\frac{1-\\pi}{\\pi}\\right)}_{\\text{Easy to estimate with } \\hat \\pi}\n\\end{align*}\n$$\n\n## Direct model\n\n[As with regression, we'll start with a simple model.]{.small}\\\nAssume our data can be modelled by a distribution of the form\n\n$$\n\\log\\left( \\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)} \\right) = \\beta_0 + \\beta^\\top x\n$$\n\n[Why does it make sense to model the *log ratio* rather than the *ratio*?]{.secondary}\n\n. . .\n\n\nFrom this eq., we can recover an estimate of the ratio we need for the [Bayes classifier]{.secondary}:\n\n$$\n\\begin{align*}\n\\log\\left( \\frac{\\P(X=x \\mid Y = 1)}{\\P(X=x \\mid Y = 0)} \\right) \n&=\n\\log\\left( \\frac{\\tfrac{\\P(X=x)}{\\P(Y = 1)}}{\\tfrac{\\P(X=x)}{\\P(Y = 0)}}  \\right) +\n\\log\\left( \\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)} \\right)\n\\\\\n&= \\underbrace{\\left( \\tfrac{1 - \\pi}{\\pi} + \\beta_0 \\right)}_{\\beta_0'} + \\beta^\\top x\n\\end{align*}\n$$\n\n## Recovering class probabilities\n\n$$\n\\text{Our model:}\\qquad\n\\log\\left( \\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)} \\right) = \\beta_0 + \\beta^\\top x\n$$\n\n. . .\n\n\\\nWe know that $\\P(Y = 1 \\mid X=x) + \\P(Y = 0 \\mid X=x) = 1$. So...\n\n$$\n\\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)}\n=\n\\frac{\\P(Y = 1 \\mid X=x)}{1 - \\P(Y = 1 \\mid X=x)}\n=\n\\exp\\left( \\beta_0 + \\beta^\\top x \\right),\n$$\n\n---\n\n$$\n\\frac{\\P(Y = 1 \\mid X=x)}{\\P(Y = 0 \\mid X=x)}\n=\n\\frac{\\P(Y = 1 \\mid X=x)}{1 - \\P(Y = 1 \\mid X=x)}\n=\n\\exp\\left( \\beta_0 + \\beta^\\top x \\right),\n$$\n\n[After algebra...]{.small}\n$$\n\\begin{aligned}\n\\P(Y = 1 \\given X=x) &= \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}},\n\\\\\\\n\\P(Y = 0 | X=x) &= \\frac{1}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\n\\end{aligned}\n$$\n\nThis is logistic regression.\n\n\n## Logistic Regression\n\n$$\\P(Y = 1 \\given X=x) = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\n= h\\left( \\beta_0 + \\beta^\\top x \\right),$$\n\nwhere $h(z) = (1 + \\exp(-z))^{-1} =  \\exp(z) / (1+\\exp(z))$\nis the [logistic function]{.secondary}.\n\n\\\n\n::: flex\n::: w-60\n\n### The \"logistic\" function is nice.\n\n* It's symmetric: $1 - h(z) = h(-z)$\n\n* Has a nice derivative: $h'(z) = \\frac{\\exp(z)}{(1 + \\exp(z))^2} = h(z)(1-h(z))$.\n\n* It's the inverse of the \"log-odds\" (logit): $\\log(p / (1-p))$.\n\n:::\n::: w-35\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](16-logistic-regression_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n## Logistic regression is a Linear Classifier\n\n<!-- Like LDA, -->\nLogistic regression is a [linear classifier]{.secondary}\n\n<!-- The _logit_ (i.e.: log odds) transformation -->\n<!-- gives a linear decision boundary -->\n$$\\log\\left( \\frac{\\P(Y = 1 \\given X=x)}{\\P(Y = 0 \\given X=x) } \\right) = \\beta_0 + \\beta^{\\top} x$$\n\n* If the log-odds are $>0$, classify as 1\\\n  [($Y=1$ is more likely)]{.small}\n\n* If the log-odds are $<0$, classify as a 0\\\n  [($Y=0$ is more likely)]{.small}\n\n\\\nThe [decision boundary]{.secondary} is the hyperplane\n$\\{x : \\beta_0 + \\beta^{\\top} x = 0\\}$\n\n\n## Visualizing the classification boundary\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(mvtnorm)\nlibrary(MASS)\ngenerate_lda_2d <- function(\n    n, p = c(.5, .5),\n    mu = matrix(c(0, 0, 1, 1), 2),\n    Sigma = diag(2)) {\n  X <- rmvnorm(n, sigma = Sigma)\n  tibble(\n    y = which(rmultinom(n, 1, p) == 1, TRUE)[, 1],\n    x1 = X[, 1] + mu[1, y],\n    x2 = X[, 2] + mu[2, y]\n  )\n}\n\ndat1 <- generate_lda_2d(100, Sigma = .5 * diag(2))\nlogit <- glm(y ~ ., dat1 |> mutate(y = y - 1), family = \"binomial\")\n\ngr <- expand_grid(x1 = seq(-2.5, 3, length.out = 100), \n                  x2 = seq(-2.5, 3, length.out = 100))\npts <- predict(logit, gr)\ng0 <- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_steps2(n.breaks = 6, name = \"log odds\") \ng0\n```\n\n::: {.cell-output-display}\n![](16-logistic-regression_files/figure-revealjs/plot-d1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Linear classifier $\\ne$ linear smoother\n\n* While logistic regression produces linear decision boundaries, it is **not** a linear smoother\n\n* AIC/BIC/Cp work if you use the likelihood correctly and count degrees-of-freedom correctly\n\n  * $\\mathrm{df} = p + 1$ ([Why?]{.secondary})\n\n* Most people use CV\n\n\n## Logistic regression in R\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlogistic <- glm(y ~ ., dat, family = \"binomial\")\n```\n:::\n\n\n\nOr we can use lasso or ridge regression or a GAM as before\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlasso_logit <- cv.glmnet(x, y, family = \"binomial\")\nridge_logit <- cv.glmnet(x, y, alpha = 0, family = \"binomial\")\ngam_logit <- gam(y ~ s(x), data = dat, family = \"binomial\")\n```\n:::\n\n\n\n\n::: aside\nglm means [generalized linear model]{.secondary}\n:::\n\n\n<!-- ## Baby example (continued from last time) -->\n\n<!-- ```{r simple-lda, echo=FALSE} -->\n<!-- library(mvtnorm) -->\n<!-- library(MASS) -->\n<!-- generate_lda_2d <- function( -->\n<!--     n, p = c(.5, .5), -->\n<!--     mu = matrix(c(0, 0, 1, 1), 2), -->\n<!--     Sigma = diag(2)) { -->\n<!--   X <- rmvnorm(n, sigma = Sigma) -->\n<!--   tibble( -->\n<!--     y = which(rmultinom(n, 1, p) == 1, TRUE)[, 1], -->\n<!--     x1 = X[, 1] + mu[1, y], -->\n<!--     x2 = X[, 2] + mu[2, y] -->\n<!--   ) -->\n<!-- } -->\n\n<!-- dat1 <- generate_lda_2d(100, Sigma = .5 * diag(2)) -->\n<!-- logit <- glm(y ~ ., dat1 |> mutate(y = y - 1), family = \"binomial\") -->\n<!-- summary(logit) -->\n<!-- ``` -->\n\n\n\n<!-- ## Calculation -->\n\n<!-- While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA. -->\n\n\n<!-- Logistic regression for two classes simplifies to a likelihood: -->\n\n<!-- Write $p_i(\\beta) = \\P(Y_i = 1 | X = x_i,\\beta)$ -->\n\n<!-- * $P(Y_i = y_i \\given X = x_i, \\beta) = p_i^{y_i}(1-p_i)^{1-y_i}$ (...Bernoulli distribution) -->\n\n<!-- * $P(\\mathbf{Y} \\given \\mathbf{X}, \\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$.  -->\n\n\n<!-- ## Calculation -->\n\n\n<!-- Write $p_i(\\beta) = \\P(Y_i = 1 | X = x_i,\\beta)$ -->\n\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\ell(\\beta)  -->\n<!-- & = \\log \\left( \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i} \\right)\\\\ -->\n<!-- &=\\sum_{i=1}^n \\left( y_i\\log(p_i(\\beta)) + (1-y_i)\\log(1-p_i(\\beta))\\right) \\\\ -->\n<!-- & =  -->\n<!-- \\sum_{i=1}^n \\left( y_i\\log(e^{\\beta^{\\top}x_i}/(1+e^{\\beta^{\\top}x_i})) - (1-y_i)\\log(1+e^{\\beta^{\\top}x_i})\\right) \\\\ -->\n<!-- & =  -->\n<!-- \\sum_{i=1}^n \\left( y_i\\beta^{\\top}x_i -\\log(1 + e^{\\beta^{\\top} x_i})\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- This gets optimized via Newton-Raphson updates and iteratively reweighed -->\n<!-- least squares. -->\n\n\n<!-- ## IRWLS for logistic regression (skip for now) -->\n\n<!-- (This is preparation for Neural Networks.) -->\n\n<!-- ```{r} -->\n<!-- logit_irwls <- function(y, x, maxit = 100, tol = 1e-6) { -->\n<!--   p <- ncol(x) -->\n<!--   beta <- double(p) # initialize coefficients -->\n<!--   beta0 <- 0 -->\n<!--   conv <- FALSE # hasn't converged -->\n<!--   iter <- 1 # first iteration -->\n<!--   while (!conv && (iter < maxit)) { # check loops -->\n<!--     iter <- iter + 1 # update first thing (so as not to forget) -->\n<!--     eta <- beta0 + x %*% beta -->\n<!--     mu <- 1 / (1 + exp(-eta)) -->\n<!--     gp <- 1 / (mu * (1 - mu)) # inverse of derivative of logistic -->\n<!--     z <- eta + (y - mu) * gp # effective transformed response -->\n<!--     beta_new <- coef(lm(z ~ x, weights = 1 / gp)) # do Weighted Least Squares -->\n<!--     conv <- mean(abs(c(beta0, beta) - betaNew)) < tol # check if the betas are \"moving\" -->\n<!--     beta0 <- betaNew[1] # update betas -->\n<!--     beta <- betaNew[-1] -->\n<!--   } -->\n<!--   return(c(beta0, beta)) -->\n<!-- } -->\n<!-- ``` -->\n\n\n<!-- ## Comparing LDA and Logistic regression -->\n\n<!-- Both decision boundaries are linear in $x$:   -->\n\n<!-- - LDA $\\longrightarrow \\alpha_0 + \\alpha_1^\\top x$  -->\n<!-- - Logit $\\longrightarrow \\beta_0 + \\beta_1^\\top x$. -->\n\n<!-- But the parameters are estimated differently. -->\n\n<!-- ## Comparing LDA and Logistic regression -->\n\n<!-- Examine the joint distribution of $(X,\\ Y)$ [(not the posterior)]{.f3}:   -->\n\n<!-- - LDA: $f(X_i,\\ Y_i) = \\underbrace{ f(X_i \\given Y_i)}_{\\textrm{Gaussian}}\\underbrace{ f(Y_i)}_{\\textrm{Bernoulli}}$ -->\n<!-- - Logistic Regression: $f(X_i,Y_i) = \\underbrace{ f(Y_i\\given X_i)}_{\\textrm{Logistic}}\\underbrace{ f(X_i)}_{\\textrm{Ignored}}$ -->\n\n<!-- * LDA estimates the joint, but Logistic estimates only the conditional (posterior) distribution. [But this is really all we need.]{.hand} -->\n\n<!-- * So logistic requires fewer assumptions. -->\n\n<!-- * But if the two classes are perfectly separable, logistic crashes (and the MLE is undefined, too many solutions) -->\n\n<!-- * LDA \"works\" even if the conditional isn't normal, but works very poorly if any X is qualitative -->\n\n\n<!-- ## Comparing with QDA (2 classes) -->\n\n\n<!-- * Recall: this gives a \"quadratic\" decision boundary (it's a curve). -->\n\n<!-- * If we have $p$ columns in $X$ -->\n<!--     - Logistic estimates $p+1$ parameters -->\n<!--     - LDA estimates $2p + p(p+1)/2 + 1$ -->\n<!--     - QDA estimates $2p + p(p+1) + 1$ -->\n\n<!-- * If $p=50$, -->\n<!--     - Logistic: 51 -->\n<!--     - LDA: 1376 -->\n<!--     - QDA: 2651 -->\n\n<!-- * QDA doesn't get used much: there are better nonlinear versions with way fewer parameters -->\n\n<!-- ## Bad parameter counting -->\n\n<!-- I've motivated LDA as needing $\\Sigma$, $\\pi$ and $\\mu_0$, $\\mu_1$ -->\n\n<!-- In fact, we don't _need_ all of this to get the decision boundary. -->\n\n<!-- So the \"degrees of freedom\" is much lower if we only want the _classes_ and not -->\n<!-- the _probabilities_. -->\n\n<!-- The decision boundary only really depends on -->\n\n<!-- * $\\Sigma^{-1}(\\mu_1-\\mu_0)$  -->\n<!-- * $(\\mu_1+\\mu_0)$,  -->\n<!-- * so appropriate algorithms estimate $<2p$ parameters. -->\n\n\n## Estimating $\\beta_0$, $\\beta$\n\nFor regression...\n\n$$\n\\text{Model:} \\qquad\n\\P(Y=y \\mid X=x) = \\mathcal N( y; \\:\\: \\beta^\\top x, \\:\\:\\sigma^2)\n$$\n\n... recall that we motivated OLS with\nthe [principle of maximum likelihood]{.secondary}\n\n$$\n\\begin{align*}\n\\hat \\beta_\\mathrm{OLS}\n&= \\argmax_{\\beta} \\prod_{i=1}^n \\P(Y_i = y_i \\mid X_i = x_i)\n\\\\\n&= \\argmin_{\\beta} \\sum_{i=1}^n -\\log\\P(Y_i = y_i \\mid X_i = x_i)\n\\\\\n\\\\\n&= \\ldots (\\text{because regression is nice})\n\\\\\n&= \\textstyle \\left( \\sum_{i=1}^n x_i x_i^\\top \\right)^{-1}\\left( \\sum_{i=1}^n y_i x_i \\right)\n\\end{align*}\n$$\n\n## Estimating $\\beta_0$, $\\beta$\n\nFor classification with logistic regression...\n\n$$\n\\begin{align*}\n\\text{Model:} &\\qquad\n\\tfrac{\\P(Y=1 \\mid X=x)}{\\P(Y=0 \\mid X=x)} = \\exp\\left( \\beta_0 +\\beta^\\top x \\right)\n\\\\\n\\text{Or alternatively:} &\\qquad \\P(Y=1 \\mid X=x) = h\\left(\\beta_0 + \\beta^\\top x \\right)\n\\\\\n&\\qquad \\P(Y=0 \\mid X=x) = h\\left(-(\\beta_0 + \\beta^\\top x)\\right)\n\\end{align*}\n$$\n... we can also apply\nthe [principle of maximum likelihood]{.secondary}\n\n$$\n\\begin{align*}\n\\hat \\beta_{0}, \\hat \\beta\n&= \\argmax_{\\beta_0, \\beta} \\prod_{i=1}^n \\P(Y_i \\mid X_i)\n\\\\\n&= \\argmin_{\\beta_0,\\beta} \\sum_{i=1}^n -\\log\\P(Y_i \\mid X_i)\n\\end{align*}\n$$\n\n. . .\n\nUnfortunately that's as far as we can get with algebra alone.\n\n<!-- ## Calculation -->\n\n<!-- While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA. -->\n\n\n<!-- Logistic regression for two classes simplifies to a likelihood: -->\n\n<!-- Write $p_i(\\beta) = \\P(Y_i = 1 | X = x_i,\\beta)$ -->\n\n<!-- * $P(Y_i = y_i \\given X = x_i, \\beta) = p_i^{y_i}(1-p_i)^{1-y_i}$ (...Bernoulli distribution) -->\n\n<!-- * $P(\\mathbf{Y} \\given \\mathbf{X}, \\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$.  -->\n\n# Next time:\n\nThe workhorse algorithm for obtaining $\\hat \\beta_0$, $\\hat \\beta$\n",
    "supporting": [
      "16-logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}