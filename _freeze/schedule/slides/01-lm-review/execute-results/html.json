{
  "hash": "999eea0f46ec9e5756cc9ede30dffae4",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"01 Linear model review\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 11 September 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## The normal linear model\n\nAssume that \n\n$$\ny_i = x_i^\\top \\beta + \\epsilon_i.\n$$\n\n::: {.incremental}\n\n1. What variables are fixed, what are parameters, and what are random?\n2. What is the distribution of $\\epsilon_i$?\n3. What is the mean of $y_i$?\n4. What is the notation $\\mathbf{X}$ or $\\mathbf{y}$?\n\n:::\n\n\n## Drawing a sample\n\n$$\ny_i = x_i^\\top \\beta + \\epsilon_i.\n$$\n\nHow would I **create** data from this model (draw a sample)?\n\n. . .\n\nSet up constants\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\np <- 3  # number of covariates\nn <- 100  # number of data points\nsigma <- 2  # stddev of \\epsilon\n```\n:::\n\n\n\n. . .\n\nCreate the data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nepsilon <- rnorm(n, sd = sigma) # this is random\nX <- matrix(runif(n * p), n, p) # treat this as fixed, but I need numbers\nbeta <- (p + 1):1 # parameter, also fixed, but I again need numbers\nY <- cbind(1, X) %*% beta + epsilon # epsilon is random, so this is\n## Equiv: Y <- beta[1] + X %*% beta[-1] + epsilon\n```\n:::\n\n\n\n\n## How do we estimate beta?\n\n1. Guess.\n2. Ordinary least squares (OLS).\n3. Maximum likelihood.\n4. Do something more creative.\n\n\n## Method 2. OLS\n\n\n::: flex\n\n::: w-50\n\n- I want to find an estimator $\\widehat\\beta$ that makes small errors on my data.\n- I measure errors with the squared difference between predictions $\\mathbf{X}\\widehat\\beta$ and the responses $\\mathbf{y}$.\n- (Don't care if the differences are positive or negative)\n\n<!--\n$$\\sum_{i=1}^n \\left\\lvert y_i - x_i^\\top \\widehat\\beta \\right\\rvert.$$\n\nThis is hard to minimize (what is the derivative of $|\\cdot|$?)\n\n-->\n\n$$\\mathrm{Error} = \\sum_{i=1}^n ( y_i - x_i^\\top \\widehat\\beta )^2.$$\n\n:::\n\n::: w-50\n\n![](gfx/islr3_4.png){width=70%}\n\n:::\n:::\n\n. . .\n\n[Why squared errors? $( y_i - x_i^\\top \\widehat\\beta )^2$ \\\nWhy not absolute errors $\\left\\lvert y_i - x_i^\\top \\widehat\\beta \\right\\rvert$?]{.secondary}\n\n## Method 2. OLS solution\n\nWe write this as\n\n$$\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.$$\n\n\n> Find the $\\beta$ which minimizes the sum of squared errors.\n\n. . .\n\nNote that this is the same as \n\n$$\\widehat\\beta = \\argmin_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.$$\n\n> Find the beta which minimizes the mean squared error.\n\n\n\n## Method 2. Ok, do it {.smaller}\n\nWe differentiate and set to zero\n\n\\begin{aligned}\n& \\frac{\\partial}{\\partial \\beta} \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2\\\\\n&= -\\frac{2}{n}\\sum_{i=1}^n x_i (y_i - x_i^\\top\\beta)\\\\\n&= \\frac{2}{n}\\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n0 &\\equiv \\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n&\\Rightarrow \\sum_{i=1}^n x_i x_i^\\top \\beta = \\sum_{i=1}^n x_i y_i\\\\\n\\\\\n&\\Rightarrow \\beta = \\left(\\sum_{i=1}^n x_i x_i^\\top\\right)^{-1}\\sum_{i=1}^n x_i y_i\n\\end{aligned}\n\n\n\n\n## In matrix notation...\n\n...this is \n\n\n$$\\hat\\beta = ( \\mathbf{X}^\\top  \\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}.$$\n\n\nThe $\\beta$ which \"minimizes the sum of squared errors\"\n\n\nAKA, the SSE.\n\n\n\n## Method 3: maximum likelihood\n\nMethod 2 didn't use anything about the distribution of $\\epsilon$.\n\nBut if we know that $\\epsilon$ has a normal distribution, we can write down the joint distribution\nof $\\mathbf{y}=(y_1,\\ldots,y_n)^\\top = \\mathbf{X}\\beta + (\\epsilon_1, \\ldots, \\epsilon_n)^\\top$:\n\n\n$$\n\\epsilon_i = \\left( y_i - x_i^\\top \\beta \\right) \\sim \\mathcal{N}(0, \\sigma^2)\n$$\n\n. . .\n\nSo the probability density of $Y = \\mathbf y$ is...\n\n\n\\begin{aligned}\nf_Y(\\mathbf{y} ; \\beta) &= \\prod_{i=1}^n f_{y_i ; \\beta}(y_i) \\\\\n  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (y_i-x_i^\\top \\beta)^2\\right) \\\\\n  &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\end{aligned}\n\n\n\n## The likelihood function\n\n$$\nf_Y(\\mathbf{y} ; \\beta) = \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n$$\n\nIn probability courses, we think of $f_Y$ as a function of $\\mathbf{y}$ with $\\beta$ fixed:\n\n1. If we integrate over $\\mathbf{y}$, it's $1$.\n2. If we want the probability of $(a,b)$, we integrate from $a$ to $b$.\n3. etc.\n\n\n\n## The likelihood function\n\n...instead, think of it as a function of $\\beta$.\n\nWe call this \"the likelihood\" of beta: $\\mathcal{L}(\\beta)$.\n\nGiven some data, we can [evaluate]{.secondary} the likelihood for any value of $\\beta$ (assuming $\\sigma$ is known).\n\nIt won't integrate to 1 over $\\beta$.\n\nBut we can maximize it with respect to $\\beta$.\n\n<!--\n(the second derivative wrt $\\beta$ is everywhere negative).\n-->\n\n## So let's maximize {.smaller}\n\nThe derivative of $\\mathcal L(\\beta)$ is ugly...\n\n$$\n\\frac{\\partial}{\\partial \\beta} \\left[ \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right) \\right] = \\text{ðŸ˜”}\n$$\n\nI claim we can maximize $\\mathcal L(\\beta)$ over $\\beta$ by instead maximizing the simpler function $\\ell(\\beta) = \\log \\mathcal L(\\beta)$. (Why?)\n\n. . .\n\n$$\n\\hat\\beta = \\argmax_\\beta \\ell(\\beta) = \\argmax_\\beta \\left[ -\\frac{n}{2}\\log (2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2 \\right]\n$$\n\n. . .\n\nWe can also throw out the constants. (Why?)\n\n. . .\n\nSo...\n\n$$\\widehat\\beta = \\argmax_\\beta -\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2 = \\argmin_\\beta \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2$$\n\n\nThe same as before!\n\n\n# Demo\n(Not in real time)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}