{
  "hash": "8a850787b40b92ae82565c64d26caf67",
  "result": {
    "markdown": "---\nlecture: \"24 Principal components, introduction\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 24 November 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n\n## Unsupervised learning\n\nIn Machine Learning, rather than calling $\\y$ the [response]{.hand}, people call it the [supervisor]{.hand}\n\nSo [unsupervised]{.hand} learning means [learning without]{.hand} $\\y$\n\nThe only data you get are the [features]{.hand} $\\{x_1,\\ldots,x_n\\}$.\n\nThis type of analysis is more often [exploratory]{.hand}\n\nWe're not necessarily using this for prediction (but we could)\n\nSo now, we get $\\X$\n\nThe two main activities are [representation learning]{.secondary} and [clustering]{.secondary}\n\n\n## Representation learning\n\nRepresentation learning is the idea that performance of ML methods is\nhighly dependent on the choice of representation\n\n\nFor this reason, much of ML is geared towards transforming the data into\nthe relevant features and then using these as inputs\n\n\nThis idea is as old as statistics itself, really,\n\nHowever, the idea is constantly revisited in a variety of fields and\ncontexts\n\n\nCommonly, these learned representations capture low-level information\nlike overall shapes\n\n\n\nIt is possible to quantify this intuition for PCA at least\n\n. . .\n\nGoal\n: Transform $\\mathbf{X}\\in \\R^{n\\times p}$ into $\\mathbf{Z} \\in \\R^{n \\times ?}$\n\n?-dimension can be bigger (feature creation) or smaller (dimension reduction) than $p$\n\n\n\n## You've done this already!\n\n* You added transformations as predictors in regression\n\n* You \"expanded\" $\\mathbf{X}$ using a basis $\\Phi$ (polynomials, splines, etc.)\n\n* You used Neural Nets to do a \"feature map\"\n\n. . .\n\nThis is the same, just no $Y$ around\n\n\n## PCA\n\nPrincipal components analysis (PCA) is an (unsupervised) dimension\nreduction technique\n\n\nIt solves various equivalent optimization problems\n\n(Maximize variance, minimize $\\ell_2$ distortions, find closest subspace of a given rank, $\\ldots$)\n\nAt its core, we are finding linear combinations of the original\n(centered) covariates $$z_{ij} = \\alpha_j^{\\top} x_i$$\n\n\nThis is expressed via the SVD: $\\X  = \\U\\D\\V^{\\top}$.\n\n. . .\n\n\n::: {.callout-important}\nWe assume throughout that $\\X - \\mathbf{11^\\top}\\overline{x} = 0$ (we center the columns)\n:::\n\nThen our new features are\n\n$$\\mathbf{Z} = \\X \\V = \\U\\D$$\n\n\n## Short SVD aside (reminder from Ridge Regression)\n\n* Any $n\\times p$ matrix can be decomposed into $\\mathbf{UDV}^\\top$.\n\n* These have properties:\n\n1. $\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_n$\n2. $\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_p$\n3. $\\mathbf{D}$ is diagonal (0 off the diagonal)\n\n\nAlmost all the methods for we'll talk about for representation learning use the SVD of some matrix.\n\n\n\n## Why? {background-color=\"#97D4E9\" .smaller}\n\n1. Given $\\X$, find a projection $\\mathbf{P}$ onto $\\R^M$ with $M \\leq p$ \nthat minimizes the reconstruction error\n$$\n\\begin{aligned}\n\\min_{\\mathbf{P}} &\\,\\, \\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P} \\rVert^2_F \\,\\,\\, \\textrm{(sum all the elements)}\\\\\n\\textrm{subject to} &\\,\\, \\textrm{rank}(\\mathbf{P}) = M,\\, \\mathbf{P} = \\mathbf{P}^T,\\, \\mathbf{P} = \\mathbf{P}^2\n\\end{aligned}\n$$\nThe conditions ensure that $\\mathbf{P}$ is a projection matrix onto $M$ dimensions.\n\n2. Maximize the variance explained by an orthogonal transformation $\\mathbf{A} \\in \\R^{p\\times M}$\n$$\n\\begin{aligned}\n\\max_{\\mathbf{A}} &\\,\\, \\textrm{trace}\\left(\\frac{1}{n}\\mathbf{A}^\\top \\X^\\top \\X \\mathbf{A}\\right)\\\\\n\\textrm{subject to} &\\,\\, \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}_M\n\\end{aligned}\n$$\n\n* In case one, the minimizer is $\\mathbf{P} = \\mathbf{V}_M\\mathbf{V}_M^\\top$\n* In case two, the maximizer is $\\mathbf{A} = \\mathbf{V}_M$.\n\n\n\n## Lower dimensional embeddings\n\nSuppose we have predictors $\\x_1$ and $\\x_2$\n\n-   We more faithfully preserve the structure of this data by keeping\n    $\\x_1$ and setting $\\x_2$ to zero than the opposite\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Lower dimensional embeddings\n\nAn important feature of the previous example is that $\\x_1$ and $\\x_2$\naren't correlated\n\nWhat if they are?\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\nWe lose a lot of structure by setting either $\\x_1$ or $\\x_2$ to zero\n\n\n\n## Lower dimensional embeddings\n\n\nThe only difference is the first is a rotation of the second\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## PCA\n\nIf we knew how to rotate our data, then we could more \neasily retain the structure.\n\n[PCA]{.secondary} gives us exactly this rotation\n\n1. Center (+scale?) the data matrix $\\X$\n2. Compute the SVD of $\\X = \\U\\D \\V^\\top$ or $\\X\\X^\\top = \\U\\D^2\\U^\\top$ or $\\X^\\top \\X = \\V\\D^2 \\V^\\top$\n3. Return $\\U_M\\D_M$, where $\\D_M$ is the largest $M$\n    singular values of $\\X$\n\n\n## PCA\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ns <- svd(X)\ntib <- rbind(X, s$u %*% diag(s$d), s$u %*% diag(c(s$d[1], 0)))\ntib <- tibble(\n  x1 = tib[, 1], x2 = tib[, 2],\n  name = rep(1:3, each = 20)\n)\nplotter <- function(set = 1, main = \"original\") {\n  tib |>\n    filter(name == set) |>\n    ggplot(aes(x1, x2)) +\n    geom_point(colour = blue) +\n    coord_cartesian(c(-2, 2), c(-2, 2)) +\n    theme(legend.title = element_blank(), legend.position = \"bottom\") +\n    ggtitle(main)\n}\ncowplot::plot_grid(\n  plotter() + labs(x = bquote(x[1]), y = bquote(x[2])),\n  plotter(2, \"rotated\") +\n    labs(x = bquote((UD)[1] == (XV)[1]), y = bquote((UD)[2] == (XV)[2])),\n  plotter(3, \"rotated and projected\") +\n    labs(x = bquote(U[1] ~ D[1] == (XV)[1]), y = bquote(U[2] ~ D[2] %==% 0)),\n  nrow = 1\n)\n```\n\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n## PCA on some pop music data\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmusic <- Stat406::popmusic_train\nstr(music)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntibble [1,269 Ã— 15] (S3: tbl_df/tbl/data.frame)\n $ artist          : Factor w/ 3 levels \"Radiohead\",\"Taylor Swift\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ danceability    : num [1:1269] 0.781 0.627 0.516 0.629 0.686 0.522 0.31 0.705 0.553 0.419 ...\n $ energy          : num [1:1269] 0.357 0.266 0.917 0.757 0.705 0.691 0.374 0.621 0.604 0.908 ...\n $ key             : int [1:1269] 0 9 11 1 9 2 6 2 1 9 ...\n $ loudness        : num [1:1269] -16.39 -15.43 -3.19 -8.37 -10.82 ...\n $ mode            : int [1:1269] 1 1 0 0 1 1 1 1 0 1 ...\n $ speechiness     : num [1:1269] 0.912 0.929 0.0827 0.0512 0.249 0.0307 0.0275 0.0334 0.0258 0.0651 ...\n $ acousticness    : num [1:1269] 0.717 0.796 0.0139 0.00384 0.832 0.00609 0.761 0.101 0.202 0.00048 ...\n $ instrumentalness: num [1:1269] 0.00 0.00 6.37e-06 7.45e-01 4.55e-06 0.00 2.46e-05 4.30e-06 0.00 0.00 ...\n $ liveness        : num [1:1269] 0.185 0.292 0.36 0.0864 0.134 0.249 0.11 0.147 0.125 0.815 ...\n $ valence         : num [1:1269] 0.645 0.646 0.635 0.623 0.919 0.651 0.16 0.395 0.186 0.472 ...\n $ tempo           : num [1:1269] 118.3 79.3 145.8 157 151.9 ...\n $ time_signature  : int [1:1269] 4 4 4 4 5 4 4 4 4 4 ...\n $ duration_ms     : int [1:1269] 107133 89648 217160 201853 180653 201106 285640 240773 302266 290520 ...\n $ explicit        : logi [1:1269] FALSE FALSE FALSE FALSE FALSE FALSE ...\n```\n:::\n\n```{.r .cell-code}\nX <- music |> select(danceability:energy, loudness, speechiness:valence)\npca <- prcomp(X, scale = TRUE) ## DON'T USE princomp()\n```\n:::\n\n\n## PCA on some pop music data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nproj_pca <- predict(pca)[,1:2] |>\n  as_tibble() |>\n  mutate(artist = music$artist)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Things to look at\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npca$rotation[, 1:2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                         PC1          PC2\ndanceability     -0.02811038  0.577020027\nenergy           -0.56077454 -0.001137424\nloudness         -0.53893087  0.085912854\nspeechiness       0.30125038  0.431188730\nacousticness      0.51172138  0.082108326\ninstrumentalness  0.01374425 -0.370058813\nliveness         -0.02282669 -0.194947054\nvalence          -0.20242211  0.540418541\n```\n:::\n\n```{.r .cell-code}\npca$sdev\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.6233469 1.3466829 1.0690017 0.9510417 0.7638669 0.6737958 0.5495869\n[8] 0.4054698\n```\n:::\n\n```{.r .cell-code}\npca$sdev^2 / sum(pca$sdev^2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.32940690 0.22669435 0.14284558 0.11306004 0.07293658 0.05675010 0.03775572\n[8] 0.02055072\n```\n:::\n:::\n\n\n## Plotting the weights\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npca$rotation[, 1:2] |>\n  as_tibble() |>\n  mutate(feature = names(X)) |>\n  pivot_longer(-feature) |>\n  ggplot(aes(value, feature, fill = feature)) +\n  facet_wrap(~name) +\n  geom_col() +\n  theme(legend.position = \"none\", axis.title = element_blank()) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  geom_vline(xintercept = 0)\n```\n\n::: {.cell-output-display}\n![](24-pca-intro_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n# Next time...\n\nWhen does PCA work?\n",
    "supporting": [
      "24-pca-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}