{
  "hash": "a1c9bf4a46ec2b50ff12f822e6dd836b",
  "result": {
    "markdown": "---\nlecture: \"28 Hierarchical clustering\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 30 November 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## From $K$-means to hierarchical clustering\n\n\n::: flex\n::: w-50\n\n[K-means]{.secondary}\n\n\n1.  It fits exactly $K$ clusters.\n\n2.  Final clustering assignments depend on the chosen initial cluster\n    centers.\n\n[Hierarchical clustering]{.secondary}\n\n1.  No need to choose the number of clusters before hand.\n\n2.  There is no random component (nor choice of starting point).\n\n\nThere is a catch: we need to choose a way to measure the distance\nbetween clusters, called the [linkage]{.secondary}.\n:::\n\n::: w-50\n\nSame data as the K-means example:\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\n# same data as K-means \"Dumb example\"\nheatmaply::ggheatmap(\n  as.matrix(dist(rbind(X1, X2, X3))),\n  showticklabels = c(FALSE, FALSE), hide_colorbar = TRUE\n)\n```\n\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n:::\n:::\n\n## Hierarchical clustering\n\n::: flex\n::: w-50\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n:::\n::: w-50\n* Given the linkage, hierarchical clustering produces a sequence of\nclustering assignments.\n* At one end, all points are in their [own]{.secondary}\ncluster.\n* At the other, all points are in [one]{.secondary}\ncluster.\n* In the middle, there are [nontrivial]{.secondary}\nsolutions.\n:::\n:::\n\n## Agglomeration\n\n\n::: flex\n::: w-50\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n* Given these data points, an agglomerative algorithm chooses a cluster sequence by combining the points into groups.\n* We can also represent the sequence of clustering assignments as a\ndendrogram\n* Cutting the dendrogram horizontally partitions the data points\ninto clusters\n\n:::\n::: w-50\n* Notation: Define $x_1,\\ldots, x_n$ to be the data\n\n* Let the dissimiliarities be $d_{ij}$ between\neach pair $x_i, x_j$\n\n* At any level, clustering assignments can be expressed by sets\n$G = \\{ i_1, i_2, \\ldots, i_r\\}$ giving the\nindicies of points in this group. Define\n$|G|$ to be the size of $G$.\n\n\nLinkage\n: The function $d(G,H)$ that takes\ntwo groups $G,\\ H$ and returns the linkage\ndistance between them.\n\n:::\n:::\n\n## Agglomerative clustering, given the linkage\n\n\n1. Start with each point in its own group\n2. Until there is only one cluster, repeatedly merge the two groups\n$G,H$ that minimize $d(G,H)$.\n\n\n::: callout-important\n$d$ measures the distance between GROUPS.\n:::\n\n\n## Single linkage\n\nIn [single linkage]{.secondary} (a.k.a nearest-neighbor\nlinkage), the linkage distance between $G,\\ H$ is the smallest\ndissimilarity between two points in different groups:\n$$d_{\\textrm{single}}(G,H) = \\min_{i \\in G, \\, j \\in H} d_{ij}$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Complete linkage\n\nIn [complete linkage]{.secondary} (i.e. farthest-neighbor\nlinkage), linkage distance between $G,H$ is the\nlargest dissimilarity between two points in\ndifferent clusters:\n$$d_{\\textrm{complete}}(G,H) = \\max_{i \\in G,\\, j \\in H} d_{ij}.$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Average linkage\n\nIn [average linkage]{.secondary}, the linkage distance\nbetween $G,H$ is the average dissimilarity\nover all points in different clusters:\n$$d_{\\textrm{average}}(G,H) = \\frac{1}{|G| \\cdot |H| }\\sum_{i \\in G, \\,j \\in H} d_{ij}.$$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Common properties\n\n[Single]{.secondary}, [complete]{.secondary}, and [average]{.secondary} linkage share the following:\n\n-   They all operate on the dissimilarities $d_{ij}$. \n    \n    This means that\n    the points we are clustering can be quite general (number of\n    mutations on a genome, polygons, faces, whatever).\n\n-   Running agglomerative clustering with any of these linkages produces\n    a dendrogram with no inversions\n\n\n- \"No inversions\" means that the linkage distance between merged clusters\nonly [increases]{.secondary} as we run the algorithm.\n\n\nIn other words, we can draw a proper dendrogram, where the height of a\nparent is always higher than the height of either daughter.\n\n(We'll return to this again shortly)\n\n\n\n## Centroid linkage\n\n[Centroid linkage]{.secondary} is \nrelatively new. We need\n$x_i \\in \\mathbb{R}^p$.\n\n\n\n$\\overline{x}_G$ and $\\overline{x}_H$ are group averages\n\n$d_{\\textrm{centroid}} = ||\\overline{x}_G - \\overline{x}_H||_2^2$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Centroid linkage\n\n::: flex\n::: w-50\n\nCentroid linkage is\n\n-   ... quite intuitive\n\n-   ... nicely analogous to $K$-means.\n\n-   ... very related to average linkage (and much, much faster)\n\nHowever, it may introduce inversions.\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-10-1.svg){fig-align='center'}\n:::\n:::\n\n:::\n\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntt <- seq(0, 2 * pi, len = 50)\ntt2 <- seq(0, 2 * pi, len = 75)\nc1 <- tibble(x = cos(tt), y = sin(tt))\nc2 <- tibble(x = 1.5 * cos(tt2), y = 1.5 * sin(tt2))\ncircles <- bind_rows(c1, c2)\ndi <- dist(circles[, 1:2])\nhc <- hclust(di, method = \"centroid\")\npar(mar = c(.1, 5, 3, .1))\nplot(hc, xlab = \"\")\n```\n\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n## Shortcomings of some linkages\n\nSingle\n: 👎 chaining --- a single pair of close points merges two clusters.  $\\Rightarrow$ clusters can be too spread out, not compact\n\nComplete linkage\n: 👎 crowding --- a point can be closer to points in other clusters than to points in its own cluster.$\\Rightarrow$ clusters are compact, not far enough apart.\n\nAverage linkage\n: tries to strike a balance these  \n: 👎 Unclear what properties the resulting clusters have when we cut an average linkage tree.  \n: 👎 Results change with a monotone increasing transformation of the dissimilarities   \n\nCentroid linkage\n: 👎 same monotonicity problem  \n: 👎 and inversions\n    \nAll linkages\n: ⁇ where do we cut?\n\n\n## Distances\n\nNote how all the methods depend on the distance function\n\nCan do lots of things besides Euclidean\n\nThis is very important\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](28-hclust_files/figure-revealjs/unnamed-chunk-12-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# Next time...\n\n\n\nNo more slides.  All done. \n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:6em;width:6em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#2c365e;overflow:visible;position:relative;\"><path d=\"M256 48a208 208 0 1 1 0 416 208 208 0 1 1 0-416zm0 464A256 256 0 1 0 256 0a256 256 0 1 0 0 512zM183.2 132.6c-1.3-2.8-4.1-4.6-7.2-4.6s-5.9 1.8-7.2 4.6l-16.6 34.7-38.1 5c-3.1 .4-5.6 2.5-6.6 5.5s-.1 6.2 2.1 8.3l27.9 26.5-7 37.8c-.6 3 .7 6.1 3.2 7.9s5.8 2 8.5 .6L176 240.5l33.8 18.3c2.7 1.5 6 1.3 8.5-.6s3.7-4.9 3.2-7.9l-7-37.8L242.4 186c2.2-2.1 3.1-5.3 2.1-8.3s-3.5-5.1-6.6-5.5l-38.1-5-16.6-34.7zm160 0c-1.3-2.8-4.1-4.6-7.2-4.6s-5.9 1.8-7.2 4.6l-16.6 34.7-38.1 5c-3.1 .4-5.6 2.5-6.6 5.5s-.1 6.2 2.1 8.3l27.9 26.5-7 37.8c-.6 3 .7 6.1 3.2 7.9s5.8 2 8.5 .6L336 240.5l33.8 18.3c2.7 1.5 6 1.3 8.5-.6s3.7-4.9 3.2-7.9l-7-37.8L402.4 186c2.2-2.1 3.1-5.3 2.1-8.3s-3.5-5.1-6.6-5.5l-38.1-5-16.6-34.7zm6.3 175.8c-28.9 6.8-60.5 10.5-93.6 10.5s-64.7-3.7-93.6-10.5c-18.7-4.4-35.9 12-25.5 28.1c24.6 38.1 68.7 63.5 119.1 63.5s94.5-25.4 119.1-63.5c10.4-16.1-6.8-32.5-25.5-28.1z\"/></svg>`{=html}\n\n. . .\n\nFINAL EXAM!! \n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:6em;width:5.25em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#a8201a;overflow:visible;position:relative;\"><path d=\"M368 128c0 44.4-25.4 83.5-64 106.4V256c0 17.7-14.3 32-32 32H176c-17.7 0-32-14.3-32-32V234.4c-38.6-23-64-62.1-64-106.4C80 57.3 144.5 0 224 0s144 57.3 144 128zM168 176a32 32 0 1 0 0-64 32 32 0 1 0 0 64zm144-32a32 32 0 1 0 -64 0 32 32 0 1 0 64 0zM3.4 273.7c7.9-15.8 27.1-22.2 42.9-14.3L224 348.2l177.7-88.8c15.8-7.9 35-1.5 42.9 14.3s1.5 35-14.3 42.9L295.6 384l134.8 67.4c15.8 7.9 22.2 27.1 14.3 42.9s-27.1 22.2-42.9 14.3L224 419.8 46.3 508.6c-15.8 7.9-35 1.5-42.9-14.3s-1.5-35 14.3-42.9L152.4 384 17.7 316.6C1.9 308.7-4.5 289.5 3.4 273.7z\"/></svg>`{=html}\n",
    "supporting": [
      "28-hclust_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}