{
  "hash": "2e2d3616d30d48680e50671243c1225e",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"00 Gradient descent\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 21 October 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Motivation: maximum likelihood estimation as optimization\n\nBy the principle of maximum likelihood, we have that\n\n$$\n\\begin{align*}\n\\hat \\beta\n&= \\argmax_{\\beta} \\prod_{i=1}^n \\P(Y_i \\mid X_i)\n\\\\\n&= \\argmin_{\\beta} \\sum_{i=1}^n -\\log\\P(Y_i \\mid X_i)\n\\end{align*}\n$$\n\nUnder the model we use for logistic regression...\n$$\n\\begin{gathered}\n\\P(Y=1 \\mid X=x) = h(\\beta^\\top x), \\qquad \\P(Y=0 \\mid X=x) = h(-\\beta^\\top x),\n\\\\\nh(z) = \\tfrac{1}{1-e^{-z}}\n\\end{gathered}\n$$\n\n... we can't simply find the argmin with algebra.\n\n## Gradient descent: the workhorse optimization algorithm\n\nWe'll see \"gradient descent\" a few times: \n\n1. solves logistic regression\n1. gradient boosting\n1. Neural networks\n\nThis seems like a good time to explain it.\n\nSo what is it and how does it work?\n\n\n## Very basic example\n\n::: flex\n::: w-65\n\nSuppose I want to minimize $f(x)=(x-6)^2$ numerically.\n\nI start at a point (say $x_1=23$)\n\nI want to \"go\" in the negative direction of the gradient.\n\nThe gradient (at $x_1=23$) is  $f'(23)=2(23-6)=34$.\n\nMove current value toward current value - 34.\n\n$x_2 = x_1 - \\gamma 34$, for $\\gamma$ small.\n\nIn general, $x_{n+1} = x_n -\\gamma f'(x_n)$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nniter <- 10\ngam <- 0.1\nx <- double(niter)\nx[1] <- 23\ngrad <- function(x) 2 * (x - 6)\nfor (i in 2:niter) x[i] <- x[i - 1] - gam * grad(x[i - 1])\n```\n:::\n\n\n\n\n:::\n\n::: w-35\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n## Why does this work?\n\n\n[Heuristic interpretation:]{.secondary}\n\n* Gradient tells me the slope.\n\n* negative gradient points toward the minimum\n\n* go that way, but not too far (or we'll miss it)\n\n## Why does this work?\n\n[More rigorous interpretation:]{.secondary}\n\n- Taylor expansion\n$$\nf(x) \\approx f(x_0) + \\nabla f(x_0)^{\\top}(x-x_0) + \\frac{1}{2}(x-x_0)^\\top H(x_0) (x-x_0)\n$$\n- replace $H$ with $\\gamma^{-1} I$\n\n- minimize this quadratic approximation in $x$:\n$$\n0\\overset{\\textrm{set}}{=}\\nabla f(x_0) + \\frac{1}{\\gamma}(x-x_0) \\Longrightarrow x = x_0 - \\gamma \\nabla f(x_0)\n$$\n\n\n## Visually\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Visually\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## What $\\gamma$? (more details than we have time for)\n\nWhat to use for $\\gamma_k$? \n\n\n[Fixed]{.secondary}\n\n- Only works if $\\gamma$ is exactly right \n- Usually does not work\n\n[Decay on a schedule]{.secondary}\n\n$\\gamma_{n+1} = \\frac{\\gamma_n}{1+cn}$ or $\\gamma_{n} = \\gamma_0 b^n$\n\n[Exact line search]{.secondary}\n\n- Tells you exactly how far to go.\n- At each iteration $n$, solve\n$\\gamma_n = \\arg\\min_{s \\geq 0} f( x^{(n)} - s f(x^{(n-1)}))$\n- Usually can't solve this.\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- matrix(0, 40, 2); x[1, ] <- c(1, 1)\ngrad <- function(x) c(2, 1) * x\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .1\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .9 # bigger gamma\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .9 # big, but decrease it on schedule\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * .9^k * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .5 # theoretically optimal\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n\n## When do we stop?\n\nFor $\\epsilon>0$, small\n\n\nCheck any / all of\n\n1. $|f'(x)| < \\epsilon$\n1. $|x^{(k)} - x^{(k-1)}| < \\epsilon$\n1. $|f(x^{(k)}) - f(x^{(k-1)})| < \\epsilon$\n\n\n## Stochastic gradient descent (SGD)\n\nIf optimizing\n$\\argmin_\\beta \\sum_{i=1}^n -\\log P_\\beta(Y_i \\mid X_i)$\nthen derivative also additive:\n$$\n\\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta} \\left[-\\log P_\\beta(Y_i \\mid X_i) \\right]\n$$\n\nIf $n$ is really big, it may take a long time to compute this\n\nSo, just sample a subset of data $\\mathcal{M} \\subset \\{ (X_i, Y_i)\\}_{i=1}^n$\nand approximate:\n$$\\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta} \\left[-\\log P_\\beta(Y_i \\mid X_i) \\right] \\approx \\frac{n}{\\vert \\mathcal M \\vert}\\sum_{i\\in\\mathcal{M}} \\left[-\\log P_\\beta(Y_i \\mid X_i) \\right]$$\n\nFor SGD need:\n\n* the gradient estimates to be unbiased (are they?)\n* decaying step size $\\gamma$  (why?)\n\n## SGD\n\n$$\n\\begin{aligned}\nf'(\\beta) &= \\frac{1}{n}\\sum_{i=1}^n f'_i(\\beta) \\approx \\frac{1}{|\\mathcal{M}_j|}\\sum_{i\\in\\mathcal{M}_j}f'_{i}(\\beta)\n\\end{aligned}\n$$\n\nInstead of drawing samples independently, better to:\n\n* Randomly order the whole dataset (N points), then iterate:\n    * grab the next M points\n    * compute a gradient estimate based on those points, take a step\n    * once you exhaust all the data, that's an \"epoch\"; start from the beginning again\n\nGradient estimates are still marginally unbiased (why?)\n\nThis is the workhorse for neural network optimization\n\n## When do we stop SGD?\n\nFor $\\epsilon>0$, small\n\n**Can we** check any / all of\n\n1. $|f'(x)| < \\epsilon$ ?\n1. $|x^{(k)} - x^{(k-1)}| < \\epsilon$ ?\n1. $|f(x^{(k)}) - f(x^{(k-1)})| < \\epsilon$ ?\n\n. . .\n\nNone of this works due to the stochasticity. Knowing when to terminate SGD is *hard*.\n\n# Practice with GD and Logistic regression\n\n\n## Gradient descent for Logistic regression\n\n$$\n\\begin{gathered}\n\\P(Y=1 \\mid X=x) = h(\\beta^\\top x), \\qquad \\P(Y=0 \\mid X=x) = h(-\\beta^\\top x),\n\\\\\n\\\\\nh(z) = \\tfrac{1}{1+e^{-z}}\n\\end{gathered}\n$$\n\n<!-- I want to model $P(Y=1| X=x)$.  -->\n\n<!-- I'll assume that $\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = ax$ for some scalar $a$. This means that -->\n<!-- $p(x) = \\frac{\\exp(ax)}{1+\\exp(ax)} = \\frac{1}{1+\\exp(-ax)}$ -->\n\n. . .\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nn <- 100\nbeta <- 2\nx <- runif(n, -5, 5)\nlogit <- function(x) 1 / (1 + exp(-x))\np <- logit(beta * x)\ny <- rbinom(n, 1, p)\ndf <- tibble(x, y)\nggplot(df, aes(x, y)) +\n  geom_point(colour = \"cornflowerblue\") +\n  stat_function(fun = ~ logit(beta * .x))\n```\n\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/generate-data-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n---\n\n$$\n\\P(Y=1 \\mid X=x) = h(\\beta^\\top x), \\qquad \\P(Y=0 \\mid X=x) = h(-\\beta^\\top x)\n$$\n\nUnder maximum likelihood\n\n$$\n\\hat \\beta = \\argmin_{\\beta} \\underbrace{\n  \\textstyle{\\sum_{i=1}^n - \\log( \\P_\\beta(Y_i=y_i \\mid X_i=x_i) )}\n}_{:= -\\ell(\\beta)}\n$$\n\n---\n\n$$\n\\begin{align*}\n\\P_\\beta(Y_i=y_i \\mid X_i=X_i) &= h\\left( [-1]^{1 - y_i} \\beta^\\top x_i \\right)\n\\\\\n\\\\\n-\\ell(\\beta) &= \\sum_{i=1}^n -\\log\\left( \\P_\\beta(Y_i=y_i \\mid X_i=X_i) \\right)\n\\\\\n&= \\sum_{i=1}^n \\log\\left( 1 + \\exp\\left( [-1]^{y_i} \\beta^\\top x_i \\right) \\right)\n\\\\\n\\\\\n-\\frac{\\partial \\ell}{\\partial \\beta}\n&= \\sum_{i=1}^n x_i[-1]^{y_i} \\frac{\\exp\\left( [-1]^{y_i} \\beta^\\top x_i \\right)}{1 + \\exp\\left( [-1]^{y_i} \\beta^\\top x_i \\right)}\n\\\\\n%&= \\sum_{i=1}^n x_i \\left( y_i - \\P_\\beta(Y_i=y_i \\mid X_i=X_i) \\right)\n\\end{align*}\n$$\n\n<!-- $$ -->\n<!-- L(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and } -->\n<!-- p(x) = \\frac{1}{1+\\exp(-ax)} -->\n<!-- $$ -->\n\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\ell(y | a, x) &= \\log \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}  -->\n<!-- = \\sum_{i=1}^n y_i\\log p(x_i) + (1-y_i)\\log(1-p(x_i))\\\\ -->\n<!-- &= \\sum_{i=1}^n\\log(1-p(x_i)) + y_i\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\\\ -->\n<!-- &=\\sum_{i=1}^n ax_i y_i + \\log\\left(1-p(x_i)\\right)\\\\ -->\n<!-- &=\\sum_{i=1}^n ax_i y_i + \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right) -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ## Reminder: the likelihood -->\n\n<!-- $$ -->\n<!-- L(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and } -->\n<!-- p(x) = \\frac{1}{1+\\exp(-ax)} -->\n<!-- $$ -->\n\n<!-- Now, we want the negative of this. Why?  -->\n\n<!-- We would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood (and scale by $1/n$) -->\n\n\n<!-- $$-\\ell(y | a, x) = \\frac{1}{n}\\sum_{i=1}^n -ax_i y_i - \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right)$$ -->\n\n<!-- ## Reminder: the likelihood -->\n\n<!-- $$ -->\n<!-- \\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and } -->\n<!-- p(x) = \\frac{1}{1+\\exp(-ax)} -->\n<!-- $$ -->\n\n<!-- This is, in the notation of our slides $f(a)$.  -->\n\n<!-- We want to minimize it in $a$ by gradient descent.  -->\n\n<!-- So we need the derivative with respect to $a$: $f'(a)$.  -->\n\n<!-- Now, conveniently, this simplifies a lot.  -->\n\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\frac{d}{d a} f(a) &= \\frac{1}{n}\\sum_{i=1}^n -x_i y_i - \\left(-\\frac{x_i \\exp(ax_i)}{1+\\exp(ax_i)}\\right)\\\\ -->\n<!-- &=\\frac{1}{n}\\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \\frac{1}{n}\\sum_{i=1}^n -x_i(y_i-p(x_i)). -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n<!-- ## Reminder: the likelihood -->\n\n<!-- $$ -->\n<!-- \\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and } -->\n<!-- p(x) = \\frac{1}{1+\\exp(-ax)} -->\n<!-- $$ -->\n\n---\n\n## Finding $\\hat\\beta = \\argmin_{\\beta} -\\ell(\\beta)$ with gradient descent:\n\n1. Input $\\beta_0,\\ \\gamma>0,\\ \\epsilon>0,\\ \\tfrac{d \\ell}{d\\beta}$.\n2. For $j=1,\\ 2,\\ \\ldots$,\n$$\\beta_j = \\beta_{j-1} - \\gamma \\tfrac{d}{d\\beta}\\left(-\\!\\ell(\\beta_{j-1}) \\right)$$\n3. Stop if $|\\beta_j - \\beta_{j-1}| < \\epsilon$ or $|d\\ell / d\\beta\\ | < \\epsilon$.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbeta.mle <- function(x, y, beta0, gamma = 0.5, jmax = 50, eps = 1e-6) {\n  beta <- double(jmax) # place to hold stuff (always preallocate space)\n  beta[1] <- beta0 # starting value\n  for (j in 2:jmax) { # avoid possibly infinite while loops\n    px <- logit(beta[j - 1] * x)\n    grad <- mean(-x * (y - px))\n    beta[j] <- beta[j - 1] - gamma * grad\n    if (abs(grad) < eps || abs(beta[j] - beta[j - 1]) < eps) break\n  }\n  beta[1:j]\n}\n```\n:::\n\n\n\n\n\n\n## Try it:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntoo_big <- beta.mle(x, y, beta0 = 5, gamma = 50)\ntoo_small <- beta.mle(x, y, beta0 = 5, gamma = 1)\njust_right <- beta.mle(x, y, beta0 = 5, gamma = 10)\n```\n:::\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nnegll <- function(beta) {\n  -beta * mean(x * y) -\n    rowMeans(log(1 / (1 + exp(outer(beta, x)))))\n}\nblah <- list_rbind(\n  map(\n    rlang::dots_list(\n      too_big, too_small, just_right, .named = TRUE\n    ), \n    as_tibble),\n  names_to = \"gamma\"\n) |> mutate(negll = negll(value))\nggplot(blah, aes(value, negll)) +\n  geom_point(aes(colour = gamma)) +\n  facet_wrap(~gamma, ncol = 1) +\n  stat_function(fun = negll, xlim = c(-2.5, 5)) +\n  scale_y_log10() + \n  xlab(\"beta\") + \n  ylab(\"negative log likelihood\") +\n  geom_vline(xintercept = tail(just_right, 1)) +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Check vs. `glm()`\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(glm(y ~ x - 1, family = \"binomial\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = y ~ x - 1, family = \"binomial\")\n\nCoefficients:\n  Estimate Std. Error z value Pr(>|z|)    \nx   1.9174     0.4785   4.008 6.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 100  degrees of freedom\nResidual deviance:  32.335  on  99  degrees of freedom\nAIC: 34.335\n\nNumber of Fisher Scoring iterations: 7\n```\n\n\n:::\n:::\n",
    "supporting": [
      "00-gradient-descent_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}