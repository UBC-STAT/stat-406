{
  "hash": "944b342dabb554b154ff3f17db4c9487",
  "result": {
    "markdown": "---\nlecture: \"00 Gradient descent\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 25 October 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n\n## Simple optimization techniques\n\n\nWe'll see \"gradient descent\" a few times: \n\n1. solves logistic regression (simple version of IRWLS)\n1. gradient boosting\n1. Neural networks\n\nThis seems like a good time to explain it.\n\nSo what is it and how does it work?\n\n\n## Very basic example\n\n::: flex\n::: w-65\n\nSuppose I want to minimize $f(x)=(x-6)^2$ numerically.\n\nI start at a point (say $x_1=23$)\n\nI want to \"go\" in the negative direction of the gradient.\n\nThe gradient (at $x_1=23$) is  $f'(23)=2(23-6)=34$.\n\nMove current value toward current value - 34.\n\n$x_2 = x_1 - \\gamma 34$, for $\\gamma$ small.\n\nIn general, $x_{n+1} = x_n -\\gamma f'(x_n)$.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nniter <- 10\ngam <- 0.1\nx <- double(niter)\nx[1] <- 23\ngrad <- function(x) 2 * (x - 6)\nfor (i in 2:niter) x[i] <- x[i - 1] - gam * grad(x[i - 1])\n```\n:::\n\n\n:::\n\n::: w-35\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n## Why does this work?\n\n\n[Heuristic interpretation:]{.secondary}\n\n* Gradient tells me the slope.\n\n* negative gradient points toward the minimum\n\n* go that way, but not too far (or we'll miss it)\n\n## Why does this work?\n\n[More rigorous interpretation:]{.secondary}\n\n- Taylor expansion\n$$\nf(x) \\approx f(x_0) + \\nabla f(x_0)^{\\top}(x-x_0) + \\frac{1}{2}(x-x_0)^\\top H(x_0) (x-x_0)\n$$\n- replace $H$ with $\\gamma^{-1} I$\n\n- minimize this quadratic approximation in $x$:\n$$\n0\\overset{\\textrm{set}}{=}\\nabla f(x_0) + \\frac{1}{\\gamma}(x-x_0) \\Longrightarrow x = x_0 - \\gamma \\nabla f(x_0)\n$$\n\n\n## Visually\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Visually\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## What $\\gamma$? (more details than we have time for)\n\nWhat to use for $\\gamma_k$? \n\n\n[Fixed]{.secondary}\n\n- Only works if $\\gamma$ is exactly right \n- Usually does not work\n\n[Decay on a schedule]{.secondary}\n\n$\\gamma_{n+1} = \\frac{\\gamma_n}{1+cn}$ or $\\gamma_{n} = \\gamma_0 b^n$\n\n[Exact line search]{.secondary}\n\n- Tells you exactly how far to go.\n- At each iteration $n$, solve\n$\\gamma_n = \\arg\\min_{s \\geq 0} f( x^{(n)} - s f(x^{(n-1)}))$\n- Usually can't solve this.\n\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nx <- matrix(0, 40, 2); x[1, ] <- c(1, 1)\ngrad <- function(x) c(2, 1) * x\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .1\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .9 # bigger gamma\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center'}\n:::\n:::\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .9 # big, but decrease it on schedule\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * .9^k * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-14-1.svg){fig-align='center'}\n:::\n:::\n\n\n##\n\n$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ngamma <- .5 # theoretically optimal\nfor (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-17-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## When do we stop?\n\nFor $\\epsilon>0$, small\n\n\nCheck any / all of\n\n1. $|f'(x)| < \\epsilon$\n1. $|x^{(k)} - x^{(k-1)}| < \\epsilon$\n1. $|f(x^{(k)}) - f(x^{(k-1)})| < \\epsilon$\n\n\n## Stochastic gradient descent\n\nSuppose $f(x) = \\frac{1}{n}\\sum_{i=1}^n f_i(x)$\n\nLike if $f(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - x^\\top_i\\beta)^2$.\n\nThen $f'(\\beta) = \\frac{1}{n}\\sum_{i=1}^n f'_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^n -2x_i^\\top(y_i - x^\\top_i\\beta)$ \n\nIf $n$ is really big, it may take a long time to compute $f'$\n\nSo, just sample some partition our data into mini-batches $\\mathcal{M}_j$\n\nAnd approximate (imagine the Law of Large Numbers, use a sample to approximate the population)\n$$f'(x) = \\frac{1}{n}\\sum_{i=1}^n f'_i(x) \\approx \\frac{1}{m}\\sum_{i\\in\\mathcal{M}_j}f'_{i}(x)$$\n\n## SGD\n\n$$\n\\begin{aligned}\nf'(\\beta) &= \\frac{1}{n}\\sum_{i=1}^n f'_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^n -2x_i^\\top(y_i - x^\\top_i\\beta)\\\\\nf'(x) &= \\frac{1}{n}\\sum_{i=1}^n f'_i(x) \\approx \\frac{1}{m}\\sum_{i\\in\\mathcal{M}_j}f'_{i}(x)\n\\end{aligned}\n$$\n\n\nUsually cycle through \"mini-batches\":\n\n* Use a different mini-batch at each iteration of GD\n* Cycle through until we see all the data\n\n\nThis is the workhorse for neural network optimization\n\n\n\n# Practice with GD and Logistic regression\n\n\n## Gradient descent for Logistic regression\n\nSuppose $Y=1$ with probability $p(x)$ and $Y=0$ with probability $1-p(x)$, $x \\in \\R$.  \n\nI want to model $P(Y=1| X=x)$. \n\nI'll assume that $\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = ax$ for some scalar $a$. This means that\n$p(x) = \\frac{\\exp(ax)}{1+\\exp(ax)} = \\frac{1}{1+\\exp(-ax)}$\n\n. . .\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nn <- 100\na <- 2\nx <- runif(n, -5, 5)\nlogit <- function(x) 1 / (1 + exp(-x))\np <- logit(a * x)\ny <- rbinom(n, 1, p)\ndf <- tibble(x, y)\nggplot(df, aes(x, y)) +\n  geom_point(colour = \"cornflowerblue\") +\n  stat_function(fun = ~ logit(a * .x))\n```\n\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/generate-data-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Reminder: the likelihood\n\n$$\nL(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n$$\n\n\n$$\n\\begin{aligned}\n\\ell(y | a, x) &= \\log \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i} \n= \\sum_{i=1}^n y_i\\log p(x_i) + (1-y_i)\\log(1-p(x_i))\\\\\n&= \\sum_{i=1}^n\\log(1-p(x_i)) + y_i\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\\\\n&=\\sum_{i=1}^n ax_i y_i + \\log\\left(1-p(x_i)\\right)\\\\\n&=\\sum_{i=1}^n ax_i y_i + \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right)\n\\end{aligned}\n$$\n\n## Reminder: the likelihood\n\n$$\nL(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n$$\n\nNow, we want the negative of this. Why? \n\nWe would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood (and scale by $1/n$)\n\n\n$$-\\ell(y | a, x) = \\frac{1}{n}\\sum_{i=1}^n -ax_i y_i - \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right)$$\n\n## Reminder: the likelihood\n\n$$\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n$$\n\nThis is, in the notation of our slides $f(a)$. \n\nWe want to minimize it in $a$ by gradient descent. \n\nSo we need the derivative with respect to $a$: $f'(a)$. \n\nNow, conveniently, this simplifies a lot. \n\n\n$$\n\\begin{aligned}\n\\frac{d}{d a} f(a) &= \\frac{1}{n}\\sum_{i=1}^n -x_i y_i - \\left(-\\frac{x_i \\exp(ax_i)}{1+\\exp(ax_i)}\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \\frac{1}{n}\\sum_{i=1}^n -x_i(y_i-p(x_i)).\n\\end{aligned}\n$$\n\n## Reminder: the likelihood\n\n$$\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n$$\n\n(Simple) gradient descent to minimize $-\\ell(a)$ or maximize $L(y|a,x)$ is:\n\n1. Input $a_1,\\ \\gamma>0,\\ j_\\max,\\ \\epsilon>0,\\ \\frac{d}{da} -\\ell(a)$.\n2. For $j=1,\\ 2,\\ \\ldots,\\ j_\\max$,\n$$a_j = a_{j-1} - \\gamma \\frac{d}{da} (-\\ell(a_{j-1}))$$\n3. Stop if $\\epsilon > |a_j - a_{j-1}|$ or $|d / da\\  \\ell(a)| < \\epsilon$.\n\n## Reminder: the likelihood\n\n$$\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"1,10-11|2-3|4-9|\"}\namle <- function(x, y, a0, gam = 0.5, jmax = 50, eps = 1e-6) {\n  a <- double(jmax) # place to hold stuff (always preallocate space)\n  a[1] <- a0 # starting value\n  for (j in 2:jmax) { # avoid possibly infinite while loops\n    px <- logit(a[j - 1] * x)\n    grad <- mean(-x * (y - px))\n    a[j] <- a[j - 1] - gam * grad\n    if (abs(grad) < eps || abs(a[j] - a[j - 1]) < eps) break\n  }\n  a[1:j]\n}\n```\n:::\n\n\n\n\n## Try it:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nround(too_big <- amle(x, y, 5, 50), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5.000 3.360 2.019 1.815 2.059 1.782 2.113 1.746 2.180 1.711 2.250 1.684\n[13] 2.309 1.669 2.344 1.663 2.359 1.661 2.364 1.660 2.365 1.660 2.366 1.660\n[25] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660\n[37] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660\n[49] 2.366 1.660\n```\n:::\n\n```{.r .cell-code}\nround(too_small <- amle(x, y, 5, 1), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5.000 4.967 4.934 4.902 4.869 4.837 4.804 4.772 4.739 4.707 4.675 4.643\n[13] 4.611 4.579 4.547 4.515 4.483 4.451 4.420 4.388 4.357 4.326 4.294 4.263\n[25] 4.232 4.201 4.170 4.140 4.109 4.078 4.048 4.018 3.988 3.957 3.927 3.898\n[37] 3.868 3.838 3.809 3.779 3.750 3.721 3.692 3.663 3.635 3.606 3.578 3.550\n[49] 3.522 3.494\n```\n:::\n\n```{.r .cell-code}\nround(just_right <- amle(x, y, 5, 10), 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 5.000 4.672 4.351 4.038 3.735 3.445 3.171 2.917 2.688 2.488 2.322 2.191\n[13] 2.094 2.027 1.983 1.956 1.940 1.930 1.925 1.922 1.920 1.919 1.918 1.918\n[25] 1.918 1.918 1.918 1.917 1.917 1.917 1.917\n```\n:::\n:::\n\n\n\n## Visual\n\n\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\nnegll <- function(a) {\n  -a * mean(x * y) -\n    rowMeans(log(1 / (1 + exp(outer(a, x)))))\n}\nblah <- list_rbind(\n  map(\n    rlang::dots_list(\n      too_big, too_small, just_right, .named = TRUE\n    ), \n    as_tibble),\n  names_to = \"gamma\"\n) |> mutate(negll = negll(value))\nggplot(blah, aes(value, negll)) +\n  geom_point(aes(colour = gamma)) +\n  facet_wrap(~gamma, ncol = 1) +\n  stat_function(fun = negll, xlim = c(-2.5, 5)) +\n  scale_y_log10() + \n  xlab(\"a\") + \n  ylab(\"negative log likelihood\") +\n  geom_vline(xintercept = tail(just_right, 1)) +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](00-gradient-descent_files/figure-revealjs/unnamed-chunk-18-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Check vs. `glm()`\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary(glm(y ~ x - 1, family = \"binomial\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x - 1, family = \"binomial\")\n\nCoefficients:\n  Estimate Std. Error z value Pr(>|z|)    \nx   1.9174     0.4785   4.008 6.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 100  degrees of freedom\nResidual deviance:  32.335  on  99  degrees of freedom\nAIC: 34.335\n\nNumber of Fisher Scoring iterations: 7\n```\n:::\n:::\n",
    "supporting": [
      "00-gradient-descent_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}