{
  "hash": "16c04ffea8d87f74261b0fe60d214a78",
  "result": {
    "markdown": "---\nlecture: \"04 Bias and variance\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 05 September 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n$$\n\n\n\n\n## {background-color=\"#e98a15\"}\n\n### We just talked about \n\n* Variance of an estimator.\n\n* Irreducible error when making predictions.\n\n* These are 2 of the 3 components of the \"Prediction Risk\" $R_n$\n\n\n\n## Component 3, the Bias\n\n\nWe need to be specific about what we mean when we say _bias_.\n\nBias is neither good nor bad in and of itself.\n\nA very simple example: let $Z_1,\\ \\ldots,\\ Z_n \\sim N(\\mu, 1)$.\n  - We don't know $\\mu$, so we try to use the data (the $Z_i$'s) to estimate it.\n  \n  - I propose 3 estimators: \n      1. $\\widehat{\\mu}_1 = 12$, \n  \n      2. $\\widehat{\\mu}_2=Z_6$, \n  \n      3. $\\widehat{\\mu}_3=\\overline{Z}$.\n  \n  - The [bias]{.secondary} (by definition) of my estimator is $E[\\widehat{\\mu_i}]-\\mu$.\n  \n. . .\n\nCalculate the bias and variance of each estimator.\n\n\n\n## Regression in general\n\nIf I want to predict $Y$ from $X$, it is almost always the case that\n\n$$\n\\mu(x) = \\Expect{Y\\given X=x} \\neq x^{\\top}\\beta\n$$\n\nSo the _bias_ of using a linear model is [not]{.secondary} zero.\n\n<br>\n\nWhy? Because\n\n$$\n\\Expect{Y\\given X=x}-x^\\top\\beta \\neq \\Expect{Y\\given X=x} - \\mu(x) = 0.\n$$\n\nWe can include as many predictors as we like, \n\nbut this doesn't change the fact that the world is [non-linear](.secondary).\n\n\n## (Continuation) Predicting new Y's\n\nSuppose we want to predict $Y$, \n\nwe know $E[Y]= \\mu \\in \\mathbb{R}$ and $\\textrm{Var}[Y] = 1$.  \n\nOur data is $\\{y_1,\\ldots,y_n\\}$\n\nLast time: we considered estimating $\\mu$ in various ways, and using $\\widehat{Y} = \\widehat{\\mu}$\n\n. . .\n\n<br><br>\n\n\nLet's try one more: $\\widehat Y_a = a\\overline{Y}_n$ for some $a \\in (0,1]$.\n\n## One can show... (wait for the proof)\n\n$\\widehat Y_a = a\\overline{Y}_n$ for some $a \\in (0,1]$\n  \n$$\nR_n(\\widehat Y_a) = \\Expect{(\\widehat Y_a-Y)^2} = (1 - a)^2\\mu^2 +\n\\frac{a^2}{n} +1 \n$$\n\n. . .\n  \nWe can minimize this in $a$ to get the best possible prediction risk for an estimator of the form $\\widehat Y_a$: \n  \n$$\n\\argmin_{a} R_n(\\widehat Y_a) = \\left(\\frac{\\mu^2}{\\mu^2 + 1/n} \\right)\n$$\n\n. . .\n\nWhat happens if $\\mu \\ll 1$?\n  \n##\n  \n::: callout-important\n::: large\nWait a minute! I'm saying there is a _better_ estimator than $\\overline{Y}_n$!\n:::\n:::\n\n\n## Bias-variance tradeoff: Estimating the mean\n\n$$\nR_n(\\widehat Y_a) = (a - 1)^2\\mu^2 +  \\frac{a^2}{n} + \\sigma^2\n$$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu = 1; n = 5; sig = 1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-bias-variance_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n## To restate\n\nIf $\\mu=$ 1 and $n=$ 5 \n\nthen it is better to predict with 0.83 $\\overline{Y}_5$ \n\nthan with $\\overline{Y}_5$ itself.  \n\n. . .\n\nFor this $a =$ 0.83 and $n=5$\n\n1. $R_5(\\widehat{Y}_a) =$ 1.17\n\n2. $R_5(\\overline{Y}_5)=$ 1.2\n\n\n\n## Prediction risk\n\n\n$$\nR_n(f) = \\Expect{(Y - f(X))^2}\n$$\n  \nWhy should we care about $R_n(f)$? \n\n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0a8754;overflow:visible;position:relative;\"><path d=\"M256 80c0-17.7-14.3-32-32-32s-32 14.3-32 32V224H48c-17.7 0-32 14.3-32 32s14.3 32 32 32H192V432c0 17.7 14.3 32 32 32s32-14.3 32-32V288H400c17.7 0 32-14.3 32-32s-14.3-32-32-32H256V80z\"/></svg>`{=html} Measures predictive accuracy on average.\n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0a8754;overflow:visible;position:relative;\"><path d=\"M256 80c0-17.7-14.3-32-32-32s-32 14.3-32 32V224H48c-17.7 0-32 14.3-32 32s14.3 32 32 32H192V432c0 17.7 14.3 32 32 32s32-14.3 32-32V288H400c17.7 0 32-14.3 32-32s-14.3-32-32-32H256V80z\"/></svg>`{=html} How much confidence should you have in $f$'s predictions.\n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 448 512\" style=\"height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0a8754;overflow:visible;position:relative;\"><path d=\"M256 80c0-17.7-14.3-32-32-32s-32 14.3-32 32V224H48c-17.7 0-32 14.3-32 32s14.3 32 32 32H192V432c0 17.7 14.3 32 32 32s32-14.3 32-32V288H400c17.7 0 32-14.3 32-32s-14.3-32-32-32H256V80z\"/></svg>`{=html} Compare with other predictors: $R_n(f)$ vs $R_n(g)$\n\n`<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 512 512\" style=\"height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#a8201a;overflow:visible;position:relative;\"><path d=\"M459.1 52.4L442.6 6.5C440.7 2.6 436.5 0 432.1 0s-8.5 2.6-10.4 6.5L405.2 52.4l-46 16.8c-4.3 1.6-7.3 5.9-7.2 10.4c0 4.5 3 8.7 7.2 10.2l45.7 16.8 16.8 45.8c1.5 4.4 5.8 7.5 10.4 7.5s8.9-3.1 10.4-7.5l16.5-45.8 45.7-16.8c4.2-1.5 7.2-5.7 7.2-10.2c0-4.6-3-8.9-7.2-10.4L459.1 52.4zm-132.4 53c-12.5-12.5-32.8-12.5-45.3 0l-2.9 2.9C256.5 100.3 232.7 96 208 96C93.1 96 0 189.1 0 304S93.1 512 208 512s208-93.1 208-208c0-24.7-4.3-48.5-12.2-70.5l2.9-2.9c12.5-12.5 12.5-32.8 0-45.3l-80-80zM200 192c-57.4 0-104 46.6-104 104v8c0 8.8-7.2 16-16 16s-16-7.2-16-16v-8c0-75.1 60.9-136 136-136h8c8.8 0 16 7.2 16 16s-7.2 16-16 16h-8z\"/></svg>`{=html} _This is hard:_  Don't know the distribution of the data (if I knew the truth, this would be easy)\n\n  \n\n## Bias-variance decomposition\n\n\n\n$$R_n(\\widehat{Y}_a)=(a - 1)^2\\mu^2 + \\frac{a^2}{n} + 1$$\n\n\n1. prediction risk  =  $\\textrm{bias}^2$  +  variance  +  irreducible error \n\n2. estimation risk  =  $\\textrm{bias}^2$  +  variance\n    \n\nWhat is $R_n(\\widehat{Y}_a)$ for our estimator $\\widehat{Y}_a=a\\overline{Y}_n$?\n\n\n\\begin{aligned}\n\\textrm{bias}(\\widehat{Y}_a) &= \\Expect{a\\overline{Y}_n} - \\mu=(a-1)\\mu\\\\\n\\textrm{var}(\\widehat f(x)) &= \\Expect{ \\left(a\\overline{Y}_n - \\Expect{a\\overline{Y}_n}\\right)^2}\n=a^2\\Expect{\\left(\\overline{Y}_n-\\mu\\right)^2}=\\frac{a^2}{n} \\\\\n\\sigma^2 &= \\Expect{(Y-\\mu)^2}=1\n\\end{aligned}\n\n\n## This decomposition holds generally {background-color=\"#97D4E9\"}\n\n\\begin{aligned}\nR_n(\\hat{Y}) \n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu + \\mu - \\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + \n2\\Expect{(Y-\\mu)(\\mu-\\hat{Y})}\\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}] + E[\\hat{Y}] - \\hat{Y})^2}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 2\\Expect{(\\mu-E[\\hat{Y}])(E[\\hat{Y}] - \\hat{Y})}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\n\n\n\n## Bias-variance decomposition\n\n\\begin{aligned}\nR_n(\\hat{Y}) \n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\n\n\n\n::: callout-important\n::: bigger\nImplication: prediction risk is proportional to estimation risk.  However, defining estimation risk requires stronger assumptions.\n:::\n:::\n\n\n. . .\n\n::: callout-tip\nIn order to make good predictions, we want our prediction risk to be small.  This means that we want to \"balance\" the bias and variance.\n:::\n\n##  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-bias-variance_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Bias-variance tradeoff: Overview\n\n[bias:]{.secondary} how well does $\\widehat{f}(x)$ approximate the truth $\\Expect{Y\\given X=x}$\n\n* If we allow more complicated possible $\\widehat{f}$, lower bias. Flexibility $\\Rightarrow$ Expressivity\n\n* But, more flexibility $\\Rightarrow$ larger variance\n\n* Complicated models are hard to estimate precisely for fixed $n$\n\n* Irreducible error\n\n. . .\n\n\n::: callout-note\nSince we can't evaluate any expectations with real data...\n:::\n\n\n\n# Next time...\n\n\nEstimating risk\n",
    "supporting": [
      "04-bias-variance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}