{
  "hash": "efe7666a4e7ea8bb39c54ba77f074a40",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"04 Bias and variance\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 16 September 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## {background-color=\"#e98a15\"}\n\n### We just talked about \n\n* Variance of an estimator.\n\n* Irreducible error when making predictions.\n\n* These are 2 of the 3 components of the \"Prediction Risk\" $R_n$\n\n\n\n## Component 3, the Bias\n\n\nWe need to be specific about what we mean when we say _bias_.\n\nBias is neither good nor bad in and of itself.\n\nA very simple example: let $Y_1,\\ \\ldots,\\ Y_n \\sim N(\\mu, 1)$.\n  - We don't know $\\mu$, so we try to use the data (the $Y_i$'s) to estimate it.\n  \n  - I propose 3 estimators: \n      1. $\\widehat{\\mu}_1 = 12$, \n  \n      2. $\\widehat{\\mu}_2=Y_6$, \n  \n      3. $\\widehat{\\mu}_3=\\overline{Y}$.\n  \n  - The [bias]{.secondary} (by definition) of my estimator is $E[\\widehat{\\mu_i}]-\\mu$.\n  \n. . .\n\nCalculate the bias and variance of each estimator.\n\n\n\n## Regression in general\n\nIf I want to predict $Y$ from $X$, it is almost always the case that\n\n$$\n\\mu(x) = \\Expect{Y\\given X=x} \\neq x^{\\top}\\beta\n$$\n\nSo the _bias_ of using a linear model is [not]{.secondary} zero.\n\n<br>\n\nWhy? Because\n\n$$\n\\Expect{Y\\given X=x}-x^\\top\\beta \\neq \\Expect{Y\\given X=x} - \\mu(x) = 0.\n$$\n\nWe can include as many predictors as we like, \n\nbut this doesn't change the fact that the world is [non-linear]{.secondary}.\n\n\n## (Continuation) Predicting new Y's\n\nSuppose we want to predict $Y$, \n\nwe know $E[Y]= \\mu \\in \\mathbb{R}$ and $\\textrm{Var}[Y] = 1$.  \n\nOur data is $\\{y_1,\\ldots,y_n\\}$\n\nWe have considered estimating $\\mu$ in various ways, and using $\\widehat{Y} = \\widehat{\\mu}$\n\n. . .\n\n<br><br>\n\n\nLet's try one more: $\\widehat Y_a = a\\overline{Y}_n$ for some $a \\in (0,1]$.\n\n## One can show... (wait for the proof)\n\n$\\widehat Y_a = a\\overline{Y}_n$ for some $a \\in (0,1]$\n  \n$$\nR_n(\\widehat Y_a) = \\Expect{(\\widehat Y_a-Y)^2} = (1 - a)^2\\mu^2 +\n\\frac{a^2}{n} +1 \n$$\n\n. . .\n  \nWe can minimize this to get the best possible prediction risk for an estimator of the form $\\widehat Y_a$: \n  \n$$\n\\argmin_{a} R_n(\\widehat Y_a) = \\left(\\frac{\\mu^2}{\\mu^2 + 1/n} \\right)\\qquad\n\\min_{a} R_n(\\widehat Y_a) = 1+\\left(\\frac{\\mu^2}{n\\mu^2 + 1} \\right).\n$$\n\n. . .\n\nIs this less than or greater than the risk we saw for $\\bar Y$?\n\n. . . \n\nAm I cheating here?\n  \n##\n  \n::: callout-important\n::: large\nWait a minute! I'm saying there is a _better_ estimator than $\\overline{Y}_n$!\n:::\n:::\n\n\n## Bias-variance tradeoff: Estimating the mean\n\n$$\nR_n(\\widehat Y_a) = (a - 1)^2\\mu^2 +  \\frac{a^2}{n} + \\sigma^2\n$$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmu = 1; n = 5; sig = 1\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](04-bias-variance_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## To restate\n\nIf $\\mu=$ 1 and $n=$ 5 \n\nthen it is better to predict with 0.83 $\\overline{Y}_5$ \n\nthan with $\\overline{Y}_5$ itself.  \n\n. . .\n\nFor this $a =$ 0.83 and $n=5$\n\n1. $R_5(\\widehat{Y}_a) =$ 1.17\n\n2. $R_5(\\overline{Y}_5)=$ 1.2\n\n\n## Bias-variance decomposition\n\n$$R_n(\\widehat{Y}_a)=(a - 1)^2\\mu^2 + \\frac{a^2}{n} + 1$$\n\n\n1. prediction risk  =  $\\textrm{bias}^2$  +  variance  +  irreducible error \n\n2. estimation risk  =  $\\textrm{bias}^2$  +  variance\n    \n\nWhat is $R_n(\\widehat{Y}_a)$ for our estimator $\\widehat{Y}_a=a\\overline{Y}_n$?\n\n\n\\begin{aligned}\n\\textrm{bias}(\\widehat{Y}_a) &= \\Expect{a\\overline{Y}_n} - \\mu=(a-1)\\mu\\\\\n\\textrm{var}(\\widehat f(x)) &= \\Expect{ \\left(a\\overline{Y}_n - \\Expect{a\\overline{Y}_n}\\right)^2}\n=a^2\\Expect{\\left(\\overline{Y}_n-\\mu\\right)^2}=\\frac{a^2}{n} \\\\\n\\sigma^2 &= \\Expect{(Y-\\mu)^2}=1\n\\end{aligned}\n\n\n## This decomposition holds generally {background-color=\"#97D4E9\"}\n\n\\begin{aligned}\nR_n(\\hat{Y}) \n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu + \\mu - \\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + \n2\\Expect{(Y-\\mu)(\\mu-\\hat{Y})}\\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}] + E[\\hat{Y}] - \\hat{Y})^2}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 2\\Expect{(\\mu-E[\\hat{Y}])(E[\\hat{Y}] - \\hat{Y})}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\n\n\n\n## Bias-variance decomposition\n\n\\begin{aligned}\nR_n(\\hat{Y}) \n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\n\n\n\n::: callout-important\n::: bigger\nImplication: prediction risk is estimation risk plus something you can't control.\nHowever, defining estimation risk requires stronger assumptions (not always just estimating a parameter).\n:::\n:::\n\n\n. . .\n\n::: callout-tip\nIn order to make good predictions, we want our prediction risk to be small.  This means that we want to \"balance\" the bias and variance.\n:::\n\n##  \n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncols = c(blue, red, green, orange)\npar(mfrow = c(2, 2), bty = \"n\", ann = FALSE, xaxt = \"n\", yaxt = \"n\", \n    family = \"serif\", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))\nlibrary(mvtnorm)\nmv <- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)\nva <- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)\n\nfor (i in 1:4) {\n  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, \n       col = blue, ann = FALSE, pty = \"s\")\n  points(0, 0, pch = 19, cex = 30, col = \"white\")\n  points(0, 0, pch = 19, cex = 18, col = green)\n  points(0, 0, pch = 19, cex = 6, col = orange)\n  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)\n  switch(i,\n    \"1\" = {\n      mtext(\"low variance\", 3, cex = 2)\n      mtext(\"low bias\", 2, cex = 2)\n    },\n    \"2\" = mtext(\"high variance\", 3, cex = 2),\n    \"3\" = mtext(\"high bias\", 2, cex = 2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![](04-bias-variance_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Bias-variance tradeoff: Overview\n\n[bias:]{.secondary} how well does $\\widehat{f}(x)$ approximate the truth $\\Expect{Y\\given X=x}$\n\n* If we allow more complicated possible $\\widehat{f}$, lower bias. Flexibility $\\Rightarrow$ Expressivity\n\n* But, more flexibility $\\Rightarrow$ larger variance\n\n* Complicated models are hard to estimate precisely for fixed $n$\n\n* Irreducible error\n\n. . .\n\n\n::: callout-danger\nSadly, that whole exercise depends on _knowing_ the truth to evaluate $E\\ldots$\n:::\n\n\n\n# Next time...\n\n\nEstimating risk\n",
    "supporting": [
      "04-bias-variance_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}