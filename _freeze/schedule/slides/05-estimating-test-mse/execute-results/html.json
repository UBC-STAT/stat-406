{
  "hash": "d29dfd91d9cadeececbcd365f2d43bd4",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"05 Estimating (Test) Risk\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 18 September 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Last time\n\n1. What is a model (formal definition)?\n1. Evaluating models (risk/loss functions)\n1. Decomposing risk (bias, variance, irreducible error)\n\n## What is a model?\n\nA model is a set of distributions that explain data $\\{ Z = (X, Y) \\}$, i.e.\n\n$$\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' f} \\}$$\n\n. . .\n\n[(Why do we have to specify that $f$ is smooth? Why can't it be any function?)]{.secondary} \\\n\n. . .\n\n### Goal of learning\n\nChoose the $P \\in \\mathcal P$ that makes the \"best\" predictions on new $X, Y$ pairs.\n\n[(Next slide: how do we formalize \"best\"?)]{.secondary}\n\n\n## How do we evaluate models?\n\n$$\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' f} \\}$$\n\n::: {.incremental}\n1. Specify how a $P \\in \\mathcal P$ makes **predictions** on new inputs $\\hat Y$. \\\n[(E.g.: $\\hat Y = f(X)$ for $P = \\mathcal N(f(X), \\sigma^2)$.)]{.secondary}\n\n2. Introduce a **loss** function $\\ell(Y, \\hat{Y})$ (a datapoint-level function). \\\n[(E.g.: $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$)]{.secondary}\n\n3. Define the **risk** of $P \\in \\mathcal P$ as the expected loss (a population-level function): \\\n[(R_n(P) = E[\\ell(Y, \\hat Y)] = E[(Y - f(X))^2])]{.secondary}\n\n4. The **best** model is the one that minimizes the risk. \\\n[($P^* = \\argmin_{P \\in \\mathcal P} R_n(P)$)]{.secondary}\n:::\n\n\n---\n\nLast time: when $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$, we showed that the **regression function** is the best model:\n\n. . .\n\n$$\n\\text{Regression function } \\triangleq E[Y \\mid X] = \\argmin_{P \\in \\mathcal P} R_n(P) = \\argmin_{P \\in \\mathcal P} E[\\ell(Y, \\hat Y)]\n$$\n\n. . .\n\n[Are we done? Have we solved learning?]{.secondary}\n\n. . .\n\nNo! We don't know what $E[Y \\mid X]$ is! We have to *estimate it from data!*\n\n$$\n\\hat f(X) \\approx E[Y \\mid X]\n$$\n\n(We'll discuss various methods for producing $\\hat f(X)$ estimators throughout this course.)\n\n\n## Decomposing risk\n\nWhen $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$, the prediction risk of $\\hat f(X)$ decomposes into two factors:\n\n$$\nR_n(\\hat f) \\quad = \\quad \\underbrace{E\\left[ \\: \\left( E[Y\\mid X] -\\hat f(X) \\right)^2 \\right]}_{(1)} \\quad - \\quad \\underbrace{E\\left[ \\: \\left( \\hat f(X) - Y\\right)^2 \\right]}_{(2)}\n$$\n\n::: {.incremental}\n1. **Estimation error**\n2. **Irreducible error** (or \"noise\")\n:::\n\n---\n\nThe **estimation error term** further reduces into two components:\n\n\\begin{aligned}\n\\underbrace{E\\left[ \\: \\left( E[Y\\mid X] -\\hat f(X) \\right)^2 \\right]}_{\\text{Estimation error}} \\quad &= \\quad \\underbrace{\\left( E[Y\\mid X] - E \\left[\\hat f(X)\\right] \\right)^2}_{(A)} \\quad \\\\\n&+ \\quad \\underbrace{E\\left[ \\: \\left( E \\left[\\hat f(X)\\right] -\\hat f(X) \\right)^2 \\right]}_{(B)}\n\\end{aligned}\n\n::: {.incremental}\nA.  **Bias**^2\nB.  **Variance**\n:::\n\n. . .\n\n::: callout-tip\nAnalogous decompositions hold for other loss/risk functions.\n:::\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncols = c(blue, red, green, orange)\npar(mfrow = c(2, 2), bty = \"n\", ann = FALSE, xaxt = \"n\", yaxt = \"n\", \n    family = \"serif\", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))\nlibrary(mvtnorm)\nmv <- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)\nva <- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)\n\nfor (i in 1:4) {\n  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, \n       col = blue, ann = FALSE, pty = \"s\")\n  points(0, 0, pch = 19, cex = 30, col = \"white\")\n  points(0, 0, pch = 19, cex = 18, col = green)\n  points(0, 0, pch = 19, cex = 6, col = orange)\n  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)\n  switch(i,\n    \"1\" = {\n      mtext(\"low variance\", 3, cex = 2)\n      mtext(\"low bias\", 2, cex = 2)\n    },\n    \"2\" = mtext(\"high variance\", 3, cex = 2),\n    \"3\" = mtext(\"high bias\", 2, cex = 2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Sources of bias and variance\n\n### What conditions give rise to a high bias estimator?\n\n::: fragment\n- Not enough covariates (small $p$)\n- Model is too simple\n- Model is _misspecified_ (doesn't accurately represent the data generating process)\n- Bad training algorithm\n:::\n\n### What conditions give rise to a high variance estimator?\n\n::: fragment\n- Not enough training samples (small $n$)\n- Model is too complicated[^1]\n- Lots of irreducible noise in training data (if my model has power to fit noise, it will)\n\n:::\n\n# How do we estimate $R_n$?\n\n\\\nSo far, $R_n$ has been a theoretical construct. \\\nWe can never know the true $R_n$ for a given $\\hat f$.\nWe also have to estimate it from data.\n\n\n\n## Don't use training error\n\n\nThe training error in regression is\n\n$$\\widehat{R}_n(\\widehat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$$\n\nHere, the $n$ is doubly used (annoying, but simple): $n$ observations to create $\\widehat{f}$ and $n$ terms in the sum.\n\n::: callout-tip\nWe also call $\\hat R_n(\\hat f)$ the **empirical risk**.\n:::\n\n. . .\n\n$\\hat R_n(\\hat f)$ is a bad estimator for $R_n(\\widehat{f})$. \\\nSo we should [__never__]{.secondary} use it.\n\n\n## Why is $\\hat R_n$ a bad estimator of $R_n$?\n\n. . .\n\n1. It doesn't say anything about predictions on new data. \\\n   [(It's a measure of how well the model fits a **fixed set** of training data.)]{.secondary}\n\n2. It can be made **arbitrarily small** by making your model more complex.\n\n\n## 1. It doesn't say anything about predictions on new data.\n\nThese all have the same $R^2$ and Training Error\n\n\n::: flex\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nans <- anscombe |>\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), \n               names_pattern = \"(.)(.)\")\nggplot(ans, aes(x, y)) + \n  geom_point(colour = orange, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = blue, linewidth = 2) +\n  facet_wrap(~set, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n:::\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nans %>% \n  group_by(set) |> \n  summarise(\n    R2 = summary(lm(y ~ x))$r.sq, \n    train_error = mean((y - predict(lm(y ~ x)))^2)\n  ) |>\n  kableExtra::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|set |   R2| train_error|\n|:---|----:|-----------:|\n|1   | 0.67|        1.25|\n|2   | 0.67|        1.25|\n|3   | 0.67|        1.25|\n|4   | 0.67|        1.25|\n\n\n:::\n:::\n\n\n\n:::\n:::\n\n## 2. It can be made **arbitrarily small** by making your model more complex.\n\nAdding \"junk\" predictors increases $R^2$ and decreases Training Error\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\np <- 10\nq <- 0:30\nx <- matrix(rnorm(n * (p + max(q))), nrow = n)\ny <- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)\n\nregress_on_junk <- function(q) {\n  x <- x[, 1:(p + q)]\n  mod <- lm(y ~ x)\n  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmap(q, regress_on_junk) |> \n  list_rbind() |>\n  mutate(q = q) |>\n  pivot_longer(-q) |>\n  ggplot(aes(q, value, colour = name)) +\n  geom_line(linewidth = 2) + xlab(\"train_error\") +\n  scale_colour_manual(values = c(blue, orange), guide = \"none\") +\n  facet_wrap(~ name, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Other things you can't use\n\nYou should not use `anova` \n\nor the $p$-values from the `lm` output for this purpose.\n\n. . .\n\n> These things are to determine whether those _parameters_ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.\n\nIn other words, they are useful for _inference problems_.\n\nThis is not the same as being useful for _prediction problems_ (i.e. how to get small $R_n$).\n\n\n## Don't use training error: the formal argument\n\nOur training error $\\hat R_n(\\hat f)$ is an **estimator** of $R_n(\\hat f)$. \\\nSo we can ask \"is $\\widehat{R}(\\hat{f})$ a good estimator for $R_n(\\hat{f})$?\"\n\n## The risk of risk\n\nLet's measure the **risk** of our empirical risk estimator:\n\n::: flex\n\n::: w-50\n![](https://images.squarespace-cdn.com/content/v1/59a53195ff7c50210a2c6b8b/1510207623511-BT2MBPENJOJXZNE0SCSQ/inceptius-meme-generator-we-have-to-go-deeper-014848.jpg){fig-alt=\"meme\"}\n:::\n\n::: w-50\n![](https://i.imgflip.com/93xy4g.jpg){fig-alt=\"meme\" width=\"380px\"}\n:::\n\n:::\n$$E[(R_n(\\hat f) - \\hat R_n(\\hat f))^2]$$\n[(What is the expectation with respect to?)]{.secondary}\n\n\n## The risk of risk\n\n$$E[(R_n(\\hat f) - \\hat R_n(\\hat f))^2]$$\n\n- $R_n(\\hat f)$ only depends on training data (since $\\hat f$ is derived from training data)\n- $\\hat R_n(\\hat f)$ also only depends on training data\n- So the expectation is with respect to our training dataset\n\n. . .\n\nAs before, we can decompose our risk-risk into **bias** and **variance**\n\n$$\nE[(R_n - \\hat R)^2] = \\underbrace{E[( R_n - E[\\hat R_n])^2]}_{\\text{bias}} + \\underbrace{E[( \\hat R_n - E[\\hat R_n])^2]}_{\\text{variance}}\n$$\n\n## Formalizing why $\\hat R_n$ is a bad estimator of $R_n$\n\nRecall that $\\hat R_n(\\hat f)$ is estimated from the training data $\\{ (X_i, Y_i) \\}_{i=1}^n$.\n\nConsider an alternative estimator built from $\\{ (X_j, Y_j) \\}_{j=1}^m$ that was not part of the training set.\n$$\\tilde R_m = {\\textstyle \\frac{1}{m} \\sum_{j=1}^m} \\ell(Y_j, \\hat Y_j(X_j)),\n$$\n\n### Which has higher bias, $\\hat R_n$ or $\\tilde R_m$?\n\n::: fragment\n- $\\tilde R_m$ has _zero_ bias.\n  - (X_j, Y_j) are i.i.d. samples from the population\n- $\\tilde R_n$ is _very_ biased.\n  - (X_i, Y_i) are i.i.d. samples from the population, but they are also used to choose $\\hat f$\n  - Using them to both choose $\\hat f$ and estimate $R_n$ is \"double dipping.\"\n:::\n\n\n\n\n# How to properly estimate $R_n$\n\n\n## Holdout sets\n\nOne option is to have a separate \"holdout\" or \"validation\" dataset.\n\n::: callout-tip\nThis option follows the logic on the previous slide. \\\nIf we randomly \"hold out\" $\\{ (X_j, Y_j) \\}_{j=1}^m$ from the training set,\nthen we can use this data to get an (nearly) unbiased estimator of $R_n$.\n$$\nR_n(\\hat f) \\approx \\tilde R_m \\triangleq {\\textstyle{\\frac 1 m \\sum_{j=1}^m \\ell ( Y_j - \\hat Y_j(X_j))}}\n$$\n:::\n\n. . .\n\n\nüëç Estimates the test error\n\nüëç Fast computationally\n\nü§Æ Estimate is random \n\nü§Æ Estimate has high variance (depends on 1 choice of split)\n\nü§Æ Estimate has a little bias (because we aren't estimating $\\hat f$ from all of the training data)\n\n\n## Aside {background-color=\"#97D4E9\"}\n\nIn my experience, CS has particular definitions of \"training\", \"validation\", and \"test\" data.\n\nI think these are not quite the same as in Statistics.\n\n* [Test data]{.secondary} - Hypothetical data you don't get to see, ever. Infinite amounts drawn from the population.\n    * _Expected test error_ or _Risk_ is an expected value over this distribution. It's _not_ a sum over some data kept aside.\n* Sometimes I'll give you \"test data\". You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.\n* [Training data]{.secondary} - This is \"holdout\" data that you get to touch.\n* [Validation set]{.secondary} - Often, we need to _choose models_. One way to do this is to split off some of your training data and pretend that it's like a \"Test Set\".\n\nWhen and how you split your training data can be very important. \n\n\n\n## Cross Validation\n\n<!--\nOne reason that $\\widehat{R}_n(\\widehat{f})$ is bad is that we are using the same data to pick $\\widehat{f}$ __AND__ to estimate $R_n$.\n\n\"Validation set\" fixes this, but holds out a particular, fixed block of data we pretend mimics the \"test data\"\n-->\n\nOur validation error $\\tilde{R}_m(\\widehat{f})$ is a random estimator.\\\n[(The split we use to divide our data into training versus validation is random.)]{.secondary}\n\n. . .\n\n\\\nA random estimator has _variance_.\nWe can reduce this variance by averageing over multiple splits.\n\n\n## Cross Validation Example\n\nWhat if we set aside one observation, say the first one $(y_1, x_1)$:\n\n- We estimate $\\widehat{f}^{(1)}$ without using the first observation.\n- We estimate $\\widetilde{R}_1(\\widehat{f}^{(1)})$ using the held-out first observation.\n\n$$\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.$$\n[(Why the notation $\\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )]{.secondary}\n\n---\n\nBut that was only one data point $(y_1, x_1)$. Why stop there?\n\nDo the same with $(y_2, x_2)$!\n\n$$\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.$$\nWe can keep doing this until we try it for every data point.\n\n. . .\n\n$$\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n \n(y_i - \\widehat{f}^{(i)}(x_i))^2$$\nThis is [__leave-one-out cross validation__]{.secondary}\n\n\n## Problems with LOO-CV\n\n<!--\nü§Æ Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.\n-->\n\nü§Æ The training sets overlap. This is bad. \n\n- Usually, averaging reduces variance: $\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.$\n- But only if the variables are independent. If not, then $\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.$\n- Since the training sets overlap a lot, that covariance can be pretty big.\n    \nü§Æ We have to estimate this model $n$ times.\n\nüéâ Bias is low because we used almost all the data to fit the model: $E[\\mbox{LOO-CV}] = R_{n-1}$ \n\n  \n## K-fold CV\n\n::: flex\n::: w-50\nTo alleviate some of these problems, people usually use $K$-fold cross validation.\n\nThe idea of $K$-fold is \n\n1. Divide the data into $K$ groups. \n1. Leave a group out and estimate with the rest.\n1. Test on the held-out group. Calculate an average risk over these $\\sim n/K$ data.\n1. Repeat for all $K$ groups.\n1. Average the average risks.\n\n\n:::\n\n\n::: w-50\nüéâ Less overlap, smaller covariance.\n\nüéâ Larger hold-out sets, smaller variance.\n\nüéâ Less computations (only need to estimate $K$ times)\n\nü§Æ LOO-CV is (nearly) unbiased for $R_n$\n\nü§Æ K-fold CV is unbiased for $R_{n(1-1/K)}$\n\nThe risk depends on how much data you use to estimate the model. $R_n$ depends on $n$.\n\n:::\n:::\n\n\n\n## A picture\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(0, 0, 0, 0))\nplot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = \"n\", yaxt = \"n\", xaxt = \"n\")\nrect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)\nrect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), \n     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)\npoints(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)\ntext(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c(\"1\", \"2\", \"3\", \"K\"), cex = 3, \n     col = red)\ntext(6, 4.5, \"Training data\", cex = 3, col = blue)\ntext(2, 1.5, \"Validation data\", cex = 3, col = red)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Code\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"11-13|14-16|\"}\n#' @param data The full data set\n#' @param estimator Function. Has 1 argument (some data) and fits a model. \n#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions\n#' @param error_fun Function. Has one arg: the test data, with fits added.\n#' @param kfolds Integer. The number of folds.\nkfold_cv <- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  n <- nrow(data)\n  fold_labels <- sample(rep(1:kfolds, length.out = n))\n  errors <- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows <- fold_labels == fold\n    train <- data[!test_rows, ]\n    test <- data[test_rows, ]\n    current_model <- estimator(train)\n    test$.preds <- predictor(current_model, test)\n    errors[fold] <- error_fun(test)\n  }\n  mean(errors)\n}\n```\n:::\n\n\n\n. . . \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2-4|\"}\nsomedata <- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))\nest <- function(dataset) lm(z ~ ., data = dataset)\npred <- function(mod, dataset) predict(mod, newdata = dataset)\nerror_fun <- function(testdata) mutate(testdata, errs = (z - .preds)^2) |> pull(errs) |> mean()\nkfold_cv(somedata, est, pred, error_fun, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8928686\n```\n\n\n:::\n:::\n\n\n\n\n## Trick\n\n__For certain \"nice\" models__ of the form\n$$\\widehat{y}_i = h_i(\\mathbf{X})^\\top \\mathbf{y}$$\nfor some vector $h_i$, one can show \n\n$$\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-[\\boldsymbol h_i(x_i)]_{i})^2}.$$\n(Proof: tedious algebra which I wouldn't wish on my worst enemy, but might - in a fit of rage - assign as homework to belligerent students.) \n\n. . .\n\n* This trick means that you only have to fit the model once rather than $n$ times!\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_nice <- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )\n```\n:::\n\n\n\n\n# Next time...\n\nMore tricks and what's up with the name \"hatvalues\"\n",
    "supporting": [
      "05-estimating-test-mse_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}