{
  "hash": "70378db499b92a9703f61a2163ddb92f",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"05 Estimating Risk and Test Error\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 23 September 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Last time\n\n1. What is a model (formal definition)?\n1. Evaluating models (risk/loss functions)\n1. Decomposing risk (bias, variance, irreducible error)\n\n## What is a model?\n\nA model is a set of distributions that explain data $\\{ Z = (X, Y) \\}$, i.e.\n\n$$\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' }f \\}$$\n\n\n[(Why do we have to specify that $f$ is smooth? Why can't it be any function?)]{.secondary} \\\n\n\n### Goal of learning\n\nChoose the $P \\in \\mathcal P$ that makes the \"best\" predictions on new $X, Y$ pairs.\n\n[(Next slide: how do we formalize \"best\"?)]{.secondary}\n\n\n## How do we evaluate models?\n\n$$\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' f} \\}$$\n\n1. Specify how a $P \\in \\mathcal P$ makes **predictions**  $\\hat Y$ on new inputs $X$. \\\n[(E.g.: $\\hat Y = f(X)$ for $P = \\mathcal N(f(X), \\sigma^2)$.)]{.secondary}\n\n2. Introduce a **loss** function $\\ell(Y, \\hat{Y})$ (a datapoint-level function). \\\n[(E.g.: $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$)]{.secondary}\n\n3. Define the **test error** of $P \\in \\mathcal P$ as the expected loss (a population-level function): \\\n[$T_n(P) = E[\\ell(Y, \\hat Y)] = E[(Y - f(X))^2]$]{.secondary}\n\n4. The **best** model is the one that minimizes the test error \\\n[($P^* = \\argmin_{P \\in \\mathcal P} T_n(P)$)]{.secondary}\n\n\n---\n\nLast time: when $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$, we showed that the **regression function** is the best model:\n\n. . .\n\n$$\n\\text{Regression function } \\triangleq E[Y \\mid X] = \\argmin_{P \\in \\mathcal P} T_n(P) = \\argmin_{P \\in \\mathcal P} E[\\ell(Y, \\hat Y)]\n$$\n\n[Are we done? Have we solved learning?]{.secondary}\n\n. . .\n\nNo! We don't know what $E[Y \\mid X]$ is! We have to *estimate it from data!*\n\n$$\n\\hat f(X) \\approx E[Y \\mid X]\n$$\n\n(We'll discuss various methods for producing $\\hat f(X)$ estimators throughout this course.)\n\n\n## Risk (Expected Test Error) and its Decomposition\n\nOur estimator $\\hat f$ is a random variable (it depends on training sample).\\\nSo let's consider the _risk_ (the _expected test error_):\n\n$$\nR_n = E_{\\hat f} \\left[ T_n(\\hat f) \\right] = E_{\\hat f, X, Y} \\left[ \\ell(Y, \\hat f(X)) \\right]\n$$\n\n. . .\n\n::: callout-note\nTest error is a metric for a fixed $\\hat f$.\nIt averages over all possible test points, but assumes a fixed training set.\n\nRisk averages over **everything** that is random:\n(1) the test data point sampled from our population, and\n(2) the training data that produces $\\hat f$\n:::\n\n\n## Risk (Expected Test Error) and its Decomposition\n\nWhen $\\ell(Y, \\hat Y) = (Y - \\hat Y)^2$, the prediction risk of $\\hat f(X)$ decomposes into two factors:\n\n$$\nR_n \\quad = \\quad \\underbrace{E_{\\hat f, X, Y} \\left[ \\: \\left( E[Y\\mid X] - \\hat f(X) \\right)^2 \\right]}_{(1)} \\quad + \\quad \\underbrace{E_{X, Y} \\left[ \\: \\left( Y - E[Y\\mid X] \\right)^2 \\right]}_{(2)}\n$$\n\n. . .\n\n1. **Estimation error** (or \"reducible error\")\n2. **Irreducible error** (or \"noise\")\n\n---\n\nThe **estimation error term** further reduces into two components:\n\n$$\n\\begin{aligned}\n\\underbrace{E_{\\hat f, X, Y} \\left[ \\: \\left( E[Y\\mid X] -\\hat f(X) \\right)^2 \\right]}_{\\text{Estimation error}} \\quad &= \\quad \\underbrace{ E_{X, Y} \\left[ \\left( E[Y\\mid X] - E \\left[\\hat f(X) \\mid X\\right] \\right)^2 \\right]}_{(A)} \\quad \\\\\n&+ \\quad \\underbrace{E_{\\hat f, X} \\left[ \\: \\left( E \\left[\\hat f(X) \\mid X\\right] -\\hat f(X) \\right)^2 \\right]}_{(B)}\n\\end{aligned}\n$$\n\n. . .\n\nA.  **Bias**^2\nB.  **Variance**\n\n\n. . .\n\n::: callout-tip\nAnalogous decompositions hold for other loss/risk functions.\n:::\n\n---\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ncols = c(blue, red, green, orange)\npar(mfrow = c(2, 2), bty = \"n\", ann = FALSE, xaxt = \"n\", yaxt = \"n\", \n    family = \"serif\", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))\nlibrary(mvtnorm)\nmv <- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)\nva <- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)\n\nfor (i in 1:4) {\n  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, \n       col = blue, ann = FALSE, pty = \"s\")\n  points(0, 0, pch = 19, cex = 30, col = \"white\")\n  points(0, 0, pch = 19, cex = 18, col = green)\n  points(0, 0, pch = 19, cex = 6, col = orange)\n  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)\n  switch(i,\n    \"1\" = {\n      mtext(\"low variance\", 3, cex = 2)\n      mtext(\"low bias\", 2, cex = 2)\n    },\n    \"2\" = mtext(\"high variance\", 3, cex = 2),\n    \"3\" = mtext(\"high bias\", 2, cex = 2)\n  )\n}\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Sources of bias and variance\n\n### What conditions give rise to a high bias estimator?\n\n::: fragment\n- Not enough covariates (small $p$)\n- Model is too simple\n- Model is _misspecified_ (doesn't accurately represent the data generating process)\n- Bad training algorithm\n:::\n\n### What conditions give rise to a high variance estimator?\n\n::: fragment\n- Not enough training samples (small $n$)\n- Model is too complicated\n- Lots of irreducible noise in training data (if my model has power to fit noise, it will)\n\n:::\n\n# How do we estimate $R_n$?\n\n\\\n$R_n$ is a theoretical construct. \\\nWe can never know the true $R_n$ for a given $\\hat f$.\nWe also have to estimate it from data.\n\n\n\n## Don't use training error\n\n\nThe training error in regression is\n\n$$\\widehat{R}_n(\\widehat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$$\n\nHere, the $n$ is doubly used (annoying, but simple): $n$ observations to create $\\widehat{f}$ and $n$ terms in the sum.\n\n::: callout-tip\nWe also call $\\hat R_n(\\hat f)$ the **empirical risk**.\n:::\n\n. . .\n\n$\\hat R_n(\\hat f)$ is a bad estimator for $R_n$. \\\nSo we should [__never__]{.secondary} use it.\n\n\n## Why is $\\hat R_n$ a bad estimator of $R_n$?\n\n. . .\n\n1. It doesn't say anything about predictions on new data. \\\n   [(It's a measure of how well the model fits a **fixed set** of training data.)]{.secondary}\n\n2. It can be made **arbitrarily small** by making your model more complex.\n\n\n## [1. It doesn't say anything about predictions on new data.]{.small}\n\nThese all have the same $R^2$ and Training Error\n\n\n::: flex\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nans <- anscombe |>\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), \n               names_pattern = \"(.)(.)\")\nggplot(ans, aes(x, y)) + \n  geom_point(colour = orange, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = blue, linewidth = 2) +\n  facet_wrap(~set, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n:::\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nans %>% \n  group_by(set) |> \n  summarise(\n    R2 = summary(lm(y ~ x))$r.sq, \n    train_error = mean((y - predict(lm(y ~ x)))^2)\n  ) |>\n  kableExtra::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|set |   R2| train_error|\n|:---|----:|-----------:|\n|1   | 0.67|        1.25|\n|2   | 0.67|        1.25|\n|3   | 0.67|        1.25|\n|4   | 0.67|        1.25|\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n## 2. It can be made **arbitrarily small** by making your model more complex.\n\nAdding \"junk\" predictors increases $R^2$ and decreases Training Error\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\np <- 10\nq <- 0:30\nx <- matrix(rnorm(n * (p + max(q))), nrow = n)\ny <- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)\n\nregress_on_junk <- function(q) {\n  x <- x[, 1:(p + q)]\n  mod <- lm(y ~ x)\n  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmap(q, regress_on_junk) |> \n  list_rbind() |>\n  mutate(q = q) |>\n  pivot_longer(-q) |>\n  ggplot(aes(q, value, colour = name)) +\n  geom_line(linewidth = 2) + xlab(\"train_error\") +\n  scale_colour_manual(values = c(blue, orange), guide = \"none\") +\n  facet_wrap(~ name, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## Other things you can't use\n\nYou should not use `anova` \n\nor the $p$-values from the `lm` output for this purpose.\n\n. . .\n\n> These things are to determine whether those _parameters_ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.\n\nIn other words, they are useful for _inference problems_.\n\nThis is not the same as being useful for _prediction problems_ (i.e. how to get small $R_n$).\n\n\n## Don't use training error: the formal argument\n\nOur training error $\\hat R_n(\\hat f)$ is an **estimator** of $R_n$.\n\nSo we can ask \"is $\\widehat{R}_n(\\hat{f})$ a good estimator for $R_n$?\"\n\n## The error of our risk estimator\n\nLet's measure the **error** of our empirical risk estimator:\n\n<!--\n::: flex\n\n::: w-50\n![](https://images.squarespace-cdn.com/content/v1/59a53195ff7c50210a2c6b8b/1510207623511-BT2MBPENJOJXZNE0SCSQ/inceptius-meme-generator-we-have-to-go-deeper-014848.jpg){fig-alt=\"meme\"}\n:::\n\n::: w-50\n![](https://i.imgflip.com/93xy4g.jpg){fig-alt=\"meme\" width=\"380px\"}\n:::\n\n:::\n-->\n$$E[(R_n - \\hat R_n(\\hat f))^2]$$\n[(What is the expectation with respect to?)]{.secondary}\n\n\n## The error of our risk estimator\n\n$$E[(R_n - \\hat R_n(\\hat f))^2]$$\n\n- $R_n$ is deterministic (we average over test data and training data)\n- $\\hat R_n(\\hat f)$ also only depends on training data\n- So the expectation is with respect to our training dataset\n\n. . .\n\nAs before, we can decompose the error of our risk estimator into **bias** and **variance**\n\n$$\nE[(R_n - \\hat R_n(\\hat f))^2] = \\underbrace{( R_n - E[\\hat R_n(\\hat f)])^2}_{\\text{bias}} + \\underbrace{E[( \\hat R_n(\\hat f) - E[\\hat R_n(\\hat f)])^2]}_{\\text{variance}}\n$$\n\n### Is the bias of $\\hat R_n(\\hat f)$ small or large? Why?\n\n## Is the bias of $\\hat R_n(\\hat f)$ small or large? Why?\n\n- Assume we have a very complex model capable of (nearly) fitting our training data\n  - I.e. $\\hat R_n(\\hat f) \\approx 0$\n- $\\text{Bias} = ( R_n - E[\\hat R_n(\\hat f)])^2 \\approx ( R_n - 0 ) = R_n$\n- (That's the worst bias we could get! 😔)\n\n\n## Formalizing why $\\hat R_n(\\hat f)$ is a bad estimator of $R_n$\n\nConsider an alternative estimator built from $\\{ (X_j, Y_j) \\}_{j=1}^m$ that was not part of the training set.\n$$\\tilde R_m(\\hat f) = {\\textstyle \\frac{1}{m} \\sum_{j=1}^m} \\ell(Y_j, \\hat f(X_j)),\n$$\nThe error of this estimator can also be decompsed into **bias** and **variance**\n$$\nE[(R_n - \\tilde R_m(\\hat f))^2] = \\underbrace{( R_n - E_{\\hat f,X_j,Y_j}[\\tilde R_m(\\hat f)])^2}_{\\text{bias}} + \\underbrace{E_{\\hat f,X_j,Y_j}[( \\tilde R_m(\\hat f) - E_{\\hat f,X_j,Y_j}[\\tilde R_m(\\hat f)])^2]}_{\\text{variance}}\n$$\n\n### Is the bias of $\\tilde R_m(\\hat f)$ small or large? Why?\n\n## Is the bias of $\\tilde R_m(\\hat f)$ small or large? Why?\n\n$\\tilde R_m(\\hat f)$ has _zero_ bias!\n\n$$\n\\begin{aligned}\nE_{\\hat f,X_j,Y_j} \\left[ \\tilde R_m(\\hat f) \\right]\n&= E_{\\hat f,X_j,Y_j} \\left[ \\frac{1}{m} \\sum_{j=1}^m \\ell(Y_j, \\hat f(X_j)) \\right] \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m E_{\\hat f,X_j,Y_j} \\left[ \\ell(Y_j, \\hat f(X_j)) \\right]\n= R_n\n\\end{aligned}\n$$\n\n\n# How to properly estimate $R_n$\n\n\n## Holdout sets\n\nOne option is to have a separate \"holdout\" or \"validation\" dataset.\n\n::: callout-tip\nThis option follows the logic on the previous slide. \\\nIf we randomly \"hold out\" $\\{ (X_j, Y_j) \\}_{j=1}^m$ from the training set,\nwe can use this data to get an (nearly) unbiased estimator of $R_n$.\n$$\nR_n \\approx \\tilde R_m(\\hat f) \\triangleq {\\textstyle{\\frac 1 m \\sum_{j=1}^m \\ell ( Y_j - \\hat Y_j(X_j))}}\n$$\n:::\n\n. . .\n\n\n👍 Estimates the test error\n\n👍 Fast computationally\n\n🤮 Estimate is random \n\n🤮 Estimate has high variance (depends on 1 choice of split)\n\n🤮 Estimate has a little bias (because we aren't estimating $\\hat f$ from all of the training data)\n\n\n## Aside {background-color=\"#97D4E9\"}\n\nIn my experience, CS has particular definitions of \"training\", \"validation\", and \"test\" data.\n\nI think these are not quite the same as in Statistics.\n\n* [Test data]{.secondary} - Hypothetical data you don't get to see, ever. Infinite amounts drawn from the population.\n    * _Expected test error_ or _Risk_ is an expected value over this distribution. It's _not_ a sum over some data kept aside.\n* Sometimes I'll give you \"test data\". You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.\n* [Training data]{.secondary} - This is \"holdout\" data that you get to touch.\n* [Validation set]{.secondary} - Often, we need to _choose models_. One way to do this is to split off some of your training data and pretend that it's like a \"Test Set\".\n\nWhen and how you split your training data can be very important. \n\n# Announcements Sept 24\n\n- Lab Section 03 on monday next week: it's a holiday. Your lab will be due Friday instead.\n- Everyone else's lab: same time as usual.\n\n# Review of Risk (Estimation)\n\nWe fixed a bunch of subtle issues in the Sept 19 Risk Estimation slides.\n\nAnd I've noticed some related confusion in my office hours, so it's *review time*!\n\n. . . \n\n::: callout-tip\nAfter this lecture, make sure to review the whole Risk Estimation slide deck to see all the fixed terminology/definitions/examples.\n:::\n\n## Risk vs. Test Error\n\n- **Risk** ($R_n$): expected error when the training data *have not yet been observed* **(random!)**\n    - depends on true data distribution, predictor\n\n- **Test Error** ($T_n$): expected error when the training data *have been observed* **(fixed!)**\n    - depends on true data distribution, predictor, training data \n\n- $R_n = \\E[T_n]$\n\n. . .\n\n- we mostly care about **risk** when *designing* a new predictor / estimator\n    - we want to know how well it will work on *future training and test data*\n- we mostly care about **test error** when we've *fit* a predictor / estimator\n    - we want to know how well it will work on *future test data*\n\n## An important clarification\n\n::: callout-important\nIn previous lectures, we've used $R_n(\\hat f)$ to denote risk. \n\nConfusing: the $\\hat \\mu$ argument looks like a *trained predictor*, so $R_n(\\hat f)$ looks like a function of *training data*. **Not true!**\n\nWe will just avoid this confusion from now on and use $R_n$ for risk.\n:::\n\n## Risk vs. Test Error: Warmup in 1D\n\n- Model: $\\mathcal{P} = \\{ P: \\quad Y \\sim \\mathcal N(\\mu, 1), \\quad \\mu\\in\\R\\}$, loss $\\ell(y,\\hat y) = (y-\\hat y)^2$\n- Training Data: $Y_i\\in\\R$ from some unknown \"true\" $P_0 \\in \\mathcal{P}$ (i.e., \"true\" $\\mu_0$)\n- Predictor: *function of training data* $\\hat\\mu : \\R^n \\to \\R$\n    - e.g., scaled empirical average $\\hat\\mu(Y_{1:n}) = \\alpha\\frac{1}{n}\\sum_{n=1}^N Y_n$ for $\\alpha > 0$\n\n. . . \n\n- **Risk** $R_n$: expected loss over *both* training and test data $R_n = E[\\ell(Y,\\hat\\mu(Y_{1:n}))]$\n    - function of true dist $P_0$ and predictor *function* $\\hat\\mu(\\cdot)$\n    - averages over randomness in training data\n\n. . . \n\n- **Test Error** $T_n$: expected loss over *only* test data $T_n(\\hat\\mu) = E[\\ell(Y,\\hat\\mu(Y_{1:n})) | Y_{1:n}]$\n    - function of true dist $P_0$, predictor *function* $\\hat\\mu(\\cdot)$, **and training data**\n    - training data (and trained predictor) are known/fixed \n\n## Risk vs. Test Error: Warmup in 1D\n\n- Model: $\\mathcal{P} = \\{ P: \\quad Y \\sim \\mathcal N(\\mu, 1), \\quad \\mu\\in\\R\\}$, loss $\\ell(y,\\hat y) = (y-\\hat y)^2$\n- Training Data: $Y_i\\in\\R$ from some unknown \"true\" $P_0 \\in \\mathcal{P}$ (i.e., \"true\" $\\mu_0$)\n- Predictor: scaled empirical average $\\hat\\mu(Y_{1:n}) = \\alpha \\frac{1}{n}\\sum_{i=1}^n Y_i$ for a fixed $\\alpha > 0$\n\n\n[(What can the risk and test error depend on?)]{.secondary}\n\n. . .\n\n$$ R_n = E[(Y - \\hat\\mu(Y_{1:n}))^2] = \\underbrace{1}_{\\text{irr.}} + \\underbrace{\\mu^2_0(\\alpha-1)^2}_{\\text{bias}^2}+\\underbrace{\\alpha^2/n}_{\\text{variance}}$$\n\n. . .\n\n$$ T_n = E[(Y - \\hat\\mu(Y_{1:n}))^2 |Y_{1:n}] = 1 + \\left(\\mu_0 - \\alpha \\frac{1}{n}\\sum_{i=1}^n Y_i\\right)^2$$\n\n. . .\n\n- $R_n$ is a function of the true distribution ($\\mu_0$) and predictor ($\\alpha$); not training data!\n- $T_n$ is a function of $\\mu_0$, $\\alpha$ *and* training data \n\n## Risk vs. Test Error: Regression\n\n- Model: $\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), 1), \\quad f(x)=ax, \\, \\text{for } a\\in\\R \\}$\n- Training Data: $X_i,Y_i\\in\\R$ pairs from some unknown \"true\" $P_0 \\in \\mathcal{P}$ (\"true\" $a_0$)\n- Predictor: a *function of the training data and a new test input* $\\hat f : \\R^n\\times \\R^{n} \\times \\R \\to \\R$\n    - e.g., the function $\\hat f(X_{1:n},Y_{1:n},x) = \\beta \\frac{Y_2 - Y_1}{X_2 - X_1} x$ for $\\beta > 0$\n\n. . .\n\n- **Risk** $R_n$: avg loss over *both* training and test data $R_n = E[\\ell(Y, \\hat f(X_{1:n}, Y_{1:n}, X))]$\n\n. . .\n\n- **Test Error** $T_n$: avg loss over *only* test data $T_n = E[\\ell(Y, \\hat f(Y_{1:n}, X_{1:n},X)) | Y_{1:n},X_{1:n}]$\n    - training data fixed/known\n\n## Risk vs. Test Error: Regression\n\n- Model: $\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), 1), \\quad f(x)=ax, \\, \\text{for } a\\in\\R \\}$ (\"true\" value $a_0$)\n- Predictor: the function $\\hat f(X_{1:n}, Y_{1:n}, x) = \\beta \\frac{Y_2 - Y_1}{X_2 - X_1} x$\n\n[(What can the risk and test error depend on?)]{.secondary}\n\n. . .\n\n$$\n\\begin{aligned}\nR_n = E[(Y - \\hat f(X))^2] \n%= E[(Y - a_0X)^2] + E[(a_0X - \\hat f(X))^2]\\\\\n%&= 1 +  E\\left[\\left(a_0X - \\beta\\frac{Y_2 - Y_1}{X_2-X_1}X\\right)^2\\right]\\\\\n&= \\text{annoying algebra...}\\\\\n% &= 1 +  E[X^2]E\\left[\\left(a_0 - \\beta\\frac{Y_2 - Y_1}{X_2-X_1}\\right)^2\\right]\\\\\n% &= 1 +  E[X^2]E\\left[\\left(a_0 - \\beta\\frac{a_0 X_2 + Z_2 - a_0 X_1 - Z_1}{X_2-X_1}\\right)^2\\right]\\\\\n&= \\underbrace{1}_{\\text{irr.}} +  \\underbrace{a_0^2(1-\\beta)^2E[X^2]}_{\\text{bias}^2} + \\underbrace{2\\beta^2E\\left[(X_2-X_1)^{-2}\\right]E[X^2]}_{\\text{variance}}.\n\\end{aligned}\n$$\n\n. . .\n\n$$ T_n = E[(Y - \\hat f(X))^2|X_{1:n},Y_{1:n}] = 1 +  E[X^2]\\left(a_0 - \\beta\\frac{Y_2 - Y_1}{X_2-X_1}\\right)^2 $$\n\n. . .\n\n- Risk $R_n$ depends only on $a_0, \\beta$ and $X$-marginal distribution\n- Test error $T_n$ depends on training data too!\n\n## Estimating risk\n\nWe can't compute $R_n$ in practice (we don't know the true data distribution)\n\nCan we estimate risk ($\\hat R_n$) with data? How do we measure the quality of our estimate?\n\n. . .\n\n$$E[(R_n - \\hat R_n)^2]$$\n[(What is the expectation with respect to?)]{.secondary}\n\n. . .\n\n- $R_n$ is just a fixed value (we already averaged over test data and training data)\n- $\\hat R_n$ depends only on training data\n- So the expectation is with respect to our training dataset\n\n. . .\n\nAs before, we can decompose the error of our risk estimator into **bias** and **variance**\n\n$$\nE[(R_n - \\hat R_n)^2] = \\underbrace{( R_n - E[\\hat R_n])^2}_{\\text{bias}} + \\underbrace{E[( \\hat R_n - E[\\hat R_n])^2]}_{\\text{variance}}\n$$\n\n## Estimating risk: empirical risk\n$$\n\\text{Recall: }E[(R_n - \\hat R_n)^2] = \\underbrace{( R_n - E[\\hat R_n])^2}_{\\text{bias}} + \\underbrace{E[( \\hat R_n - E[\\hat R_n])^2]}_{\\text{variance}}\n$$\n\n- **Empirical risk:** just use the training loss $\\hat R_n = \\frac{1}{n}\\sum_{i=1}^n \\ell(Y_i, \\hat f(X_i))$\n\n\n[(What is the bias and variance with a *really flexible model*?)]{.secondary}\n\n. . .\n\n- If our model is so flexible that it can nearly fit all data always, then $\\hat R_n \\approx 0$\n- $\\text{Variance} = E[( \\hat R_n - E[\\hat R_n])^2] \\approx E[(0 - 0)^2] = 0$ \n- $\\text{Bias}^2 = ( R_n - E[\\hat R_n(\\hat f)])^2 \\approx ( R_n - 0 ) = R_n$\n\n. . .\n\n[(Is that good or bad?)]{.secondary}\n\n## Estimating risk: holdout sets\n$$\n\\text{Recall: }E[(R_n - \\hat R_n)^2] = \\underbrace{( R_n - E[\\hat R_n])^2}_{\\text{bias}} + \\underbrace{E[( \\hat R_n - E[\\hat R_n])^2]}_{\\text{variance}}\n$$\n**Holdout:** train $\\hat f$ with $n-m$ data, estimate with $m$ data:  $\\hat R_n = \\frac{1}{m}\\sum_{j=1}^{m} \\ell(Y_j, \\hat f(X_j))$ \n\n. . . \n\n- bias [(increasing, decreasing, or either with $m$?)]{.secondary}\n$$\n\\text{Bias}^2 = (R_n - R_{n-m})^2\n$$\n\n. . . \n\n- variance [(increasing, decreasing, or either with $m$?)]{.secondary}\n\n$$\n\\text{Variance}\n= \\underbrace{\\frac{1}{m}E[(\\tilde \\ell - E[\\tilde\\ell| \\hat f])^2]}_{\\text{estimation variance}}\n+ \\underbrace{E[(E[\\tilde \\ell| \\hat f] - R_{n-m})^2]}_{\\text{training variance}} \\quad \\text{where }\\tilde \\ell = \\ell(Y_1, \\hat f(X_1))\n$$\n\n<!-- \n= \\underbrace{E[(\\hat R_n - E[\\hat R_n | \\hat f])^2]}_{\\text{estimation variance}}  + \\underbrace{E[(E[ \\hat R_n | \\hat f] - E[\\hat R_n])^2]}_{\\text{training variance}}\n-->\n\n## Estimating risk: leave-one-out cross-val. (LOO-CV)\n\n- **LOO-CV estimate:** $\\hat R_n = \\frac{1}{n} \\sum_{i=1}^n \\tilde R_i$, where:\n    - Remove the first observation and train ($\\hat f_1$), estimate $\\tilde R_1 = \\ell(Y_1, \\hat f_1(X_1))$\n    - Remove the second observation and train ($\\hat f_2$), estimate $\\tilde R_2 = \\ell(Y_2, \\hat f_2(X_2))$\n    - Wash, rinse, repeat...\n\n. . .\n\n[(Bias? Hint: if we train with $n-m$, we get $(R_n - R_{n-m})^2$...)]{.secondary}\n\n. . .\n\n- Bias is $(R_n - R_{n-1})^2$ -- quite small!\n- Variance is small too! Roughly: $E[( \\hat R_n - E[\\hat R_n])^2]\\approx \\frac{1}{n} E[(\\tilde R_1 - R_{n-1})^2]$\n    - assumption: ignoring $\\tilde R_j$, $\\tilde R_k$ covariance for $j\\neq k$\n    - reasonable if $n$ large enough that one data point doesn't influence $\\hat f$ too much\n\n. . .\n\n- **major downside:**  have to train $n$ models (*super* expensive!)\n\n# Picking up from last time...\n\n<!--\n## Cross Validation\n\nOne reason that $\\widehat{R}_n(\\widehat{f})$ is bad is that we are using the same data to pick $\\widehat{f}$ __AND__ to estimate $R_n$.\n\n\"Validation set\" fixes this, but holds out a particular, fixed block of data we pretend mimics the \"test data\"\n\nOur validation error $\\tilde{R}_m(\\widehat{f})$ is a random estimator.\\\n[(The split we use to divide our data into training versus validation is random.)]{.secondary}\n\n. . .\n\n\\\nA random estimator has _variance_.\nWe can reduce this variance by averageing over multiple splits.\n\n\n## Cross Validation Example\n\nWhat if we set aside one observation, say the first one $(y_1, x_1)$:\n\n- We estimate $\\widehat{f}^{(1)}$ without using the first observation.\n- We estimate $\\widetilde{R}_1(\\widehat{f}^{(1)})$ using the held-out first observation.\n\n$$\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.$$\n[(Why the notation $\\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )]{.secondary}\n\n---\n\nBut that was only one data point $(y_1, x_1)$. Why stop there?\n\nDo the same with $(y_2, x_2)$!\n\n$$\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.$$\nWe can keep doing this until we try it for every data point.\n\n. . .\n\n$$\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n \n(y_i - \\widehat{f}^{(i)}(x_i))^2$$\nThis is [__leave-one-out cross validation__]{.secondary}\n\n## Problems with LOO-CV\n\n🤮 Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.\n\n🤮 The training sets overlap. This is bad. \n\n- Usually, averaging reduces variance: $\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.$\n- But only if the variables are independent. If not, then $\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.$\n- Since the training sets overlap a lot, that covariance can be pretty big.\n    \n🤮 We have to estimate this model $n$ times.\n\n🎉 Bias is low because we used almost all the data to fit the model: $E[\\mbox{LOO-CV}] = R_{n-1}$ \n\n-->\n\n## K-fold CV\n\n$K$-fold cross validation finds a *decent* tradeoff between these options.\n\nThe idea of $K$-fold CV is \n\n1. Divide the data into $K$ groups. \n1. Leave one group out and train with the remaining $K-1$ groups.\n1. Test on the held-out group. Calculate an average risk over these $\\sim n/K$ data.\n1. Repeat for all $K$ groups.\n1. Average the average risks.\n\n## K-fold CV: illustration\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(0, 0, 0, 0))\nplot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = \"n\", yaxt = \"n\", xaxt = \"n\")\nrect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)\nrect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), \n     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)\npoints(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)\ntext(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c(\"1\", \"2\", \"3\", \"K\"), cex = 3, \n     col = red)\ntext(6, 4.5, \"Training data\", cex = 3, col = blue)\ntext(2, 1.5, \"Validation data\", cex = 3, col = red)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## K-fold CV vs. LOO CV\n\n- 🎉 Less costly than LOO CV (i.e., actually possible; only need to train $K$ times)\n- 💩 K-fold CV has higher sq. bias $(R_n - R_{n(1-1/K)})^2$ than LOO CV $(R_n - R_{n-1})^2$\n- a bit painful to compare variance...\n    - I hereby invoke the sacred incantation: \"this exercise is left to the reader\"\n\nThe overall risk $R_n$ depends on $n$.\n\n:::callout-tip\nIn practice, most people just default to using 5-fold or 10-fold. This is probably fine in most cases.\n:::\n\n\n\n## K-fold CV: Code\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"11-13|14-16|\"}\n#' @param data The full data set\n#' @param estimator Function. Has 1 argument (some data) and fits a model. \n#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions\n#' @param error_fun Function. Has one arg: the test data, with fits added.\n#' @param kfolds Integer. The number of folds.\nkfold_cv <- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  n <- nrow(data)\n  fold_labels <- sample(rep(1:kfolds, length.out = n))\n  errors <- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows <- fold_labels == fold\n    train <- data[!test_rows, ]\n    test <- data[test_rows, ]\n    current_model <- estimator(train)\n    test$.preds <- predictor(current_model, test)\n    errors[fold] <- error_fun(test)\n  }\n  mean(errors)\n}\n```\n:::\n\n\n\n\n. . . \n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-line-numbers=\"2-4|\"}\nsomedata <- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))\nest <- function(dataset) lm(z ~ ., data = dataset)\npred <- function(mod, dataset) predict(mod, newdata = dataset)\nerror_fun <- function(testdata) mutate(testdata, errs = (z - .preds)^2) |> pull(errs) |> mean()\nkfold_cv(somedata, est, pred, error_fun, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.8928686\n```\n\n\n:::\n:::\n\n\n\n\n\n<!--\n## Trick\n\n__For certain \"nice\" models__ of the form\n$$\\widehat{y}_i = h_i(\\mathbf{X})^\\top \\mathbf{y}$$\nfor some vector $h_i$, one can show \n\n$$\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-[\\boldsymbol h_i(x_i)]_{i})^2}.$$\n(Proof: tedious algebra. QED)\n\nwhich I wouldn't wish on my worst enemy, but might - in a fit of rage - assign as homework to belligerent students.) \n\n. . .\n\n* This trick means that you only have to fit the model once rather than $n$ times!\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_nice <- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )\n```\n:::\n\n\n\n\n-->\n\n# Next time...\n\nLOO tricks and \"hatvalues\"\n",
    "supporting": [
      "05-estimating-test-mse_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}