{
  "hash": "10917c80a4a5827e0058b39979078911",
  "result": {
    "markdown": "---\nlecture: \"05 Estimating test MSE\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 06 September 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n$$\n\n\n\n\n\n## Estimating prediction risk\n\nLast time, we saw\n\n$R_n(\\widehat{f}) = E[(Y-\\widehat{f}(X))^2]$\n\n\nprediction risk  =  $\\textrm{bias}^2$  +  variance  +  irreducible error \n\n\nWe argued that we want procedures that produce $\\widehat{f}$ with small $R_n$.\n\n\n> How do we estimate $R_n$?\n\n\n\n## Don't use training error\n\n\nThe training error in regression is\n\n$$\\widehat{R}_n(\\widehat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2$$\n\nHere, the $n$ is doubly used (annoying, but simple): $n$ observations to create $\\widehat{f}$ and $n$ terms in the sum.\n\n::: callout-important\nTraining error is a bad estimator for $R_n(\\widehat{f})$.\n:::\n\n\nSo we should [__never__]{.secondary} use it.\n\n## These all have the same $R^2$ and Training Error\n\n\n::: flex\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nans <- anscombe |>\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), \n               names_pattern = \"(.)(.)\")\nggplot(ans, aes(x, y)) + \n  geom_point(colour = orange, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = blue, linewidth = 2) +\n  facet_wrap(~set, labeller = label_both)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n::: w-50\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nans %>% \n  group_by(set) |> \n  summarise(\n    R2 = summary(lm(y ~ x))$r.sq, \n    train_error = mean((y - predict(lm(y ~ x)))^2)\n  ) |>\n  kableExtra::kable(digits = 2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> set </th>\n   <th style=\"text-align:right;\"> R2 </th>\n   <th style=\"text-align:right;\"> train_error </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 1.25 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 1.25 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 1.25 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4 </td>\n   <td style=\"text-align:right;\"> 0.67 </td>\n   <td style=\"text-align:right;\"> 1.25 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n:::\n:::\n\n## Adding \"junk\" predictors increases $R^2$ and decreases Training Error\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nn <- 100\np <- 10\nq <- 0:30\nx <- matrix(rnorm(n * (p + max(q))), nrow = n)\ny <- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)\n\nregress_on_junk <- function(q) {\n  x <- x[, 1:(p + q)]\n  mod <- lm(y ~ x)\n  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nmap(q, regress_on_junk) |> \n  list_rbind() |>\n  mutate(q = q) |>\n  pivot_longer(-q) |>\n  ggplot(aes(q, value, colour = name)) +\n  geom_line(linewidth = 2) + xlab(\"train_error\") +\n  scale_colour_manual(values = c(blue, orange), guide = \"none\") +\n  facet_wrap(~ name, scales = \"free_y\")\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Other things you can't use\n\nYou should not use `anova` \n\nor the $p$-values from the `lm` output for this purpose.\n\n> These things are to determine whether those _parameters_ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.\n\nNot the same as _\"are they useful for prediction = do they help me get smaller $R_n$?\"_\n\n\n\n## Risk of Risk\n\nWhile it's crummy, Training Error is an [estimator]{.secondary} of $R_n(\\hat{f})$\n\nRecall, $R_n(\\hat{f})$ is a _parameter_ (a property of the data distribution)\n\nSo we can ask \"is $\\widehat{R}(\\hat{f})$ a good estimator for $R_n(\\hat{f})$?\"\n\nBoth are just numbers, so perhaps a good way to measure is\n\n$$\nE[(R_n - \\widehat{R})^2] \n= \\cdots \n= (R_n - E[\\widehat{R}])^2 + \\Var{\\widehat{R}}\n$$\n\nChoices you make determine how good this is.\n\nWe can try to balance it's bias and variance...\n\n# Things you can use.\n\n\n## Held out sets\n\nOne option is to have a separate \"held out\" or \"validation set\".\n\n\nüëç Estimates the test error\n\nüëç Fast computationally\n\nü§Æ Estimate is random \n\nü§Æ Estimate has high variance (depends on 1 choice of split)\n\nü§Æ Estimate has some bias because we only used some of the data\n\n\n## Aside {background-color=\"#97D4E9\"}\n\nIn my experience, CS has particular definitions of \"training\", \"validation\", and \"test\" data.\n\nI think these are not quite the same as in Statistics.\n\n* [Test data]{.secondary} - Hypothetical data you don't get to see, ever. Infinite amounts drawn from the population.\n    * _Expected test error_ or _Risk_ is an expected value over this distribution. It's _not_ a sum over some data kept aside.\n* Sometimes I'll give you \"test data\". You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.\n* [Training data]{.secondary} - This is data that you get to touch.\n* [Validation set]{.secondary} - Often, we need to _choose models_. One way to do this is to split off some of your training data and pretend that it's like a \"Test Set\".\n\nWhen and how you split your training data can be very important. \n\n\n\n## Intuition for CV\n\n\nOne reason that $\\widehat{R}_n(\\widehat{f})$ is bad is that we are using the same data to pick $\\widehat{f}$ __AND__ to estimate $R_n$.\n\n\"Validation set\" fixes this, but holds out a particular, fixed block of data we pretend mimics the \"test data\"\n\n. . .\n\nWhat if we set aside one observation, say the first one $(y_1, x_1)$.\n\nWe estimate $\\widehat{f}^{(1)}$ without using the first observation.\n\nThen we test our prediction:\n\n$$\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.$$\n\n\n(why the notation $\\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )\n\n---\n\n## Keep going\n\nBut that was only one data point $(y_1, x_1)$. Why stop there?\n\nDo the same with $(y_2, x_2)$! Get an estimate $\\widehat{f}^{(2)}$ \nwithout using it, then\n\n$$\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.$$\n\nWe can keep doing this until we try it for every data point.\n\nAnd then average them! (Averages are good)\n\n\n$$\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n \n(y_i - \\widehat{f}^{(i)}(x_i))^2$$\n\n. . .\n\nThis is [__leave-one-out cross validation__]{.secondary}\n\n\n## Problems with LOO-CV\n\nü§Æ Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.\n\nü§Æ The training sets overlap. This is bad. \n\n- Usually, averaging reduces variance: $\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.$\n- But only if the variables are independent. If not, then $\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.$\n- Since the training sets overlap a lot, that covariance can be pretty big.\n    \nü§Æ We have to estimate this model $n$ times.\n\nüéâ Bias is low because we used almost all the data to fit the model: $E[\\mbox{LOO-CV}] = R_{n-1}$ \n\n  \n## K-fold CV\n\n::: flex\n::: w-50\nTo alleviate some of these problems, people usually use $K$-fold cross validation.\n\nThe idea of $K$-fold is \n\n1. Divide the data into $K$ groups. \n1. Leave a group out and estimate with the rest.\n1. Test on the held-out group. Calculate an average risk over these $\\sim n/K$ data.\n1. Repeat for all $K$ groups.\n1. Average the average risks.\n\n\n:::\n\n\n::: w-50\nüéâ Less overlap, smaller covariance.\n\nüéâ Larger hold-out sets, smaller variance.\n\nüéâ Less computations (only need to estimate $K$ times)\n\nü§Æ LOO-CV is (nearly) unbiased for $R_n$\n\nü§Æ K-fold CV is unbiased for $R_{n(1-1/K)}$\n\nThe risk depends on how much data you use to estimate the model. $R_n$ depends on $n$.\n\n:::\n:::\n\n\n\n## A picture\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(0, 0, 0, 0))\nplot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = \"n\", yaxt = \"n\", xaxt = \"n\")\nrect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)\nrect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), \n     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)\npoints(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)\ntext(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c(\"1\", \"2\", \"3\", \"K\"), cex = 3, \n     col = red)\ntext(6, 4.5, \"Training data\", cex = 3, col = blue)\ntext(2, 1.5, \"Validation data\", cex = 3, col = red)\n```\n\n::: {.cell-output-display}\n![](05-estimating-test-mse_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Code\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#' @param data The full data set\n#' @param estimator Function. Has 1 argument (some data) and fits a model. \n#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions\n#' @param error_fun Function. Has one arg: the test data, with fits added.\n#' @param kfolds Integer. The number of folds.\nkfold_cv <- function(data, estimator, predictor, \n                     error_fun, kfolds = 5) {\n  n <- nrow(data)\n  fold_labels <- sample(rep(1:kfolds, length.out = n))\n  errors <- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows <- fold_labels == fold\n    train <- data[!test_rows, ]\n    test <- data[test_rows, ]\n    current_model <- estimator(train)\n    test$.preds <- predictor(current_model, test)\n    errors[fold] <- error_fun(test)\n  }\n  mean(errors)\n}\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsomedata <- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))\nest <- function(dataset) lm(z ~ ., data = dataset)\npred <- function(mod, dataset) predict(mod, newdata = dataset)\nerror_fun <- function(testdata) mutate(testdata, errs = (z - .preds)^2) |> pull(errs) |> mean()\nkfold_cv(somedata, est, pred, error_fun, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9532271\n```\n:::\n:::\n\n\n\n## Trick\n\n__For a certain \"nice\" models__, one can show \n\n(after pages of tedious algebra which I wouldn't wish on my worst enemy, but might, in a fit of rage assign as homework to belligerent students) \n\n\n$$\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-h_{ii})^2} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\widehat{e}_i^2}{(1-h_{ii})^2}.$$\n\n* This trick means that you only have to fit the model once rather than $n$ times!\n\n* You still have to calculate this for each model!\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_nice <- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )\n```\n:::\n\n\n\"Nice\" requires:\n1. $\\widehat{y}_i = h_i(\\mathbf{X})^\\top \\mathbf{y}$ for some vector $h_i$\n2. $e^{(i)} = \\frac{\\widehat{e}_i}{(1-h_{ii})}$\n\n\n# Next time...\n\nMore tricks and what's up with the name \"hatvalues\"\n",
    "supporting": [
      "05-estimating-test-mse_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}