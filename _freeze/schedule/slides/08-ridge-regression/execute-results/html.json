{
  "hash": "4771cd464c87f258c8bee97d86506e12",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"08 Ridge regression\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 29 September 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## Recap\n\nGeneral idea:\n\n1. Pick a *model* (a family of distributions).\n1. Pick a *loss function* (measures how wrong predictions are).\n1. Design a *predictor/estimator* that minimizes the *prediction/estimation risk*.\n\nRisk includes [bias]{.secondary}, [variance]{.secondary}, and [irreducible error]{.secondary}.\n\nSo far, trade bias/variance via *variable selection* using a risk estimate.\n\n. . .\n\n**Today:** trade bias/variance via [*regularization*]{.secondary} \n\n- use **all** predictors\n- shrink $\\hat\\beta$ towards 0 (increase bias, reduce variance)\n\n\n$$\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n<!--\n## Regularization\n\n\n* Another way to control bias and variance is through [regularization]{.secondary} or\n[shrinkage]{.secondary}.  \n\n\n* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\n\n* I mean [ALL]{.tertiary}.\n\n* But, make your estimates of $\\beta$ \"smaller\"\n-->\n\n## OLS: bias and variance\n\nMinimize \n\n$$\\min_\\beta \\frac{1}{n}\\sum_i (y_i - x_i^T\\beta)^2 \\quad \\mbox{ subject to } \\quad \\beta \\in \\R^p$$\n\n. . .\n\nCan rewrite using matrix-vector notation:\n\n$$\\min_\\beta \\frac{1}{n}\\| \\y - \\X\\beta\\|_2^2\\quad \\mbox{ subject to } \\quad \\beta \\in \\R^p$$\n\n. . .\n\nSolve by setting derivative to 0: $\\bls = (\\X^\\top\\X)^{-1}\\X^\\top\\y$\n\n- unbiased: $\\E[\\bls] = \\E[\\E[\\bls \\mid \\X]] = \\E[(\\X^\\top\\X)^{-1}\\X^\\top\\X\\beta] = \\beta$\n- variance: $\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1}$ ??\n\n## Singular Value Decomposition (SVD) {background-color=\"#97D4E9\"}\n\nA rectangular $n \\times p$ matrix $X$ has *singular value decomposition* $X = U D V^\\top$\n\n- $U$ is $n\\times n$,  *orthonormal*: $U^\\top U = UU^\\top = I$\n- $V$ is $p\\times p$, *orthonormal*: $V^\\top V = VV^\\top = I$\n- $D$ is rectangular $n\\times p$, *diagonal and nonnegative* (*singular values*)\n\n. . .\n\ne.g. $n = 4, p = 2$, singular values $d_1, d_2$:\n\n$$D = \\left[\\begin{array} dd_1 & 0 \\\\ 0 & d_2 \\\\ 0 & 0 \\\\ 0 & 0 \\end{array}\\right] \\qquad D^\\top D = \\left[\\begin{array} dd_1^2 & 0 \\\\ 0 & d_2^2\\end{array}\\right].$$\n\n. . .\n\nIf $X$ has (almost) linearly dependent columns, some singular values in $D$ are (near) zero\n\n$X$ is *(nearly) rank-deficient* or *ill-conditioned*\n\n## The variance of OLS\n\nLet's use the SVD $X = UDV^T$ to examine the OLS variance $\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1}$:\n\n\n$$(\\X^\\top \\X)^{-1} = (VD^\\top U^\\top UDV^\\top)^{-1} = V (D^\\top D)^{-1}V^\\top =  V\\left[\\begin{array}dd_1^{-2} & 0 & 0\\\\ 0 & \\ddots & 0 \\\\ 0 & 0 & d_p^{-2}\\end{array}\\right]V^\\top$$ \n\n. . .\n\n[Multicollinearity:]{.secondary} a linear combination of variables is nearly equal to another variable. \n\n* so $\\X$ is ill-conditioned\n* so some singular values $d_j\\approx 0$\n* so $d^{-2}_j$ is large\n* so $\\bls$ has large, unstable values; i.e., **large variance**\n\n## How to stop large, unstable values of $\\widehat{\\beta}$?\n\nIdea: **constrain** the values of $\\beta$ to be small. For $s > 0$:\n$$\n\\minimize_\\beta \\underbrace{\\frac{1}{n}\\|\\y - \\X\\beta\\|_2^2}_{\\text{objective function}} \\quad \\st \\underbrace{\\|\\beta\\|_2^2 < s}_{\\text{constraint}}.\n$$\n\nRecall, for a vector $\\beta \\in \\R^p$\n\n$\\ell_2$-norm\n: $\\|\\beta\\|_2 = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_p^2} = \\left(\\sum_{j=1}^p |\\beta_j|^2\\right)^{1/2}$\n(so $\\|\\beta\\|_2^2 = \\sum_j \\beta_j^2$)\n\n. . .\n\nCompare this to ordinary least squares:\n\n$$\n\\minimize_\\beta \\underbrace{\\frac{1}{n}\\|\\y-\\X\\beta\\|_2^2}_{\\text{same objective}} \n\\quad \\st \\underbrace{\\beta \\in \\R^p}_{\\textbf{no constraint}}\n$$\n\n## Geometry of constrained regression (contours)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(mvtnorm)\nnorm_ball <- function(q = 1, len = 1000) {\n  tg <- seq(0, 2 * pi, length = len)\n  out <- tibble(x = cos(tg), b = (1 - abs(x)^q)^(1 / q), bm = -b) |>\n    pivot_longer(-x, values_to = \"y\")\n  out$lab <- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipse_data <- function(\n  n = 75, xlim = c(-2, 3), ylim = c(-2, 3),\n  mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  expand_grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)) |>\n    rowwise() |>\n    mutate(z = dmvnorm(c(x, y), mean, Sigma))\n}\n\nlballmax <- function(ed, q = 1, tol = 1e-6, niter = 20) {\n  ed <- filter(ed, x > 0, y > 0)\n  feasible <- (ed$x^q + ed$y^q)^(1 / q) <= 1\n  best <- ed[feasible, ]\n  best[which.max(best$z), ]\n}\n\n\nnb <- norm_ball(2)\ned <- ellipse_data()\nbols <- data.frame(x = 1, y = 1)\nbhat <- lballmax(ed, 2)\nggplot(nb, aes(x, y)) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal() +\n  geom_label(\n    data = bols,\n    mapping = aes(label = bquote(\"hat(beta)[ols]\")),\n    parse = TRUE, \n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  theme_bw(base_size = 24) +\n  geom_label(\n    data = bhat,\n    mapping = aes(label = bquote(\"hat(beta)[s]^R\")),\n    parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/plotting-functions-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Ridge regression\n\nAn equivalent way to write\n\n$$\\brt = \\argmin_{\\beta} \\frac{1}{n}\\|\\y - \\X\\beta\\|_2^2 \\st  || \\beta ||_2^2 \\leq s$$\n\nis as a [*regularized*]{.secondary} (or *penalized*) optimization with *regularization weight* $\\lambda$:\n\n$$\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\|\\y-\\X\\beta\\|_2^2 + \\lambda || \\beta ||_2^2.$$\n\nFor every $\\lambda$ there is a unique $s$ (and vice versa) that makes $\\brt = \\brl$. We will work with $\\lambda$.\n\n## Ridge regression\n\n$$\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\|\\y-\\X\\beta\\|_2^2 + \\lambda || \\beta ||_2^2$$\n\nObserve:\n\n* $\\lambda = 0$ makes $\\brl = \\bls$\n* $\\lambda \\to\\infty$ makes $\\brl \\to 0$\n* Any $0 < \\lambda < \\infty$ penalizes larger values of $\\beta$, effectively shrinking them.\n* $\\lambda$ is a **tuning parameter:** we need to pick it\n\n<!--\n\n## Visualizing ridge regression (2 coefficients)\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nb <- c(1, 1)\nn <- 1000\nlams <- c(1, 5, 10)\nols_loss <- function(b1, b2) colMeans((y - X %*% rbind(b1, b2))^2) / 2\npen <- function(b1, b2, lambda = 1) lambda * (b1^2 + b2^2) / 2\ngr <- expand_grid(\n  b1 = seq(b[1] - 0.5, b[1] + 0.5, length.out = 100),\n  b2 = seq(b[2] - 0.5, b[2] + 0.5, length.out = 100)\n)\n\nX <- mvtnorm::rmvnorm(n, c(0, 0), sigma = matrix(c(1, .3, .3, .5), nrow = 2))\ny <- drop(X %*% b + rnorm(n))\n\nbols <- coef(lm(y ~ X - 1))\nbridge <- coef(MASS::lm.ridge(y ~ X - 1, lambda = lams * sqrt(n)))\n\npenalties <- lams |>\n  set_names(~ paste(\"lam =\", .)) |>\n  map(~ pen(gr$b1, gr$b2, .x)) |>\n  as_tibble()\ngr <- gr |>\n  mutate(loss = ols_loss(b1, b2)) |>\n  bind_cols(penalties)\n\ng1 <- ggplot(gr, aes(b1, b2)) +\n  geom_raster(aes(fill = loss)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = 0.5))\n\ng2 <- gr |>\n    pivot_longer(starts_with(\"lam\")) |>\n    mutate(name = factor(name, levels = paste(\"lam =\", lams))) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"penalty\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ng3 <- gr |> \n  mutate(across(starts_with(\"lam\"), ~ loss + .x)) |>\n  pivot_longer(starts_with(\"lam\")) |>\n  mutate(name = factor(name, levels = paste(\"lam =\", lams))) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"loss + pen\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ncowplot::plot_grid(g1, g2, g3, rel_widths = c(2, 1, 1), nrow = 1)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center'}\n:::\n:::\n\n\n\n\n## The effect on the estimates\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngr |> \n  mutate(z = ols_loss(b1, b2) + max(lams) * pen(b1, b2)) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = z)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = tibble(\n    b1 = c(bols[1], bridge[,1]),\n    b2 = c(bols[2], bridge[,2]),\n    estimate = factor(c(\"ols\", paste0(\"ridge = \", lams)), \n                      levels = c(\"ols\", paste0(\"ridge = \", lams)))\n  ),\n  aes(shape = estimate), size = 3) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2]), colour = orange, size = 4)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center'}\n:::\n:::\n\n\n\n\n-->\n\n## Example data\n\n`prostate` data from [ESL]\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(prostate, package = \"ElemStatLearn\")\nprostate |> as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 97 × 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    <dbl>   <dbl> <int>  <dbl> <int> <dbl>   <int> <int>  <dbl> <lgl>\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ℹ 87 more rows\n```\n\n\n:::\n:::\n\n\n\n\n::: notes\n\nUse `lpsa` as response.\n\n:::\n\n\n## Ridge regression path\n\nWe can look at coefficients as we vary $\\lambda$ (a \"path\" or \"coefficient trace\")\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nY <- prostate$lpsa\nX <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, xvar = \"lambda\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/process-prostate-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n**Model selection:** choose some $\\lambda$ (a vertical line on this plot)\n\n## Ridge regression: closed-form\n\nRidge regression has a closed-form solution like OLS (set the derivative in $\\beta$ to 0):\n\n$$\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\nCompare to OLS:\n\n$$\\bls = (\\X^\\top \\X)^{-1}\\X^\\top \\y$$\n\n* What does the $+\\lambda I$ do?\n\n* What about bias and variance?\n\n* Is $\\brl$ faster/slower/neither to compute? \n\n* Is $\\brl$ more/less numerically stable to compute?\n\n## Ridge regression: bias and variance\n\nRidge regression has a closed-form solution like OLS (set the derivative in $\\beta$ to 0):\n\n$$\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n\n* bias: $\\E[\\brl | \\X] = (\\X^\\top\\X + \\lambda I)^{-1}\\X^\\top \\X\\beta \\neq \\beta$\n\n* variance: $\\Var{\\brl \\mid \\X} = \\sigma^2 (\\X^\\top\\X + \\lambda I)^{-1}\\X^\\top \\X(\\X^\\top\\X + \\lambda I)^{-1}$\n\n. . .\n\nUsing the SVD $\\X = UDV^T$, $\\Var{\\brl \\mid \\X} = \\sigma^2 VG V^\\top$ where $G$ is diagonal with entries\n\n$$ g_j = \\frac{d_j^2}{(d_j^2 + \\lambda)^2} \\qquad \\left(\\text{compare to OLS:} \\quad g_j = \\frac{1}{d_j^{2}} \\right)$$\n\nMitigates the issue of [multicollinearity]{.secondary} (ill-conditioned $\\X$)!\n\n<!--\n* This is easy to calculate in `R` for any $\\lambda$.\n\n* However, computations and interpretation are simplified if we examine the \n[Singular Value Decomposition]{.secondary} of $\\X = \\mathbf{UDV}^\\top$.\n\n* Recall: any matrix has an SVD.\n\n* Here $\\mathbf{D}$ is diagonal and $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal: $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$.\n\n\n## Solving the minization\n\n$$\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n* Note that $\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top$.\n\n\n* Then,\n\n\n$$\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.$$\n\n* For computations, now we only need to invert $\\mathbf{D}$.\n\n\n## Comparing with OLS\n\n\n* $\\mathbf{D}$ is a diagonal matrix\n\n$$\\bls = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y$$\n\n$$\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.$$\n\n\n* Notice that $\\bls$ depends on $d_j/d_j^2$ while $\\brl$ depends on $d_j/(d_j^2 + \\lambda)$.\n\n* Ridge regression makes the coefficients smaller relative to OLS.\n\n* But if $\\X$ has small singular values, ridge regression compensates with $\\lambda$ in the denominator.\n\n\n\n## Ridge regression and multicollinearity\n\n[Multicollinearity:]{.secondary} a linear combination of predictor variables is nearly equal to another predictor variable. \n\nSome comments:\n\n* A better phrase: $\\X$ is ill-conditioned\n\n* AKA \"(numerically) rank-deficient\".\n\n* $\\X = \\mathbf{U D V}^\\top$ ill-conditioned $\\Longleftrightarrow$ some elements of $\\mathbf{D} \\approx 0$\n\n* $\\bls= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y$, so small entries of $\\mathbf{D}$ $\\Longleftrightarrow$ huge elements of $\\mathbf{D}^{-1}$\n\n* Means huge variance: $\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top$\n\n-->\n\n## Tuning the regularization weight $\\lambda$\n\nUse **cross-validation** (it's built into `glmnet` if you use `cv.glmnet`!)\n\n`plot` shows mean CV estimate $\\pm$ standard error\n\nLeft line is minimum risk estimate; right is the largest $\\lambda$ within 1$\\sigma$\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge <- cv.glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, main = \"Ridge\")\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Ridge regression: summary\n\n\nRidge regression addresses [multicollinearity]{.secondary} by preventing division by near-zero numbers\n\nConclusion\n: $\\bls = (\\X^{\\top}\\X)^{-1}\\X^\\top \\y$ can be unstable, while $\\brl=(\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top\\y$ is not.\n\nAside\n: Engineering approach to solving linear systems is to always do this with small $\\lambda$. The thinking is about the numerics rather than the statistics.\n\n### Which $\\lambda$ to use?\n\nComputational\n: Use CV and pick the $\\lambda$ that makes this smallest (maybe within $1\\sigma$).\n\nIntuition (bias)\n: As $\\lambda\\rightarrow\\infty$, bias ⬆\n\nIntuition (variance)\n: As $\\lambda\\rightarrow\\infty$, variance ⬇\n\n## Regularization for variable selection?\n\nRidge regression is a *regularized* approach to prediction.\n\n* nice bias/variance tradeoff\n* closed-form / easy-to-compute solution\n* no predictor variable selection.\n    * (NB: picking a $\\lambda$ is still *model selection*)\n\nIs there a regularization approach to variable selection?\n\n. . .\n\ne.g., best (in-sample) linear regression model of size $s$:\n: $\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ (\\text{\\# of nonzero $\\beta_j$}) \\leq s$\n\nThat is a *super nasty* optimization. What to do?...\n\n<!--\n## Can we get the best of both worlds?\n\nTo recap:\n\n* Deciding which predictors to include, adding quadratic terms, or interactions is [model selection]{.secondary} (more precisely variable selection within a linear model).\n\n* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  \n\n* If the LM is **true**, \n    1. OLS is unbiased, but Variance depends on $\\mathbf{D}^{-2}$. Can be big.\n    2. Ridge is biased (can you find the bias?). But Variance is smaller than OLS.\n\n* Ridge regression does not perform variable selection.\n\n* But [picking]{.hand} $\\lambda=3.7$ and thereby [deciding]{.hand} to predict with $\\widehat{\\beta}^R_{3.7}$ is [model selection]{.secondary}.\n\n\n\n## Can we get the best of both worlds?\n\nRidge regression \n: $\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_2^2 \\leq s$ \n\nBest (in-sample) linear regression model of size $s$\n: $\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_0 \\leq s$\n\n\n$||\\beta||_0$ is the number of nonzero elements in $\\beta$\n\nFinding the best in-sample linear model (of size $s$, among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\n\nRidge regression is convex (easy to solve), but doesn't do __variable__ selection\n\nCan we somehow \"interpolate\" to get both?\n\nNote: selecting $\\lambda$ is still __model__ selection, but we've included __all__ the variables.\n-->\n\n\n# Next time...\n\nThe lasso, interpolating variable selection and model selection\n",
    "supporting": [
      "08-ridge-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}