{
  "hash": "0d294af11c16282c26a79293bba589a5",
  "result": {
    "markdown": "---\nlecture: \"08 Ridge regression\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 27 September 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n\n\n## Recap\n\nSo far, we have emphasized __model selection__ as\n\n[Decide which predictors we would like to use in our linear model]{.hand}\n\nOr similarly:\n\n[Decide which of a few linear models to use]{.hand}\n\nTo do this, we used a risk estimate, and chose the \"model\" with the lowest estimate\n\n. . .\n\nMoving forward, we need to generalize this to\n\n[Decide which of possibly infinite prediction functions]{.hand} $f\\in\\mathcal{F}$ [to use]{.hand}\n\nThankfully, this isn't really any different. We still use those same risk estimates.\n\n\n[Remember:]{.secondary} We were choosing models that balance bias and variance (and hence have low prediction risk).\n\n$$\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n## Regularization\n\n\n* Another way to control bias and variance is through [regularization]{.secondary} or\n[shrinkage]{.secondary}.  \n\n\n* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\n\n* I mean [ALL]{.tertiary}.\n\n* But, make your estimates of $\\beta$ \"smaller\"\n\n\n\n## Brief aside on optimization {background-color=\"#97D4E9\"}\n\n* An optimization problem has 2 components:\n\n    1. The \"Objective function\": e.g. $\\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2$.\n    2. The \"constraint\": e.g. \"fewer than 5 non-zero entries in $\\beta$\".\n    \n* A constrained minimization problem is written\n\n\n$$\\min_\\beta f(\\beta)\\;\\; \\mbox{ subject to }\\;\\; C(\\beta)$$\n\n* $f(\\beta)$ is the objective function\n* $C(\\beta)$ is the constraint\n\n\n## Ridge regression (constrained version)\n\nOne way to do this for regression is to solve (say):\n$$\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\sum_j \\beta^2_j < s\n$$\nfor some $s>0$.\n\n* This is called \"ridge regression\".\n* Call the [minimizer]{.secondary} of this problem $\\brt$\n\n. . .\n\nCompare this to ordinary least squares:\n\n$$\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 \n\\quad \\st \\beta \\in \\R^p\n$$\n\n\n\n## Geometry of ridge regression (contours)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(mvtnorm)\nnorm_ball <- function(q = 1, len = 1000) {\n  tg <- seq(0, 2 * pi, length = len)\n  out <- tibble(x = cos(tg), b = (1 - abs(x)^q)^(1 / q), bm = -b) |>\n    pivot_longer(-x, values_to = \"y\")\n  out$lab <- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipse_data <- function(\n  n = 75, xlim = c(-2, 3), ylim = c(-2, 3),\n  mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  expand_grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)) |>\n    rowwise() |>\n    mutate(z = dmvnorm(c(x, y), mean, Sigma))\n}\n\nlballmax <- function(ed, q = 1, tol = 1e-6, niter = 20) {\n  ed <- filter(ed, x > 0, y > 0)\n  feasible <- (ed$x^q + ed$y^q)^(1 / q) <= 1\n  best <- ed[feasible, ]\n  best[which.max(best$z), ]\n}\n\n\nnb <- norm_ball(2)\ned <- ellipse_data()\nbols <- data.frame(x = 1, y = 1)\nbhat <- lballmax(ed, 2)\nggplot(nb, aes(x, y)) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal() +\n  geom_label(\n    data = bols,\n    mapping = aes(label = bquote(\"hat(beta)[ols]\")),\n    parse = TRUE, \n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  theme_bw(base_size = 24) +\n  geom_label(\n    data = bhat,\n    mapping = aes(label = bquote(\"hat(beta)[s]^R\")),\n    parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/plotting-functions-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Brief aside on norms\n\nRecall, for a vector $z \\in \\R^p$\n\n\n$$\\snorm{z}_2 = \\sqrt{z_1^2 + z_2^2 + \\cdots + z^2_p} = \\sqrt{\\sum_{j=1}^p z_j^2}$$\n\n\nSo, \n\n$$\\snorm{z}^2_2 = z_1^2 + z_2^2 + \\cdots + z^2_p = \\sum_{j=1}^p z_j^2.$$\n\n\n## Other norms we should remember:\n\n$\\ell_q$-norm\n: $\\left(\\sum_{j=1}^p |z_j|^q\\right)^{1/q}$\n\n$\\ell_1$-norm (special case)\n: $\\sum_{j=1}^p |z_j|$\n\n$\\ell_0$-norm\n: $\\sum_{j=1}^p I(z_j \\neq 0 ) = \\lvert \\{j : z_j \\neq 0 \\}\\rvert$\n\n$\\ell_\\infty$-norm\n: $\\max_{1\\leq j \\leq p} |z_j|$\n\n::: aside\nRecall what a norm is: <https://en.wikipedia.org/wiki/Norm_(mathematics)>\n:::\n\n\n## Ridge regression\n\nAn equivalent way to write\n\n$$\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2$$\n\n\nis in the [Lagrangian]{.secondary} form\n\n\n$$\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2.$$\n\n\n\n\nFor every $\\lambda$ there is a unique $s$ (and vice versa) that makes \n\n$$\\brt = \\brl$$\n\n## Ridge regression\n\n$\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2$\n\n$\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2$\n\nObserve:\n\n* $\\lambda = 0$ (or $s = \\infty$) makes $\\brl = \\bls$\n* Any $\\lambda > 0$ (or $s <\\infty$)  penalizes larger values of $\\beta$, effectively shrinking them.\n\n\n$\\lambda$ and $s$ are known as [tuning parameters]{.secondary}\n\n\n## Visualizing ridge regression (2 coefficients)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nb <- c(1, 1)\nn <- 1000\nlams <- c(1, 5, 10)\nols_loss <- function(b1, b2) colMeans((y - X %*% rbind(b1, b2))^2) / 2\npen <- function(b1, b2, lambda = 1) lambda * (b1^2 + b2^2) / 2\ngr <- expand_grid(\n  b1 = seq(b[1] - 0.5, b[1] + 0.5, length.out = 100),\n  b2 = seq(b[2] - 0.5, b[2] + 0.5, length.out = 100)\n)\n\nX <- mvtnorm::rmvnorm(n, c(0, 0), sigma = matrix(c(1, .3, .3, .5), nrow = 2))\ny <- drop(X %*% b + rnorm(n))\n\nbols <- coef(lm(y ~ X - 1))\nbridge <- coef(MASS::lm.ridge(y ~ X - 1, lambda = lams * sqrt(n)))\n\npenalties <- lams |>\n  set_names(~ paste(\"lam =\", .)) |>\n  map(~ pen(gr$b1, gr$b2, .x)) |>\n  as_tibble()\ngr <- gr |>\n  mutate(loss = ols_loss(b1, b2)) |>\n  bind_cols(penalties)\n\ng1 <- ggplot(gr, aes(b1, b2)) +\n  geom_raster(aes(fill = loss)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = 0.5))\n\ng2 <- gr |>\n    pivot_longer(starts_with(\"lam\")) |>\n    mutate(name = factor(name, levels = paste(\"lam =\", lams))) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"penalty\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ng3 <- gr |> \n  mutate(across(starts_with(\"lam\"), ~ loss + .x)) |>\n  pivot_longer(starts_with(\"lam\")) |>\n  mutate(name = factor(name, levels = paste(\"lam =\", lams))) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"loss + pen\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ncowplot::plot_grid(g1, g2, g3, rel_widths = c(2, 1, 1), nrow = 1)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-1-1.png){fig-align='center'}\n:::\n:::\n\n\n## The effect on the estimates\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngr |> \n  mutate(z = ols_loss(b1, b2) + max(lams) * pen(b1, b2)) |>\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = z)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = tibble(\n    b1 = c(bols[1], bridge[,1]),\n    b2 = c(bols[2], bridge[,2]),\n    estimate = factor(c(\"ols\", paste0(\"ridge = \", lams)), \n                      levels = c(\"ols\", paste0(\"ridge = \", lams)))\n  ),\n  aes(shape = estimate), size = 3) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2]), colour = orange, size = 4)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-2-1.png){fig-align='center'}\n:::\n:::\n\n\n\n## Example data\n\n`prostate` data from [ESL]\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(prostate, package = \"ElemStatLearn\")\nprostate |> as_tibble()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 97 × 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    <dbl>   <dbl> <int>  <dbl> <int> <dbl>   <int> <int>  <dbl> <lgl>\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ℹ 87 more rows\n```\n:::\n:::\n\n\n::: notes\n\nUse `lpsa` as response.\n\n:::\n\n\n## Ridge regression path\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nY <- prostate$lpsa\nX <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\n```\n:::\n\n\n<br>\n\n::: flex\n::: w-60\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ridge, xvar = \"lambda\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](08-ridge-regression_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n::: w-35\n\nModel selection here: \n\n* means [choose]{.secondary} some $\\lambda$ \n\n* A value of $\\lambda$ is a vertical line.\n\n* This graphic is a \"path\" or \"coefficient trace\"\n\n* Coefficients for varying $\\lambda$\n:::\n:::\n\n\n## Solving the minimization\n\n* One nice thing about ridge regression is that it has a closed-form solution (like OLS)\n\n\n$$\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n* This is easy to calculate in `R` for any $\\lambda$.\n\n* However, computations and interpretation are simplified if we examine the \n[Singular Value Decomposition]{.secondary} of $\\X = \\mathbf{UDV}^\\top$.\n\n* Recall: any matrix has an SVD.\n\n* Here $\\mathbf{D}$ is diagonal and $\\mathbf{U}$ and $\\mathbf{V}$ are orthonormal: $\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}$.\n\n## Solving the minization\n\n$$\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$$\n\n* Note that $\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top$.\n\n\n* Then,\n\n\n$$\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.$$\n\n* For computations, now we only need to invert $\\mathbf{D}$.\n\n\n## Comparing with OLS\n\n\n* $\\mathbf{D}$ is a diagonal matrix\n\n$$\\bls = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y$$\n\n$$\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.$$\n\n\n* Notice that $\\bls$ depends on $d_j/d_j^2$ while $\\brl$ depends on $d_j/(d_j^2 + \\lambda)$.\n\n* Ridge regression makes the coefficients smaller relative to OLS.\n\n* But if $\\X$ has small singular values, ridge regression compensates with $\\lambda$ in the denominator.\n\n\n\n## Ridge regression and multicollinearity\n\n[Multicollinearity:]{.secondary} a linear combination of predictor variables is nearly equal to another predictor variable. \n\nSome comments:\n\n* A better phrase: $\\X$ is ill-conditioned\n\n* AKA \"(numerically) rank-deficient\".\n\n* $\\X = \\mathbf{U D V}^\\top$ ill-conditioned $\\Longleftrightarrow$ some elements of $\\mathbf{D} \\approx 0$\n\n* $\\bls= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y$, so small entries of $\\mathbf{D}$ $\\Longleftrightarrow$ huge elements of $\\mathbf{D}^{-1}$\n\n* Means huge variance: $\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top$\n\n\n## Ridge regression and ill-posed $\\X$\n\n\nRidge Regression fixes this problem by preventing the division by a near-zero number\n\nConclusion\n: $(\\X^{\\top}\\X)^{-1}$ can be really unstable, while $(\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}$ is not.\n\nAside\n: Engineering approach to solving linear systems is to always do this with small $\\lambda$. The thinking is about the numerics rather than the statistics.\n\n### Which $\\lambda$ to use?\n\nComputational\n: Use CV and pick the $\\lambda$ that makes this smallest.\n\nIntuition (bias)\n: As $\\lambda\\rightarrow\\infty$, bias ⬆\n\nIntuition (variance)\n: As $\\lambda\\rightarrow\\infty$, variance ⬇\n\nYou should think about why.\n\n\n\n## Can we get the best of both worlds?\n\nTo recap:\n\n* Deciding which predictors to include, adding quadratic terms, or interactions is [model selection]{.secondary} (more precisely variable selection within a linear model).\n\n* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  \n\n* If the LM is **true**, \n    1. OLS is unbiased, but Variance depends on $\\mathbf{D}^{-2}$. Can be big.\n    2. Ridge is biased (can you find the bias?). But Variance is smaller than OLS.\n\n* Ridge regression does not perform variable selection.\n\n* But [picking]{.hand} $\\lambda=3.7$ and thereby [deciding]{.hand} to predict with $\\widehat{\\beta}^R_{3.7}$ is [model selection]{.secondary}.\n\n\n\n## Can we get the best of both worlds?\n\nRidge regression \n: $\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_2^2 \\leq s$ \n\nBest (in-sample) linear regression model of size $s$\n: $\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_0 \\leq s$\n\n\n$||\\beta||_0$ is the number of nonzero elements in $\\beta$\n\nFinding the best in-sample linear model (of size $s$, among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\n\nRidge regression is convex (easy to solve), but doesn't do __variable__ selection\n\nCan we somehow \"interpolate\" to get both?\n\n\nNote: selecting $\\lambda$ is still __model__ selection, but we've included __all__ the variables.\n\n\n# Next time...\n\nThe lasso, interpolating variable selection and model selection\n",
    "supporting": [
      "08-ridge-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}