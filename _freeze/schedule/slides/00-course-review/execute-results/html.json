{
  "hash": "b6ded9b0a3f77a81a411beb4157a71d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"00 Review and bonus clickers\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 04 December 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n<!--\n## Office hours and such \n\n(also in the Canvas announcement)\n\n1. Yes, there is lab as usual tomorrow. (But no Zoom OH)\n1. Homework 5 due tonight.\n1. Office hours next week:\n    - Monday 5-6pm on Zoom (use the link on Canvas, TA)\n    - Tuesday 3-4:30pm in ESB 4192 (me)\n    - Wednesday 10-11am in ESB 4192 (TA)\n    - Thursday 10-11am in ESB 3174 (me)\n    - Friday 2-3pm on Zoom (use the link on Canvas, TA)\n\n[Final Exam on Monday, December 18 from 12-2pm]{.secondary}\n\n## Grades etc.\n\n- Effort score done as soon as possible\n- HW 5, aiming for Friday Dec 15, but no guarantees\n- Clickers and Labs should be done soon\n- The Final is autograded\n- It usually takes me a few days to get the final grades in\n- Generally, no curves, no roundin' up, etc.\n-->\n\n##\n\n![](https://i.vas3k.blog/7vx.jpg){fig-align=\"center\"}\n\n\n## Big picture\n\n- What is a model?\n- How do we evaluate models?\n- How do we decide which models to use?\n- How do we improve models?\n\n## General stuff\n\n- Linear algebra (SVD, matrix multiplication, matrix properties, etc.)\n- Optimization (derivitive + set to 0, gradient descent, Newton's method, etc.)\n- Probability (conditional probability, Bayes rule, etc.)\n- Statistics (likelihood, MLE, confidence intervals, etc.)\n\n\n## 1. Model selection\n\n- What is a statistical model?\n- What is the difference between training error, test error, and risk?\n- What is the (theoretical) predictor with the lowest risk for regression? For classification?\n  - Why can we not obtain these predictor in practice?\n- What is the bias-variance tradeoff?\n- What is the goal of model selection?\n- What is the difference between AIC / BIC / CV / Held-out validation?\n\n## 2. Regression\n\n- What do we mean by regression?\n- What is regularization?\n  - What is the goal of regularization?\n  - What is the difference between L1 and L2 regularization?\n- How do we do non-linear regression?\n  - What are splies?\n  - What are kernel smoothers?\n  - What is knn?\n  - What are decision trees?\n- What is the curse of dimensionality?\n\n## 3. Classification\n\n- What is classification?\n  - What is the difference between a generative versus descriminative classification model?\n- What is a decision boundary? When is it linear?\n- Compare logistic regression to discriminant analysis.\n  - What are the assumptions made by each method?\n  - What are the shapes of the decision boundaries?\n- What are the positives and negatives of trees?\n- How do we measure performance of classification beyond 0-1 loss?\n  - What is a probabilistic notion of classification performance?\n  - How do we measure the goodness of uncertainty estimates?\n\n\n## 4. Modern methods\n\n- What is the bootstrap?\n- What is the difference between bagging and boosting?\n  - When do we prefer one over the other (think bias-variance tradeoff)?\n- What is the difference between random forests and bagging?\n- How do we understand Neural Networks?\n  - What is the difference between neural networks and other non-linear methods?\n  - What is the difference between increasing width and increasing depth? (Number of parameters, expressivity)\n  - How do we train neural networks? What is backpropagation?\n  - Why are we surprised that neural networks \"work\"?\n\n## 5. Unsupervised learning\n\n- What is unsupervised learning?\n- What is dimensionality reduction?\n  - What is the difference between PCA vs KPCA?\n  - What do the principle components represent?\n- What is clustering?\n  - What is the difference between k-means and hierarchical clustering?\n\n# Pause for course evals\n\nCurrently at 51/144.\n\n# A few clicker questions\n\n## \n\n### The singular value decomposition applies to any matrix.\n\n<hr>\n\na. True\nb. False\n\n##\n\n### Which of the following is true about the training error?\n\n<hr>\n\na. It will decrease as we add interaction terms\nb. It will decrease as we add more training data\nc. It will decrease as we add more regularization\nd. It will decrease as we remove useless predictors\n\n##\n\n(Multiple answer)\n\n### Which of the following is an advantage to using LOO-CV over k-fold CV?\n \n<hr>\n \na. The bias of LOO-CV, as a risk estimator, is lower than that of k-fold CV.\nb. The variance of LOO-CV, as a risk estimator, is lower than that of k-fold CV. \nc. It can be computed more quickly than k-fold CV for kernel smoothers.\nc. It can be computed more quickly than k-fold CV for ridge regression.\n\n##\n\n(Multiple answer)\n\n### Which of the following reduce the bias of linear regression?\n\n<hr>\n\na. Adding a ridge penalty\nb. Adding a lasso penalty\nc. Adding interaction terms / nonlinear basis functions\nd. Adding more training data\n\n<!-- ##  -->\n\n<!-- (Multiple answer) -->\n\n<!-- ### Which of the following are true about the Bayes' classifier? -->\n\n<!-- <hr> -->\n\n<!-- a. It is the classifier with the lowest 0-1 risk -->\n<!-- b. It depends on $P(Y=1)$ and $P(Y=0)$ -->\n<!-- c. It has a linear classification boundary -->\n<!-- d. It cannot be estimated with a finite amount of data -->\n\n##\n\n(multiple answer)\n\n### The decision boundary for classification problems...\n\n<hr>\n\na. Is the set of points where $P(Y=1|X) = P(Y=0|X)$\nb. Is the set of points where $P(Y=1|X) / P(Y=0|X) = P(Y=1) / P(Y=0)$\nc. Is the set of points where $P(Y=1|X) / P(Y=0|X) = P(Y=0) / P(Y=1)$\nd. Is linear for all discriminant analysis predictors\n\n\n##\n\n(multiple answer)\n\n### Which of the following properties of boosting are true?\n\n<hr>\n\na. The risk can be estimated without a holdout set\nb. The component predictors can be trained in parallel\nc. The predictive uncertainty can be estimated by the variance of the predictors\nd. The bias of the ensemble is lower than the bias of the component predictors\ne. The variance of the ensemble is lower than the variance of the component predictors\n\n##\n\n(multiple answer)\n\n### Which of the following statements are true about PCA and KPCA?\n\n<hr>\n\na. PCA requires specifying the number of principled components, while KPCA does not\nb. KPCA requires the data to be centered, while PCA does not\nc. PCA is a linear method, while KPCA is a non-linear method\nd. After performing KPCA, the principle components can be used to reduce the dimensionality of new (previously unseen) test data\n\n\n<!--\n### Which would you prefer to hear about (briefly)?\n\n<hr>\n\na. Daniel's thoughts on stuff (grad school / undergrad school / life / etc.)\nb. [Epidemiological forecasting](https://dajmcdon.github.io/epi-modelling-calgary-2023/)\nc. [Software for epidemiological forecasting](https://dajmcdon.github.io/cdc-tooling-packages/)\nd. [Analysis of classical music](https://dajmcdon.github.io/chopin-2022-slides/)\ne. [Economic forecasting models](https://dajmcdon.github.io/dsges/)\n-->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}