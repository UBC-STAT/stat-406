{
  "hash": "e454d1dd9817f0cbebfff1fedb411787",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"18 The bootstrap\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 29 October 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Uncertainty with one sample?\n\nWe get data $X_{1:N},Y_{1:N}$. We compute *something* from the data $\\hat\\theta = f(X_{1:N},Y_{1:N})$.\n\nThat $\\ \\hat\\theta$ is a *point estimate* (regression coeffs, median of $Y$, risk estimate...)\n\nHow do we understand the *uncertainty* in $\\hat\\theta\\approx \\theta$ due to *randomness* in $X_{1:N}, Y_{1:N}$?\n\n. . .\n\n*If we know* the **sampling distribution** of $\\hat\\theta$, we can use variance / confidence intervals.\n\n**We usually don't know that in practice** (don't know real data dist, $f$ can be nasty)\n\nFor some $\\hat\\theta$, things are nice anyway. E.g. for empirical mean $\\hat\\theta = \\frac{1}{N}\\sum_{n=1}^N X_n$ with iid $X_n$:\n\n- $\\Var{\\hat\\theta} = \\Var{X_1} / n$.\n- Confidence Intervals: if $X_n$ are iid and $n$ \"big\", then **CLT**: $\\hat\\theta \\overset{\\text{approx}}{\\sim}\\mathcal{N}(\\mu, \\sigma^2/n)$ \n\n## Unknown sampling distribution?\n\nWhat if you are skeptical of CLT / don't know the sampling distribution?\n\n. . .\n\nI fit LDA on some data.\n\nI get a new $X_0$ and produce $\\hat\\theta = \\Pr(Y_0 =1 \\given X_0)$.\n\nCan I get a 95% confidence interval for $\\theta = \\Pr(Y_0=1 \\given X_0)$?\n\n. . .\n\n**The bootstrap gives this to you.**\n\n## Etymology of the \"bootstrap\"\n\n- \"to build itself up incrementally, starting from very little\"\n- \"to create something valuable from very little/nothing\" \n\n- attributed to a late 1800s physics textbook question\n    - \"why can a man not lift himself up by pulling on his bootstraps?\"\n- became a sarcastic comment about self-driven socioeconomic advancement\n    - as these things go: co-opted by others and taken seriously\n- fun fact: same etymology for the term in computing\n    - \"booting up your computer\"\n\n<!--![](http://rackjite.com/wp-content/uploads/rr11014aa.jpg)-->\n\n\n## Generic Bootstrap Procedure\n\nThe bootstrap is *super general-purpose*. Given data $X_{1:N}, Y_{1:N}$:\n\n1. Compute $\\hat\\theta = f(X_{1:N}, Y_{1:N})$\n1. For $b=1,\\dots, B$, resample data w/ replacement: $X^{(b)}_{1:N}, Y^{(b)}_{1:N}$ (*bootstrap samples*)\n1. For $b=1,\\dots, B$, recompute $\\hat\\theta^{(b)} = f(X^{(b)}_{1:N}, Y^{(b)}_{1:N}$) (*bootstrap estimates*)\n1. Variance of $\\hat\\theta \\approx$ the empirical variance of $\\hat\\theta^{(b)}$\n1. $(1-\\alpha)$% Confidence Interval for $\\theta$: $\\left[2\\hat\\theta - \\widehat{F}_{\\text{boot}}(1-\\alpha/2),\\ 2\\hat\\theta - \\widehat{F}_{\\text{boot}}(\\alpha/2)\\right]$\n    - $\\widehat{F}_{\\text{boot}}$ is the empirical CDF of the samples $\\hat\\theta^{(b)}$\n\n## The bootstrap is very flexible\n\ne.g., for LDA:\n\n1. Produce the original estimate $\\hat\\theta=\\widehat{\\Pr}(Y_0=1 \\given X_0)$\n1. Resample your training data $B$ times with replacement\n1. Refit LDA for each one to produce $\\widehat{\\Pr}\\nolimits_b(Y_0 =1 \\given X_0)$.\n1. Variance: empirical variance of $\\widehat{\\Pr}\\nolimits_b(Y_0 =1 \\given X_0)$.\n1. CI: $\\left[2\\widehat{\\Pr}(Y_0 =1 \\given X_0) - \\widehat{F}_{\\text{boot}}(1-\\alpha/2),\\ 2\\widehat{\\Pr}(Y_0 =1 \\given X_0) - \\widehat{F}_{\\text{boot}}(\\alpha/2)\\right]$\n    - $\\widehat{F}_{\\text{boot}}$ is the empirical CDF of the samples $\\widehat{\\Pr}\\nolimits_b(Y_0 =1 \\given X_0)$\n\n## A basic example\n\n* Let $X_i\\sim \\textrm{Exponential}(1/5)$. The pdf is $f(x) = \\frac{1}{5}e^{-x/5}$\n\n* I know if I estimate the mean with $\\bar{X}$, then by the CLT (if $n$ is big), \n\n$$\\frac{\\sqrt{n}(\\bar{X}-E[X])}{s} \\approx N(0, 1).$$\n\n* This gives me a 95% confidence interval like\n$$\\bar{X} \\pm 2s/\\sqrt{n}$$\n\n* But I don't want to estimate the mean, I want to estimate the median.\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data.frame(x = c(0, 12)), aes(x)) +\n  stat_function(fun = function(x) dexp(x, 1 / 5), color = orange) +\n  geom_vline(xintercept = 5, color = blue) + # mean\n  geom_vline(xintercept = qexp(.5, 1 / 5), color = red) + # median\n  annotate(\"label\",\n    x = c(2.5, 5.5, 10), y = c(.15, .15, .05),\n    label = c(\"median\", \"bar(x)\", \"pdf\"), parse = TRUE,\n    color = c(red, blue, orange), size = 6\n  )\n```\n\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n## How to assess uncertainty in the median?\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n* I give you a sample of size 500, you give me the sample median.\n\n* How do you get a CI?\n\n* You can use the bootstrap!\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(406406406)\nx <- rexp(n, 1 / 5)\n(med <- median(x)) # sample median\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.611615\n```\n\n\n:::\n\n```{.r .cell-code}\nB <- 100\nalpha <- 0.05\nFhat <- map_dbl(1:B, ~ median(sample(x, replace = TRUE))) # repeat B times, \"empirical distribution\"\nCI <- 2 * med - quantile(Fhat, probs = c(1 - alpha / 2, alpha / 2))\n```\n:::\n\n\n\n\n## Empirical distribution\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nbootdf = tibble(boots = Fhat)\nggplot(bootdf, aes(boots)) + \n  stat_ecdf(colour = orange) +\n  geom_vline(xintercept = quantile(Fhat, probs = c(.05, .95))) +\n  geom_hline(yintercept = c(.05, .95), linetype = \"dashed\") +\n  annotate(\n    \"label\", x = c(3.2, 3.9), y = c(.2, .8), \n    label = c(\"hat(F)[boot](.05)\", \"hat(F)[boot](.95)\"), \n    parse = TRUE\n  )\n```\n\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-4-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Result\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nggplot(data.frame(Fhat), aes(Fhat)) +\n  geom_density(color = orange) +\n  geom_vline(xintercept = CI, color = orange, linetype = 2) +\n  geom_vline(xintercept = med, col = blue) +\n  geom_vline(xintercept = qexp(.5, 1 / 5), col = red) +\n  annotate(\"label\",\n    x = c(3.15, 3.5, 3.75), y = c(.5, .5, 1),\n    color = c(orange, red, blue),\n    label = c(\"widehat(F)\", \"true~median\", \"widehat(median)\"),\n    parse = TRUE\n  ) +\n  xlab(\"x\") +\n  geom_rug(aes(2 * med - Fhat))\n```\n\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## How does this work?\n\nThe Fundamental Premise (TM): **a sufficiently large sample looks like the population**.\n\nSo, *sampling from the sample looks like sampling from the population.*\n\n::: flex\n::: w-50\n\nPopulation:\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\nMedians for samples of size $N=100$:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.055229\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 4.155205\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.346588\n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n::: w-50\n\nOne Sample ( $N = 100$ ):\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nMedians for samples of size $N=100$:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.126391\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.063069\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 3.534019\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n<!-- ![](gfx/boot1.png)-->\n\n<!-- ## Approximations -->\n\n<!-- ![](gfx/boot2.png) -->\n\n\n## Bootstrap error sources\n\n<!-- ![](gfx/boot2.png){.center} -->\n\nSimulation error\n: using only $B$ samples to estimate $F$ with $\\hat{F}$.\n\nStatistical error\n: our data depended on a sample from the population. We don't have the whole population so we make an error by using a sample (Note: this part is what __always__ happens with data, and what the science of statistics analyzes.)\n\nSpecification error\n: If we use the parametric bootstrap, and our model is wrong, then we are overconfident.\n\n\n## Types of intervals\n\nLet $\\hat{\\theta}$ be our sample statistic, $\\hat\\theta^{(b)}$ be the resamples\n\nOur interval is\n\n$$\n[2\\hat{\\theta} - \\hat\\theta^{(b)}_{1-\\alpha/2},\\ 2\\hat{\\theta} - \\hat\\theta^{(b)}_{\\alpha/2}]\n$$\n\nwhere $\\theta^{(b)}_q$ is the $q$ quantile of $\\hat\\theta^{(b)}$.\n\n<hr>\n\n* Called the \"Pivotal Interval\"\n* Has the correct $1-\\alpha$% coverage under very mild conditions on $\\hat{\\theta}$\n\n## Types of intervals\n\nLet $\\hat{\\theta}$ be our sample statistic, $\\hat\\theta^{(b)}$ be the resamples\n\n$$\n[\\hat{\\theta} - z_{1-\\alpha/2}\\hat{s},\\ \\hat{\\theta} + z_{1-\\alpha/2}\\hat{s}]\n$$\n\nwhere $\\hat{s} = \\sqrt{\\Var{\\hat\\theta^{(b)}}}$\n\n<hr>\n\n* Called the \"Normal Interval\"\n* Only works if the distribution of $\\hat{\\theta^{(b)}}$ is approximately Normal.\n* Common and really tempting, but not likely to work well\n* Don't do this\n\n## Types of intervals\n\nLet $\\hat{\\theta}$ be our sample statistic, $\\hat\\theta^{(b)}$ be the resamples\n\n$$\n[\\hat\\theta^{(b)}_{\\alpha/2},\\ \\hat\\theta^{(b)}_{1-\\alpha/2}]\n$$\n\nwhere $\\hat\\theta^{(b)}_q$ is the $q$ quantile of $\\hat\\theta^{(b)}$.\n\n<hr>\n\n* Called the \"Percentile Interval\"\n* Better than the Normal Interval, more assumptions than the Pivotal Interval\n* We teach this one in DSCI100 because it's easy and \"looks right\" (it's not, in general)\n    * Unlike Pivotal, doesn't follow from the fundamental premise of bootstrap\n* It *does* have (asymp) right coverage if $\\exists$ monotone $m$ so that $m(\\hat\\theta) \\sim N(m(\\theta), c^2)$\n\n## Slightly harder example\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n::: flex\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(fatcats, aes(Bwt, Hwt)) +\n  geom_point(color = blue) +\n  xlab(\"Cat body weight (Kg)\") +\n  ylab(\"Cat heart weight (g)\")\n```\n\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-11-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n:::\n\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncats.lm <- lm(Hwt ~ 0 + Bwt, data = fatcats)\nsummary(cats.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Hwt ~ 0 + Bwt, data = fatcats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.8138 -0.9014 -0.2155  0.7548 22.5957 \n\nCoefficients:\n    Estimate Std. Error t value Pr(>|t|)    \nBwt  3.88297    0.08401   46.22   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.789 on 143 degrees of freedom\nMultiple R-squared:  0.9373,\tAdjusted R-squared:  0.9368 \nF-statistic:  2136 on 1 and 143 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nconfint(cats.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2.5 %   97.5 %\nBwt 3.716912 4.049036\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n\n## When we fit models, we examine diagnostics\n\n\n::: flex\n::: w-50\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqqnorm(residuals(cats.lm), pch = 16, col = blue)\nqqline(residuals(cats.lm), col = orange, lwd = 2)\n```\n\n::: {.cell-output-display}\n![](18-the-bootstrap_files/figure-revealjs/unnamed-chunk-13-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\nThe tails are too fat. I don't believe that CI...\n\n:::\n\n::: w-50\n\n**We bootstrap**\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- 500\nalpha <- .05\nbhats <- map_dbl(1:B, ~ {\n  newcats <- fatcats |>\n    slice_sample(prop = 1, replace = TRUE)\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   97.5%     2.5% \n3.731614 4.030176 \n```\n\n\n:::\n\n```{.r .cell-code}\nconfint(cats.lm) # Original CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2.5 %   97.5 %\nBwt 3.716912 4.049036\n```\n\n\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n\n## An alternative\n\n* So far, I didn't use any information about the data-generating process. \n\n* We've done the [non-parametric bootstrap]{.secondary}\n\n* This is easiest, and most common for the methods in this module\n\n. . . \n\n[But there's another version]{.secondary}\n\n* You could try a \"parametric bootstrap\"\n\n* This assumes knowledge about the DGP\n\n\n\n## Same data\n\n::: flex\n::: w-50\n\n[Non-parametric bootstrap]{.secondary}\n\nSame as before\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- 500\nalpha <- .05\nbhats <- map_dbl(1:B, ~ {\n  newcats <- fatcats |>\n    slice_sample(prop = 1, replace = TRUE)\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # NP Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   97.5%     2.5% \n3.726857 4.038470 \n```\n\n\n:::\n\n```{.r .cell-code}\nconfint(cats.lm) # Original CI\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       2.5 %   97.5 %\nBwt 3.716912 4.049036\n```\n\n\n:::\n:::\n\n\n\n:::\n\n::: w-50\n[Parametric bootstrap]{.secondary}\n\n1. Assume that the linear model is TRUE.\n2. Then, $\\texttt{Hwt}_i = \\widehat{\\beta}\\times \\texttt{Bwt}_i + \\widehat{e}_i$, $\\widehat{e}_i \\approx \\epsilon_i$\n3. The $\\epsilon_i$ is random $\\longrightarrow$ just resample $\\widehat{e}_i$.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nB <- 500\nbhats <- double(B)\ncats.lm <- lm(Hwt ~ 0 + Bwt, data = fatcats)\nr <- residuals(cats.lm)\nbhats <- map_dbl(1:B, ~ {\n  newcats <- fatcats |> mutate(\n    Hwt = predict(cats.lm) +\n      sample(r, n(), replace = TRUE)\n  )\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # Parametric Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   97.5%     2.5% \n3.707015 4.035300 \n```\n\n\n:::\n:::\n\n\n\n\n:::\n\n:::\n\n\n\n<!--\n## More details\n\n> See \"All of Statistics\" by Larry Wasserman, Chapter 8.3\n\nThere's a handout with the proofs on Canvas (under Modules)\n\n-->\n\n# Next time...\n\nBootstrap for bagging and random forests\n",
    "supporting": [
      "18-the-bootstrap_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}