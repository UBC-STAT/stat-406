{
  "hash": "e9f41d67eb342e4a7057779c88e4d819",
  "result": {
    "markdown": "---\nlecture: \"25 Principal components, the troubles\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 23 November 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## PCA\n\nIf we knew how to rotate our data, then we could more easily retain the structure.\n\n[PCA]{.secondary} gives us exactly this rotation\n\n\n\nPCA works when the data can be represented (in a lower dimension) as [lines]{.hand} (or planes, or hyperplanes). \n\n\nSo, in two dimensions:  \n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/good-pca-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n## PCA reduced\n  \nHere, we can capture a lot of the variation and underlying\nstructure with just 1 dimension, \n\ninstead of the original 2 (the colouring is for visualizing). \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/pca-reduced-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## PCA bad\n\nWhat about other data structures?  Again in two dimensions\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/spiral-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## PCA bad\n\nHere, we have failed miserably.  \n\nThere is actually only 1 dimension\nto this data (imagine walking up the spiral going \nfrom purple to yellow).  \n\nHowever, when we write it as 1 PCA dimension,\nall the points are all \"mixed up\". \n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/spiral-reduced-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Explanation\n  \n::: flex\n::: w-65\nPCA wants to minimize distances (equivalently maximize\n    variance).  \n    \nThis means it [slices]{.hand} through the data at the [meatiest]{.secondary} point, and then the next one, and so on.  \n\nIf the data are [curved]{.secondary} this is going to induce artifacts.  \n\nPCA also looks at things as being [close]{.hand} if they are near each other in a Euclidean sense.  \n\n\nOn the spiral, our intuition says that things are close only if the distance is constrained to go along the curve.  \n    \nIn other words, purple and blue are close, blue and yellow are not. \n:::\n\n::: w-35\n    \n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n## Kernel PCA\n\nClassical PCA comes from $\\X=  \\U\\D\\V^{\\top}$, the SVD of the (centered) data\n\n\nHowever, we can just as easily get it from the outer product\n$\\mathbf{K} = \\X\\X^{\\top} =  \\U\\D^2\\U^{\\top}$\n\n\nThe intuition behind KPCA is that $\\mathbf{K}$ is an expansion into a\n[kernel space]{.hand}, where\n$$\\mathbf{K}_{i,i'} = k(x_i,\\ x_{i'}) = \\langle x_i,x_{i'} \\rangle$$\n\nWe saw this trick before with feature expansion.\n\n## Procedure {background-colour=\"#97D4E9\"}\n\n1. Specify a kernel function $k$  \nmany people use $k(x,x') = \\exp\\left( -d(x, x')/\\gamma\\right)$ \nwhere $d(x,x') = \\norm{x-x'}_2^2$\n2. Form $\\mathbf{K}_{i,i'} = k(x_i,x_{i'})$\n3. Double center $\\mathbf{K} = \\mathbf{PKP}$ where $\\mathbf{P} = \\mathbf{I}_n - \\mathbf{11}^\\top / n$\n4. Take eigendecomposition $\\mathbf{K} = \\U\\D^2\\U^{\\top}$\n\nThe scores are still $\\mathbf{Z} = \\U_M\\D_M$\n\n\n::: callout-note\nWe don't explicitly\ngenerate the feature map $\\longrightarrow$ there are NO loadings\n:::\n\n\n## An alternate view\n\nTo get the first PC\nin classical PCA, we solve\n$$\\max_\\alpha \\Var{\\X\\alpha} \\quad \\textrm{ subject to } \\quad \\left|\\left| \\alpha \\right|\\right|_2^2 = 1$$\n\n\nIn the kernel setting we solve\n$$\\max_{g \\in \\mathcal{H}_k} \\Var{g(X)} \\quad \\textrm{ subject to } \\quad\\left|\\left| g \\right|\\right|_{\\mathcal{H}_k} = 1$$\n\nHere $\\mathcal{H}_k$ is a function space determined by $k(x,x')$.\n\n## Example kernels\n\n$k(x,x') = x^\\top x'$\n: gives back regular PCA\n\n$k(x,x') = (1+x^\\top x')^d$\n: gives a function space which contains all $d^{th}$-order \n: polynomials.\n\n$k(x,x') = \\exp(-\\norm{x-x'}_2^2/\\gamma)$\n: gives a function space spanned by the infinite Fourier basis\n\n> For more details see [ESL 14.5] \n\n\n\n## Kernel PCA on the spiral\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nn <- nrow(df_spiral)\nI_M <- (diag(n) - tcrossprod(rep(1, n)) / n)\nkp <- (tcrossprod(as.matrix(df_spiral[, 1:2])) + 1)^2\nKp <- I_M %*% kp %*% I_M\nEp <- eigen(Kp, symmetric = TRUE)\npolydf <- tibble(\n  x = Ep$vectors[, 1] * Ep$values[1],\n  y = jit,\n  z = df_spiral$z\n)\nkg <- exp(-as.matrix(dist(df_spiral[, 1:2]))^2 / 1)\nKg <- I_M %*% kg %*% I_M\nEg <- eigen(Kg, symmetric = TRUE)\ngaussdf <- tibble(\n  x = Eg$vectors[, 1] * Eg$values[1],\n  y = jit,\n  z = df_spiral$z\n)\ndfkern <- bind_rows(df_spiral, df_spiral2, polydf, gaussdf)\ndfkern$method <- rep(c(\"data\", \"pca\", \"kpoly (d = 2)\", \"kgauss (gamma = 1)\"), each = n)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/plot-kpca-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## KPCA: summary\n\nKernel PCA seeks to generalize the notion of\nsimilarity using a kernel map\n\n\nThis can be interpreted as finding smooth,\northogonal directions in an RKHS\n\n\nThis can allow us to start picking up nonlinear (in the original feature\nspace) aspects of our data\n\n\nThis new representation can be passed to a\nsupervised method to form a semisupervised\nlearner\n\n[This kernel is different than kernel smoothing!!]{.secondary}\n\n* Just like with PCA (and lots of other things) the way you measure distance is important\n* The choice of Kernel is important\n* The embedding dimension must be chosen\n\n\n## Basic semi-supervised (see [ISLR 6.3.1])\n\n1. You get data $\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$.\n\n2. You do something unsupervised on $\\X$ to create new features (like PCA).\n\n3. You use the learned features to find a predictor $\\hat{f}$ (say, do OLS on them)\n\n## Quick example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmusic <- Stat406::popmusic_train\nX <- music |> select(danceability:energy, loudness, speechiness:valence)\npca <- prcomp(X, scale = TRUE)\nZ <- predict(pca)[, 1:2]\nZgrid <- expand.grid(\n  Z1 = seq(min(Z[,1]), max(Z[,1]), len = 100L), \n  Z2 = seq(min(Z[,2]), max(Z[,2]), len = 100L)\n)\nout <- class::knn(Z, Zgrid, music$artist, k = 6)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](25-pca-issues_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Why?\n\nRecall that for nonparametric regression, we must deal with the curse of dimensionality.\n\n* This did KNN in 2 dimensions instead of 8: less curse\n\n* However, the dimensions were learned independently of $\\y$: may not be helpful\n\n* There's a bias-variance tradeoff here...\n\n## What is PCA / KPCA estimating?\n\nImagine that the Population data lie near a low-dimensional linear manifold...\n\n* Think of the spiral. (1D manifold in 2D space)\n* Or a flat piece of paper (2D manifold in 3D space)\n\nPCA / KPCA are estimating the manifold.\n\n* This works well under some conditions\n* And if it's true, then we have reduced variance but added a bit of bias\n* And the downstream method may work [better]{.secondary}, because there are \nfewer predictors.\n\nWe just need the data [near]{.secondary} the manifold. But if not, then we \nintroduced a [lot]{.secondary} of bias.\n\nPCA estimates linear manifolds, KPCA estimates non-linear\n\n# Next time...\n\nMore PCA vs. KPCA\n",
    "supporting": [
      "25-pca-issues_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}