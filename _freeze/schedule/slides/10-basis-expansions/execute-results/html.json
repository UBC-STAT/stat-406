{
  "hash": "ff4a6934d354ee1548b88422f0eb53d8",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"10 Basis expansions\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 07 October 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n# Quick Review of Ridge and Lasso\n\n## OLS: low bias, (potentially) high variance\n\n$$\n\\begin{gathered}\n\\text{Model:} \\quad y = x^\\top \\beta + \\epsilon, \\qquad \\epsilon \\sim N(0, \\sigma^2)\n\\\\\n\\text{OLS:} \\quad \\bls = \\argmin_\\beta \\| \\y - \\X\\beta\\|_2^2\\quad\n\\end{gathered}\n$$\n\n- Bias: $\\E[\\bls] - \\beta = \\E[\\E[\\bls \\mid \\X]] - \\beta = \\ldots = 0$\n- variance: $\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1}$\n\n[When is $(\\X^\\top \\X)^{-1}$ large?]{.secondary}\n\n. . . \n\nWhen we have *nearly colinear features*\\\nNearly colinear features $\\Rightarrow$ small singular values $\\Rightarrow$ large matrix inverse\\\n[(Colinearity is more likely when $n$ is small)]{.secondary}\n\n\n## Reducing variance (at the cost of additional bias)\n\n0. *Manual variable selection*\n1. *Ridge regression*: $\\min_\\beta \\| \\y - \\X\\beta\\|_2^2 + \\lambda \\Vert \\beta \\Vert_2^2$\n2. *Lasso*: $\\min_\\beta \\| \\y - \\X\\beta\\|_2^2 + \\lambda \\Vert \\beta \\Vert_1$\n\n. . .\n\n*Ridge* shrinks all parameters towards 0.\\\n*Lasso* performs automatic variable selection.\\\n\n. . .\n\n\\\nIncreasing $\\lambda$ *increases bias* and *decreases variance*.\\\n[(For ridge, larger lambda $\\rightarrow$ smaller $\\beta$.)]{.small}\\\n[(For lasso, larger lambda $\\rightarrow$ sparser $\\beta$.)]{.small}\n\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Computing ridge and lasso predictors\n\n- *OLS:* $\\bls = (\\X^\\top \\X)^{-1}\\X^\\top \\y$\n- *Ridge:* $\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y$\n- *Lasso:* No closed form solution ðŸ˜”\n  - (Convex optimization problem solvable with iterative algorithms.)\n\n\n## (Optional) Proof that ridge shrinks parameters\n\n(This proof is not too hard if you use facts about SVDs and eigenvalues. I recommend working through it.)\n\nLet $\\mathbf{UDV^\\top} = X$ be the SVD of $\\mathbf X$.\n\n$$\n\\begin{align}\n\\brl &= (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1} \\X^\\top \\y\n     \\\\\n     &= (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1} {\\color{blue} \\X^\\top \\X (\\X^\\top \\X)^{-1}} \\X^\\top \\y\n     \\\\\n     &= (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1} \\X^\\top \\X \\underbrace{\\left( (\\X^\\top \\X)^{-1} \\X^\\top \\y \\right)}_{\\bls}\n     \\\\\n     &= (\\mathbf V \\mathbf D^2 \\mathbf V^\\top + \\lambda \\mathbf{I})^{-1} \\mathbf V \\mathbf D^2 \\mathbf V^\\top \\bls\n     \\\\\n     &= (\\mathbf V (\\mathbf D^2 + \\lambda I) \\mathbf V^\\top)^{-1} \\mathbf V \\mathbf D^2 \\mathbf V^\\top \\bls\n     \\\\\n     &= \\mathbf V (\\mathbf D^2)(\\mathbf D^2 + \\lambda I)^{-1} \\mathbf V^\\top \\bls\n\\end{align}\n$$\n\n---\n\n$(\\mathbf D^2)(\\mathbf D^2 + \\lambda I)^{-1}$ is a diagonal matrix with entries $d_i^2/(d_i^2 + \\lambda) < 1$.\n\nSo $\\mathbf V (\\mathbf D^2)(\\mathbf D^2 + \\lambda I)^{-1} \\mathbf V^\\top$ is a matrix that *shrinks* all coefficients of any vector multiplied against it.\n\n$$\n\\Vert \\mathbf V (\\mathbf D^2)(\\mathbf D^2 + \\lambda I)^{-1} \\mathbf V^\\top \\bls \\Vert_2 < \\Vert \\bls \\Vert_2.\n$$\nSo $\\Vert \\brl \\Vert_2 < \\Vert \\bls \\Vert_2$\n\n\n# Now onto new stuff\n(But first, more clickers!)\n\n## What about nonlinear things\n\n\n$$\\text{Our usual model:} \\quad \\Expect{Y \\given X=x} = \\sum_{j=1}^p x_j\\beta_j$$\n\nNow we relax this assumption of linearity:\n\n$$\\Expect{Y \\given X=x} = f(x)$$\n\nHow do we estimate $f$?\n\n. . . \n\nFor this lecture, we use $x \\in \\R$ (1 dimensional)\n\nHigher dimensions are possible, but complexity grows [exponentially]{.secondary}.\n\nWe'll see some special techniques for $x\\in\\R^p$ later this Module.\n\n\n## Start simple\n\nFor any $f : \\R \\rightarrow [0,1]$\n\n$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2 + \\frac{1}{3!}f'''(x_0)(x-x_0)^3 + R_3(x-x_0)$$\n\nSo we can linearly regress $y_i = f(x_i)$ on the polynomials.\n\nThe more terms we use, the smaller $R$.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(406406)\ndata(arcuate, package = \"Stat406\") \narcuate <- arcuate |> slice_sample(n = 220)\narcuate %>% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) +\n  geom_smooth(color = orange, formula = y ~ poly(x, 3), method = \"lm\", se = FALSE)\n```\n\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Same thing, different orders\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\narcuate %>% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) + \n  geom_smooth(aes(color = \"a\"), formula = y ~ poly(x, 4), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"b\"), formula = y ~ poly(x, 7), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"c\"), formula = y ~ poly(x, 25), method = \"lm\", se = FALSE) +\n  scale_color_manual(name = \"Taylor order\",\n    values = c(green, red, orange), labels = c(\"4 terms\", \"7 terms\", \"25 terms\"))\n```\n\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Still a \"linear smoother\"\n\nReally, this is still linear regression, just in a transformed space.\n\nIt's not linear in $x$, but it is linear in $(x,x^2,x^3)$ (for the 3rd-order case)\n\nSo, we're still doing OLS with\n\n$$\\X=\\begin{bmatrix}1& x_1 & x_1^2 & x_1^3 \\\\ \\vdots&&&\\vdots\\\\1& x_n & x_n^2 & x_n^3\\end{bmatrix}$$\n\nSo we can still use our nice formulas for LOO-CV, GCV, Cp, AIC, etc.\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmax_deg <- 20\ncv_nice <- function(mdl) mean( residuals(mdl)^2 / (1 - hatvalues(mdl))^2 ) \ncvscores <- map_dbl(seq_len(max_deg), ~ cv_nice(lm(fa ~ poly(position, .), data = arcuate)))\n```\n:::\n\n\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\ng1 <- ggplot(tibble(cvscores, degrees = seq(max_deg)), aes(degrees, cvscores)) +\n  geom_point(colour = blue) +\n  geom_line(colour = blue) + \n  labs(ylab = 'LOO-CV', xlab = 'polynomial degree') +\n  geom_vline(xintercept = which.min(cvscores), linetype = \"dotted\") \ng2 <- ggplot(arcuate, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_smooth(\n    colour = orange, \n    formula = y ~ poly(x, which.min(cvscores)), \n    method = \"lm\", \n    se = FALSE\n  )\nplot_grid(g1, g2, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-5-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Other bases\n\nPolynomials\n: $x \\mapsto \\left(1,\\ x,\\ x^2, \\ldots, x^p\\right)$ (technically, not quite this, they are orthogonalized)\n\nLinear splines\n: $x \\mapsto \\bigg(1,\\ x,\\ (x-k_1)_+,\\ (x-k_2)_+,\\ldots, (x-k_p)_+\\bigg)$ for some $\\{k_1,\\ldots,k_p\\}$\n\n<!--\nCubic splines\n: $x \\mapsto \\bigg(1,\\ x,\\ x^2,\\ x^3,\\ (x-k_1)^3_+,\\ (x-k_2)^3_+,\\ldots, (x-k_p)^3_+\\bigg)$ for some choices $\\{k_1,\\ldots,k_p\\}$\n-->\n\nFourier series\n: $x \\mapsto \\bigg(1,\\ \\cos(2\\pi x),\\ \\sin(2\\pi x),\\ \\cos(2\\pi 2 x),\\ \\sin(2\\pi 2 x), \\ldots, \\cos(2\\pi p x),\\ \\sin(2\\pi p x)\\bigg)$\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\nlibrary(ggplot2)\n\nrelu_shifted <- function(x, shift) {pmax(0, x - shift)}\n\n# Create a sequence of x values\nx_vals <- seq(-3, 3, length.out = 1000)\n\n# Create a data frame with all the shifted functions\ndata <- data.frame(\n  x = rep(x_vals, 5),\n  polynomial = c(x_vals, x_vals^2, x_vals^3, x_vals^4, x_vals^5),\n  linear.splines = c(relu_shifted(x_vals, 2), relu_shifted(x_vals, 1), relu_shifted(x_vals, 0), relu_shifted(x_vals, -1), relu_shifted(x_vals, -2)),\n  fourier = c(cos(pi / 2 * x_vals), sin(pi / 2 * x_vals), cos(pi / 4 * x_vals), sin(pi / 4 * x_vals), cos(pi * x_vals)),\n  function_label = rep(c(\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"), each = length(x_vals))\n)\n\n# Plot using ggplot2\ng1 <- ggplot(data, aes(x = x, y = polynomial, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\ng2 <- ggplot(data, aes(x = x, y = linear.splines, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\ng3 <- ggplot(data, aes(x = x, y = fourier, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\n\nplot_grid(g1, g2, g3, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-6-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## How do you choose?\n\n[Procedure 1:]{.secondary}\n\n1. Pick your favorite basis. (Think if the data might \"prefer\" one basis over another.)\n    - [How \"smooth\" is the response you're trying to model?]{.small}\n  \n<!-- This is not as easy as it sounds. For instance, if $f$ is a step function, linear splines will do well with good knots, but polynomials will be terrible unless you have __lots__ of terms. -->\n\n2. Perform OLS on different orders.\n\n3. Use model selection criterion to choose the order.\n\n---\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-7-1.svg){fig-align='center'}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/unnamed-chunk-8-1.svg){fig-align='center'}\n:::\n:::\n\n\n\nWhat bases do you think will work best for $f1$, $f2$ and $f3$?\n\n. . .\n\n[*Answer: $f1$ was made from polynomial bases, $f2$ from fourier, $f3$ from linear splines*]{.secondary}\n\n\n---\n\n## How do you choose?\n\n[Procedure 2:]{.secondary}\n\n1. Use a bunch of high-order bases, say Linear splines and Fourier series and whatever else you like.\n\n2. Use Lasso or Ridge regression or elastic net. (combining bases can lead to multicollinearity, but we may not care)\n\n3. Use model selection criteria to choose the tuning parameter.\n\n\n## Try both procedures\n\n1. Split `arcuate` into 75% training data and 25% testing data.\n\n2. Estimate polynomials up to 20 as before and choose best order.\n\n3. Do ridge, lasso and elastic net $\\alpha=.5$ on 20th order polynomials, splines with 20 knots, and Fourier series with $p=20$. Choose tuning parameter (using `lambda.1se`).\n\n4. Repeat 1-3 10 times (different splits)\n\n\n##\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(glmnet)\nmapto01 <- function(x, pad = .005) (x - min(x) + pad) / (max(x) - min(x) + 2 * pad)\nx <- mapto01(arcuate$position)\nXmat <- cbind(\n  poly(x, 20), \n  splines::bs(x, df = 20, degree = 1), \n  cos(2 * pi * outer(x, 1:20)), sin(2 * pi * outer(x, 1:20))\n)\ny <- arcuate$fa\nrmse <- function(z, s) sqrt(mean( (z - s)^2 ))\nnzero <- function(x) with(x, nzero[match(lambda.1se, lambda)])\nsim <- function(maxdeg = 20, train_frac = 0.75) {\n  n <- nrow(arcuate)\n  train <- as.logical(rbinom(n, 1, train_frac))\n  test <- !train # not precisely 25%, but on average\n  polycv <- map_dbl(seq(maxdeg), ~ cv_nice(lm(y ~ Xmat[,seq(.)], subset = train))) # figure out which order to use\n  bpoly <- lm(y[train] ~ Xmat[train, seq(which.min(polycv))]) # now use it\n  lasso <- cv.glmnet(Xmat[train, ], y[train])\n  ridge <- cv.glmnet(Xmat[train, ], y[train], alpha = 0)\n  elnet <- cv.glmnet(Xmat[train, ], y[train], alpha = .5)\n  tibble(\n    methods = c(\"poly\", \"lasso\", \"ridge\", \"elnet\"),\n    rmses = c(\n      rmse(y[test], cbind(1, Xmat[test, 1:which.min(polycv)]) %*% coef(bpoly)),\n      rmse(y[test], predict(lasso, Xmat[test,])),\n      rmse(y[test], predict(ridge, Xmat[test,])),\n      rmse(y[test], predict(elnet, Xmat[test,]))\n    ),\n    nvars = c(which.min(polycv), nzero(lasso), nzero(ridge), nzero(elnet))\n  )\n}\nset.seed(12345)\nsim_results <- map(seq(20), sim) |> list_rbind() # repeat it 20 times\n```\n:::\n\n\n\n## \n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nsim_results |>  \n  pivot_longer(-methods) |> \n  ggplot(aes(methods, value, fill = methods)) + \n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_y\") + \n  ylab(\"\") +\n  theme(legend.position = \"none\") + \n  xlab(\"\") +\n  scale_fill_viridis_d(begin = .2, end = 1)\n```\n\n::: {.cell-output-display}\n![](10-basis-expansions_files/figure-revealjs/sim-results-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Common elements\n\nIn all these cases, we transformed $x$ to a [higher-dimensional space]{.secondary}\n\nUsed $p+1$ dimensions with polynomials\n\nUsed $p+4$ dimensions with cubic splines\n\nUsed $2p+1$ dimensions with Fourier basis\n\n## Featurization\n\nEach case applied a [feature map]{.secondary} to $x$, call it $\\Phi$\n\nWe used new \"features\" $\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)$\nw/ a linear model\n\n$$f(x) = \\Phi(x)^\\top \\beta$$ \n\nNeural networks (coming in module 4) build upon this idea\n\n<!-- You've also probably seen it in earlier courses when you added interaction terms or other transformations. -->\n\n. . .\n\n\\\nSome methods (Support Vector Machines and other Kernel Machines) allow $k=\\infty$\\\n[See [ISLR] 9.3.2 for baby overview or [ESL] 5.8 (note ðŸ˜±)]{.small}\n\n\n# Next time...\n\nKernel regression and nearest neighbors\n",
    "supporting": [
      "10-basis-expansions_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}