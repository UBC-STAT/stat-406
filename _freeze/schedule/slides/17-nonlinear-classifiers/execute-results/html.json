{
  "hash": "4951fbf8f4326b1e6e6ddfdaee56cc62",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"17 Nonlinear classifiers\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 23 October 2024\n\n\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Two lectures ago\n\n\nWe discussed logistic regression\n\n$$\\begin{aligned}\nPr(Y = 1 \\given X=x)  & = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}} \\\\\nPr(Y = 0 \\given X=x) & = \\frac{1}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}=1-\\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\\end{aligned}$$\n\n## Make it nonlinear\n\nWe can make logistic regression have non-linear decision boundaries by mapping the features to a higher dimension (just like with linear regression)\n\nSay:\n\n__Polynomials__\n\n$(x_1, x_2) \\mapsto \\left(1,\\ x_1,\\ x_1^2,\\ x_2,\\ x_2^2,\\ x_1 x_2\\right)$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat1 <- generate_lda_2d(100, Sigma = .5 * diag(2)) |> mutate(y = as.factor(y))\nlogit_poly <- glm(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1, family = \"binomial\")\n```\n:::\n\n\n\n\n\n\n\n## Visualizing the classification boundary\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\ngr <- expand_grid(x1 = seq(-2.5, 3, length.out = 100), x2 = seq(-2.5, 3, length.out = 100))\npts_logit <- predict(logit_poly, gr)\ng0 <- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts_logit), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_viridis_b(n.breaks = 6, alpha = .5, name = \"log odds\") +\n  ggtitle(\"Polynomial logit\") +\n  theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\"))\nplot_grid(g0)\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/plot-d1-1.svg){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n\nA linear decision boundary in the higher-dimensional space corresponds to a non-linear decision boundary in low dimensions.\n\n\n## KNN classifiers\n\n<<<<<<< HEAD\n=======\n* We saw $k$-nearest neighbors in the last module.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(class)\nknn3 <- knn(dat1[, -1], gr, dat1$y, k = 3)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngr$nn03 <- knn3\nggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = knn3), aes(x1, x2, fill = disc), alpha = .5) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, blue), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-3-1.svg){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Choosing $k$ is very important\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(406406406)\nks <- c(1, 2, 5, 10, 20)\nnn <- map(ks, ~ as_tibble(knn(dat1[, -1], gr[, 1:2], dat1$y, .x)) |> \n  set_names(sprintf(\"k = %02s\", .x))) |>\n  list_cbind() |>\n  bind_cols(gr)\npg <- pivot_longer(nn, starts_with(\"k =\"), names_to = \"k\", values_to = \"knn\")\n\nggplot(pg, aes(x1, x2)) +\n  geom_raster(aes(fill = knn), alpha = .6) +\n  facet_wrap(~ k) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  geom_point(data = dat1, mapping = aes(x1, x2, shape = as.factor(y)), size = 4) +\n  theme_bw(base_size = 18) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  theme(\n    legend.title = element_blank(),\n    legend.key.height = unit(3, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-4-1.svg){fig-align='center' width=1536}\n:::\n:::\n\n\n\n\n\n* How should we choose $k$?\n\n* Scaling is also very important. \"Nearness\" is determined by distance, so better to standardize your data first.\n\n* If there are ties, break randomly. So even $k$ is strange.\n\n\n## `knn.cv()` (leave one out)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkmax <- 20\nerr <- map_dbl(1:kmax, ~ mean(knn.cv(dat1[, -1], dat1$y, k = .x) != dat1$y))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-6-1.svg){fig-align='center' width=511.68}\n:::\n:::\n\n\n\n\n\nI would use the _largest_ (odd) `k` that is close to the minimum.  \nThis produces simpler, smoother, decision boundaries.\n\n\n\n## Final version\n\n\n::: flex\n::: w-50\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nkopt <- max(which(err == min(err)))\nkopt <- kopt + 1 * (kopt %% 2 == 0)\ngr$opt <- knn(dat1[, -1], gr[, 1:2], dat1$y, k = kopt)\ntt <- table(knn(dat1[, -1], dat1[, -1], dat1$y, k = kopt), dat1$y, dnn = c(\"predicted\", \"truth\"))\nggplot(dat1, aes(x1, x2)) +\n  theme_bw(base_size = 24) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = gr, aes(x1, x2, fill = opt), alpha = .6) +\n  geom_point(aes(shape = y), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-7-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n\n:::\n\n::: w-50\n\n* Best $k$: 19\n\n* Misclassification error: 0.17\n\n* Confusion matrix:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         truth\npredicted  1  2\n        1 41  6\n        2 11 42\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n\n\n>>>>>>> 1c36b39 (Update last of classification slides)\n## Trees\n\n::: flex\n\n::: w-50\nWe saw regression trees last module\n\nClassification trees are \n\n- More natural\n- Slightly different computationally\n\nEverything else is pretty much the same\n:::\n\n::: w-50\n![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg)\n:::\n:::\n\n\n\n## Axis-parallel splits\n\nLike with regression trees, classification trees operate by greedily splitting the predictor space\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::: flex\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnames(bakeoff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"winners\"                  \n [2] \"series\"                   \n [3] \"age\"                      \n [4] \"occupation\"               \n [5] \"hometown\"                 \n [6] \"percent_star\"             \n [7] \"percent_technical_wins\"   \n [8] \"percent_technical_bottom3\"\n [9] \"percent_technical_top3\"   \n[10] \"technical_highest\"        \n[11] \"technical_lowest\"         \n[12] \"technical_median\"         \n[13] \"judge1\"                   \n[14] \"judge2\"                   \n[15] \"viewers_7day\"             \n[16] \"viewers_28day\"            \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmalltree <- tree(\n  winners ~ technical_median + percent_star,\n  data = bakeoff\n)\n```\n:::\n\n\n\n\n\n:::\n\n\n::: w-50\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(5, 5, 0, 0) + .1)\nplot(bakeoff$technical_median, bakeoff$percent_star,\n  pch = c(\"-\", \"+\")[bakeoff$winners + 1], cex = 2, bty = \"n\", las = 1,\n  ylab = \"% star baker\", xlab = \"times above median in technical\",\n  col = orange, cex.axis = 2, cex.lab = 2\n)\npartition.tree(smalltree,\n  add = TRUE, col = blue,\n  ordvars = c(\"technical_median\", \"percent_star\")\n)\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/plot-partition-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n## When do trees do well?\n\n::: flex\n::: w-50\n![](gfx/8.7.png)\n:::\n\n::: w-50\n\n[2D example]{.hand}\n\n[Top Row:]{.primary} \n\ntrue decision boundary is linear\n\n🍎 linear classifier \n\n👎 tree with axis-parallel splits\n\n[Bottom Row:]{.primary}\n\ntrue decision boundary is non-linear\n\n🤮 A linear classifier can't capture the true decision boundary\n\n🍎 decision tree is successful.\n:::\n:::\n\n\n\n\n## How do we build a tree?\n\n\n1. Divide the predictor space into\n$J$ non-overlapping regions $R_1, \\ldots, R_J$ \n\n  > this is done via greedy, recursive binary splitting\n\n2. Every observation that falls into a given region $R_j$ is given the same prediction\n\n  > determined by majority (or plurality) vote in that region.\n\n\n\n[Important:]{.hand}\n\n* Trees can only make rectangular regions that are aligned with the coordinate axis.\n\n* We use a *greedy* (not optimal) algorithm to fit the tree\n\n\n## Flashback: Constructing Trees for Regression\n\n* While ($\\mathtt{depth} \\ne \\mathtt{max.depth}$):\n    * For each existing region $R_k$\n        * For a given *splitting variable* $j$ and *split value* $s$,\n          define\n          $$\n          \\begin{align}\n          R_k^> &= \\{x \\in R_k : x^{(j)} > s\\} \\\\\n          R_k^< &= \\{x \\in R_k : x^{(j)} > s\\}\n          \\end{align}\n          $$\n        * Choose $j$ and $s$ \n          to *maximize quality of fit*; i.e.\n          $$\\min |R_k^>| \\cdot \\widehat{Var}(R_k^>) + |R_k^<| \\cdot  \\widehat{Var}(R_k^<)$$\n\n. . .\n\n[We have to change this last line for classification]{.secondary}\n\n\n\n\n\n## How do we measure quality of fit?\n\n\nLet $p_{mk}$ be the proportion of training observations in the $m^{th}$\nregion that are from the $k^{th}$ class.\n\n| |  |\n|---|---|\n| __classification error rate:__ | $E = 1 - \\max_k (\\widehat{p}_{mk})$|\n| __Gini index:__   | $G = \\sum_k \\widehat{p}_{mk}(1-\\widehat{p}_{mk})$ |\n| __cross-entropy:__ | $D = -\\sum_k \\widehat{p}_{mk}\\log(\\widehat{p}_{mk})$|\n\n\nBoth Gini and cross-entropy measure the purity of the classifier (small if all $p_{mk}$ are near zero or 1).  \n\nClassification error is hard to optimize.\n\nWe build a classifier by growing a tree that minimizes $G$ or $D$.\n\n\n<!--\n## Pruning the tree\n\n\n* Cross-validation can be used to directly prune the tree, \n\n* But it is computationally expensive (combinatorial complexity).\n\n* Instead, we use _weakest link pruning_, (Gini version)\n\n$$\\sum_{m=1}^{|T|} \\sum_{k \\in R_m} \\widehat{p}_{mk}(1-\\widehat{p}_{mk}) + \\alpha |T|$$\n\n* $|T|$ is the number of terminal nodes.  \n\n* Essentially, we are trading training fit (first term) with model complexity (second) term (compare to lasso).\n\n* Now, cross-validation can be used to pick $\\alpha$.\n-->\n\n\n\n## Advantages and disadvantages of trees (again)\n\n🎉 Trees are very easy to explain (much easier than even linear regression).  \n\n🎉 Some people believe that decision trees mirror human decision.  \n\n🎉 Trees can easily be displayed graphically no matter the dimension of the data.\n\n🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.\n\n💩 Trees aren't very good at prediction.\n\n💩 Trees are highly variable. Small changes in training data $\\Longrightarrow$ big changes in the tree.\n\nTo fix these last two, we can try to grow many trees and average their performance. \n\n. . .\n\nWe do this next module\n",
    "supporting": [
      "17-nonlinear-classifiers_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}