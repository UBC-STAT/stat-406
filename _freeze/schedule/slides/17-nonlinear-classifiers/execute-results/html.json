{
  "hash": "4951fbf8f4326b1e6e6ddfdaee56cc62",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"17 Nonlinear classifiers\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 23 October 2024\n\n\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n## Two lectures ago\n\n\nWe discussed logistic regression\n\n$$\\begin{aligned}\nPr(Y = 1 \\given X=x)  & = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}} \\\\\nPr(Y = 0 \\given X=x) & = \\frac{1}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}=1-\\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\\end{aligned}$$\n\n## Make it nonlinear\n\nWe can make logistic regression have non-linear decision boundaries by mapping the features to a higher dimension (just like with linear regression)\n\nSay:\n\n__Polynomials__\n\n$(x_1, x_2) \\mapsto \\left(1,\\ x_1,\\ x_1^2,\\ x_2,\\ x_2^2,\\ x_1 x_2\\right)$\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat1 <- generate_lda_2d(100, Sigma = .5 * diag(2)) |> mutate(y = as.factor(y))\nlogit_poly <- glm(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1, family = \"binomial\")\n```\n:::\n\n\n\n\n\n\n\n## Visualizing the classification boundary\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(cowplot)\ngr <- expand_grid(x1 = seq(-2.5, 3, length.out = 100), x2 = seq(-2.5, 3, length.out = 100))\npts_logit <- predict(logit_poly, gr)\ng0 <- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts_logit), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_viridis_b(n.breaks = 6, alpha = .5, name = \"log odds\") +\n  ggtitle(\"Polynomial logit\") +\n  theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\"))\nplot_grid(g0)\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/plot-d1-1.svg){fig-align='center' width=480}\n:::\n:::\n\n\n\n\n\nA linear decision boundary in the higher-dimensional space corresponds to a non-linear decision boundary in low dimensions.\n\n\n## KNN classifiers\n\n<<<<<<< HEAD\n=======\n* We saw $k$-nearest neighbors in the last module.\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(class)\nknn3 <- knn(dat1[, -1], gr, dat1$y, k = 3)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngr$nn03 <- knn3\nggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = knn3), aes(x1, x2, fill = disc), alpha = .5) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, blue), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-3-1.svg){fig-align='center' width=768}\n:::\n:::\n\n\n\n\n\n\n## Choosing $k$ is very important\n\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(406406406)\nks <- c(1, 2, 5, 10, 20)\nnn <- map(ks, ~ as_tibble(knn(dat1[, -1], gr[, 1:2], dat1$y, .x)) |> \n  set_names(sprintf(\"k = %02s\", .x))) |>\n  list_cbind() |>\n  bind_cols(gr)\npg <- pivot_longer(nn, starts_with(\"k =\"), names_to = \"k\", values_to = \"knn\")\n\nggplot(pg, aes(x1, x2)) +\n  geom_raster(aes(fill = knn), alpha = .6) +\n  facet_wrap(~ k) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  geom_point(data = dat1, mapping = aes(x1, x2, shape = as.factor(y)), size = 4) +\n  theme_bw(base_size = 18) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  theme(\n    legend.title = element_blank(),\n    legend.key.height = unit(3, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-4-1.svg){fig-align='center' width=1536}\n:::\n:::\n\n\n\n\n\n* How should we choose $k$?\n\n* Scaling is also very important. \"Nearness\" is determined by distance, so better to standardize your data first.\n\n* If there are ties, break randomly. So even $k$ is strange.\n\n\n## `knn.cv()` (leave one out)\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nkmax <- 20\nerr <- map_dbl(1:kmax, ~ mean(knn.cv(dat1[, -1], dat1$y, k = .x) != dat1$y))\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-6-1.svg){fig-align='center' width=511.68}\n:::\n:::\n\n\n\n\n\nI would use the _largest_ (odd) `k` that is close to the minimum.  \nThis produces simpler, smoother, decision boundaries.\n\n\n\n## Final version\n\n\n::: flex\n::: w-50\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nkopt <- max(which(err == min(err)))\nkopt <- kopt + 1 * (kopt %% 2 == 0)\ngr$opt <- knn(dat1[, -1], gr[, 1:2], dat1$y, k = kopt)\ntt <- table(knn(dat1[, -1], dat1[, -1], dat1$y, k = kopt), dat1$y, dnn = c(\"predicted\", \"truth\"))\nggplot(dat1, aes(x1, x2)) +\n  theme_bw(base_size = 24) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = gr, aes(x1, x2, fill = opt), alpha = .6) +\n  geom_point(aes(shape = y), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/unnamed-chunk-7-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n\n:::\n\n::: w-50\n\n* Best $k$: 19\n\n* Misclassification error: 0.17\n\n* Confusion matrix:\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n         truth\npredicted  1  2\n        1 41  6\n        2 11 42\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n:::\n\n\n\n>>>>>>> 1c36b39 (Update last of classification slides)\n## Trees\n\n::: flex\n\n::: w-50\nWe saw regression trees last module\n\nClassification trees are \n\n- More natural\n- Slightly different computationally\n\nEverything else is pretty much the same\n:::\n\n::: w-50\n![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg)\n:::\n:::\n\n\n\n## Axis-parallel splits\n\nLike with regression trees, classification trees operate by greedily splitting the predictor space\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n\n\n\n\n::: flex\n::: w-50\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnames(bakeoff)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"winners\"                  \n [2] \"series\"                   \n [3] \"age\"                      \n [4] \"occupation\"               \n [5] \"hometown\"                 \n [6] \"percent_star\"             \n [7] \"percent_technical_wins\"   \n [8] \"percent_technical_bottom3\"\n [9] \"percent_technical_top3\"   \n[10] \"technical_highest\"        \n[11] \"technical_lowest\"         \n[12] \"technical_median\"         \n[13] \"judge1\"                   \n[14] \"judge2\"                   \n[15] \"viewers_7day\"             \n[16] \"viewers_28day\"            \n```\n\n\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsmalltree <- tree(\n  winners ~ technical_median + percent_star,\n  data = bakeoff\n)\n```\n:::\n\n\n\n\n\n:::\n\n\n::: w-50\n\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\npar(mar = c(5, 5, 0, 0) + .1)\nplot(bakeoff$technical_median, bakeoff$percent_star,\n  pch = c(\"-\", \"+\")[bakeoff$winners + 1], cex = 2, bty = \"n\", las = 1,\n  ylab = \"% star baker\", xlab = \"times above median in technical\",\n  col = orange, cex.axis = 2, cex.lab = 2\n)\npartition.tree(smalltree,\n  add = TRUE, col = blue,\n  ordvars = c(\"technical_median\", \"percent_star\")\n)\n```\n\n::: {.cell-output-display}\n![](17-nonlinear-classifiers_files/figure-html/plot-partition-1.svg){fig-align='center' width=576}\n:::\n:::\n\n\n\n\n:::\n:::\n\n\n## When do trees do well?\n\n::: flex\n::: w-50\n![](gfx/8.7.png)\n:::\n\n::: w-50\n\n[2D example]{.hand}\n\n[Top Row:]{.primary} \n\ntrue decision boundary is linear\n\nðŸŽ linear classifier \n\nðŸ‘Ž tree with axis-parallel splits\n\n[Bottom Row:]{.primary}\n\ntrue decision boundary is non-linear\n\nðŸ¤® A linear classifier can't capture the true decision boundary\n\nðŸŽ decision tree is successful.\n:::\n:::\n\n\n\n\n## How do we build a tree?\n\n\n1. Divide the predictor space into\n$J$ non-overlapping regions $R_1, \\ldots, R_J$ \n\n  > this is done via greedy, recursive binary splitting\n\n2. Every observation that falls into a given region $R_j$ is given the same prediction\n\n  > determined by majority (or plurality) vote in that region.\n\n\n\n[Important:]{.hand}\n\n* Trees can only make rectangular regions that are aligned with the coordinate axis.\n\n* We use a *greedy* (not optimal) algorithm to fit the tree\n\n\n## Flashback: Constructing Trees for Regression\n\n* While ($\\mathtt{depth} \\ne \\mathtt{max.depth}$):\n    * For each existing region $R_k$\n        * For a given *splitting variable* $j$ and *split value* $s$,\n          define\n          $$\n          \\begin{align}\n          R_k^> &= \\{x \\in R_k : x^{(j)} > s\\} \\\\\n          R_k^< &= \\{x \\in R_k : x^{(j)} > s\\}\n          \\end{align}\n          $$\n        * Choose $j$ and $s$ \n          to *maximize quality of fit*; i.e.\n          $$\\min |R_k^>| \\cdot \\widehat{Var}(R_k^>) + |R_k^<| \\cdot  \\widehat{Var}(R_k^<)$$\n\n. . .\n\n[We have to change this last line for classification]{.secondary}\n\n\n\n\n\n## How do we measure quality of fit?\n\n\nLet $p_{mk}$ be the proportion of training observations in the $m^{th}$\nregion that are from the $k^{th}$ class.\n\n| |  |\n|---|---|\n| __classification error rate:__ | $E = 1 - \\max_k (\\widehat{p}_{mk})$|\n| __Gini index:__   | $G = \\sum_k \\widehat{p}_{mk}(1-\\widehat{p}_{mk})$ |\n| __cross-entropy:__ | $D = -\\sum_k \\widehat{p}_{mk}\\log(\\widehat{p}_{mk})$|\n\n\nBoth Gini and cross-entropy measure the purity of the classifier (small if all $p_{mk}$ are near zero or 1).  \n\nClassification error is hard to optimize.\n\nWe build a classifier by growing a tree that minimizes $G$ or $D$.\n\n\n<!--\n## Pruning the tree\n\n\n* Cross-validation can be used to directly prune the tree, \n\n* But it is computationally expensive (combinatorial complexity).\n\n* Instead, we use _weakest link pruning_, (Gini version)\n\n$$\\sum_{m=1}^{|T|} \\sum_{k \\in R_m} \\widehat{p}_{mk}(1-\\widehat{p}_{mk}) + \\alpha |T|$$\n\n* $|T|$ is the number of terminal nodes.  \n\n* Essentially, we are trading training fit (first term) with model complexity (second) term (compare to lasso).\n\n* Now, cross-validation can be used to pick $\\alpha$.\n-->\n\n\n\n## Advantages and disadvantages of trees (again)\n\nðŸŽ‰ Trees are very easy to explain (much easier than even linear regression).  \n\nðŸŽ‰ Some people believe that decision trees mirror human decision.  \n\nðŸŽ‰ Trees can easily be displayed graphically no matter the dimension of the data.\n\nðŸŽ‰ Trees can easily handle qualitative predictors without the need to create dummy variables.\n\nðŸ’© Trees aren't very good at prediction.\n\nðŸ’© Trees are highly variable. Small changes in training data $\\Longrightarrow$ big changes in the tree.\n\nTo fix these last two, we can try to grow many trees and average their performance. \n\n. . .\n\nWe do this next module\n",
    "supporting": [
      "17-nonlinear-classifiers_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}