{
  "hash": "c83beceb629524632f50358324cbb45f",
  "result": {
    "markdown": "---\nlecture: \"07 Greedy selection\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 05 September 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n$$\n\n\n\n\n## Recap\n\nModel Selection means _select a family of distributions for your data_.\n\nIdeally, we'd do this by comparing the $R_n$ for one family with that for\nanother.\n\nWe'd use whichever has smaller $R_n$.\n\nBut $R_n$ depends on the truth, so we estimate it with $\\widehat{R}$.\n\nThen we use whichever has smaller $\\widehat{R}$.\n\n## Example\n\nThe truth:\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndat <- tibble(\n  x1 = rnorm(100), x2 = rnorm(100),\n  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)\n)\n```\n:::\n\n\nModel 1: `y ~ x1 + x2`\n\nModel 2: `y ~ x1 + x2 + x1*x2`\n\nModel 3: `y ~ x2 + sin(x1 * x2)`\n\n. . .\n\n(What are the families for each of these?)\n\n\n## Fit each model and estimate $R_n$\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nforms <- list(\"y ~ x1 + x2\", \"y ~ x1 * x2\", \"y ~ x2 + sin(x1*x2)\") |> \n  lapply(as.formula)\nfits <- map(forms, ~ lm(.x, data = dat))\nmap(fits, ~ tibble(\n  R2 = summary(.x)$r.sq,\n  training_error = mean(residuals(.x)^2),\n  loocv = mean( (residuals(.x) / (1 - hatvalues(.x)))^2 ),\n  AIC = AIC(.x),\n  BIC = BIC(.x)\n)) |> list_rbind()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 √ó 5\n     R2 training_error loocv   AIC   BIC\n  <dbl>          <dbl> <dbl> <dbl> <dbl>\n1 0.589           21.3  22.9  598.  608.\n2 0.595           21.0  23.4  598.  611.\n3 0.586           21.4  23.0  598.  609.\n```\n:::\n:::\n\n\n## Model Selection vs. Variable Selection\n\nModel selection is very comprehensive\n\nYou choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.\n\nVariable selection is a subset of this. It means \n> choosing which predictors to include in a predictive model\n\nEliminating a predictor, means removing it from the model.\n\nSome [procedures]{.hand} automatically search predictors, and eliminate some.\n\nWe call this variable selection. But the procedure is implicitly selecting a model\nas well.\n\n. . .\n\nMaking this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them.\n\n## Selecting variables / predictors with linear methods\n\n::: flex\n::: w-50\n\nSuppose we have a pile of predictors.\n\nWe estimate models with different subsets of predictors and use CV / Cp / AIC \n/ BIC to decide which is preferred.\n\nSometimes you might have a few plausible subsets. Easy enough to choose with our criterion.\n\nSometimes you might just have a bunch of predictors, then what do you do?\n:::\n\n::: w-50\n__All subsets__: estimate model based on every possible subset of size $|\\mathcal{S}| \\leq \\min\\{n, p\\}$, use one with \nlowest risk estimate\n\n__Forward selection__: start with $\\mathcal{S}=\\varnothing$, add predictors greedily\n\n__Backward selection__: start with $\\mathcal{S}=\\{1,\\ldots,p\\}$, remove greedily\n\n__Hybrid__: combine forward and backward smartly\n:::\n:::\n\n\n## Costs and benefits\n\n\n__All subsets__: \n\nüëç estimates each subset\n\nüí£ takes $2^p$ model fits when $p<n$. If $p=50$, this is about $10^{15}$ models. \n\n__Forward selection__: \n\nüëç computationally feasible\n\nüí£ ignores some models, correlated predictors means bad performance\n\n__Backward selection__: \n\nüëç computationally feasible\n\nüí£ ignores some models, correlated predictors means bad performance\n\nüí£ doesn't work if $p>n$\n\n__Hybrid__: \n\nüëç visits more models than forward/backward\n\nüí£ slower\n\n\n## Synthetic example\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123)\nn <- 406\ndf <- tibble( # like data.frame, but columns can be functions of preceding\n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  x5 = x1 + rnorm(n, sd = .1), # correlated with x1\n  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)\n  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only\n)\n```\n:::\n\n\n. . .\n\n$\\mathbf{x}_1$ and $\\mathbf{x}_2$ are the true predictors\n\nBut the rest are correlated with them\n\n\n## Full model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfull <- lm(y ~ ., data = df)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7739 -1.4283 -0.0929  1.4257  7.5869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  0.03383    0.27700   0.122  0.90287   \nx1           6.70481    2.06743   3.243  0.00128 **\nx2          -0.43945    1.71650  -0.256  0.79807   \nx3           1.37293    1.11524   1.231  0.21903   \nx4          -1.19911    1.17850  -1.017  0.30954   \nx5          -0.53918    1.07089  -0.503  0.61490   \nx6          -1.88547    1.21652  -1.550  0.12196   \nx7          -1.25245    1.10743  -1.131  0.25876   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.231 on 398 degrees of freedom\nMultiple R-squared:  0.6411,\tAdjusted R-squared:  0.6347 \nF-statistic: 101.5 on 7 and 398 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\n## True model\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntruth <- lm(y ~ x1 + x2, data = df)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4519 -1.3873 -0.1941  1.3498  7.5533 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   0.1676     0.2492   0.673   0.5015    \nx1            3.0316     0.1146  26.447   <2e-16 ***\nx2            0.2447     0.1109   2.207   0.0279 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.233 on 403 degrees of freedom\nMultiple R-squared:  0.6357,\tAdjusted R-squared:  0.6339 \nF-statistic: 351.6 on 2 and 403 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\n## All subsets\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(leaps)\ntrythemall <- regsubsets(y ~ ., data = df)\nsummary(trythemall)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df)\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n```\n:::\n:::\n\n\n\n## BIC and Cp\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ntibble(\n  BIC = summary(trythemall)$bic, \n  Cp = summary(trythemall)$cp,\n  size = 1:7\n) |>\n  pivot_longer(-size) |>\n  ggplot(aes(size, value, colour = name)) + \n  geom_point() + \n  geom_line() + \n  facet_wrap(~name, scales = \"free_y\") + \n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](07-greedy-selection_files/figure-revealjs/more-all-subsets1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Forward stepwise\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstepup <- regsubsets(y ~ ., data = df, method = \"forward\")\nsummary(stepup)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"forward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: forward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n```\n:::\n:::\n\n\n\n## BIC and Cp\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ntibble(\n  BIC = summary(stepup)$bic,\n  Cp = summary(stepup)$cp,\n  size = 1:7\n) |>\n  pivot_longer(-size) |>\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange),\n    guide = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](07-greedy-selection_files/figure-revealjs/more-step-forward-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n## Backward selection\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nstepdown <- regsubsets(y ~ ., data = df, method = \"backward\")\nsummary(stepdown)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"backward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: backward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\"\n```\n:::\n:::\n\n\n\n## BIC and Cp\n\n\n::: {.cell layout-align=\"center\" output-location='column'}\n\n```{.r .cell-code}\ntibble(\n  BIC = summary(stepdown)$bic,\n  Cp = summary(stepdown)$cp,\n  size = 1:7\n) |>\n  pivot_longer(-size) |>\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )\n```\n\n::: {.cell-output-display}\n![](07-greedy-selection_files/figure-revealjs/more-step-backward-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n# somehow, for this seed, everything is the same\n\n\n## Randomness and prediction error\n\nAll of that was for one data set\n\nIf we want to know how they compare, we should repeat many times\n  \n  1. Generate training data\n  1. Estimate with different algorithms\n  1. Predict held-out set data\n  1. Examine prediction MSE (on held-out set)\n  \n. . .\n\nI'm not going to do all subsets, just the truth, forward selection, backward, and the full model\n\nFor forward/backward selection, I'll use Cp to choose the final model\n\nThis Cp is using the training set.\n\n\n## Code for simulation\n\n... Annoyingly, no predict method for `regsubsets`, so we make one.\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredict.regsubsets <- function(object, newdata, risk_estimate = c(\"cp\", \"bic\"), ...) {\n  risk_estimate <- match.arg(risk_estimate)\n  chosen <- coef(object, which.min(summary(object)[[risk_estimate]]))\n  predictors <- names(chosen)\n  if (object$intercept) predictors <- predictors[-1]\n  X <- newdata[, predictors]\n  if (object$intercept) X <- cbind2(1, X)\n  drop(as.matrix(X) %*% chosen)\n}\n```\n:::\n\n\n##\n\n\n::: {.cell layout-align=\"center\" hash='07-greedy-selection_cache/revealjs/replication-exercise_4c7c1335d3dcb7312186d46f8a9d1442'}\n\n```{.r .cell-code}\nsimulate_and_estimate_them_all <- function(n = 406) {\n  N <- 2 * n # generate 2x the amount of data (half train, half test)\n  df <- tibble( # generate data\n    x1 = rnorm(N), \n    x2 = rnorm(N, mean = 2), \n    x3 = rexp(N),\n    x4 = x2 + rnorm(N, sd = .1), \n    x5 = x1 + rnorm(N, sd = .1),\n    x6 = x1 - x2 + rnorm(N, sd = .1), \n    x7 = x1 + x3 + rnorm(N, sd = .1),\n    y = x1 * 3 + x2 / 3 + rnorm(N, sd = 2.2)\n  )\n  train <- df[1:n, ] # half the data for training\n  test <- df[(n + 1):N, ] # half the data for evaluation\n  \n  oracle <- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs\n  full <- lm(y ~ ., data = train)\n  stepup <- regsubsets(y ~ ., data = train, method = \"forward\")\n  stepdown <- regsubsets(y ~ ., data = train, method = \"backward\")\n  \n  tibble(\n    y = test$y,\n    oracle = predict(oracle, newdata = test),\n    full = predict(full, newdata = test),\n    stepup = predict(stepup, newdata = test),\n    stepdown = predict(stepdown, newdata = test),\n    truth = drop(as.matrix(test[, c(\"x1\", \"x2\")]) %*% c(3, 1/3))\n  )\n}\n\nset.seed(12345)\nour_sim <- map(1:50, ~ simulate_and_estimate_them_all(406)) |>\n  list_rbind(names_to = \"sim\")\n```\n:::\n\n\n## What is \"Oracle\"\n\n::: flex\n::: w-70\n<a title=\"Helen Simonsson, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Delfi_Apollons_tempel.jpg\"><img width=\"800\" alt=\"Delfi Apollons tempel\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Delfi_Apollons_tempel.jpg/512px-Delfi_Apollons_tempel.jpg\"></a>\n:::\n\n::: w-30\n![](https://www.worldhistory.org/img/r/p/750x750/186.jpg.webp?v=1628028003)\n:::\n:::\n\n\n## Results\n\n\n\n::: {.cell layout-align=\"center\" output-location='column' hash='07-greedy-selection_cache/revealjs/synth-results_28ce75daf6c1698b7987d02628ea19cf'}\n\n```{.r .cell-code}\nour_sim2 <- our_sim %>%\n  group_by(sim) %>%\n  summarise(across(oracle:truth, ~ mean((y - .)^2)), \n            .groups = \"drop\") %>%\n  transmute(across(oracle:stepdown, ~ . / truth - 1))\n  \nour_sim2 %>%\n  pivot_longer(everything(), \n               names_to = \"method\", \n               values_to = \"mse\") %>%\n  ggplot(aes(method, mse, fill = method)) +\n  geom_boxplot(notch = TRUE) +\n  geom_hline(yintercept = 0, linewidth = 2) +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(\n    labels = scales::label_percent()\n  )+\n  ylab(\"% increase in mse relative\\n to the truth\")\n```\n\n::: {.cell-output-display}\n![](07-greedy-selection_files/figure-revealjs/synth-results-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n\n# Next time...\n\n[Module 2]{.large}\n\n[regularization, constraints, and nonparametrics]{.secondary}\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}