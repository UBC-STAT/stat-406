{
  "hash": "3c5683f23b8b76620620eb2b5397af37",
  "result": {
    "markdown": "---\nlecture: \"12 To(o) smooth or not to(o) smooth?\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 02 October 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n\n\n## Last time...\n\n\nWe've been discussing smoothing methods in 1-dimension:\n\n$$\\Expect{Y\\given X=x} = f(x),\\quad x\\in\\R$$\n\nWe looked at basis expansions, e.g.:\n\n$$f(x) \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_k x^k$$ \n\nWe looked at local methods, e.g.:\n\n$$f(x_i) \\approx  s_i^\\top \\y$$\n\n. . .\n\nWhat if $x \\in \\R^p$ and $p>1$?\n\n::: aside\nNote that $p$ means the dimension of $x$, not the dimension of the space of the polynomial basis or something else. That's why I put $k$ above.\n:::\n\n\n\n## Kernels and interactions\n\nIn multivariate nonparametric regression, you estimate a [surface]{.secondary} over the input variables.\n\nThis is trying essentially to find $\\widehat{f}(x_1,\\ldots,x_p)$.\n\nTherefore, this function [by construction]{.secondary} includes interactions, handles categorical data, etc. etc.\n\nThis is in contrast with explicit [linear models]{.secondary} which need you to specify these things.\n\nThis extra complexity (automatically including interactions, as well as other things) comes with tradeoffs.\n\n. . . \n\nMore complicated functions (smooth Kernel regressions vs. linear models) tend to have [lower bias]{.secondary} but [higher variance]{.secondary}.\n\n\n\n## Issue 1\n\nFor $p=1$, one can show that for kernels (with the correct bandwidth)\n\n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\n\n::: callout-danger\n\n_you don't need to memorize these formulas_ but you should know the intuition\n\n:::\n\n## Issue 1\n\nFor $p=1$, one can show that for kernels (with the correct bandwidth)\n\n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\n\nRecall, this decomposition is **squared bias** + **variance** + **irreducible error**\n\n* It depends on the **choice** of $h$\n\n$$\\textrm{MSE}(\\hat{f}) = C_1 h^4 + \\frac{C_2}{nh} + \\sigma^2$$ \n\n* Using $h = cn^{-1/5}$ **balances** squared bias and variance, leads to the above rate. (That balance minimizes the MSE)\n\n## Issue 1\n\n\nFor $p=1$, one can show that for kernels (with the correct bandwidth)\n\n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\n\n### Intuition: \n\nas you collect data, use a smaller bandwidth and the MSE (on future data) decreases\n\n\n## Issue 1\n\nFor $p=1$, one can show that for kernels (with the correct bandwidth)\n\n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\n\n[How does this compare to just using a linear model?]{.primary}\n\n[Bias]{.secondary}\n    \n1. The bias of using a linear model **if the truth nonlinear** is a number $b > 0$ which doesn't depend on $n$.\n2. The bias of using kernel regression is $C_1/n^{4/5}$. This goes to 0 as $n\\rightarrow\\infty$.\n  \n[Variance]{.secondary}\n\n1. The variance of using a linear model is $C/n$ **no matter what**\n2. The variance of using kernel regression is $C_2/n^{4/5}$.\n\n## Issue 1\n\n\nFor $p=1$, one can show that for kernels (with the correct bandwidth)\n\n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\n\n### To conclude: \n\n* bias of kernels goes to zero, bias of lines doesn't (unless the truth is linear).\n\n* but variance of lines goes to zero faster than for kernels.\n\nIf the linear model is **right**, you win. \n\nBut if it's wrong, you (eventually) lose as $n$ grows.\n\nHow do you know if you have enough data? \n\nCompare of the kernel version with CV-selected tuning parameter with the estimate of the risk for the linear model.\n\n\n# ☠️☠️ Danger ☠️☠️\n\nYou can't just compare the CVM for the kernel version to the CVM for the LM. This is because you used CVM to select the tuning parameter, so we're back to the usual problem of using the data twice. You have to do [another]{.hand} CV to estimate the risk of the kernel version at CV selected tuning parameter.\n️\n\n\n## Issue 2\n\nFor $p>1$, there is more trouble.\n\nFirst, lets look again at \n$$\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2$$ \n\nThat is for $p=1$. It's not __that much__ slower than $C/n$, the variance for linear models.\n\nIf $p>1$ similar calculations show,\n\n$$\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .$$\n\n## Issue 2\n\n$$\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .$$\n\n\nWhat if $p$ is big (and $n$ is really big)?\n\n1. Then $(C_1 + C_2) / n^{4/(4+p)}$ is still big.\n2. But $Cp / n$ is small.\n3. So unless $b$ is big, we should use the linear model.\n  \nHow do you tell? Do model selection to decide.\n\nA [very, very]{.secondary} questionable rule of thumb: if $p>\\log(n)$, don't do smoothing.\n\n\n# Next time...\n\nCompromises if _p_ is big\n\nAdditive models and trees\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}