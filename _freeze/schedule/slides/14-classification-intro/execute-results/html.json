{
  "hash": "9bcb73fe51987917f8d5a597637c43d5",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"14 Classification\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 14 October 2024\n\n\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## An Overview of Classification\n\n\n\n* A person arrives at an emergency room with a set of symptoms that\ncould be 1 of 3 possible conditions. Which one is it?\n\n* An online banking service must be able to determine whether each\ntransaction is fraudulent or not, using a customer's location, past\ntransaction history, etc.\n\n* Given a set of individuals sequenced DNA, can we determine whether\nvarious mutations are associated with different phenotypes?\n\n. . .\n\nThese problems are [not]{.secondary} regression\nproblems. They are [classification]{.secondary} problems.\n\n. . .\n\nClassification involves a **categorical response variable** (no notion of \"order\"/\"distance\").\n\n\n## Setup\n\nIt begins just like regression: suppose we have observations\n$$\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$$\n\nAgain, we want to estimate a function that maps $X$ to $Y$ to\npredict as yet observed data.\n\n(This function is known as a [classifier]{.secondary})\n\n\nThe same constraints apply:\n\n* We want a classifier that predicts test data, not just the training\ndata.\n\n* Often, this comes with the introduction of some bias to get lower\nvariance and better predictions.\n\n\n## How do we measure quality?\n\nBefore in regression, we have $y_i \\in \\mathbb{R}$ and use $(y - \\hat{y})^2$ loss to measure accuracy.\n\nInstead, let $y \\in \\mathcal{K} = \\{1,\\ldots, K\\}$\n\n(This is arbitrary, sometimes other numbers, such as $\\{-1,1\\}$ will be\nused)\n\nWe will usually convert categories/\"factors\" (e.g. $\\{\\textrm{cat},\\textrm{dog}\\}$) to integers.\n\n\nWe again make predictions $\\hat{y}=k$ based on the data\n\n\n* We get zero loss if we predict the right class\n* We lose $\\ell(k,k')$ on $(k\\neq k')$ for incorrect predictions\n\n## How do we measure quality?\n\nExample: You're trying to build a fun widget to classify images of cats and dogs.\n\n| Loss | Predict Dog | Predict Cat |\n|:---: | :---: | :---: |\n| Actual Dog | 0 | ? |\n| Actual Cat | ? | 0 |\n\n. . .\n\nUse the zero-one loss (1 if wrong, 0 if right). *Type of error doesn't matter.*\n\n| Loss | Predict Dog | Predict Cat |\n|:---: | :---: | :---: |\n| Actual Dog | 0 | 1 |\n| Actual Cat | 1 | 0 |\n\n## How do we measure quality?\n\nExample: Suppose you have a fever of 39ยบ C. You get a rapid test on campus.\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | ? (Infect others) |\n| Are - | ? (Isolation) | 0 |\n\n. . .\n\nUse a weighted loss; *type of error matters!*\n\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | (LARGE) |\n| Are - | 1 | 0 |\n\n\nNote that one class is \"important\": we sometimes call that one *positive*. Errors are *false positive* and *false negative*.\n\nIn practice, you have to design your loss (just like before) to reflect what you care about.\n\n\n## How do we measure quality?\n\nWe're going to use $g(x)$ to be our classifier. It takes values in $\\mathcal{K}$.\n\nConsider the risk\n$$R_n(g) = E [\\ell(Y,g(X))]$$ If we use the law of\ntotal probability, this can be written\n$$R_n(g) = E\\left[\\sum_{y=1}^K \\ell(y,\\; g(X)) Pr(Y = y \\given X)\\right]$$\nWe minimize this over a class of options $\\mathcal{G}$, to produce\n$$g_*(X) = \\argmin_{g\\in\\mathcal{G}} E\\left[\\sum_{y=1}^K \\ell(y,g(X)) Pr(Y = y \\given X)\\right]$$\n\n## How do we measure quality?\n\n$g_*$ is named the [Bayes' classifier]{.secondary} for loss $\\ell$ in class $\\mathcal{G}$. \n\n$R_n(g_*)$ is the called the [Bayes' limit]{.secondary} or [Bayes' Risk]{.secondary}. \n\nIt's the best we could hope to do *even if we knew the distribution of the data* (recall irreducible error!)\n\nBut we don't, so we'll try to do our best to estimate $g_*$.\n\n\n## Best classifier overall\n\n\nSuppose we actually *know* the distribution of everything, and we've picked $\\ell$ to be the [zero-one loss]{.secondary}\n\n$$\\ell(y,\\ g(x)) = \\begin{cases}0 & y=g(x)\\\\1 & y\\neq g(x) \\end{cases}$$\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | 1 |\n| Are - | 1 | 0 |\n\nThen \n\n$$R_n(g) = \\Expect{\\ell(Y,\\ g(X))} = Pr(g(X) \\neq Y)$$\n\n## Best classifier overall\n\nWant to classify a new observation $(X,Y)$ such that\n$g(X) = Y$ with as high probability as possible. Under zero-one loss, we have\n\n$$g_* = \\argmin_{g} Pr(g(X) \\neq Y) = \\argmin_g 1- \\Pr(g(X) = Y) = \\argmax_g \\Pr(g(X) = Y)$$\n\n. . .\n\n$$\n\\begin{aligned}\ng_* &= \\argmax_{g} E[\\Pr(g(X) = Y | X)]\\\\\n &= \\argmax_{g} E\\left[\\sum_{k\\in\\mathcal{K}}1[g(X) = k]\\Pr(Y=k | X)\\right]\n\\end{aligned}\n$$\n\n. . .\n\nFor each $x$, only one $k$ can satisfy $g(x) = k$. So for each $x$,\n\n$$\ng_*(x) = \\argmax_{k\\in\\mathcal{K}} \\Pr(Y = k | X = x).\n$$\n\n## Estimating $g_*$ Approach 1: Empirical risk minimization\n\n1. Choose some class of classifiers $\\mathcal{G}$. \n\n2. Find $\\argmin_{g\\in\\mathcal{G}} \\sum_{i = 1}^n I(g(x_i) \\neq y_i)$\n\n\n## Estimating $g_*$ Approach 2: Class densities\n\nConsider 2 classes $\\{0,1\\}$: using **Bayes' theorem** (and being loose with notation),\n\n$$\\begin{aligned}\n\\Pr(Y=1 \\given X=x) &= \\frac{\\Pr(X=x\\given Y=1) \\Pr(Y=1)}{\\Pr(X=x)}\\\\\n&=\\frac{\\Pr(X=x\\given Y = 1) \\Pr(Y = 1)}{\\sum_{k \\in \\{0,1\\}} \\Pr(X=x\\given Y = k) \\Pr(Y = k)} \\\\ \n&= \\frac{p_1(x) \\pi}{ p_1(x)\\pi + p_0(x)(1-\\pi)}\\end{aligned}$$\n\n* We call $p_k(x)$ the [class (conditional) densities]{.secondary}\n\n* $\\pi$ is the [marginal probability]{.secondary} $P(Y=1)$\n\n* Similar formula for $\\Pr(Y=0\\given X=x) = p_0(x)(1-\\pi)/(\\dots)$\n\n## Estimating $g_*$ Approach 2: Class densities\n\nRecall $g_*(x) = \\argmax_k \\Pr(Y=k|x)$; so we classify 1 if\n\n$$\\frac{p_1(x) \\pi}{ p_1(x)\\pi + p_0(x)(1-\\pi)} > \\frac{p_0(x) (1-\\pi)}{ p_1(x)\\pi + p_0(x)(1-\\pi)}$$\n\ni.e.,  the [Bayes' Classifier]{.secondary} (best classifier for 0-1 loss) can be rewritten \n\n$$g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} > \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}$$\n\n\n### Estimate everything in the expression above.\n\n* We need to estimate $p_0$, $p_1$, $\\pi$, $1-\\pi$\n* Easily extended to more than two classes\n\n\n## Estimating $g_*$ Approach 3: Regression discretization\n\n\n0-1 loss natural, but discrete. Let's try using [squared error]{.secondary}: $\\ell(y,\\ f(x)) = (y - f(x))^2$\n\n**What will be the optimal classifier here?** (hint: think about regression)\n\n. . .\n\nThe \"Bayes' Classifier\" (sort of...minimizes risk) is just the regression function!\n$$f_*(x) = \\Pr(Y = 1 \\given X=x) = E[ Y \\given X = x] $$ \n\nIn this case, $0\\leq f_*(x)\\leq 1$ not discrete... How do we get a class prediction?\n\n. . .\n\n**Discretize the output**:\n\n$$g(x) = \\begin{cases}0 & f_*(x) < 1/2\\\\1 & \\textrm{else}\\end{cases}$$\n\n1. Estimate $\\hat f(x) = E[Y|X=x] = \\Pr(Y=1|X=x)$ using any method we've learned so far. \n2. Predict 0 if $\\hat{f}(x)$ is less than 1/2, else predict 1.\n\n## Claim: Classification is easier than regression\n\n\n1. Let $\\hat{f}$ be any estimate of $f_*$\n\n2. Let $\\widehat{g} (x) = \\begin{cases}0 & \\hat f(x) < 1/2\\\\1 & else\\end{cases}$\n\n[Proof by picture.]{.hand}\n\n## Claim: Classification is easier than regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\nx <- 1:99 / 100\ny <- rbinom(99, 1, \n            .25 + .5 * (x > .3 & x < .5) + \n              .6 * (x > .7))\ndmat <- as.matrix(dist(x))\nksm <- function(sigma) {\n  gg <-  dnorm(dmat, sd = sigma) \n  sweep(gg, 1, rowSums(gg), '/') %*% y\n}\nfstar <- ksm(.04)\ngg <- tibble(x = x, fstar = fstar, y = y) %>%\n  ggplot(aes(x)) +\n  geom_point(aes(y = y), color = blue) +\n  geom_line(aes(y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1)) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5)\ngg\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Claim: Classification is easier than regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngg + geom_hline(yintercept = .5, color = green)\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## Claim: Classification is easier than regression\n\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntib <- tibble(x = x, fstar = fstar, y = y)\nggplot(tib) +\n  geom_vline(data = filter(tib, fstar > 0.5), aes(xintercept = x), alpha = .5, color = green) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5) + \n  geom_point(aes(x = x, y = y), color = blue) +\n  geom_line(aes(x = x, y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1))\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n\n\n## How to find a classifier\n\n**Why did we go through that math?**\n\nEach of these approaches has strengths/drawbacks:\n\n* [Empirical risk minimization:]{.secondary} Minimize $R_n(g)$ in some family $\\mathcal{G}$\n    \n> (This can be quite challenging as, unlike in regression, the training error is nonconvex)\n\n* [Density estimation:]{.secondary} Estimate $\\pi$ and $p_k$\n\n> (We have to estimate class densities to classify. Too roundabout?)\n\n* [Regression:]{.secondary} Find an estimate $\\hat{f}\\approx E[Y|X=x]$ and compare the predicted value to 1/2\n\n> (Unnatural, estimates whole regression function when we'll just discretize anyway)\n\n# Next time...\nEstimating the densities\n",
    "supporting": [
      "14-classification-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}