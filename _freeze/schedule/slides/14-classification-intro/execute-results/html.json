{
  "hash": "8828d518ad4c4391935b78421ba9265f",
  "result": {
    "markdown": "---\nlecture: \"14 Classification\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 09 October 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n$$\n\n\n\n\n\n## An Overview of Classification\n\n\n\n* A person arrives at an emergency room with a set of symptoms that\ncould be 1 of 3 possible conditions. Which one is it?\n\n* An online banking service must be able to determine whether each\ntransaction is fraudulent or not, using a customer's location, past\ntransaction history, etc.\n\n* Given a set of individuals sequenced DNA, can we determine whether\nvarious mutations are associated with different phenotypes?\n\n. . .\n\nThese problems are [not]{.secondary} regression\nproblems. They are [classification]{.secondary} problems.\n\n\n## The Set-up\n\nIt begins just like regression: suppose we have observations\n$$\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}$$\n\nAgain, we want to estimate a function that maps $X$ to $Y$ to\npredict as yet observed data.\n\n(This function is known as a [classifier]{.secondary})\n\n\nThe same constraints apply:\n\n* We want a classifier that predicts test data, not just the training\ndata.\n\n* Often, this comes with the introduction of some bias to get lower\nvariance and better predictions.\n\n\n## How do we measure quality?\n\nBefore in regression, we have $y_i \\in \\mathbb{R}$ and use squared error loss to measure accuracy: $(y - \\hat{y})^2$.\n\nInstead, let $y \\in \\mathcal{K} = \\{1,\\ldots, K\\}$\n\n(This is arbitrary, sometimes other numbers, such as $\\{-1,1\\}$ will be\nused)\n\nWe can always take \"factors\": $\\{\\textrm{cat},\\textrm{dog}\\}$ and convert to integers, which is what we assume.\n\n\nWe again make predictions $\\hat{y}=k$ based on the data\n\n\n* We get zero loss if we predict the right class\n* We lose $\\ell(k,k')$ on $(k\\neq k')$ for incorrect predictions\n\n\n## How do we measure quality?\n\nSuppose you have a fever of 39ยบ C. You get a rapid test on campus.\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | Infect others |\n| Are - | Isolation | 0 |\n\n## How do we measure quality?\n\nSuppose you have a fever of 39ยบ C. You get a rapid test on campus.\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | 1 |\n| Are - | 1 | 0 |\n\n\n## How do we measure quality?\n\n> We're going to use $g(x)$ to be our classifier. It takes values in $\\mathcal{K}$.\n\n\n## How do we measure quality?\n\nAgain, we appeal to risk\n$$R_n(g) = E [\\ell(Y,g(X))]$$ If we use the law of\ntotal probability, this can be written\n$$R_n(g) = E_X \\sum_{y=1}^K \\ell(y,\\; g(X)) Pr(Y = y \\given X)$$\nWe minimize this over a class of options $\\mathcal{G}$, to produce\n$$g_*(X) = \\argmin_{g\\in\\mathcal{G}} E_X \\sum_{y=1}^K \\ell(y,g(X)) Pr(Y = y \\given X)$$\n\n## How do we measure quality?\n\n$g_*$ is named the [Bayes' classifier]{.secondary} for loss $\\ell$ in class $\\mathcal{G}$. \n\n$R_n(g_*)$ is the called the [Bayes' limit]{.secondary} or [Bayes' Risk]{.secondary}. \n\n[It's the best we could hope to do in terms of]{.hand} $\\ell$ [if we knew the distribution of the data.]{.hand}\n\n. . .\n\nBut we don't, so we'll try to do our best to estimate $g_*$.\n\n\n## Best classifier overall\n\n(for now, we limit to 2 classes)\n\nOnce we make a specific choice for $\\ell$, we can find $g_*$ exactly (pretending we know the distribution)\n\n\nBecause $Y$ takes only a few values, [zero-one]{.secondary}\nloss is natural (but not the only option)\n$$\\ell(y,\\ g(x)) = \\begin{cases}0 & y=g(x)\\\\1 & y\\neq g(x) \\end{cases} \\Longrightarrow R_n(g) = \\Expect{\\ell(Y,\\ g(X))} = Pr(g(X) \\neq Y),$$\n\n## Best classifier overall\n\n| Loss | Test + | Test - |\n|:---: | :---: | :---: |\n| Are + | 0 | 1 |\n| Are - | 1 | 0 |\n\n## Best classifier overall\n\nThis means we want to \nclassify a new observation $(x_0,y_0)$ such that\n$g(x_0) = y_0$ as often as possible\n\n\nUnder this loss, we have\n$$\n\\begin{aligned}\ng_*(X) &= \\argmin_{g} Pr(g(X) \\neq Y) \\\\\n&= \\argmin_{g} \\left[ 1 - Pr(Y = g(x) | X=x)\\right]  \\\\\n&= \\argmax_{g} Pr(Y = g(x) | X=x )\n\\end{aligned}\n$$\n\n\n## Estimating $g_*$\n\n\n\n### Classifier approach 1 (empirical risk minimization):\n\n1. Choose some class of classifiers $\\mathcal{G}$. \n\n2. Find $\\argmin_{g\\in\\mathcal{G}} \\sum_{i = 1}^n I(g(x_i) \\neq y_i)$\n\n\n## Bayes' Classifier and class densities (2 classes)\n\nUsing **Bayes' theorem**, and recalling that $f_*(X) = E[Y \\given X]$\n\n$$\\begin{aligned}\nf_*(X) & = E[Y \\given X] = Pr(Y = 1 \\given X) \\\\ \n&= \\frac{Pr(X\\given Y=1) Pr(Y=1)}{Pr(X)}\\\\\n& =\\frac{Pr(X\\given Y = 1) Pr(Y = 1)}{\\sum_{k \\in \\{0,1\\}} Pr(X\\given Y = k) Pr(Y = k)} \\\\ & = \\frac{p_1(X) \\pi}{ p_1(X)\\pi + p_0(X)(1-\\pi)}\\end{aligned}$$\n\n* We call $p_k(X)$ the [class (conditional) densities]{.secondary}\n\n* $\\pi$ is the [marginal probability]{.secondary} $P(Y=1)$\n\n## Bayes' Classifier and class densities (2 classes)\n\nThe Bayes' Classifier (best classifier for 0-1 loss) can be rewritten \n\n$$g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} > \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}$$\n\n\n### Approach 2: estimate everything in the expression above.\n\n* We need to estimate $p_1$, $p_2$, $\\pi$, $1-\\pi$\n* Easily extended to more than two classes\n\n\n## An alternative easy classifier\n\n\nZero-One loss was natural, but try something else\n\n\nLet's try using [squared error loss]{.secondary} instead:\n$\\ell(y,\\ f(x)) = (y - f(x))^2$\n\n\nThen, the Bayes' Classifier (the function that minimizes the Bayes Risk) is\n$$g_*(x) = f_*(x) = E[ Y \\given X = x] = Pr(Y = 1 \\given X)$$ \n(recall that $f_* \\in [0,1]$ is _still_ the regression function)\n\nIn this case, our \"class\" will actually just be a probability. But this isn't a class, so it's a bit unsatisfying.\n\nHow do we get a class prediction?\n\n. . .\n\nDiscretize the probability:\n\n$$g(x) = \\begin{cases}0 & f_*(x) < 1/2\\\\1 & \\textrm{else}\\end{cases}$$\n\n## Estimating $g_*$\n\n### Approach 3:\n\n1. Estimate $f_*$ using any method we've learned so far. \n2. Predict 0 if $\\hat{f}(x)$ is less than 1/2, else predict 1.\n\n\n\n\n## Claim: Classification is easier than regression\n\n\n1. Let $\\hat{f}$ be any estimate of $f_*$\n\n2. Let $\\widehat{g} (x) = \\begin{cases}0 & \\hat f(x) < 1/2\\\\1 & else\\end{cases}$\n\n[Proof by picture.]{.hand}\n\n## Claim: Classification is easier than regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\nset.seed(12345)\nx <- 1:99 / 100\ny <- rbinom(99, 1, \n            .25 + .5 * (x > .3 & x < .5) + \n              .6 * (x > .7))\ndmat <- as.matrix(dist(x))\nksm <- function(sigma) {\n  gg <-  dnorm(dmat, sd = sigma) \n  sweep(gg, 1, rowSums(gg), '/') %*% y\n}\nfstar <- ksm(.04)\ngg <- tibble(x = x, fstar = fstar, y = y) %>%\n  ggplot(aes(x)) +\n  geom_point(aes(y = y), color = blue) +\n  geom_line(aes(y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1)) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5)\ngg\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-1-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Claim: Classification is easier than regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ngg + geom_hline(yintercept = .5, color = green)\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-2-1.svg){fig-align='center'}\n:::\n:::\n\n\n## Claim: Classification is easier than regression\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code  code-fold=\"true\"}\ntib <- tibble(x = x, fstar = fstar, y = y)\nggplot(tib) +\n  geom_vline(data = filter(tib, fstar > 0.5), aes(xintercept = x), alpha = .5, color = green) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5) + \n  geom_point(aes(x = x, y = y), color = blue) +\n  geom_line(aes(x = x, y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1))\n```\n\n::: {.cell-output-display}\n![](14-classification-intro_files/figure-revealjs/unnamed-chunk-3-1.svg){fig-align='center'}\n:::\n:::\n\n\n\n\n## How to find a classifier\n\n[Why did we go through that math?]{.hand}\n\nEach of these approaches suggests a way to find a classifier\n\n* [Empirical risk minimization:]{.secondary} Choose a set\nof classifiers $\\mathcal{G}$ and find $g \\in \\mathcal{G}$ that minimizes\nsome estimate of $R_n(g)$\n    \n> (This can be quite challenging as, unlike in regression, the\ntraining error is nonconvex)\n\n* [Density estimation:]{.secondary} Estimate $\\pi$ and $p_k$\n\n* [Regression:]{.secondary} Find an\nestimate $\\hat{f}$ of $f^*$ and compare the predicted value to 1/2\n\n\n\n\n##\n\nEasiest classifier when $y\\in \\{0,\\ 1\\}$:\n\n(stupidest version of the third case...)\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nghat <- round(predict(lm(y ~ ., data = trainingdata)))\n```\n:::\n\n\nThink about why this may not be very good. (At least 2 reasons I can think of.)\n\n\n# Next time:\n\nEstimating the densities\n",
    "supporting": [
      "14-classification-intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}