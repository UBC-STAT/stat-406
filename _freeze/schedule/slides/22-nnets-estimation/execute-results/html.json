{
  "hash": "96f0c3d8b2fce723db5685d59b8b3d56",
  "result": {
    "engine": "knitr",
    "markdown": "---\nlecture: \"22 Neural nets - optimization\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 13 November 2024\n\n\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n$$\n\n\n\n\n\n## This lecture\n\n1. How do we train (optimize) NN?\n2. Challenges with NN optimization (and solutions)\n\n\n# How do we train (optimize) NN?\n\n## Neural network review (L hidden layers)\n\n\n::: flex\n::: w-40\n\n$$\n\\begin{aligned}\n&\\boldsymbol a^{(0)} = \\boldsymbol x \\\\\n&\\text{For }  t \\in 1 \\ldots L:\\\\\n&\\qquad \\boldsymbol p^{(t)} = \\boldsymbol W^{(t)} a^{(t-1)} \\\\ \n&\\qquad \\boldsymbol a^{(t)} = \\begin{bmatrix} g(p^{(t)}_1) & \\ldots & g(p^{(t)}_D) \\end{bmatrix}^\\top \\\\ \n&\\hat f_\\mathrm{NN}(x) = \\boldsymbol \\beta^\\top \\boldsymbol a^{(L)}\n\\end{aligned}\n$$\n\n### Terms\n\n* Weights ( $\\boldsymbol \\beta \\in \\R^{K_T}, \\boldsymbol{W}^{(t)} \\in \\R^{D \\times D}$ )\n\n* Preactivations ( $\\boldsymbol p^{(t)} \\in \\R^D$ )\n\n* (Hidden) activations ( $\\boldsymbol a^{(t)} \\in \\R^D$ )\n\n:::\n::: w-55\n![](gfx/two-layer-net.png){fig-align=\"center\" width=600}\n\n### Predictions:\n- Regression: $\\hat Y_j = \\hat f_\\mathrm{NN}(x)$\n- Binary classification: $\\hat Y_j = I[\\hat f_\\mathrm{NN}(x) \\geq 0]$\n\n:::\n:::\n\n\n## Training neural networks: the procedure\n\n\n\n1. Choose the architecture:\\\n   [How many layers, units per layer, activation function $g$?]{.small}\n\n2. Choose the **loss** function:\n   [Measures \"goodness\" of NN predictions]{.small}\n  \n   * Regression: $\\ell(\\hat f_\\mathrm{NN}(x_i), y_i) = \\frac{1}{2}(y_i - \\hat f_\\mathrm{NN}(x_i))^2$\\\n   [(the 1/2 just makes the derivative nice)]{.small}\n  \n   * Classification : $\\ell(\\hat f_\\mathrm{NN}(x_i), y_i) = \\log\\left( 1 + \\exp\\left( (1 - y_i) \\hat f_\\mathrm{NN}(x_i) \\right) \\right)$\n\n3. Choose paramteters $\\boldsymbol W^{(t)}$, $\\beta$ to minimize the loss over training data\n\n   * $\\mathrm{argmin}_{\\boldsymbol W^{(t)}, \\beta} \\sum_{i=1}^n \\ell(\\hat f_\\mathrm{NN}(x_i), y_i)$\n   * We will solve this optimization through *stochastic gradient descent*.\n\n\n## Training neural networks with Stochastic Gradient Descent\n\n$$ \\text{Goal:} \\quad \\argmin_{w_{\\ell,k}^{(t)}, \\beta_{m,\\ell}} \\sum_{i=1}^n \\underbrace{\\ell(\\hat f_\\mathrm{NN}(x_i), y_i)}_{\\ell_i} $$\n\n**SGD update rule:** for a random subset $\\mathcal M \\subset \\{1, \\ldots, n \\}$\n\n$$\n\\begin{align}\n  \\boldsymbol W^{(t)} &\\leftarrow \\boldsymbol W^{(t)} - \\gamma \\: \\frac{n}{\\left| \\mathcal M \\right|} \\sum_{i \\in \\mathcal M} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}} \\\\\n  \\beta &\\leftarrow \\beta - \\gamma \\: \\frac{n}{\\left| \\mathcal M \\right|} \\sum_{i \\in \\mathcal M} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol \\beta}\n\\end{align}\n$$\n\n[Recall that $\\frac{\\partial }{\\partial \\boldsymbol W^{(t)}}\\sum_{i=1}^n \\ell_i \\approx \\frac{N}{\\left| \\mathcal M \\right|} \\sum_{i \\in \\mathcal M} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}}$ ]{.small}\n\n## Training neural networks with Stochastic Gradient Descent\n\n$$ \\text{Goal:} \\quad \\argmin_{w_{\\ell,k}^{(t)}, \\beta_{m,\\ell}} \\sum_{i=1}^n \\underbrace{\\ell(\\hat f_\\mathrm{NN}(x_i), y_i)}_{\\ell_i} $$\n\n$$ \\text{Need:} \\qquad \\frac{\\partial \\ell_i}{\\partial \\boldsymbol \\beta}, \\quad \\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}} $$\n\n**Solution**: Compute $\\frac{\\partial \\ell_i}{\\partial \\boldsymbol \\beta}$ and $\\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}}$ via the *chain rule* \\\n[In NN parlance, \"chain rule\" = \"backpropagation\"]{.secondary}\n\n\n## Backpropagation (Automated Chain Rule)\n \nConsider the *computation graph* corresponding to the 1-hidden layer NN\n\n[ $$\n\\begin{aligned}\n\\boldsymbol p^{(1)} = \\boldsymbol W^{(1)} \\boldsymbol x,\n\\qquad\n\\boldsymbol a^{(1)} = \\begin{bmatrix} g(p^{(1)}_1) & \\ldots & g(p^{(1)}_D) \\end{bmatrix}^\\top,\n\\qquad\nf_\\mathrm{NN}(x) = \\boldsymbol \\beta^\\top \\boldsymbol a^{(1)}\n\\end{aligned}\n$$ ]{.small}\n\n![](gfx/comp_graph_forward.png){fig-align=\"center\" width=1000 fig-caption=\"Forward computation graph\"}\n\n* Each node in the computation graph is computed from a simple and differentiable function.\n\n* So we can compute $\\partial \\ell_i / \\partial \\boldsymbol \\beta$ through *recursive* application of the chain rule.\n\n\n## Backpropagation (Automated Chain Rule)\n\n![](gfx/comp_graph_forward.png){fig-align=\"center\" width=1000 fig-caption=\"Forward computation graph\"}\n\n$$\n\\begin{aligned}\n  \\partial \\ell_i / \\partial \\boldsymbol W^{(1)}\n  &=\n  \\underbrace{\\left( \\partial \\ell_i / \\partial \\boldsymbol p^{(1)} \\right)}_{\\text{compute recursively}}\n  \\underbrace{\\Bigl( \\partial \\overbrace{\\boldsymbol W^{(1)} \\boldsymbol x}^{\\boldsymbol p^{(1)}} / \\partial \\boldsymbol W^{(1)} \\Bigr)^\\top}_{\\text{easy!}}\n  \\\\\n  \\partial \\ell_i / \\partial p^{(1)}_j\n  &=\n  \\underbrace{\\left( \\partial \\ell_i / \\partial a^{(1)}_j \\right)}_{\\text{compute recursively}}\n  \\underbrace{\\Bigl( \\partial \\overbrace{\\boldsymbol g(p^{(1)}_j)}^{a_j^{(1)}} / \\partial p_j^{(1)} \\Bigr)^\\top}_{\\text{easy!}}\n  \\\\\n  &\\vdots\n\\end{aligned}\n$$\n\n## Backpropagation (Automated Chain Rule)\n\n![](gfx/comp_graph_forward.png){fig-align=\"center\" width=400 fig-caption=\"Forward computation graph\"}\n\nApplying these rules recursively, we end up with the *reverse computation graph*\n\n![](gfx/comp_graph_backward.png){fig-align=\"center\" width=1000 fig-caption=\"Backward computation graph\"}\n\n## Backpropagation (Automated Chain Rule)\n\n![](gfx/comp_graph_backward.png){fig-align=\"center\" width=1000 fig-caption=\"Backward computation graph\"}\n\n### Observations\n\n* If this process looks automatable, it is!\\\n  [Modern *autodifferentiation* frameworks automatically perform these recursive  gradients computations for any function that can be written as a *computation graph* (i.e. the composition of simple differentiable primative functions).]{.small}\n  \n* We do not need to compute $\\partial \\ell_i / \\partial \\boldsymbol x$ in order to perform gradient descent. [Why? Can you generalize this rule?]{.secondary}\n\n* However, we do need to compute $\\partial \\ell_i / \\partial \\boldsymbol a^{(1)}$. [Why? Can you generalize this rule?]{.secondary}\n\n<!-- ## Chain rule {.smaller} -->\n\n\n<!-- We want $\\frac{\\partial}{\\partial B} \\hat{R}_i$ and $\\frac{\\partial}{\\partial W_{t}}\\hat{R}_i$ for all $t$. -->\n\n<!-- [Regression:]{.secondary} $\\hat{R}_i = \\frac{1}{2}(y_i - \\hat{y}_i)^2$ -->\n\n\n<!-- $$\\begin{aligned} -->\n<!-- \\frac{\\partial\\hat{R}_i}{\\partial B} &= -(y_i - \\hat{y}_i)\\frac{\\partial \\hat{y_i}}{\\partial B} =\\underbrace{-(y_i - \\hat{y}_i)}_{-r_i}  \\boldsymbol{A}^{(T)}\\\\ -->\n<!-- \\frac{\\partial}{\\partial \\boldsymbol{W}_T} \\hat{R}_i &= -(y_i - \\hat{y}_i)\\frac{\\partial\\hat{y_i}}{\\partial \\boldsymbol{W}_T} = -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\boldsymbol{A}^{(T)}} \\frac{\\partial \\boldsymbol{A}^{(T)}}{\\partial \\boldsymbol{W}_T}\\\\  -->\n<!-- &= -\\left(r_i  B \\odot g'(\\boldsymbol{W}_T \\boldsymbol{A}^{(T)}) \\right) \\left(\\boldsymbol{A}^{(T-1)}\\right)^\\top\\\\ -->\n<!-- \\frac{\\partial}{\\partial \\boldsymbol{W}_{T-1}} \\hat{R}_i &= -(y_i - \\hat{y}_i)\\frac{\\partial\\hat{y_i}}{\\partial \\boldsymbol{W}_{T-1}} = -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\boldsymbol{A}^{(T)}} \\frac{\\partial \\boldsymbol{A}^{(T)}}{\\partial \\boldsymbol{W}_{T-1}}\\\\ -->\n<!-- &= -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\boldsymbol{A}^{(T)}} \\frac{\\partial \\boldsymbol{A}^{(T)}}{\\partial \\boldsymbol{W}_{T}}\\frac{\\partial \\boldsymbol{W}_{T}}{\\partial \\boldsymbol{A}^{(T-1)}}\\frac{\\partial \\boldsymbol{A}^{(T-1)}}{\\partial \\boldsymbol{W}_{T-1}}\\\\ -->\n<!-- \\cdots &= \\cdots -->\n<!-- \\end{aligned}$$ -->\n\n\n\n<!-- ## Mapping it out {.smaller} -->\n\n<!-- Given current $\\boldsymbol{W}_t, B$, we want to get new, $\\widetilde{\\boldsymbol{W}}_t,\\ \\widetilde B$ for $t=1,\\ldots,T$ -->\n\n<!-- * Squared error for regression, cross-entropy for classification -->\n\n<!-- ::: flex -->\n<!-- ::: w-50 -->\n\n<!-- [Feed forward]{.tertiary} `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0a8754;overflow:visible;position:relative;\"><path d=\"M169.4 470.6c12.5 12.5 32.8 12.5 45.3 0l160-160c12.5-12.5 12.5-32.8 0-45.3s-32.8-12.5-45.3 0L224 370.8 224 64c0-17.7-14.3-32-32-32s-32 14.3-32 32l0 306.7L54.6 265.4c-12.5-12.5-32.8-12.5-45.3 0s-12.5 32.8 0 45.3l160 160z\"/></svg>`{=html} -->\n\n<!-- $$\\boldsymbol{A}^{(0)} = \\boldsymbol{X}  \\in \\R^{n\\times p}$$ -->\n\n<!-- Repeat, $t= 1,\\ldots, T$ -->\n\n<!-- 1. $\\boldsymbol{Z}_{t} = \\boldsymbol{A}^{(t-1)}\\boldsymbol{W}_t \\in \\R^{n\\times K_t}$ -->\n<!-- 1. $\\boldsymbol{A}^{(t)} = g(\\boldsymbol{Z}_{t})$ (component wise) -->\n<!-- 1. $\\dot{\\boldsymbol{A}}^{(t)} = g'(\\boldsymbol{Z}_t)$ -->\n\n<!-- $$\\begin{cases} -->\n<!-- \\hat{\\boldsymbol{y}} =\\boldsymbol{A}^{(T)} B \\in \\R^n \\\\ -->\n<!-- \\hat{\\Pi} = \\left(1 + \\exp\\left(-\\boldsymbol{A}^{(T)}\\boldsymbol{B}\\right)\\right)^{-1} \\in \\R^{n \\times M}\\end{cases}$$ -->\n\n<!-- ::: -->\n\n<!-- ::: w-50 -->\n\n\n<!-- [Back propogate]{.secondary} `<svg aria-hidden=\"true\" role=\"img\" viewBox=\"0 0 384 512\" style=\"height:1em;width:0.75em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#e98a15;overflow:visible;position:relative;\"><path d=\"M214.6 41.4c-12.5-12.5-32.8-12.5-45.3 0l-160 160c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0L160 141.2V448c0 17.7 14.3 32 32 32s32-14.3 32-32V141.2L329.4 246.6c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3l-160-160z\"/></svg>`{=html} -->\n\n<!-- $$-r = \\begin{cases} -->\n<!-- -\\left(\\boldsymbol{y} - \\widehat{\\boldsymbol{y}}\\right) \\\\ -->\n<!-- -\\left(1 - \\widehat{\\Pi}\\right)[y]\\end{cases}$$ -->\n\n\n<!-- $$ -->\n<!-- \\begin{aligned} -->\n<!-- \\frac{\\partial}{\\partial \\boldsymbol{B}} \\widehat{R} &= \\left(\\boldsymbol{A}^{(T)}\\right)^\\top \\boldsymbol{r}\\\\ -->\n<!-- -\\boldsymbol{\\Gamma} &\\leftarrow -\\boldsymbol{r}\\\\ -->\n<!-- \\boldsymbol{W}_{T+1} &\\leftarrow \\boldsymbol{B} -->\n<!-- \\end{aligned} -->\n<!-- $$ -->\n\n\n<!-- Repeat, $t = T,...,1$, -->\n\n<!-- 1. $-\\boldsymbol{\\Gamma} \\leftarrow -\\left(\\boldsymbol{\\Gamma} \\boldsymbol{W}_{t+1}\\right) \\odot\\dot{\\boldsymbol{A}}^{(t)}$ -->\n<!-- 1. $\\frac{\\partial R}{\\partial \\boldsymbol{W}_t} = -\\left(\\boldsymbol{A}^{(t)}\\right)^\\top \\Gamma$ -->\n\n<!-- ::: -->\n<!-- ::: -->\n\n\n\n<!-- ## Simple example -->\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- n <- 200 -->\n<!-- df <- tibble( -->\n<!--   x = seq(.05, 1, length = n), -->\n<!--   y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function -->\n<!-- ) -->\n<!-- testdata <- matrix(seq(.05, 1, length.out = 1e3), ncol = 1) -->\n<!-- library(neuralnet) -->\n<!-- nn_out <- neuralnet(y ~ x, data = df, hidden = c(10, 5, 15), threshold = 0.01, rep = 3) -->\n<!-- nn_preds <- map(1:3, ~ compute(nn_out, testdata, .x)$net.result) -->\n<!-- yhat <- nn_preds |> bind_cols() |> rowMeans() # average over the runs -->\n<!-- ``` -->\n\n<!-- ```{r} -->\n<!-- #| eval: false -->\n<!-- #| code-fold: true -->\n<!-- # This code will reproduce the analysis, takes some time -->\n<!-- set.seed(406406406) -->\n<!-- n <- 200 -->\n<!-- df <- tibble( -->\n<!--   x = seq(.05, 1, length = n), -->\n<!--   y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function -->\n<!-- ) -->\n<!-- testx <- matrix(seq(.05, 1, length.out = 1e3), ncol = 1) -->\n<!-- library(neuralnet) -->\n<!-- library(splines) -->\n<!-- fstar <- sin(1 / testx) -->\n<!-- spline_test_err <- function(k) { -->\n<!--   fit <- lm(y ~ bs(x, df = k), data = df) -->\n<!--   yhat <- predict(fit, newdata = tibble(x = testx)) -->\n<!--   mean((yhat - fstar)^2) -->\n<!-- } -->\n<!-- Ks <- 1:15 * 10 -->\n<!-- SplineErr <- map_dbl(Ks, ~ spline_test_err(.x)) -->\n\n<!-- Jgrid <- c(5, 10, 15) -->\n<!-- NNerr <- double(length(Jgrid)^3) -->\n<!-- NNplot <- character(length(Jgrid)^3) -->\n<!-- sweep <- 0 -->\n<!-- for (J1 in Jgrid) { -->\n<!--   for (J2 in Jgrid) { -->\n<!--     for (J3 in Jgrid) { -->\n<!--       sweep <- sweep + 1 -->\n<!--       NNplot[sweep] <- paste(J1, J2, J3, sep = \" \") -->\n<!--       nn_out <- neuralnet(y ~ x, df, -->\n<!--         hidden = c(J1, J2, J3), -->\n<!--         threshold = 0.01, rep = 3 -->\n<!--       ) -->\n<!--       nn_results <- sapply(1:3, function(x) { -->\n<!--         compute(nn_out, testx, x)$net.result -->\n<!--       }) -->\n<!--       # Run them through the neural network -->\n<!--       Yhat <- rowMeans(nn_results) -->\n<!--       NNerr[sweep] <- mean((Yhat - fstar)^2) -->\n<!--     } -->\n<!--   } -->\n<!-- } -->\n\n<!-- bestK <- Ks[which.min(SplineErr)] -->\n<!-- bestspline <- predict(lm(y ~ bs(x, bestK), data = df), newdata = tibble(x = testx)) -->\n<!-- besthidden <- as.numeric(unlist(strsplit(NNplot[which.min(NNerr)], \" \"))) -->\n<!-- nn_out <- neuralnet(y ~ x, df, hidden = besthidden, threshold = 0.01, rep = 3) -->\n<!-- nn_results <- sapply(1:3, function(x) compute(nn_out, testdata, x)$net.result) -->\n<!-- # Run them through the neural network -->\n<!-- bestnn <- rowMeans(nn_results) -->\n<!-- plotd <- data.frame( -->\n<!--   x = testdata, spline = bestspline, nnet = bestnn, truth = fstar -->\n<!-- ) -->\n<!-- save.image(file = \"data/nnet-example.Rdata\") -->\n<!-- ``` -->\n\n<!-- ```{r fun-nnet-spline, echo=FALSE, fig.align='center', fig.width=10, fig.height=4} -->\n<!-- load(\"data/nnet-example.Rdata\") -->\n<!-- plotd |> -->\n<!--   pivot_longer(-x) |> -->\n<!--   ggplot(aes(x, value, color = name)) + -->\n<!--   geom_line(linewidth = 1.5) + -->\n<!--   ylab(\"y\") + -->\n<!--   scale_color_manual(values = c(red, orange, blue)) + -->\n<!--   theme(legend.title = element_blank()) + -->\n<!--   geom_point( -->\n<!--     data = df, mapping = aes(x, y), -->\n<!--     color = \"black\", alpha = .4, shape = 16 -->\n<!--   ) -->\n<!-- ``` -->\n\n\n<!-- ## Different architectures -->\n\n<!-- ```{r nnet-vs-spline-plots, echo=FALSE, fig.align='center',fig.height=6,fig.width=12} -->\n<!-- library(cowplot) -->\n<!-- doppler_nnet <- data.frame(x = NNplot, err = NNerr) -->\n<!-- spl <- data.frame(x = Ks, err = SplineErr) -->\n<!-- best <- c(min(NNerr), min(SplineErr)) -->\n<!-- rel <- function(x) abs(x) / .01 -->\n<!-- g1 <- ggplot(doppler_nnet, aes(x, rel(err), group = 1)) + -->\n<!--   ggtitle(\"Neural Nets\") + -->\n<!--   xlab(\"architecture\") + -->\n<!--   theme(axis.text.x = element_text(angle = 90, vjust = .5)) + -->\n<!--   geom_line(color = orange, linewidth = 1.5) + -->\n<!--   ylab(\"Increase in error over f*\") + -->\n<!--   scale_y_continuous(labels = scales::percent_format()) + -->\n<!--   geom_hline(yintercept = rel(best[1]), color = red, linewidth = 1.5) + -->\n<!--   geom_hline(yintercept = rel(best[2]), color = green, linewidth = 1.5) -->\n<!-- g2 <- ggplot(spl, aes(x, rel(err))) + -->\n<!--   ggtitle(\"Splines\") + -->\n<!--   xlab(\"degrees of freedom\") + -->\n<!--   geom_line(color = orange, linewidth = 1.5) + -->\n<!--   ylab(\"Increase in error over f*\") + -->\n<!--   scale_y_continuous(labels = scales::percent_format()) + -->\n<!--   geom_hline(yintercept = rel(best[1]), color = red, linewidth = 1.5) + -->\n<!--   geom_hline(yintercept = rel(best[2]), color = green, linewidth = 1.5) -->\n<!-- plot_grid(g1, g2) -->\n<!-- ``` -->\n\n## NN training: zooming back out\n\n* NN parameters solve the optimization problem\\\n  $$ \\argmin_{\\boldsymbol W^{(t)}, \\boldsymbol \\beta} \\sum_{i=1}^n \\ell( \\hat f_\\mathrm{NN}(\\boldsymbol x_i), y_i) $$\n  \n* We solve the optimization problem with *stochastic* gradient descent\\\n  [ $$\n  \\begin{align}\n    \\boldsymbol W^{(t)} \\leftarrow \\boldsymbol W^{(t)} - \\gamma \\: \\frac{n}{\\left| \\mathcal M \\right|} \\sum_{i \\in \\mathcal M} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}}, \\qquad\n    \\beta \\leftarrow \\beta - \\gamma \\: \\frac{n}{\\left| \\mathcal M \\right|} \\sum_{i \\in \\mathcal M} \\frac{\\partial \\ell_i}{\\partial \\boldsymbol \\beta}\n  \\end{align}\n  $$ ]{.small}\n  \n* We compute the gradients $\\frac{\\partial \\ell_i}{\\partial \\boldsymbol \\beta}$ and $\\frac{\\partial \\ell_i}{\\partial \\boldsymbol W^{(t)}}$ using the chain rule (i.e. backpropagation)\n\n  * Procedure (intuitivly): reformulate $\\hat f_\\mathrm{NN}(\\boldsymbol x_i)$ as a computation graph, and then \"reverse\" the computation graph.\n  \n  * Modern NN libraries perform this automatically.\n  \n  \n# Challenges with NN training\n(and why they're not actually challenges)\n\n## Challenges with NN training\n\n### Major Challenges\n\n1. Computing $\\sum_{i=1}^n \\partial \\ell_i / \\partial \\boldsymbol W^{(t)}$ requires *a lot* of computation!\n\n1. $\\ell_i$ is a non-convex function of $\\boldsymbol W^{(t)}$ and $\\boldsymbol \\beta$!\\\n   [Extra credit: convince yourself that this is true for any neural network with any activation function.]\n\n   * Gradient descent might get stuck in sub-optimal local minima!\n   \n   * Gradient descent might be unstable / sensitive to initial conditions.\n\n### Minor Challenges\n\n* How do we choose the step size of gradient descent?\n\n\n## Solution: Stochastic Gradient Descent(?!?!)*\n[*It doesn't solve the problem of choosing the step size. That remains a dark art.]{.small}\n\n1. We have already seen how [SGD]{.secondary} can reduce the computational burden of gradient descent by approximating $\\sum_{i=1}^n \\partial \\ell_i / \\partial \\boldsymbol W^{(t)}$ with the *randomized* partial sum $\\frac{n}{|\\mathcal M|} \\sum_{i \\in \\mathcal M} \\partial \\ell_i / \\partial \\boldsymbol W^{(t)}$\n\n2. I claim that SGD also helps (but doesn't guarantee) convergence to \"good\" minima.\n\n   * [I can't theoretically prove this to be true. No one can. (Though some statisticians have made progress.)]{.small}\n   \n   * [Empirically, SGD-trained NN outperforms non-stochasticly trained NN.]{.small}\n   \n   * [I can offer some hand-wavy intuition...]{.small}\n  \n\n## Intuition: why SGD helps optimization\n\n[The stochastisity (noise) in SGD turns out to be a *feature, not a bug*.]{.secondary}\\\nConsider the loss as a function of $w^{(i)}_{ij}$\n\n::: flex\n::: w-65\n* Imagine the loss (as a function of $w^{(i)}_{ij}$) has a \"narrow\" minimum and a \"wide\" minimum\n\n* The steps made by SGD are too imprecise to converge to the \"narrow\" minimum.\\\n  [A more precise optimization method, like non-stochastic gradient descent, would have no problem converging to these.]{.small}\n  \n* The \"wider\" minima that SGD converges to usually yield better predictors.\\\n  [Intuitively, a \"wide\" minimum means that the loss is robust to small perturbations in $w^{(i)}_{ij}$. A robust set of parameters is likely less dependent on any one training set.]{.small}\n:::\n\n::: w-30\n![](gfx/sgd_vs_gd_toy_1d.png){fig-align=\"center\" fig-caption=\"Toy depiction of SGD vs GD converging to 'narrow' vs 'wide' optima.\"}\n\n![](gfx/sgd_vs_gd_toy_2d.png){fig-align=\"center\" fig-caption=\"Toy depiction of SGD vs GD converging to 'narrow' vs 'wide' optima.\"}\n:::\n:::\n  \n<!-- ## Estimation procedures (training) -->\n\n\n<!-- Back-propagation -->\n\n<!-- [Advantages:]{.secondary} -->\n\n<!-- -   It's updates only depend on local -->\n<!--     information in the sense that if objects in the hierarchical model -->\n<!--     are unrelated to each other, the updates aren't affected -->\n\n<!--     (This helps in many ways, most notably in parallel architectures) -->\n\n<!-- -   It doesn't require second-derivative information -->\n\n<!-- -   As the updates are only in terms of $\\hat{R}_i$, the algorithm can -->\n<!--     be run in either batch  or online  mode -->\n\n<!-- [Down sides:]{.tertiary} -->\n\n<!-- -   It can be very slow -->\n\n<!-- -   Need to choose the learning rate -->\n<!--     $\\gamma_t$ -->\n\n## Other Optimization Tricks\n\n* Don't initialize the neural network parameters to be $0$.\\\n  [In general, initialize the parameters to be i.i.d. Gaussian with a reasonable stddev. Deep learning libraries do this automatically.]{.small}\n  \n* Use [batch normalization]()\n\n* Scale input features to be zero mean and unit variance\n\n* Use small minibatches.\\\n  [Small minibatches = noisier (higher variance) gradients = convergence to \"better\" minima]{.secondary}\n  \n* Reduce step size during optimization.\n\n* Use a slightly fancier optimization algorithm than SGD:\n   * SGD with momentum\n   * Adam (SGD with adaptively tuned learning rates)\n\n\n\n\n# Next time...\n\nWhy do NN work so well?\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}