{
  "hash": "c2d1d313773fd70fb7f0dbbd04920336",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 6: Ridge Regression\"\nauthor: \"Geoff Pleiss\"\ndate: last-modified\nformat: html\n---\n\n\n\n## Learning Objectives\n\nBy the end of this lecture, you should be able to:\n\n1. Implement ridge regression using both constrained and penalized formulations\n2. Derive the closed-form solution for ridge regression\n3. Articulate how the regularization parameter affects bias, variance, and the learned parameters\n\n## Overview\n\n- In the last lecture, we saw that the risk of a learned model decomposes into three components: bias, variance, and irreducible error.\n- We also saw that there is often a tradeoff between bias and variance: more flexible models tend to have lower bias but higher variance, while less flexible models tend to have higher bias but lower variance.\n- Over the next few lectures, we will explore a set of techniques to help us navigate this tradeoff.\n- We have already seen one such technique: adding (or removing) covariates to our model to reduce bias (or reduce variance).\n- We will start with **ridge regularization**, a technique that will help us reduce the variance of our learned models by introducing some bias.\n\n## Motivation: Risk Analysis of Ordinary Least Squares\n\n- Consider the OLS estimator for linear regression:\n\n  $$ \\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y. $$\n\n    where $\\boldsymbol X \\in \\mathbb R^{n \\times p}$ is the design matrix (with rows $x_i^\\top$) and $\\boldsymbol Y \\in \\mathbb R^n$ is the vector of training responses.\n\n- Now let's assume that our data i.i.d. generated according to the linear model\n  $$ Y = X^\\top \\beta + \\epsilon, \\qquad \\epsilon \\sim \\mathcal N(0, \\sigma^2), $$\n\n  for some true parameter vector $\\beta \\in \\mathbb R^p$.\n\n- By this model (and again assuming that our data are i.i.d.), we can write the following model for our training data in *matrix form*:\n\n$$ \\boldsymbol Y = \\boldsymbol X \\beta + \\boldsymbol \\epsilon, \\qquad \\boldsymbol \\epsilon \\sim \\mathcal N(\\boldsymbol 0, \\sigma^2 I). $$\n\n- Let's now analyze the bias and variance of the OLS estimator $\\hat{\\beta}_\\mathrm{OLS}$ under this model!\n\n### Bias of OLS\n\nI claim that the OLS estimator is unbiased, i.e.,\n\n$$ \\mathbb E[\\hat{\\beta}_\\mathrm{OLS}] = \\beta. $$\n\n:::{.callout-tip collapse=\"true\" title=\"Derivation\"}\n\\begin{align*}\n\\mathbb E[\\hat{\\beta}_\\mathrm{OLS}]\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y \\right] \\\\\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\mid \\boldsymbol X \\right]}_{= \\boldsymbol X \\beta} \\right] \\\\\n&= \\mathbb E\\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\right] \\\\\n&= \\beta\n\\end{align*}\n:::\n\nSince we have an unbiased estimator, our **average model** ($\\mathbb E[\\hat f_\\mathcal{D}(X) \\mid X ]$ from last lecture) will produce the prediction $X^\\top \\beta$,\nso the bias component of the risk will be zero!\n\n:::{.callout-caution title=\"Check Your Assumptions!\"}\nWe have shown that the bias associated with OLS is zero *under the assumption that the linear model is correct*.\n\nIn practice, it's very unlikely that the linear model is exactly correct (i.e. the true value of $Y$ may depend on covariates we don't have access to, interaction terms, etc.). In that case (i.e. when the linear model is too simple of an approximation for the ground-truth data-generating process), the bias of OLS is non-zero.\n:::\n\n### (Co-)Variance of OLS\n\nThe covariance of this estimator is a little more complicated to derive, but we can use similar techniques to show that:\n\n$$ \\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\mathbb E \\left[ \\left( \\hat \\beta_\\mathrm{OLS}  - \\beta \\right) \\left(\\hat \\beta_\\mathrm{OLS} - \\beta \\right)^\\top \\right]= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]. $$\n\n:::{.callout-tip collapse=\"true\" title=\"Derivation\"}\n\\begin{align*}\n\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS})\n&= \\mathbb E \\left[ \\hat \\beta_\\mathrm{OLS} \\hat \\beta_\\mathrm{OLS} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\boldsymbol Y^\\top \\mid \\boldsymbol X \\right]}_{\\boldsymbol X \\beta \\beta^\\top \\boldsymbol X^\\top + \\sigma^2 \\boldsymbol I} \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\beta^\\top \\underbrace{\\boldsymbol X^\\top \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1}}_{= \\boldsymbol I} \\right] \\\\\n  &\\:\\:\\:+ \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]\n  - \\beta \\beta^\\top \\\\\n&= \\beta \\beta^\\top - \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right].\n\\end{align*}\n:::\n\n### When is Variance Large? (An SVD Analysis)\n\n- To understand when the variance of OLS becomes problematic, we can use the **singular value decomposition** (SVD) of $\\boldsymbol X$.\n\n- Recall that any rectangular $n \\times p$ matrix $\\boldsymbol X$ has an SVD $\\boldsymbol X = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top$\n\n:::{.callout-tip collapse=\"true\" title=\"SVD Review\"}\n\n- $\\boldsymbol U$ is $n \\times p$ with orthonormal columns: $\\boldsymbol U^\\top \\boldsymbol U = \\boldsymbol I$\n- $\\boldsymbol V$ is $p \\times p$ and orthonormal (rows and columns): $\\boldsymbol V^\\top \\boldsymbol V = \\boldsymbol V \\boldsymbol V^\\top = \\boldsymbol I$\n- $\\boldsymbol D$ is $p \\times p$, diagonal, and nonnegative (contains the singular values)\n\n- Using the SVD, we can rewrite the covariance of OLS as:\n\n$$\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\sigma^2 (\\boldsymbol X^\\top \\boldsymbol X)^{-1} = \\sigma^2 (\\boldsymbol V \\boldsymbol D^2 \\boldsymbol V^\\top)^{-1} = \\sigma^2 \\boldsymbol V (\\boldsymbol D^2)^{-1} \\boldsymbol V^\\top$$\n\n:::\n\n- This means that $\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\sigma^2 \\boldsymbol V \\text{diag}(d_1^{-2}, d_2^{-2}, \\ldots, d_p^{-2}) \\boldsymbol V^\\top$, where $d_1, d_2, \\ldots, d_p$ are the singular values.\n\n- **Key insight**: When $\\boldsymbol X$ has small singular values (i.e., when $d_j \\approx 0$ for some $j$), then $d_j^{-2}$ becomes very large, leading to high variance in $\\hat{\\beta}_\\mathrm{OLS}$.\n\n- This situation arises when $\\boldsymbol X$ is **ill-conditioned** or **nearly rank-deficient**, which happens when we have **multicollinearity**: a linear combination of predictor variables is nearly equal to another predictor variable.\n\n- In such cases, $\\hat{\\beta}_\\mathrm{OLS}$ has large, unstable values with high variance.\n\n## Intuition: Fight Variance by \"Shrinking\" Large Coefficients\n\n- **Main idea**: To combat the high variance problem, we can constrain the values of $\\beta$ to be small.\n\n### Constrained Optimization Problem\n\n- Instead of solving the unconstrained OLS problem:\n  $$\\min_\\beta \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2$$\n\n- We can solve a **constrained** optimization problem for some $s > 0$:\n  $$\\min_\\beta \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq s$$\n\n- Here, $\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2$ is the squared $\\ell_2$-norm of $\\beta$.\n\n- This constraint prevents the coefficients from becoming arbitrarily large,\n  which may happen when $\\boldsymbol X$ is ill-conditioned (and thus $\\mathbb E[(\\boldsymbol X^\\top \\boldsymbol X)^{-1}]$ has large entries).\n\n![Illustration of constrained vs. unconstrained optimization solutions ($\\hat \\beta_\\mathrm{OLS}$ vs $\\hat \\beta_\\mathrm{ridge}$) on a 2D toy problem. Contours represent level sets of mean squared error (MSE), i.e. $\\frac{1}{N} \\Vert \\boldsymbol Y - \\boldsymbol X \\hat \\beta \\Vert_2^2$, for various values of $\\hat \\beta$](../figures/ridge.svg)\n\n\n### Graphical Perspective\n\n- Intuitively, why does this constraint reduce variance?\n\n- Recall from the previous lecture that high variance indicates that our learned model is very sensitive to the training data.\n\n- The OLS solution $\\hat \\beta_\\mathrm{OLS}$ depends on the training data $\\mathcal D = \\{(x_i, y_i)\\}_{i=1}^n$.\n\n- If we had a different training set $\\mathcal D'$, we can change the OLS optimization landscape a lot,\nwhich can lead to a solution $\\hat \\beta_\\mathrm{OLS}'$ that is very far away from $\\hat \\beta_\\mathrm{OLS}$.\n\n- If we are instead constrained to have our solution inside the ball of radius $\\sqrt{s}$,\nthen the optimal solutions for $\\mathcal D$ and $\\mathcal D'$ ($\\hat \\beta_s$ and $\\hat \\beta'_s$) will be less far apart!\n\n![Illustration of why ridge reduces variance. Two different training samples ($\\mathcal D$ and $\\mathcal D'$) yield different mean squared error (MSE) optimization problems with different solutions that may be far apart. If we instead constrain the MSE to live inside a ball, the resulting solutions will be less far apart.](../figures/ridge_two_datasets.svg)\n\n:::{.callout-warning title=\"The Trade-off\"}\nBy constraining the coefficients, we can gain a significant reduction in variance, though we will introduce some bias (as we will soon see).\n:::\n\n## Ridge Regression\n\n### Regularization\n\n- An equivalent way to write the constrained optimization problem\n  $$\\hat{\\beta}_s = \\mathrm{argmin}_{\\beta} \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq s$$\n  is as a **regularized** (or **penalized**) optimization with regularization weight $\\lambda$:\n  $$\\hat{\\beta}_\\lambda = \\mathrm{argmin}_{\\beta} \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2$$\n\n- For every $\\lambda$ there is a unique $s$ (and vice versa) that makes $\\hat{\\beta}_s = \\hat{\\beta}_\\lambda$. We will work with the $\\lambda$ formulation.\n\n### Closed Form\n\n- Ridge regression has a closed-form solution (set the derivative with respect to $\\beta$ to zero):\n  $$\\hat{\\beta}_\\lambda = (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol Y$$\n\n- Compare to OLS: $\\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y$\n\n- The $+ \\lambda \\boldsymbol I$ term **stabilizes** the inversion when $\\boldsymbol X^\\top \\boldsymbol X$ is ill-conditioned, preventing division by near-zero singular values.\n\n### Shrinkage\n\n- Using the SVD $\\boldsymbol X = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top$, we can show that:\n  $$\\hat{\\beta}_\\lambda = \\boldsymbol V (\\boldsymbol D^2 + \\lambda \\boldsymbol I)^{-1} \\boldsymbol D \\boldsymbol U^\\top \\boldsymbol Y$$\n\n- Notice that OLS depends on $d_j/d_j^2 = 1/d_j$ while ridge depends on $d_j/(d_j^2 + \\lambda)$.\n\n- **Shrinkage effect**: Ridge regression makes the coefficients smaller relative to OLS, but when $\\boldsymbol X$ has small singular values, ridge compensates with $\\lambda$ in the denominator.\n\n:::{.callout-note title=\"Key Observations\"}\n- $\\lambda = 0$ makes $\\hat{\\beta}_\\lambda = \\hat{\\beta}_\\mathrm{OLS}$\n\n  <details>\n    <summary>Why?</summary>\n  When $\\lambda = 0$, $(\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol y = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol y = \\hat \\beta_\\mathrm{OLS}$.\n  </details>\n\n- $\\lambda \\to \\infty$ makes $\\hat{\\beta}_\\lambda \\to \\boldsymbol{0}$\n\n  <details>\n    <summary>Why?</summary>\n\n  - We can write $(\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} = \\frac{1}{\\lambda} (\\frac{1}{\\lambda} \\boldsymbol X^\\top \\boldsymbol X + \\boldsymbol I)^{-1}$.\n  - As $\\lambda \\to \\infty$, $\\frac{1}{\\lambda} \\to 0$, so $\\frac{1}{\\lambda} (\\frac{1}{\\lambda} \\boldsymbol X^\\top \\boldsymbol X + \\boldsymbol I)^{-1} \\to \\frac{1}{\\lambda} \\boldsymbol I^{-1} = \\boldsymbol 0$.\n  </details>\n\n- Any $0 < \\lambda < \\infty$ penalizes larger values of $\\beta$, effectively shrinking them\n:::\n\n## Effect on Bias/Variance\n\n- **Effect on Bias**: Ridge regression introduces some bias because it shrinks the unbiased OLS coefficients towards zero.\n\n:::{.callout-tip collapse=\"true\" title=\"A Fun Mathematical Interpretation\"}\nNote that we can rewrite the ridge estimator as:\n\n\\begin{align*}\n\\hat{\\beta}_\\lambda\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} {\\color{blue} \\boldsymbol I} \\boldsymbol X^\\top \\boldsymbol Y \\\\\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} {\\color{blue} \\boldsymbol X^\\top \\boldsymbol X} \\underbrace{{\\color{blue} \\left( \\boldsymbol X^\\top \\boldsymbol X \\right)} \\boldsymbol X^\\top \\boldsymbol Y}_{\\hat \\beta_\\mathrm{OLS}} \\\\\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol X \\hat \\beta_\\mathrm{OLS}.\n\\end{align*}\n\nSo $\\hat{\\beta}_\\lambda$ is a transformed version of $\\hat \\beta_\\mathrm{OLS}$.\nSince $\\hat \\beta_\\mathrm{OLS}$ is unbiased,\nand $(\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol X$ is not the identity, $\\hat{\\beta}_\\lambda$ must be biased!\n:::\n\n- **Effect on Variance**: As we've discussed, ridge regression reduces variance by preventing coefficients from becoming too large.\n\n### Picking the Regularization Parameter\n\n- With ridge regression, we're accepting a slight increase in bias with the hope that we'll get a large decrease in variance.\n- As $\\lambda$ increases, bias increases and variance decreases.\n- We want to choose the best value of $\\lambda$ that hits the sweet spot in the bias-variance tradeoff.\n- We can use **cross-validation** to select the value of $\\lambda$ that minimizes risk.\n\n## Example\n\nLet's see ridge regression in action on the prostate cancer dataset from Lab 1:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(prostate, package = \"ElemStatLearn\")\n\nY <- prostate$lpsa\nX <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, xvar = \"lambda\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](lecture_06_ridge_regression_files/figure-html/process-prostate-1.svg){fig-align='center' width=768}\n:::\n:::\n\n\n\n- This first plot shows the values of the $\\hat \\beta_\\mathrm{OLS}$ coefficients as a function of $\\log(\\lambda)$.\n- When $\\log(\\lambda)$ is very negative (i.e., $\\lambda$ is close to zero), the coefficients are at their largest values.\n- As $\\log(\\lambda)$ increases, the coefficients shrink towards zero.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(ridge, main = \"Ridge\")\n```\n\n::: {.cell-output-display}\n![](lecture_06_ridge_regression_files/figure-html/unnamed-chunk-1-1.svg){fig-align='center' width=1056}\n:::\n:::\n\n\n\n- This second plot shows the cross-validation estimation of risk as a function of $\\log(\\lambda)$.\n- We can see that the risk is minimized at some intermediate value of $\\lambda$,\nindicating that it is worthwhile introducing some bias to reduce variance!\n- However, the risk with $\\lambda = 0$ (i.e., OLS) is not much worse than the optimal risk,\nindicating that we may not have been in the high-variance regime with our OLS estimator.\n- (Maybe we're actually in the high bias regime? We'll learn some techniques to address that soon!)\n\n## Summary\n\n- We have now introduced our first method to help us navigate the bias-variance tradeoff:\n  ridge regression, which introduces bias to reduce variance.\n\n- **The Problem**: OLS has high variance when $\\boldsymbol X$ is ill-conditioned (multicollinearity), which occurs when some singular values are near zero, making $(\\boldsymbol X^\\top \\boldsymbol X)^{-1}$ unstable.\n\n- **The Solution**: Ridge regression constrains coefficients via $\\|\\beta\\|_2^2 \\leq s$ (or equivalently adds penalty $\\lambda \\|\\beta\\|_2^2$), which shrinks coefficients toward zero and stabilizes the solution with closed form $\\hat{\\beta}_\\lambda = (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol Y$.\n\n- **The Tradeoff**: Ridge introduces bias but significantly reduces variance, and we can use cross-validation to find the optimal $\\lambda$ that minimizes prediction risk in the bias-variance tradeoff.\n",
    "supporting": [
      "lecture_06_ridge_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}