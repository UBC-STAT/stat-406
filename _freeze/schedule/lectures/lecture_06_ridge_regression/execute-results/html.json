{
  "hash": "525e60e941951c2ebe587f3fe5d34b26",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 5: The Bias-Variance Tradeoff\"\nauthor: \"Geoff Pleiss\"\ndate: last-modified\nformat: html\n---\n\n\n\n## Learning Objectives\n\nBy the end of this lecture, you should be able to:\n\n1. Implement ridge regression using both constrained and penalized formulations\n2. Derive the closed-form solution for ridge regression\n3. Articulate how the regularization parameter affects bias, variance, and the learned parameters\n\n## Overview\n\n- In the last lecture, we saw that the risk of a learned model decomposes into three components: bias, variance, and irreducible error.\n- We also saw that there is often a tradeoff between bias and variance: more flexible models tend to have lower bias but higher variance, while less flexible models tend to have higher bias but lower variance.\n- Over the next few lectures, we will explore a set of techniques to help us navigate this tradeoff.\n- We have already seen one such technique: adding (or removing) covariates to our model to reduce bias (or reduce variance).\n- We will start with **ridge regularization**, a technique that will help us reduce the variance of our learned models by introducing some bias.\n\n## Motivation: Risk Analysis of Ordinary Least Squares\n\n- Consider the OLS estimator for linear regression:\n\n$$ \\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y. $$\n\n- Now let's assume that our data i.i.d. generated according to the linear model\n  $$ Y = X^\\top \\beta + \\epsilon, \\qquad \\epsilon \\sim \\mathcal N(0, \\sigma^2), $$\n\n  for some true parameter vector $\\beta \\in \\mathbb R^p$.\n\n- By this model (and again assuming that our data are i.i.d.), we can write the following model for our training data in *matrix form*:\n\n$$ \\boldsymbol Y = \\boldsymbol X \\beta + \\boldsymbol \\epsilon, \\qquad \\boldsymbol \\epsilon \\sim \\mathcal N(\\boldsymbol 0, \\sigma^2 I). $$\n\n- Let's now analyze the bias and variance of the OLS estimator $\\hat{\\beta}_\\mathrm{OLS}$ under this model!\n\n- (Important note: we are not analyzing the bias and variance of the **risk** of the OLS estimator, but you should be able to derive those following the techniques that we are about to use!)\n\n### Bias of OLS\n\nI claim that the OLS estimator is unbiased, i.e.,\n\n$$ \\mathbb E[\\hat{\\beta}_\\mathrm{OLS}] = \\beta. $$\n\n:::{.callout-tip collapse=\"true\" title=\"Derivation\"}\n\\begin{align*}\n\\mathbb E[\\hat{\\beta}_\\mathrm{OLS}]\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y \\right] \\\\\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\mid \\boldsymbol X \\right]}_{= \\boldsymbol X \\beta} \\right] \\\\\n&= \\mathbb E\\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\right] \\\\\n&= \\beta\n\\end{align*}\n:::\n\nSince we have an unbiased estimator, our **average model** ($\\mathbb E[\\hat f_\\mathcal{D}(X) \\mid X ]$ from last lecture) will produce the prediction $X^\\top \\beta$,\nso the bias component of the risk will be zero!\n\n:::{.callout-caution title=\"Check Your Assumptions!\"}\nWe have shown that the bias associated with OLS is zero *under the assumption that the linear model is correct*.\n\nIn practice, it's very unlikely that the linear model is exactly correct (i.e. the true value of $Y$ may depend on covariates we don't have access to, interaction terms, etc.). In that case (i.e. when the linear model is too simple of an approximation for the ground-truth data-generating process), the bias of OLS is non-zero.\n:::\n\n### (Co-)Variance of OLS\n\nThe covariance of this estimator is a little more complicated to derive, but we can use similar techniques to show that:\n\n$$ \\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\mathbb E \\left[ \\left( \\hat \\beta_\\mathrm{OLS}  - \\beta \\right) \\left(\\hat \\beta_\\mathrm{OLS} - \\beta \\right)^\\top \\right]= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]. $$\n\n:::{.callout-tip collapse=\"true\" title=\"Derivation\"}\n\\begin{align*}\n\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS})\n&= \\mathbb E \\left[ \\hat \\beta_\\mathrm{OLS} \\hat \\beta_\\mathrm{OLS} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\boldsymbol Y^\\top \\mid \\boldsymbol X \\right]}_{\\boldsymbol X \\beta \\beta^\\top \\boldsymbol X^\\top + \\sigma^2 \\boldsymbol I} \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\beta^\\top \\underbrace{\\boldsymbol X^\\top \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1}}_{= \\boldsymbol I} \\right] \\\\\n  &\\:\\:\\:+ \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]\n  - \\beta \\beta^\\top \\\\\n&= \\beta \\beta^\\top - \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right].\n\\end{align*}\n:::\n\n### When is Variance Large? (An SVD Analysis)\n\n## Intuition: Fight Variance by \"Shrinking\" Large Coefficients\n\n### Constrained Optimization Problem\n\n### Graphical Perspective\n\n## Ridge Regression\n\n### Regularization\n\n### Closed Form\n\n### Shrinkage\n\n## Effect on Bias/Variance\n\n### Picking the Regularization Parameter\n\n## Example\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(prostate, package = \"ElemStatLearn\")\n\nY <- prostate$lpsa\nX <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, xvar = \"lambda\", lwd = 3)\n```\n\n::: {.cell-output-display}\n![](lecture_06_ridge_regression_files/figure-html/process-prostate-1.svg){fig-align='center' width=768}\n:::\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nridge <- cv.glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, main = \"Ridge\")\n```\n\n::: {.cell-output-display}\n![](lecture_06_ridge_regression_files/figure-html/unnamed-chunk-1-1.svg){fig-align='center' width=1056}\n:::\n:::\n\n\n\n## Summary\n",
    "supporting": [
      "lecture_06_ridge_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}