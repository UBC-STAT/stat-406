{
  "hash": "95ddc4f60d445348af3f42c8a2a93c53",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 14: Clustering\"\nauthor: \"Geoff Pleiss\"\ndate: last-modified\nformat: html\n---\n\n## Learning Objectives\n\nBy the end of this lecture, you should be able to:\n\n1. Formulate a clustering problem and explain its goals/applications.\n2. Implement and apply k-means clustering to a dataset.\n3. Measure clustering quality and choose an appropriate number of clusters for a given dataset.\n4. Determine when clustering (via k-means) is more appropriate than dimensionality reduction (via PCA).\n5. Identify when an alternative clustering algorithm or variant of k-means is more appropriate for a given dataset.\n\n\n## Motivation\n\n- In the last lecture, we explored dimensionality reduction (through PCA) as our first **unsupervised learning method.**\n- PCA, and its basis expansion/kernel variants, reduce the complexity of data by representing a $\\mathbb R^p$ set of covariates by a $\\mathbb R^k$ vector with $k \\ll p$.\n- In this lecture, we'll explore an alternative simplification:\n  representing a $\\mathbb R^p$ set of covariates by a $\\{1, 2, \\ldots, k\\}$ integer.\n- This integer represents a category, that each observation belongs to,\n  which we refer to as a **cluster label**.\n- We refer to this unsupervised learning method as **clustering**.\n\n**To restate:**\n\n- Clustering maps a set of covariates $X \\in \\mathbb R^p$ to a discrete label $C \\in \\{1, 2, \\ldots, k\\}$.\n- PCA (and its variants) maps a set of covariates $X \\in \\mathbb R^p$ to a continuous vector $Z \\in \\mathbb R^k, k \\ll p$.\n\n:::{.callout-note title=\"Isn't Clustering Just Classification?\"}\nWhile clustering and classification both assign labels to observations, there is a key difference.\n\n- In classification, we have **labeled training data** $\\mathcal D = \\{ (X_1, Y_1), \\ldots, (X_N, Y_N) \\}$.\nWe aim to learn a function to predict the responses $Y_1, \\ldots, Y_N$\nfrom the covariates $X_1, \\ldots, X_N$.\n- In clustering, we do **not** have labeled training data; i.e. our training data are just the covariates $\\mathcal D = \\{ X_1, \\ldots, X_N \\}$.\nThe cluster labels do not correspond to any response; they are simply a way to describe the covariates by a much simpler representation (an integer label rather than a $p$-dimensional vector).\n\n| Mapping | Supervised Learning Problem | Unsupervised Learning Problem |\n|---------|------------------------------|-------------------------------|\n| $\\hat f_{\\mathcal D}(X): \\mathbb R^p \\to \\mathbb R^k$ | Regression | Dimensionality Reduction |\n| $\\hat f_{\\mathcal D}(X): \\mathbb R^p \\to \\{1, 2, \\ldots, k\\}$ | Classification | Clustering |\n:::\n\n### Example of Clustering\n\nConsider the following synthetic dataset of $n=140$ set of $p=2$-dimensional covariates: $\\mathcal D \\in \\{ X_1, \\ldots X_{140} \\}, X_i \\in \\mathbb R^2$.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(Stat406)\nlibrary(mvtnorm)\nlibrary(ggplot2)\nset.seed(406406406)\nX1 <- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))\nX2 <- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))\nX3 <- rmvnorm(40, c(4, 4))\ndata <- rbind(X1, X2, X3)\ntibble(\n  x1 = data[, 1],\n  x2 = data[, 2],\n) |> ggplot(aes(x = x1, y = x2)) +\n  geom_point(size = 2) +\n  theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](lecture_14_clustering_files/figure-html/create-fake-data-1.png){width=384}\n:::\n:::\n\n\n- You'll notice that the data are not uniformly distributed throughout the 2D space.\n- If you squint, it looks like the data belong to three groups, or **clusters**.\n- If we apply a clustering algorithm to learn a mapping $\\hat f_{\\mathcal D}(X): \\mathbb R^2 \\to \\{1, 2, 3\\}$, we might get the following result:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# data: 140 x 2 matrix with our covariates\nassignments <- kmeans(data, centers = 3)$cluster\ntibble(\n  x1 = data[, 1],\n  x2 = data[, 2],\n  cluster = as.factor(assignments)\n) |> ggplot(aes(x = x1, y = x2, colour = cluster)) +\n  geom_point(size = 2) +\n  scale_colour_manual(values = c(\"blue\", \"orange\", \"green\"))\n```\n\n::: {.cell-output-display}\n![](lecture_14_clustering_files/figure-html/clustering-example-1.png){width=480}\n:::\n:::\n\n\n- These cluster labels that we assign to the points may or may not be semantically meaningful.\n  However, it provides us with a way to analyze and summarize the data more easily, and gives us a toehold for discovering patterns and further analysis.\n\n\n\n## The Simplest Clustering Algorithm: K-Means\n\n- While there are many clustering algorithms, the simplest and most widely used is **k-means clustering**.\n- K-means clustering aims to partition $n$ observations into $k$ clusters such that the **within-cluster variation** is minimized.\n- Intuitively, we want to group the data so that the points in each cluster are as similar to each other as possible.\n- As with k-nearest neighbours and kernel methods, similarity will be measured through Euclidean distance.\n\n### Mathematical Formalization\n\nAs with (almost) all other learning procedures we've discussed, we will formalize this clustering procedure as an optimization problem.\n\n- Assume we have fixed $k$, i.e. the number of clusters we hope to partition our data into. (We'll discuss how to choose $k$ momentarily.)\n\n- **What we are trying to learn**: $\\hat f_{\\mathcal D}(X) : \\mathbb R^p \\to \\{ 1, \\ldots, k \\}$, or some function that maps covariates onto clusters.\n\n- For any given $\\hat f_{\\mathcal D}(X) : \\mathbb R^p \\to \\{ 1, \\ldots, k \\}$, let $C_1, \\ldots, C_k \\subset \\mathcal D$ represent the **clusters** induced by this mapping; i.e.\n\n  $$C_j = \\{ X_i \\in \\mathcal D : \\hat f_{\\mathcal D}(X_i) = j \\}$$\n\n  In other words, $C_j$ is the set of all training covariates assigned to cluster $j$.\n\n- The **within-distance variation** of this clustering is defined as:\n\n  $$ W_j := \\frac{1}{|C_j|^2} \\sum_{i, {i'} \\in C_j} \\| X_i - X_{i'} \\|_2^2 $$\n\n  where $|C_j|$ is the number of points in cluster $j$.\n  It is the **average pairwise-distance** between points in cluster $j$.\n\n- We want to minimize the total within-cluster variation, but weight each cluster by its size so larger clusters contribute more.\n- Mathematically, this gives us:\n\n  $$ \\mathrm{argmin}_{\\hat f_{\\mathcal D}(X)} \\frac{1}{2}  \\sum_{j=1}^k \\vert C_j \\vert W_j = \\frac{1}{2} \\sum_{j=1}^k \\frac{1}{\\vert C_j \\vert} \\sum_{i, i' \\in C_j} \\Vert X_i - X_{i'} \\Vert^2_2. $$\n\n  The fraction $1/2$ is for mathematical convenience (it does not affect the maximization), as will become clear shortly.\n\n\n### Cluster Centroids: A Crucial Simplification\n\n- This optimization problem is unwieldy, especially because we have to consider all pairwise distances between points in each cluster.\n(This is, worst case scenario, $O(n^2)$ pairwise distances to consider!)\n\n- Fortunately, we can make use of the following identity to simplify from a pair-wise summation to a per-data-point summation:\n\n  $$ \\frac{1}{2} \\sum_{i, i' \\in C_j} \\| X_i - X_{i'} \\|_2^2 = |C_j| \\sum_{i \\in C_j} \\| X_i - \\mu_j \\|_2^2 $$\n\n  where $\\mu_j$ is the **centroid** of cluster $j$, or the empirical average of the covariates within that cluster:\n\n  $$ \\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} X_i. $$\n\n  :::{.callout-tip title=\"Deriving this Identity\"}\n  If you're confused about where this identity comes from, consider our favourite trick that we used when deriving the bias-variance tradeoff:\n  adding and subtracting zero:\n\n  $$ \\sum_{i, i' \\in C_j} \\| X_i - X_{i'} \\|_2^2 = \\sum_{i, i' \\in C_j} \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2. $$\n\n  Just like we did with the bias-variance tradeoff, expand the square and you'll find that the cross terms vanish!\n\n  <details>\n    <Summary>Details</Summary>\n  While we could go through this derivation step-by-step, it's perhaps simpler if we replace the summations with expectations over the empirical distribution of points in cluster $C_j$.\n\n  $$\n  \\sum_{i, i' \\in C_j} \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2\n  = \\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[ \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2 \\right]\n  $$\n\n  We also note that $\\mu_j$ is the expectation of $X$ under the empirical distribution $\\hat P_{C_j}$:\n\n  $$\n  \\begin{aligned}\n  &\\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[\\Vert (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\Vert_2^2 \\right]\n  \\\\\n  =&\n  \\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[\\Vert  (X_i - \\mathbb E[X]) + (\\mathbb E[X] - X_{i'}) \\Vert_2^2 \\right]\n  \\end{aligned}\n  $$\n\n  and now this derivation should look exactly like what we did in the bias-variance tradeoff!\n  Follow those steps to see the cross terms disappear!\n  </details>\n  :::\n\n- Intuitively, these centroids represent the \"average\" point in each cluster.\n  They are represented on the plots below by black crosses.\n\n\n  ::: {.cell}\n  \n  ```{.r .cell-code  code-fold=\"true\"}\n  centroids <- as.data.frame(kmeans(data, centers = 3)$centers)\n  ggplot() +\n    geom_point(data = tibble(\n      x1 = data[, 1],\n      x2 = data[, 2],\n      cluster = as.factor(assignments)\n    ), aes(x = x1, y = x2, colour = cluster), size = 2) +\n    geom_point(data = centroids, aes(x = V1, y = V2), colour = \"black\", size = 5, shape = 3) +\n    scale_colour_manual(values = c(\"blue\", \"orange\", \"green\"))\n  ```\n  \n  ::: {.cell-output-display}\n  ![](lecture_14_clustering_files/figure-html/kmeans-centroids-1.png){width=480}\n  :::\n  :::\n\n\n### Solving the Optimization Problem\n\nEven after simplifying the optimization problem using centroids, it is still impossible to solve it exactly. Instead, we rely on an iterative algorithm to approximate a solution.\n\n#### The Idea\n\n- We produce a series of clustering assignments\n\n  $$ f_{\\mathcal D}^{(0)}(X), f_{\\mathcal D}^{(1)}(X), f_{\\mathcal D}^{(2)}(X), \\ldots f_{\\mathcal D}^{(t)}(X), \\ldots $$\n\n  so that $\\sum_{j=1}^k |C_j| W_j$ decreases as $t$ increases.\n\n- $f_{\\mathcal D}^{(0)}(X)$ will essentially be random:\n\n  1. Randomly initialize $\\mu_1, \\ldots, \\mu_k$ somewhere in $\\mathbb R^p$.\n  2. Assign each point to the cluster corresponding to the nearest $\\mu_j$ value:\n\n    $$ f_{\\mathcal D}^{(0)}(X_i) = \\mathrm{argmin}_{j \\in \\{1, \\ldots, k\\}} \\| X_i - \\mu_j \\|_2^2. $$\n\n  3. Update $\\mu_1, \\ldots, \\mu_k$ to be the **centroids** of the clusters induced by $f_{\\mathcal D}^{(0)}(X)$:\n\n    $$ \\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} X_i. $$\n\n- To get $f_{\\mathcal D}^{(1)}(X)$ from $f_{\\mathcal D}^{(0)}(X)$, we repeat steps 2 and 3 above.\n\n- More generally, we repeat steps 2 and 3 until the cluster centroids are stable (i.e. when the distance that the cluster centroids change is $< \\epsilon$ for some pre-defined constant $\\epsilon$. Most implementations will define this constant for you.)\n\n![Visualization of k-means iterations (from https://ai.plainenglish.io/)](../figures/kmeans.gif){width=\"500px\"}\n\n- While this algorithm does not guarantee that we find the optimal clustering assignment, it does guarantee that the within-cluster variation $\\sum_{j=1}^k |C_j| W_j$ decreases at each iteration, and thus will eventually converge to some (possibly local) minimum.\n\n:::{.callout-warning title=\"K-Means May be Sensitive to Initialization\"}\nBecause k-means starts with a random initialization of the cluster centroids, it may converge to different clustering assignments on different runs.\nMost implementations of k-means (including R's built-in `kmeans` function) will try many random initializations and return the clustering assignment with the lowest within-cluster variation.\n\nHowever, note that you may get different clustering assignments on different runs of k-means, especially if the clusters are not well-separated.\n:::\n\n\n## Choosing the Number of Clusters $k$\n\n- So far, we've assumed that the number of clusters $k$ is fixed.\n  Now we're going to discuss how to perform **model selection** to choose the best value of $k$.\n- Since this algorithm is an unsupervised method, there's not really a notion of risk (or cross-validation for that matter), so we're going to have to rely on some heuristic methods to choose $k$.\n\n#### What Do We Want From Our Clustering?\n\nIn general, there are three key desiderata for our clusters.\nWe've already talked about one of them:\n\n1. **Points within clusters should be similar to each other.**\n  This quantity is exactly what k-means optimizes for:\n\n  $$ W := \\frac{1}{2} \\sum_{j=1}^k \\vert C_j \\vert W_j = \\frac{1}{2}\\sum_{j=1}^k \\sum_{i \\in C_j} \\left\\| X_i - \\mu_j \\right\\|_2^2 $$\n\n2. **Points in different clusters should be dissimilar to each other.**\n  This quantity is not directly optimized by k-means, but is still important.\n  It is measured by the **between-cluster variation** quantity,\n  which measures the weighted average distance between pairs of cluster centroids:\n\n  $$ B := \\frac{1}{2} \\sum_{j=1}^k \\sum_{j'=1}^k \\frac{|C_j| |C_{j'}|}{n} \\| \\mu_j - \\mu_{j'} \\|_2^2. $$\n\n  As with the within-cluster variation, this summation over all pairs can be simplified to a summation over individual clusters:\n\n  $$ B = \\sum_{j=1}^k |C_j| \\| \\mu_j - \\mu \\|_2^2 $$\n\n  where $\\mu = \\frac{1}{n} \\sum_{i=1}^n X_i$ is the overall mean of the covariates.\n\n3. We don't want too many clusters.\n  If we let $k=n$, then each point is its own cluster,\n  which is not a useful clustering!\n\n:::{.callout-tip collapse=true title=\"The `kmeans` function in R automatically computes all of these variables\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nkmeans(data, centers = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-means clustering with 3 clusters of sizes 49, 39, 42\n\nCluster means:\n        [,1]      [,2]\n1 -0.9528507  2.141750\n2  1.8193830 -1.531834\n3  4.0791847  3.836696\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1]  98.81053 111.78974  81.12076\n (between_SS / total_SS =  80.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n\n\n:::\n:::\n\n\nYou can guess what each of these variable names mean ;)\n:::\n\nWe want to *minimize within-cluster variation* and *maximize between-cluster variation*.\nHowever, these quantities will often be at odds with one another.\n\n:::{.callout-note title=\"Quiz\"}\nAs the total number of clusters $k$ increases...\n\n- Will the within-cluster variation $W$ increase or decrease?\n- Will the between-cluster variation $B$ increase or decrease?\n\n<details>\n  <summary>Answer</summary>\n- As we add more clusters, each cluster will have fewer points.\n  Thus, the points in each cluster will be closer together, and the within-cluster variation decreases.\n\n- As we add more clusters, the between-cluster variation increases!\n  (In my opinion, the intuition for this idea is best understood in reverse.\n   $B$ essentially measures the empirical variance of our $X_i$ if we were to replace each $X_i$ with its cluster centroid $\\mu_{f_{\\mathcal D}(X_i)}$.\n   As we decrease the number of clusters, we are replacing many $X_i$ with the same centroid, which reduces the empirical variance of the data.)\n\nIf you want to understand see this relationship more rigorously,\ntry to describe it via the [law ot total variance](https://en.wikipedia.org/wiki/Law_of_total_variance).\n</details>\n:::\n\nA simple heuristic to balance these three criteria is the following ratio:\n\n$$ \\mathrm{CH} := \\frac{B/(k-1)}{W/(n-k)}. $$\n\n- Note that both $B$ increases and $W$ decreases as $k$ increases.\n- The normalization of both factors by $k-1$ and $n-k$ ensures that we don't just keep increasing $k$ to maximize this quantity.\n- This quantity is known as the **Calinski-Harabasz (CH) index**.\n\n:::{.callout-note title=\"K-means Clustering With Model Selection\"}\nFor all $k \\in \\{2, 3, \\ldots, k_{\\max}\\}$ (where $k_{\\max} < n$ is some maximum number of clusters we want to consider):\n\n- Run k-means clustering with $k$ clusters to get cluster assignments $C_1, \\ldots, C_k$.\n- Compute the CH index for this clustering assignment:\n\nChoose the number of clusters $\\hat k$ that maximizes the CH index.\n:::\n\n\n## K-Means vs PCA\n\n- We now have learned about two unsupervised learning methods: PCA and k-means clustering.\n- Both are useful when you want to simplify a complex dataset for further analysis\nbut you don't have labeled training data.\n- However, when should you use one method over the other?\n- The answer depends on your analysis, goals, and the data.\n  If you want to compress the data into a distinct number of categories, clustering is likely more appropriate.\n- However, if you want to represent data on a spectrum (or to visualize the data), PCA is more important.\n\n**Example clustering problem**:\nYou are studying birds, and you have collected measurements of their beak length, beak depth, wing length, and weight.\nYou suspect there might be several distinct species of birds in your dataset.\nSince species are distinct categories, it makes sense to use clustering to group the birds into categories that may correspond to species or sub-species.\n\n**Example dimensionality reduction problem**:\nYou are studying mental health data. You have collected survey responses from individuals on various aspects of their mental health, including stress levels, anxiety, depression, and overall well-being.\nYou want to reduce these multiple dimensions into a smaller set of underlying factors that capture the main variations in mental health.\nSince many mental health factors exist on a spectrum, dimensionality reduction is more appropriate here.\n\n\n## Other Clustering Variants\n\nThere are many alternative flavours of clustering that can be more appropriate for different applications/data types:\n\n- **Hierarchical clustering** organizes clusters in a tree-like structure, creating hierarchical relationships between the different clusters. This algorithm is especially popular for biological data.\n\n- **Gaussian mixture models** are a generalization of k-means where the cluster distances are generalized and cluster membership becomes \"fuzzy\" or probabilistic. You may learn about this method in a more advanced machine learning class.\n\n- **Spectral clustering** is similar to k-means clustering, except it is designed for *graphical data* (i.e. if your data is a social network, a \"musical influence\" chart, etc.)\n\nYou don't need to know any of these, but be aware that they're out there.\nAlso, if k-means doesn't immediately work for your data, with some googling you'll likely find a variant that does!\n\n\n## Summary\n\n- Clustering is an unsupervised learning method that maps covariates $X \\in \\mathbb R^p$ to discrete cluster labels $C \\in \\{1, 2, \\ldots, k\\}$.\n- K-means clustering is a simple and widely used clustering algorithm that aims to minimize within-cluster variation, as measured by Euclidean distance.\n- the centroids to be the mean of the points in each cluster.\n- The number of clusters $k$ can be chosen using heuristic methods like the Calinski-Harabasz index.\n- There are many other clustering algorithms and variants that may be more appropriate for different data types (e.g. graph data) and applications (e.g. biological data).\n",
    "supporting": [
      "lecture_14_clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}