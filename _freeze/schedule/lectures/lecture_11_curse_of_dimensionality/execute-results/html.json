{
  "hash": "88749f2ca3089438b9f592594f66a4ae",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 11: k-Nearest Neighbours, Curse of Dimensionality\"\nauthor: \"Geoff Pleiss\"\ndate: last-modified\nformat: html\n---\n\n\n\n## Learning Objectives\n\nBy the end of this lecture, you should be able to:\n\n1. Implement k-nearest neighbours/kernel smoothing for regression and classification\n2. Analyze the bias-variance tradeoff as a function of k/bandwidth\n3. Estimate the degrees of freedom for kNN/kernel smoothed models\n4. Derive how prediction error scales with dimensionality for basis expansions, kNN, and kernel smoothing\n5. Compare the dimensional scaling of parametric vs nonparametric methods\n\n\n## Motivation\n\n- In the last lecture, we considered ridge regression with $d > n$ basis expansions.\n- When we took ridge regression with Fourier bases to the limit, we ended up with the following predictor:\n\n  $$\n  \\hat{f}(X) = \\sum_{i=1}^n \\alpha_i K(X, X_i)\n  $$\n\n  - $K(X, X_i) = \\exp\\left( -\\tfrac{1}{2 \\gamma^2} \\Vert X - X_i \\Vert_2^2 \\right)$ is a kernel function that measures similarity between $X$ and $X_i$.\n  - $\\gamma$, the bandwidth, is a hyperparameter that can be tuned with cross validation.\n  - The coefficients $\\alpha_i$ can be computed in closed form, and depend on the ridge penalty $\\lambda$.\n\n- This **nonparametric model** has two notable properties:\n\n  1. It defines its predictions through similarity between training points (rather than a fixed set of parameters $\\beta$)\n\n  2. Its degrees of freedom scales with $n$ (the number of training points) rather than being fixed at $p$ (the number of covariates).\n\n\n### The Best Possible Model?\n\nThere are many advantages of this nonparametric model:\n\n1. It is **infinitely powerful.** Recall that an infinite Fourier series can represent (almost) any function, so this predictive model can represent almost any $\\mathbb E[Y|X]$.\n(In contrast, linear regression can only represent linear $\\mathbb E[Y|X]$, polynomial regression with $d^\\mathrm{th}$ degree polynomials can only represent $\\mathbb E[Y|X]$ that are polynomials of degree $d$, etc.)\n\n2. It is **simple.** Amazingly, we can write this complex model in a single line of math, and we can even derive the $\\alpha_i$ coefficients in closed form.\n\n3. We can **control its variance.** Even though the model is infinitely powerful, with a large enough ridge penalty $\\lambda$ we can construct a model that balances bias and variance well.\n\nSo why would we ever consider using anything else?\nWhy would a linear model ever be preferable to this?\n\n\n### The Drawback\n\n- While this model is extremely powerful (and often very useful),\nit suffers from a phenomenon known as the **curse of dimensionality**.\n- Briefly, this model (potentially) becomes exponentially less effective as we increase the number of covariates $p$.\n- To understand this phenomenon, we will consider a related nonparametric model known as **k-nearest neighbours** (kNN) which will be easier to analyze.\n\n\n## Our Second Nonparametric Model: k-Nearest Neighbours\n\n- If we analyze our kernel function:\n\n  $$\n  K(X, X_i) = \\exp\\left( -\\tfrac{1}{2 \\gamma^2} \\Vert X - X_i \\Vert_2^2 \\right),\n  $$\n\n  we see that it is largest when $X$ is close to $X_i$, and decays to zero as $X$ moves away from $X_i$.\n\n- It is likely that this kernel function will be $\\approx 1$ for only a few training points $X_i$ that are close to $X$, and $\\approx 0$ for most other training points.\n\n- Therefore, what if we make the following approximation to the kernel function:\n\n  $$\n  K(X, X_i) \\approx \\begin{cases}\n  1 & \\text{if } X_i \\text{ is one of the $k$ nearest neighbours of } X \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n  $$\n\n  where the **nearest neighbours** of $X$ are the $k$ training points $X_i$ that are closest to $X$ in Euclidean distance.\n\n  - Here, $k$ (the number of nearest neighbours) is a hyperparameter that we will touch upon in a second.\n\n  - This plot depicts the **k-nearest neighbours** (kNN) function with $k=3$. The red points are the 3 nearest training $X_i$s of $X = 85$ (the red vertical line).\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code  code-fold=\"true\"}\n    library(tidyverse)\n    library(cowplot)\n    data(arcuate, package = \"Stat406\")\n    set.seed(406406)\n    arcuate_unif <- arcuate |> slice_sample(n = 40) |> arrange(position)\n    test_position <- 85\n    nn <-  3\n    seq_range <- function(x, n = 101) seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out = n)\n    neibs <- sort.int(abs(arcuate_unif$position - test_position), index.return = TRUE)$ix[1:nn]\n    arcuate_unif$neighbours = seq_len(40) %in% neibs\n    ggplot(arcuate_unif, aes(position, fa, colour = neighbours)) +\n      geom_point() +\n      scale_colour_manual(values = c(\"blue\", \"red\")) +\n      geom_vline(xintercept = test_position, colour = \"red\") +\n      annotate(\"rect\", fill = \"red\", alpha = .25, ymin = -Inf, ymax = Inf,\n              xmin = min(arcuate_unif$position[neibs]),\n              xmax = max(arcuate_unif$position[neibs])\n      ) +\n      theme(legend.position = \"none\")\n    ```\n    \n    ::: {.cell-output-display}\n    ![](lecture_11_curse_of_dimensionality_files/figure-html/plot-knn-1.png){width=384}\n    :::\n    :::\n\n\n\n- We could now approximate the $\\alpha_i$ coefficients with an equal weighting of the responses of the $k$ nearest neighbours:\n\n  $$\n  \\alpha_i \\approx \\begin{cases}\n  \\frac{1}{k} Y_i & \\text{if } X_i \\text{ is one of the $k$ nearest neighbours of } X \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n  $$\n\n- The result is the following **k-nearest neighbours (kNN) predictor**:\n\n  $$\n  \\hat{f}_\\mathcal{D}(X) = \\frac{1}{k} \\sum_{i \\in N_k(X)} Y_i,\n  \\qquad \\frac{1}{k} \\sum_{i \\in N_k(X)} Y_i \\approx \\sum_{i=1}^n \\alpha_i K(X, X_i)\n  $$\n\n  where $N_k(X)$ is the set of indices of the $k$ nearest neighbours of $X$.\n\n:::{.callout-tip title=\"kNN is a Common Method\"}\nWhile we have derived kNN as an approximation to kernel ridge regression,\nit is a very well-used method in practice.\n(It is arguably the most common nonparametric method, and used in practice more than kernel ridge regression.)\n:::\n\n### Bias and Variance as a Function of k\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nplot_knn <- function(k) {\n  ggplot(arcuate_unif, aes(position, fa)) +\n    geom_point(colour = \"blue\") +\n    geom_line(\n      data = tibble(\n        position = seq_range(arcuate_unif$position),\n        fa = FNN::knn.reg(\n          arcuate_unif$position, matrix(position, ncol = 1),\n          y = arcuate_unif$fa,\n          k = k\n        )$pred\n      ),\n      colour = \"orange\", linewidth = 2\n    ) + ggtitle(paste(\"k =\", k))\n}\n\ng1 <- plot_knn(1)\ng2 <- plot_knn(5)\ng3 <- plot_knn(length(arcuate_unif$position))\nplot_grid(g1, g2, g3, ncol = 3)\n```\n\n::: {.cell-output-display}\n![](lecture_11_curse_of_dimensionality_files/figure-html/knn-changing-k-1.png){width=1152}\n:::\n:::\n\n\n\n- The above plots show the kNN regression function for different values of $k$.\n- $k=1$ and $k=40$ are poor fits and have high risk, while $k=5$ is likely a good predictor.\n- When $k$ is too large or too small, we are not balancing bias and variance well.\n  But which extreme are we in?\n\n:::{.callout-note collapse=\"true\" title=\"(Intuitive) Bias-Variance Tradeoff for kNN\"}\n- As $k \\to 1$, our prediction at a point $X$ depends only on a single training point $(X_i, Y_i)$, so it is very dependent on our *specific training sample*.\n- As $k \\to \\infty$, all of our training points are used to make predictions at any point $X$.\n  $\\hat{f}_\\mathcal{D}(X) \\to \\frac{1}{n} \\sum_{i=1}^n Y_i$ becomes the sample mean of the $Y_i$, which is not going to be a good estimate of $\\mathbb E[Y|X]$ regardless of our specific training sample.\n- Thus, **small $k$ leads to high variance, while large $k$ leads to high bias.**\n:::\n\n### Degrees of Freedom of kNN\n\n- Another way we can analyze the bias-variance tradeoff of kNN is through its degrees of freedom.\n- First, we note that the predictions on the training data $\\hat{\\boldsymbol Y} = [ \\hat Y_1, \\ldots, \\hat Y_n ]^\\top$ can be written in matrix form as:\n\n  $$\n  \\hat{\\boldsymbol Y} = \\underbrace{\\boldsymbol H}_{n \\times n} \\underbrace{\\boldsymbol Y}_{n \\times 1},\n  \\qquad\n  H_{ij} = \\begin{cases}\n  \\frac{1}{k} & \\text{if } X_j \\text{ is one of the $k$ nearest neighbours of } X_i \\\\\n  0 & \\text{otherwise}\n  \\end{cases}\n  $$\n\n- For predictive models that can be written in this form, recall that the degrees of freedom is given by:\n\n  $$\n  \\mathrm{df}_\\mathrm{kNN} = \\mathrm{trace}(\\boldsymbol H) = \\sum_{i=1}^n H_{ii}\n  $$\n\n- Finally, noting that $X_i$ is always a nearest neighbour of itself, we have $H_{ii} = \\frac{1}{k}$ for all $i$, and thus:\n\n  $$\n  \\mathrm{df}_\\mathrm{kNN} = \\sum_{i=1}^n H_{ii} = \\sum_{i=1}^n \\frac{1}{k} = \\frac{n}{k}\n  $$\n\n- Increasing $k$ leads to fewer degrees of freedom (i.e. a less flexible model, lower variance, etc.), but note that $\\mathrm{df} = O(n)$ for any fixed $k$.\n\n- Thus, kNN is **non-parametric** because its flexibility increases with the amount of training data.\n\n\n## The Curse of Dimensionality\n\nNow, onto the bad stuff.\n\n- With kNN as our easy-to-analyze nonparametric model, we can now analyze how its performance scales with the number of covariates $p$.\n\n- For kNN (or nonparametric models in general) to perform well, $X$ needs to have \"similar enough\" training points $X_i$ nearby.\n\n- However, as the number of covariates $p$ increases, Euclidean distance breaks down as a meaningful measure of similarity, making it unlikely that $X$'s nearest neighbours are actually similar.\n\n### Euclidean Distance Breaks Down in High Dimensions\n\nTo understand why distance breaks down in high dimensions, consider the following thought experiment:\n\n- Suppose we have $x_1, x_2, \\ldots, x_n$ training points distributed *uniformly* within a $p$-dimensional ball of radius 1.\n- For a test point $x$ at the center of the ball, consider its $k = n/10$ nearest neighbours.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](lecture_11_curse_of_dimensionality_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n\n- The red points are the $n/10$ nearest neighbours of $x$ (the black point at the center), which are all contained within the inner dotted circle.\n- The inner circle is a lot smaller than the outer circle, but this $2D$ plot gives the wrong intuition for higher dimensions!\n\n#### $p=2$\n\n- What is the radius of the inner circle relative to the outer circle?\n\n- In expectation, the $n/10$ nearest neighbours of $x$ will take up $10\\%$ of the area of the outer circle.\n\n- Since the area of a circle scales with the square of its radius, the inner circle has a radius of $\\sqrt{0.1} \\approx 0.316$.\n\n#### $p=3$\n\n- Now consider $p=3$ dimensions (so that our radius 1 ball is a unit sphere).\n- The sub-space containing its $n/10$ nearest neighbours is now a sphere with $10\\%$ of the volume of the outer sphere.\n- Since the volume of a sphere scales with the cube of its radius, the inner sphere has a radius of $0.1^{1/3} \\approx 0.464$.\n\n#### Most Points Live Near the Boundary as $p$ Increases\n\nMore generally, for $p$ dimensions, the radius $r$ of the inner ball containing the $n/10$ nearest neighbours is given by:\n\n  $$\n  r = 0.1^{1/p}\n  $$\n\nwhere $p$ is the number of dimensions (covariates).\nThis number grows very quickly as $p$ increases:\n\n- When $p=10$, $r = (0.1)^{1/10} \\approx 0.794$(!)\n- When $p=100$, $r = (0.1)^{1/100} \\approx 0.977$(!!)\n- When $p=1000$, $r = (0.1)^{1/1000} \\approx 0.999$(!!!)\n\nIn other words, in a $1000$-dimensional space, even the $10\\%$ of nearest neighbours are going to live on the boundary of the unit ball!\n\n**Why is this problematic?**\n\n- As dimensionality increases, all points become maximally far apart from one another.\n- With such large distances, we can't meaningfully distinguish between \"similar\" and \"different\" inputs.\n- Distance becomes (exponentially) meaningless in high dimensions.\n\n## How to Overcome this Curse of Dimensionality\n\n- Nonparametric methods, which typically make predictions from distance-based similarity, become exponentially less effective as the number of covariates $p$ increases.\n- To meaningfully distinguish between \"similar\" and \"different\" inputs, we need an exponentially large amount of training data $n$ as $p$ increases.\n- The theoretical risk of kNN (and other nonparametric methods) after we tune $k$ to optimally balance bias and variance scales as:\n\n  $$\n  R_n^{(\\mathrm{kNN})} = \\underbrace{\\frac{C_1^{(\\mathrm{kNN})}}{n^{4/(4+p)}}}_{\\mathrm{bias}^2} +\n  \\underbrace{\\frac{C_2^{(\\mathrm{kNN})}}{n^{4/(4+p)}}}_{\\mathrm{var}} +\n  \\sigma^2\n  $$\n\n  where $C_1^{(\\mathrm{kNN})}, C_2^{(\\mathrm{kNN})}$ are constants that depend on the distribution of $(X, Y)$.\n\n  - To halve the bias or variance, we need to increase $n$ by a factor of $2^{(4+p)/4}$.\n  - For $p=4$, this is a factor of $2$.\n  - For $p=36$, this is a factor of $1024$.\n\n- In contrast, the risk of linear regression scales as:\n\n  $$\n  R_n^{(\\mathrm{OLS})} = \\underbrace{C_1^{(\\mathrm{OLS})}}_{\\mathrm{bias}^2} +\n  \\underbrace{\\frac{C_2^{(\\mathrm{OLS})}}{n/p}}_{\\mathrm{var}} +\n  \\sigma^2\n  $$\n\n  - To halve the variance, we only need to increase $n$ by a factor of $2$ (regardless of $p$).\n  - (Of course, the bias $C_1^{(\\mathrm{OLS})}$ may be large if $\\mathbb E[Y|X]$ is not linear.)\n\n### In Practice\n\n- Surprisingly, kNN and other nonparametric methods can work well in practice, even for moderate $p$.\n- For example, on the classic classification dataset MNIST ($p=784$), kNN achieves $97.3\\%$ accuracy from just $60,000$ training points.\n- The reason for this empirical success cannot be easily formalized, but the intuition is that real-world data often has \"low-dimensional structure\" (e.g. images of handwritten digits lie on a low-dimensional manifold within $\\mathbb R^{784}$) where Euclidean distance is still meaningful.\n- So you should not avoid nonparametric methods altogether on high-dimensional data, but be aware of their limitations.\n\n\n## Summary\n- kNN is a simple nonparametric method that makes predictions based on the $k$ nearest training points.\n- However, nonparametric methods like kNN suffer from the curse of dimensionality, where distance becomes meaningless in high dimensions, and exponentially more training data is required to reduce risk.\n- High dimensional spaces are weird and defy our common $\\mathbb R^2$ and $\\mathbb R^3$ intuitions.\n",
    "supporting": [
      "lecture_11_curse_of_dimensionality_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}