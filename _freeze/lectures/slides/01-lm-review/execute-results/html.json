{
  "hash": "38b6dd679b6e7739db41a0031babf1c9",
  "result": {
    "markdown": "---\nlecture: \"01 Linear model review\"\nformat: revealjs\nmetadata-files: \n  - _metadata.yml\n---\n---\n---\n\n## {{< meta lecture >}} {.large background-image=\"gfx/smooths.svg\" background-opacity=\"0.3\"}\n\n[Stat 406]{.secondary}\n\n[{{< meta author >}}]{.secondary}\n\nLast modified -- 16 August 2023\n\n\n\n$$\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n$$\n\n\n\n\n\n## The normal linear model\n\nAssume that \n\n$$\ny_i = x_i^\\top \\beta + \\epsilon_i.\n$$\n\n::: {.incremental}\n\n1. What is the mean of $y_i$?\n2. What is the distribution of $\\epsilon_i$?\n3. What is the notation $\\mathbf{X}$ or $\\mathbf{y}$?\n\n:::\n\n\n## Drawing a sample\n\n$$\ny_i = x_i^\\top \\beta + \\epsilon_i.\n$$\n\nHow would I **create** data from this model (draw a sample)?\n\n. . .\n\nSet up parameters\n\n\n::: {.cell}\n\n```{.r .cell-code}\np <- 3\nn <- 100\nsigma <- 2\n```\n:::\n\n\n. . .\n\nCreate the data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nepsilon <- rnorm(n, sd = sigma) # this is random\nX <- matrix(runif(n * p), n, p) # treat this as fixed, but I need numbers\nbeta <- (p + 1):1 # parameter, also fixed, but I again need numbers\nY <- cbind(1, X) %*% beta + epsilon # epsilon is random, so this is\n## Equiv: Y <- beta[1] + X %*% beta[-1] + epsilon\n```\n:::\n\n\n\n## How do we estimate beta?\n\n1. Guess.\n2. Ordinary least squares (OLS).\n3. Maximum likelihood.\n4. Do something more creative.\n\n\n## Method 2. OLS\n\nI want to find an estimator $\\widehat\\beta$ that makes small errors on my data.\n\nI measure errors with the difference between predictions $\\mathbf{X}\\widehat\\beta$ and the responses $\\mathbf{y}$.\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\nDon't care if the differences are positive or negative\n\n$$\\sum_{i=1}^n \\left\\lvert y_i - x_i^\\top \\widehat\\beta \\right\\rvert.$$\n\n\nThis is hard to minimize (what is the derivative of $|\\cdot|$?)\n\n\n$$\\sum_{i=1}^n ( y_i - x_i^\\top \\widehat\\beta )^2.$$\n\n:::\n\n::: {.column width=\"50%\"}\n\n![](gfx/islr3_4.png){width=70%}\n\n:::\n::::\n\n\n\n## Method 2. OLS solution\n\nWe write this as\n\n$$\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.$$\n\n\n> Find the $\\beta$ which minimizes the sum of squared errors.\n\n. . .\n\nNote that this is the same as \n\n$$\\widehat\\beta = \\argmin_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.$$\n\n> Find the beta which minimizes the mean squared error.\n\n\n\n## Method 2. Ok, do it {.smaller}\n\nWe differentiate and set to zero\n\n\\begin{aligned}\n& \\frac{\\partial}{\\partial \\beta} \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2\\\\\n&= -\\frac{2}{n}\\sum_{i=1}^n x_i (y_i - x_i^\\top\\beta)\\\\\n&= \\frac{2}{n}\\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n0 &\\equiv \\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n&\\Rightarrow \\sum_{i=1}^n x_i x_i^\\top \\beta = \\sum_{i=1}^n x_i y_i\\\\\n&\\Rightarrow \\beta = \\left(\\sum_{i=1}^n x_i x_i^\\top\\right)^{-1}\\sum_{i=1}^n x_i y_i\n\\end{aligned}\n\n\n\n\n## In matrix notation...\n\n...this is \n\n\n$$\\hat\\beta = ( \\mathbf{X}^\\top  \\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}.$$\n\n\nThe $\\beta$ which \"minimizes the sum of squared errors\"\n\n\nAKA, the SSE.\n\n\n\n## Method 3: maximum likelihood\n\nMethod 2 didn't use anything about the distribution of $\\epsilon$.\n\nBut if we know that $\\epsilon$ has a normal distribution, we can write down the joint distribution\nof $\\mathbf{y}=(y_1,\\ldots,y_n)^\\top$:\n\n\n\\begin{aligned}\nf_Y(\\mathbf{y} ; \\beta) &= \\prod_{i=1}^n f_{y_i ; \\beta}(y_i)\\\\\n  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (y_i-x_i^\\top \\beta)^2\\right)\\\\\n  &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\end{aligned}\n\n\n\n## Method 3: maximum likelihood\n\n$$\nf_Y(\\mathbf{y} ; \\beta) = \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n$$\n\nIn probability courses, we think of $f_Y$ as a function of $\\mathbf{y}$ with $\\beta$ fixed:\n\n1. If we integrate over $\\mathbf{y}$, it's $1$.\n2. If we want the probability of $(a,b)$, we integrate from $a$ to $b$.\n3. etc.\n\n\n\n## Turn it around...\n\n...instead, think of it as a function of $\\beta$.\n\nWe call this \"the likelihood\" of beta: $\\mathcal{L}(\\beta)$.\n\nGiven some data, we can [evaluate]{.secondary} the likelihood for any value of $\\beta$ (assuming $\\sigma$ is known).\n\nIt won't integrate to 1 over $\\beta$.\n\nBut it is \"convex\", \n\nmeaning we can maximize it (the second derivative wrt $\\beta$ is everywhere negative).\n\n\n\n## So let's maximize {.smaller}\n\nThe derivative of this thing is kind of ugly.\n\nBut if we're trying to maximize over $\\beta$, we can take an increasing transformation without changing anything.\n\nI choose $\\log_e$.\n\n\n\\begin{aligned}\n\\mathcal{L}(\\beta) &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\\\\\n\\ell(\\beta) &=-\\frac{n}{2}\\log (2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\n\\end{aligned}\n\n\nBut we can ignore constants, so this gives\n\n\n$$\\widehat\\beta = \\argmax_\\beta -\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2$$\n\n\nThe same as before!\n\n\n# Next time...\n\nRecall how to do this in `R`",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}