---
lecture: "05 Estimating (Test) Risk"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Last time

1. What is a model (formal definition)?
1. Evaluating models (risk/loss functions)
1. Decomposing risk (bias, variance, irreducible error)

## What is a model?

A model is a set of distributions that explain data $\{ Z = (X, Y) \}$, i.e.

$$\mathcal{P} = \{ P: \quad Y \mid X \sim \mathcal N( f(X), \sigma^2) \quad \text{for some ``smooth'' f} \}$$

. . .

[(Why do we have to specify that $f$ is smooth? Why can't it be any function?)]{.secondary} \

. . .

### Goal of learning

Choose the $P \in \mathcal P$ that makes the "best" predictions on new $X, Y$ pairs.

[(Next slide: how do we formalize "best"?)]{.secondary}


## How do we evaluate models?

$$\mathcal{P} = \{ P: \quad Y \mid X \sim \mathcal N( f(X), \sigma^2) \quad \text{for some ``smooth'' f} \}$$

::: {.incremental}
1. Specify how a $P \in \mathcal P$ makes **predictions** on new inputs $\hat Y$. \
[(E.g.: $\hat Y = f(X)$ for $P = \mathcal N(f(X), \sigma^2)$.)]{.secondary}

2. Introduce a **loss** function $\ell(Y, \hat{Y})$ (a datapoint-level function). \
[(E.g.: $\ell(Y, \hat Y) = (Y - \hat Y)^2$)]{.secondary}

3. Define the **risk** of $P \in \mathcal P$ as the expected loss (a population-level function): \
[(R_n(P) = E[\ell(Y, \hat Y)] = E[(Y - f(X))^2])]{.secondary}

4. The **best** model is the one that minimizes the risk. \
[($P^* = \argmin_{P \in \mathcal P} R_n(P)$)]{.secondary}
:::


---

Last time: when $\ell(Y, \hat Y) = (Y - \hat Y)^2$, we showed that the **regression function** is the best model:

. . .

$$
\text{Regression function } \triangleq E[Y \mid X] = \argmin_{P \in \mathcal P} R_n(P) = \argmin_{P \in \mathcal P} E[\ell(Y, \hat Y)]
$$

. . .

[Are we done? Have we solved learning?]{.secondary}

. . .

No! We don't know what $E[Y \mid X]$ is! We have to *estimate it from data!*

$$
\hat f(X) \approx E[Y \mid X]
$$

(We'll discuss various methods for producing $\hat f(X)$ estimators throughout this course.)


## Decomposing risk

When $\ell(Y, \hat Y) = (Y - \hat Y)^2$, the prediction risk of $\hat f(X)$ decomposes into two factors:

$$
R_n(\hat f) \quad = \quad \underbrace{E\left[ \: \left( E[Y\mid X] -\hat f(X) \right)^2 \right]}_{(1)} \quad - \quad \underbrace{E\left[ \: \left( \hat f(X) - Y\right)^2 \right]}_{(2)}
$$

::: {.incremental}
1. **Estimation error**
2. **Irreducible error** (or "noise")
:::

---

The **estimation error term** further reduces into two components:

\begin{aligned}
\underbrace{E\left[ \: \left( E[Y\mid X] -\hat f(X) \right)^2 \right]}_{\text{Estimation error}} \quad &= \quad \underbrace{\left( E[Y\mid X] - E \left[\hat f(X)\right] \right)^2}_{(A)} \quad \\
&+ \quad \underbrace{E\left[ \: \left( E \left[\hat f(X)\right] -\hat f(X) \right)^2 \right]}_{(B)}
\end{aligned}

::: {.incremental}
A.  **Bias**^2
B.  **Variance**
:::

. . .

::: callout-tip
Analogous decompositions hold for other loss/risk functions.
:::

---

```{r}
#| fig-height: 6
#| fig-width: 8
#| code-fold: true
cols = c(blue, red, green, orange)
par(mfrow = c(2, 2), bty = "n", ann = FALSE, xaxt = "n", yaxt = "n", 
    family = "serif", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))
library(mvtnorm)
mv <- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)
va <- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)

for (i in 1:4) {
  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, 
       col = blue, ann = FALSE, pty = "s")
  points(0, 0, pch = 19, cex = 30, col = "white")
  points(0, 0, pch = 19, cex = 18, col = green)
  points(0, 0, pch = 19, cex = 6, col = orange)
  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)
  switch(i,
    "1" = {
      mtext("low variance", 3, cex = 2)
      mtext("low bias", 2, cex = 2)
    },
    "2" = mtext("high variance", 3, cex = 2),
    "3" = mtext("high bias", 2, cex = 2)
  )
}
```


## Sources of bias and variance

### What conditions give rise to a high bias estimator?

::: fragment
- Not enough covariates (small $p$)
- Model is too simple
- Model is _misspecified_ (doesn't accurately represent the data generating process)
- Bad training algorithm
:::

### What conditions give rise to a high variance estimator?

::: fragment
- Not enough training samples (small $n$)
- Model is too complicated[^1]
- Lots of irreducible noise in training data (if my model has power to fit noise, it will)

:::

# How do we estimate $R_n$?

\
So far, $R_n$ has been a theoretical construct. \
We can never know the true $R_n$ for a given $\hat f$.
We also have to estimate it from data.



## Don't use training error


The training error in regression is

$$\widehat{R}_n(\widehat{f}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$

Here, the $n$ is doubly used (annoying, but simple): $n$ observations to create $\widehat{f}$ and $n$ terms in the sum.

::: callout-tip
We also call $\hat R_n(\hat f)$ the **empirical risk**.
:::

. . .

$\hat R_n(\hat f)$ is a bad estimator for $R_n(\widehat{f})$. \
So we should [__never__]{.secondary} use it.


## Why is $\hat R_n$ a bad estimator of $R_n$?

. . .

1. It doesn't say anything about predictions on new data. \
   [(It's a measure of how well the model fits a **fixed set** of training data.)]{.secondary}

2. It can be made **arbitrarily small** by making your model more complex.


## 1. It doesn't say anything about predictions on new data.

These all have the same $R^2$ and Training Error


::: flex
::: w-50

```{r}
#| code-fold: true
#| fig-height: 8
#| fig-width: 8
ans <- anscombe |>
  pivot_longer(everything(), names_to = c(".value", "set"), 
               names_pattern = "(.)(.)")
ggplot(ans, aes(x, y)) + 
  geom_point(colour = orange, size = 3) + 
  geom_smooth(method = "lm", se = FALSE, color = blue, linewidth = 2) +
  facet_wrap(~set, labeller = label_both)
```

:::
::: w-50

```{r}
ans %>% 
  group_by(set) |> 
  summarise(
    R2 = summary(lm(y ~ x))$r.sq, 
    train_error = mean((y - predict(lm(y ~ x)))^2)
  ) |>
  kableExtra::kable(digits = 2)
```

:::
:::

## 2. It can be made **arbitrarily small** by making your model more complex.

Adding "junk" predictors increases $R^2$ and decreases Training Error

```{r}
n <- 100
p <- 10
q <- 0:30
x <- matrix(rnorm(n * (p + max(q))), nrow = n)
y <- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)

regress_on_junk <- function(q) {
  x <- x[, 1:(p + q)]
  mod <- lm(y ~ x)
  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))
}
```

```{r}
#| code-fold: true
map(q, regress_on_junk) |> 
  list_rbind() |>
  mutate(q = q) |>
  pivot_longer(-q) |>
  ggplot(aes(q, value, colour = name)) +
  geom_line(linewidth = 2) + xlab("train_error") +
  scale_colour_manual(values = c(blue, orange), guide = "none") +
  facet_wrap(~ name, scales = "free_y")
```


## Other things you can't use

You should not use `anova` 

or the $p$-values from the `lm` output for this purpose.

. . .

> These things are to determine whether those _parameters_ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.

In other words, they are useful for _inference problems_.

This is not the same as being useful for _prediction problems_ (i.e. how to get small $R_n$).


## Don't use training error: the formal argument

Our training error $\hat R_n(\hat f)$ is an **estimator** of $R_n(\hat f)$. \
So we can ask "is $\widehat{R}(\hat{f})$ a good estimator for $R_n(\hat{f})$?"

## The risk of risk

Let's measure the **risk** of our empirical risk estimator:

::: flex

::: w-50
![](https://images.squarespace-cdn.com/content/v1/59a53195ff7c50210a2c6b8b/1510207623511-BT2MBPENJOJXZNE0SCSQ/inceptius-meme-generator-we-have-to-go-deeper-014848.jpg){fig-alt="meme"}
:::

::: w-50
![](https://i.imgflip.com/93xy4g.jpg){fig-alt="meme" width="380px"}
:::

:::
$$E[(R_n(\hat f) - \hat R_n(\hat f))^2]$$
[(What is the expectation with respect to?)]{.secondary}


## The risk of risk

$$E[(R_n(\hat f) - \hat R_n(\hat f))^2]$$

- $R_n(\hat f)$ only depends on training data (since $\hat f$ is derived from training data)
- $\hat R_n(\hat f)$ also only depends on training data
- So the expectation is with respect to our training dataset

. . .

As before, we can decompose our risk-risk into **bias** and **variance**

$$
E[(R_n - \hat R)^2] = \underbrace{E[( R_n - E[\hat R_n])^2]}_{\text{bias}} + \underbrace{E[( \hat R_n - E[\hat R_n])^2]}_{\text{variance}}
$$

## Formalizing why $\hat R_n$ is a bad estimator of $R_n$

Recall that $\hat R_n(\hat f)$ is estimated from the training data $\{ (X_i, Y_i) \}_{i=1}^n$.

Consider an alternative estimator built from $\{ (X_j, Y_j) \}_{j=1}^m$ that was not part of the training set.
$$\tilde R_m = {\textstyle \frac{1}{m} \sum_{j=1}^m} \ell(Y_j, \hat Y_j(X_j)),
$$

### Which has higher bias, $\hat R_n$ or $\tilde R_m$?

::: fragment
- $\tilde R_m$ has _zero_ bias.
  - (X_j, Y_j) are i.i.d. samples from the population
- $\tilde R_n$ is _very_ biased.
  - (X_i, Y_i) are i.i.d. samples from the population, but they are also used to choose $\hat f$
  - Using them to both choose $\hat f$ and estimate $R_n$ is "double dipping."
:::




# How to properly estimate $R_n$


## Holdout sets

One option is to have a separate "holdout" or "validation" dataset.

::: callout-tip
This option follows the logic on the previous slide. \
If we randomly "hold out" $\{ (X_j, Y_j) \}_{j=1}^m$ from the training set,
then we can use this data to get an (nearly) unbiased estimator of $R_n$.
$$
R_n(\hat f) \approx \tilde R_m \triangleq {\textstyle{\frac 1 m \sum_{j=1}^m \ell ( Y_j - \hat Y_j(X_j))}}
$$
:::

. . .


üëç Estimates the test error

üëç Fast computationally

ü§Æ Estimate is random 

ü§Æ Estimate has high variance (depends on 1 choice of split)

ü§Æ Estimate has a little bias (because we aren't estimating $\hat f$ from all of the training data)


## Aside {background-color="#97D4E9"}

In my experience, CS has particular definitions of "training", "validation", and "test" data.

I think these are not quite the same as in Statistics.

* [Test data]{.secondary} - Hypothetical data you don't get to see, ever. Infinite amounts drawn from the population.
    * _Expected test error_ or _Risk_ is an expected value over this distribution. It's _not_ a sum over some data kept aside.
* Sometimes I'll give you "test data". You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.
* [Training data]{.secondary} - This is "holdout" data that you get to touch.
* [Validation set]{.secondary} - Often, we need to _choose models_. One way to do this is to split off some of your training data and pretend that it's like a "Test Set".

When and how you split your training data can be very important. 



## Cross Validation

<!--
One reason that $\widehat{R}_n(\widehat{f})$ is bad is that we are using the same data to pick $\widehat{f}$ __AND__ to estimate $R_n$.

"Validation set" fixes this, but holds out a particular, fixed block of data we pretend mimics the "test data"
-->

Our validation error $\tilde{R}_m(\widehat{f})$ is a random estimator.\
[(The split we use to divide our data into training versus validation is random.)]{.secondary}

. . .

\
A random estimator has _variance_.
We can reduce this variance by averageing over multiple splits.


## Cross Validation Example

What if we set aside one observation, say the first one $(y_1, x_1)$:

- We estimate $\widehat{f}^{(1)}$ without using the first observation.
- We estimate $\widetilde{R}_1(\widehat{f}^{(1)})$ using the held-out first observation.

$$\widetilde{R}_1(\widehat{f}^{(1)}) = (y_1 -\widehat{f}^{(1)}(x_1))^2.$$
[(Why the notation $\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )]{.secondary}

---

But that was only one data point $(y_1, x_1)$. Why stop there?

Do the same with $(y_2, x_2)$!

$$\widetilde{R}_1(\widehat{f}^{(2)}) = (y_2 -\widehat{f}^{(2)}(x_2))^2.$$
We can keep doing this until we try it for every data point.

. . .

$$\mbox{LOO-CV} = \frac{1}{n}\sum_{i=1}^n \widetilde{R}_1(\widehat{f}^{(i)}) = \frac{1}{n}\sum_{i=1}^n 
(y_i - \widehat{f}^{(i)}(x_i))^2$$
This is [__leave-one-out cross validation__]{.secondary}


## Problems with LOO-CV

<!--
ü§Æ Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.
-->

ü§Æ The training sets overlap. This is bad. 

- Usually, averaging reduces variance: $\Var{\overline{X}} = \frac{1}{n^2}\sum_{i=1}^n \Var{X_i} = \frac{1}{n}\Var{X_1}.$
- But only if the variables are independent. If not, then $\Var{\overline{X}} = \frac{1}{n^2}\Var{ \sum_{i=1}^n X_i} = \frac{1}{n}\Var{X_1} + \frac{1}{n^2}\sum_{i\neq j} \Cov{X_i}{X_j}.$
- Since the training sets overlap a lot, that covariance can be pretty big.
    
ü§Æ We have to estimate this model $n$ times.

üéâ Bias is low because we used almost all the data to fit the model: $E[\mbox{LOO-CV}] = R_{n-1}$ 

  
## K-fold CV

::: flex
::: w-50
To alleviate some of these problems, people usually use $K$-fold cross validation.

The idea of $K$-fold is 

1. Divide the data into $K$ groups. 
1. Leave a group out and estimate with the rest.
1. Test on the held-out group. Calculate an average risk over these $\sim n/K$ data.
1. Repeat for all $K$ groups.
1. Average the average risks.


:::


::: w-50
üéâ Less overlap, smaller covariance.

üéâ Larger hold-out sets, smaller variance.

üéâ Less computations (only need to estimate $K$ times)

ü§Æ LOO-CV is (nearly) unbiased for $R_n$

ü§Æ K-fold CV is unbiased for $R_{n(1-1/K)}$

The risk depends on how much data you use to estimate the model. $R_n$ depends on $n$.

:::
:::



## A picture

```{r}
#| code-fold: true
#| fig-height: 6
#| fig-width: 10
par(mar = c(0, 0, 0, 0))
plot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = "n", yaxt = "n", xaxt = "n")
rect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)
rect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), 
     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)
points(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)
text(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c("1", "2", "3", "K"), cex = 3, 
     col = red)
text(6, 4.5, "Training data", cex = 3, col = blue)
text(2, 1.5, "Validation data", cex = 3, col = red)
```


## Code


```{r}
#| code-line-numbers: "11-13|14-16|"
#' @param data The full data set
#' @param estimator Function. Has 1 argument (some data) and fits a model. 
#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions
#' @param error_fun Function. Has one arg: the test data, with fits added.
#' @param kfolds Integer. The number of folds.
kfold_cv <- function(data, estimator, predictor, error_fun, kfolds = 5) {
  n <- nrow(data)
  fold_labels <- sample(rep(1:kfolds, length.out = n))
  errors <- double(kfolds)
  for (fold in seq_len(kfolds)) {
    test_rows <- fold_labels == fold
    train <- data[!test_rows, ]
    test <- data[test_rows, ]
    current_model <- estimator(train)
    test$.preds <- predictor(current_model, test)
    errors[fold] <- error_fun(test)
  }
  mean(errors)
}
```

. . . 

```{r}
#| code-line-numbers: "2-4|"
somedata <- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))
est <- function(dataset) lm(z ~ ., data = dataset)
pred <- function(mod, dataset) predict(mod, newdata = dataset)
error_fun <- function(testdata) mutate(testdata, errs = (z - .preds)^2) |> pull(errs) |> mean()
kfold_cv(somedata, est, pred, error_fun, 5)
```


## Trick

__For certain "nice" models__ of the form
$$\widehat{y}_i = h_i(\mathbf{X})^\top \mathbf{y}$$
for some vector $h_i$, one can show 

$$\mbox{LOO-CV} = \frac{1}{n} \sum_{i=1}^n \frac{(y_i -\widehat{y}_i)^2}{(1-[\boldsymbol h_i(x_i)]_{i})^2}.$$
(Proof: tedious algebra which I wouldn't wish on my worst enemy, but might - in a fit of rage - assign as homework to belligerent students.) 

. . .

* This trick means that you only have to fit the model once rather than $n$ times!

```{r}
cv_nice <- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )
```


# Next time...

More tricks and what's up with the name "hatvalues"
