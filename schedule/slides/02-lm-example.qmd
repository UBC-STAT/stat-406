---
lecture: "02 Linear model example"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Economic mobility


```{r}
data("mobility", package = "Stat406")
mobility
```

::: {.notes}

Note how many observations and predictors it has.

We'll use `Mobility` as the response

:::



## A linear model


$$\mbox{Mobility}_i = \beta_0 + \beta_1 \, \mbox{State}_i + \beta_2 \, \mbox{Urban}_i + \cdots + \epsilon_i$$ 
    
or equivalently

$$E \left[ \biggl. \mbox{mobility} \, \biggr| \, \mbox{State}, \mbox{Urban}, 
    \ldots \right]  = \beta_0 + \beta_1 \, \mbox{State} + 
    \beta_2 \, \mbox{Urban} + \cdots$$



## Analysis


-   Randomly split into a training (say 3/4) and a test set (1/4)

-   Use training set to fit a model 

-   Fit the "full" model

-   "Look" at the fit

. . .

```{r}
set.seed(20220914)
mob <- mobility[complete.cases(mobility), ] |>
	select(-Name, -ID, -State)
n <- nrow(mob)
set <- sample.int(n, floor(n * .75), FALSE)
train <- mob[set, ]
test <- mob[setdiff(1:n, set), ]
full <- lm(Mobility ~ ., data = train)
```

::: {.notes}

Why don't we include `Name` or `ID`?

:::


## Results

(dispatch happening here!)

```{r}
summary(full)
```

## Diagnostic plots

NB: the line in the QQ plot isn't right for either `geom_qq_line` or `plot.lm`...

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
stuff <- tibble(
  residuals = residuals(full),
  fitted = fitted(full),
  stdresiduals = rstandard(full)
)
ggplot(stuff, aes(fitted, residuals)) +
  geom_point() +
  geom_smooth(
    se = FALSE,
    colour = "steelblue",
    linewidth = 2
  ) +
  ggtitle("Residuals vs Fitted")
```

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
ggplot(stuff, aes(sample = stdresiduals)) +
  geom_qq(size = 2) +
  geom_qq_line(linewidth = 2, color = "steelblue") +
  labs(
    x = "Theoretical quantiles",
    y = "Standardized residuals",
    title = "Normal Q-Q"
  )
```

## Can also just use dispatched `plot`

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
plot(full, 1)
```

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
plot(full, 2)
```



## Fit a reduced model

```{r}
reduced <- lm(
  Mobility ~ Commute + Gini_99 + Test_scores + HS_dropout +
    Manufacturing + Migration_in + Religious + Single_mothers,
  data = train
)

summary(reduced)$coefficients 

reduced |>
  broom::glance() |>
  print(width = 120)
```


## Diagnostic plots for reduced model

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
plot(reduced, 1)
```

```{r}
#| output-location: column
#| fig-width: 8
#| fig-height: 3.5
plot(reduced, 2)
```




## How do we decide which model is better?

::: flex

::: w-50

* Goodness of fit versus prediction power

```{r}
map( # smaller AIC is better
  list(full = full, reduced = reduced),
  ~ c(aic = AIC(.x), rsq = summary(.x)$r.sq)
)
```

* Use both models to predict `Mobility` 

* Compare both sets of predictions

:::

::: w-50

```{r}
mses <- function(preds, obs) round(mean((obs - preds)^2), 5)
c(
  full = mses(
    predict(full, newdata = test),
    test$Mobility
  ),
  reduced = mses(
    predict(reduced, newdata = test),
    test$Mobility
  )
)
```


```{r}
#| fig-height: 4
#| fig-width: 8
#| code-fold: true
test$full <- predict(full, newdata = test)
test$reduced <- predict(reduced, newdata = test)
test |>
  select(Mobility, full, reduced) |>
  pivot_longer(-Mobility) |>
  ggplot(aes(Mobility, value)) +
  geom_point(color = "orange") +
  facet_wrap(~name, 2) +
  xlab("observed mobility") +
  ylab("predicted mobility") +
  geom_abline(slope = 1, intercept = 0, colour = "darkblue")
```

:::
:::


# Next time... 


[Module 1]{.secondary .large}

Selecting models
