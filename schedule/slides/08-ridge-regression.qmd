---
lecture: "08 Ridge regression"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Recap

General idea:

1. Pick a *model* (a family of distributions).
1. Pick a *loss function* (measures how wrong predictions are).
1. Design a *predictor/estimator* that minimizes the *prediction/estimation risk*.

Risk includes [bias]{.secondary}, [variance]{.secondary}, and [irreducible error]{.secondary}.

So far, trade bias/variance via *variable selection* using a risk estimate.

. . .

**Today:** trade bias/variance via [*regularization*]{.secondary} 

- use **all** predictors
- shrink $\hat\beta$ towards 0 (increase bias, reduce variance)


$$
\newcommand{\brt}{\widehat{\beta}^R_{s}}
\newcommand{\brl}{\widehat{\beta}^R_{\lambda}}
\newcommand{\bls}{\widehat{\beta}_{ols}}
\newcommand{\blt}{\widehat{\beta}^L_{s}}
\newcommand{\bll}{\widehat{\beta}^L_{\lambda}}
$$



<!--
## Regularization


* Another way to control bias and variance is through [regularization]{.secondary} or
[shrinkage]{.secondary}.  


* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.

* I mean [ALL]{.tertiary}.

* But, make your estimates of $\beta$ "smaller"
-->

## OLS: bias and variance

Minimize 

$$\min_\beta \frac{1}{n}\sum_i (y_i - x_i^T\beta)^2 \quad \mbox{ subject to } \quad \beta \in \R^p$$

. . .

Can rewrite using matrix-vector notation:

$$\min_\beta \frac{1}{n}\| \y - \X\beta\|_2^2\quad \mbox{ subject to } \quad \beta \in \R^p$$

. . .

Solve by setting derivative to 0: $\bls = (\X^\top\X)^{-1}\X^\top\y$

- unbiased: $\E[\bls] = \E[\E[\bls \mid \X]] = \E[(\X^\top\X)^{-1}\X^\top\X\beta] = \beta$
- variance: $\Var{\bls} =  \sigma^2(\X^\top \X)^{-1}$ ??

## Singular Value Decomposition (SVD) {background-color="#97D4E9"}

A rectangular $n \times p$ matrix $X$ has *singular value decomposition* $X = U D V^\top$

- $U$ is $n\times n$,  *orthonormal*: $U^\top U = UU^\top = I$
- $V$ is $p\times p$, *orthonormal*: $V^\top V = VV^\top = I$
- $D$ is rectangular $n\times p$, *diagonal and nonnegative* (*singular values*)

. . .

e.g. $n = 4, p = 2$, singular values $d_1, d_2$:

$$D = \left[\begin{array} dd_1 & 0 \\ 0 & d_2 \\ 0 & 0 \\ 0 & 0 \end{array}\right] \qquad D^\top D = \left[\begin{array} dd_1^2 & 0 \\ 0 & d_2^2\end{array}\right].$$

. . .

If $X$ has (almost) linearly dependent columns, some singular values in $D$ are (near) zero

$X$ is *(nearly) rank-deficient* or *ill-conditioned*

## The variance of OLS

Let's use the SVD $X = UDV^T$ to examine the OLS variance $\Var{\bls} =  \sigma^2(\X^\top \X)^{-1}$:


$$(\X^\top \X)^{-1} = (VD^\top U^\top UDV^\top)^{-1} = V (D^\top D)^{-1}V^\top =  V\left[\begin{array}dd_1^{-2} & 0 & 0\\ 0 & \ddots & 0 \\ 0 & 0 & d_p^{-2}\end{array}\right]V^\top$$ 

. . .

[Multicollinearity:]{.secondary} a linear combination of variables is nearly equal to another variable. 

* so $\X$ is ill-conditioned
* so some singular values $d_j\approx 0$
* so $d^{-2}_j$ is large
* so $\bls$ has large, unstable values; i.e., **large variance**

## How to stop large, unstable values of $\widehat{\beta}$?

Idea: **constrain** the values of $\beta$ to be small. For $s > 0$:
$$
\minimize_\beta \underbrace{\frac{1}{n}\|\y - \X\beta\|_2^2}_{\text{objective function}} \quad \st \underbrace{\|\beta\|_2^2 < s}_{\text{constraint}}.
$$

Recall, for a vector $\beta \in \R^p$

$\ell_2$-norm
: $\|\beta\|_2 = \sqrt{\beta_1^2 + \beta_2^2 + \dots + \beta_p^2} = \left(\sum_{j=1}^p |\beta_j|^2\right)^{1/2}$
(so $\|\beta\|_2^2 = \sum_j \beta_j^2$)

. . .

Compare this to ordinary least squares:

$$
\minimize_\beta \underbrace{\frac{1}{n}\|\y-\X\beta\|_2^2}_{\text{same objective}} 
\quad \st \underbrace{\beta \in \R^p}_{\textbf{no constraint}}
$$

## Geometry of constrained regression (contours)

```{r plotting-functions}
#| code-fold: true
#| fig-width: 6
#| fig-height: 6
library(mvtnorm)
norm_ball <- function(q = 1, len = 1000) {
  tg <- seq(0, 2 * pi, length = len)
  out <- tibble(x = cos(tg), b = (1 - abs(x)^q)^(1 / q), bm = -b) |>
    pivot_longer(-x, values_to = "y")
  out$lab <- paste0('"||" * beta * "||"', "[", signif(q, 2), "]")
  return(out)
}

ellipse_data <- function(
  n = 75, xlim = c(-2, 3), ylim = c(-2, 3),
  mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {
  expand_grid(
    x = seq(xlim[1], xlim[2], length.out = n),
    y = seq(ylim[1], ylim[2], length.out = n)) |>
    rowwise() |>
    mutate(z = dmvnorm(c(x, y), mean, Sigma))
}

lballmax <- function(ed, q = 1, tol = 1e-6, niter = 20) {
  ed <- filter(ed, x > 0, y > 0)
  feasible <- (ed$x^q + ed$y^q)^(1 / q) <= 1
  best <- ed[feasible, ]
  best[which.max(best$z), ]
}


nb <- norm_ball(2)
ed <- ellipse_data()
bols <- data.frame(x = 1, y = 1)
bhat <- lballmax(ed, 2)
ggplot(nb, aes(x, y)) +
  xlim(-2, 2) +
  ylim(-2, 2) +
  geom_path(colour = red) +
  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  geom_point(data = bols) +
  coord_equal() +
  geom_label(
    data = bols,
    mapping = aes(label = bquote("hat(beta)[ols]")),
    parse = TRUE, 
    nudge_x = .3, nudge_y = .3
  ) +
  geom_point(data = bhat) +
  xlab(bquote(beta[1])) +
  ylab(bquote(beta[2])) +
  theme_bw(base_size = 24) +
  geom_label(
    data = bhat,
    mapping = aes(label = bquote("hat(beta)[s]^R")),
    parse = TRUE,
    nudge_x = -.4, nudge_y = -.4
  )
```

## Ridge regression

An equivalent way to write

$$\brt = \argmin_{\beta} \frac{1}{n}\|\y - \X\beta\|_2^2 \st  || \beta ||_2^2 \leq s$$

is as a [*regularized*]{.secondary} (or *penalized*) optimization with *regularization weight* $\lambda$:

$$\brl = \argmin_{ \beta} \frac{1}{n}\|\y-\X\beta\|_2^2 + \lambda || \beta ||_2^2.$$

For every $\lambda$ there is a unique $s$ (and vice versa) that makes $\brt = \brl$. We will work with $\lambda$.

## Ridge regression

$$\brl = \argmin_{ \beta} \frac{1}{n}\|\y-\X\beta\|_2^2 + \lambda || \beta ||_2^2$$

Observe:

* $\lambda = 0$ makes $\brl = \bls$
* $\lambda \to\infty$ makes $\brl \to 0$
* Any $0 < \lambda < \infty$ penalizes larger values of $\beta$, effectively shrinking them.
* $\lambda$ is a **tuning parameter:** we need to pick it

<!--

## Visualizing ridge regression (2 coefficients)

```{r}
#| code-fold: true
#| fig-width: 16
#| fig-height: 9
#| dev: png
b <- c(1, 1)
n <- 1000
lams <- c(1, 5, 10)
ols_loss <- function(b1, b2) colMeans((y - X %*% rbind(b1, b2))^2) / 2
pen <- function(b1, b2, lambda = 1) lambda * (b1^2 + b2^2) / 2
gr <- expand_grid(
  b1 = seq(b[1] - 0.5, b[1] + 0.5, length.out = 100),
  b2 = seq(b[2] - 0.5, b[2] + 0.5, length.out = 100)
)

X <- mvtnorm::rmvnorm(n, c(0, 0), sigma = matrix(c(1, .3, .3, .5), nrow = 2))
y <- drop(X %*% b + rnorm(n))

bols <- coef(lm(y ~ X - 1))
bridge <- coef(MASS::lm.ridge(y ~ X - 1, lambda = lams * sqrt(n)))

penalties <- lams |>
  set_names(~ paste("lam =", .)) |>
  map(~ pen(gr$b1, gr$b2, .x)) |>
  as_tibble()
gr <- gr |>
  mutate(loss = ols_loss(b1, b2)) |>
  bind_cols(penalties)

g1 <- ggplot(gr, aes(b1, b2)) +
  geom_raster(aes(fill = loss)) +
  scale_fill_viridis_c(direction = -1) +
  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +
  theme(legend.position = "bottom") +
  guides(fill = guide_colourbar(barwidth = 20, barheight = 0.5))

g2 <- gr |>
    pivot_longer(starts_with("lam")) |>
    mutate(name = factor(name, levels = paste("lam =", lams))) |>
  ggplot(aes(b1, b2)) +
  geom_raster(aes(fill = value)) +
  scale_fill_viridis_c(direction = -1, name = "penalty") +
  facet_wrap(~name, ncol = 1) +
  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +
  theme(legend.position = "bottom") +
  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))

g3 <- gr |> 
  mutate(across(starts_with("lam"), ~ loss + .x)) |>
  pivot_longer(starts_with("lam")) |>
  mutate(name = factor(name, levels = paste("lam =", lams))) |>
  ggplot(aes(b1, b2)) +
  geom_raster(aes(fill = value)) +
  scale_fill_viridis_c(direction = -1, name = "loss + pen") +
  facet_wrap(~name, ncol = 1) +
  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +
  theme(legend.position = "bottom") +
  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))

cowplot::plot_grid(g1, g2, g3, rel_widths = c(2, 1, 1), nrow = 1)
```

## The effect on the estimates

```{r}
#| code-fold: true
#| fig-width: 8
#| fig-height: 5
#| dev: png
gr |> 
  mutate(z = ols_loss(b1, b2) + max(lams) * pen(b1, b2)) |>
  ggplot(aes(b1, b2)) +
  geom_raster(aes(fill = z)) +
  scale_fill_viridis_c(direction = -1) +
  geom_point(data = tibble(
    b1 = c(bols[1], bridge[,1]),
    b2 = c(bols[2], bridge[,2]),
    estimate = factor(c("ols", paste0("ridge = ", lams)), 
                      levels = c("ols", paste0("ridge = ", lams)))
  ),
  aes(shape = estimate), size = 3) +
  geom_point(data = data.frame(b1 = b[1], b2 = b[2]), colour = orange, size = 4)
```

-->

## Example data

`prostate` data from [ESL]

```{r load-prostate}
data(prostate, package = "ElemStatLearn")
prostate |> as_tibble()
```

::: notes

Use `lpsa` as response.

:::


## Ridge regression path

We can look at coefficients as we vary $\lambda$ (a "path" or "coefficient trace")

```{r process-prostate, echo=TRUE, dev="svg", message=FALSE,warning=FALSE, fig.height = 4, fig.width=8, fig.align='center'}
Y <- prostate$lpsa
X <- model.matrix(~ ., data = prostate |> dplyr::select(-train, -lpsa))
library(glmnet)
ridge <- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)
plot(ridge, xvar = "lambda", lwd = 3)
```

**Model selection:** choose some $\lambda$ (a vertical line on this plot)

## Ridge regression: closed-form

Ridge regression has a closed-form solution like OLS (set the derivative in $\beta$ to 0):

$$\brl = (\X^\top\X + \lambda \mathbf{I})^{-1}\X^\top \y$$

Compare to OLS:

$$\bls = (\X^\top \X)^{-1}\X^\top \y$$

* What does the $+\lambda I$ do?

* What about bias and variance?

* Is $\brl$ faster/slower/neither to compute? 

* Is $\brl$ more/less numerically stable to compute?

## Ridge regression: bias and variance

Ridge regression has a closed-form solution like OLS (set the derivative in $\beta$ to 0):

$$\brl = (\X^\top\X + \lambda \mathbf{I})^{-1}\X^\top \y$$


* bias: $\E[\brl | \X] = (\X^\top\X + \lambda I)^{-1}\X^\top \X\beta \neq \beta$

* variance: $\Var{\brl \mid \X} = \sigma^2 (\X^\top\X + \lambda I)^{-1}\X^\top \X(\X^\top\X + \lambda I)^{-1}$

. . .

Using the SVD $\X = UDV^T$, $\Var{\brl \mid \X} = \sigma^2 VG V^\top$ where $G$ is diagonal with entries

$$ g_j = \frac{d_j^2}{(d_j^2 + \lambda)^2} \qquad \left(\text{compare to OLS:} \quad g_j = \frac{1}{d_j^{2}} \right)$$

Mitigates the issue of [multicollinearity]{.secondary} (ill-conditioned $\X$)!

<!--
* This is easy to calculate in `R` for any $\lambda$.

* However, computations and interpretation are simplified if we examine the 
[Singular Value Decomposition]{.secondary} of $\X = \mathbf{UDV}^\top$.

* Recall: any matrix has an SVD.

* Here $\mathbf{D}$ is diagonal and $\mathbf{U}$ and $\mathbf{V}$ are orthonormal: $\mathbf{U}^\top\mathbf{U} = \mathbf{I}$.


## Solving the minization

$$\brl = (\X^\top\X + \lambda \mathbf{I})^{-1}\X^\top \y$$

* Note that $\mathbf{X}^\top\mathbf{X} = \mathbf{VDU}^\top\mathbf{UDV}^\top = \mathbf{V}\mathbf{D}^2\mathbf{V}^\top$.


* Then,


$$\brl = (\X^\top \X + \lambda \mathbf{I})^{-1}\X^\top \y = (\mathbf{VD}^2\mathbf{V}^\top + \lambda \mathbf{I})^{-1}\mathbf{VDU}^\top \y
= \mathbf{V}(\mathbf{D}^2+\lambda \mathbf{I})^{-1} \mathbf{DU}^\top \y.$$

* For computations, now we only need to invert $\mathbf{D}$.


## Comparing with OLS


* $\mathbf{D}$ is a diagonal matrix

$$\bls = (\X^\top\X)^{-1}\X^\top \y = (\mathbf{VD}^2\mathbf{V}^\top)^{-1}\mathbf{VDU}^\top \y = \mathbf{V}\color{red}{\mathbf{D}^{-2}\mathbf{D}}\mathbf{U}^\top \y = \mathbf{V}\color{red}{\mathbf{D}^{-1}}\mathbf{U}^\top \y$$

$$\brl = (\X^\top \X + \lambda \mathbf{I})^{-1}\X^\top \y = \mathbf{V}\color{red}{(\mathbf{D}^2+\lambda \mathbf{I})^{-1}} \mathbf{DU}^\top \y.$$


* Notice that $\bls$ depends on $d_j/d_j^2$ while $\brl$ depends on $d_j/(d_j^2 + \lambda)$.

* Ridge regression makes the coefficients smaller relative to OLS.

* But if $\X$ has small singular values, ridge regression compensates with $\lambda$ in the denominator.



## Ridge regression and multicollinearity

[Multicollinearity:]{.secondary} a linear combination of predictor variables is nearly equal to another predictor variable. 

Some comments:

* A better phrase: $\X$ is ill-conditioned

* AKA "(numerically) rank-deficient".

* $\X = \mathbf{U D V}^\top$ ill-conditioned $\Longleftrightarrow$ some elements of $\mathbf{D} \approx 0$

* $\bls= \mathbf{V D}^{-1} \mathbf{U}^\top \y$, so small entries of $\mathbf{D}$ $\Longleftrightarrow$ huge elements of $\mathbf{D}^{-1}$

* Means huge variance: $\Var{\bls} =  \sigma^2(\X^\top \X)^{-1} = \sigma^2 \mathbf{V D}^{-2} \mathbf{V}^\top$

-->

## Tuning the regularization weight $\lambda$

Use **cross-validation** (it's built into `glmnet` if you use `cv.glmnet`!)

`plot` shows mean CV estimate $\pm$ standard error

Left line is minimum risk estimate; right is the largest $\lambda$ within 1$\sigma$

```{r, fig.width=11,fig.align="center",dev="svg",fig.height=4}
ridge <- cv.glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)
plot(ridge, main = "Ridge")
```

## Ridge regression: summary


Ridge regression addresses [multicollinearity]{.secondary} by preventing division by near-zero numbers

Conclusion
: $\bls = (\X^{\top}\X)^{-1}\X^\top \y$ can be unstable, while $\brl=(\X^{\top}\X + \lambda \mathbf{I})^{-1}\X^\top\y$ is not.

Aside
: Engineering approach to solving linear systems is to always do this with small $\lambda$. The thinking is about the numerics rather than the statistics.

### Which $\lambda$ to use?

Computational
: Use CV and pick the $\lambda$ that makes this smallest (maybe within $1\sigma$).

Intuition (bias)
: As $\lambda\rightarrow\infty$, bias ⬆

Intuition (variance)
: As $\lambda\rightarrow\infty$, variance ⬇

## Regularization for variable selection?

Ridge regression is a *regularized* approach to prediction.

* nice bias/variance tradeoff
* closed-form / easy-to-compute solution
* no predictor variable selection.
    * (NB: picking a $\lambda$ is still *model selection*)

Is there a regularization approach to variable selection?

. . .

e.g., best (in-sample) linear regression model of size $s$:
: $\minimize \frac{1}{n}||\y-\X\beta||_2^2 \ \st\ (\text{\# of nonzero $\beta_j$}) \leq s$

That is a *super nasty* optimization. What to do?...

<!--
## Can we get the best of both worlds?

To recap:

* Deciding which predictors to include, adding quadratic terms, or interactions is [model selection]{.secondary} (more precisely variable selection within a linear model).

* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  

* If the LM is **true**, 
    1. OLS is unbiased, but Variance depends on $\mathbf{D}^{-2}$. Can be big.
    2. Ridge is biased (can you find the bias?). But Variance is smaller than OLS.

* Ridge regression does not perform variable selection.

* But [picking]{.hand} $\lambda=3.7$ and thereby [deciding]{.hand} to predict with $\widehat{\beta}^R_{3.7}$ is [model selection]{.secondary}.



## Can we get the best of both worlds?

Ridge regression 
: $\minimize \frac{1}{n}||\y-\X\beta||_2^2 \ \st\ ||\beta||_2^2 \leq s$ 

Best (in-sample) linear regression model of size $s$
: $\minimize \frac{1}{n}||\y-\X\beta||_2^2 \ \st\ ||\beta||_0 \leq s$


$||\beta||_0$ is the number of nonzero elements in $\beta$

Finding the best in-sample linear model (of size $s$, among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)

Ridge regression is convex (easy to solve), but doesn't do __variable__ selection

Can we somehow "interpolate" to get both?

Note: selecting $\lambda$ is still __model__ selection, but we've included __all__ the variables.
-->


# Next time...

The lasso, interpolating variable selection and model selection
