---
lecture: "16 Logistic regression"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Last time


* We showed that with two classes, the [Bayes' classifier]{.secondary} is

$$g_*(X) = \begin{cases}
1 & \textrm{ if } \frac{p_1(X)}{p_0(X)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

where

* $p_1(X) = \P(X \given Y=1)$
* $p_0(X) = \P(X \given Y=0)$
* $\pi = \P(Y=1)$

. . .

### This lecture

How do we estimate $p_1(X), p_0(X), \pi$?

## Warmup: estimating $\pi = Pr(Y=1)$

A good estimator:

$$
\hat \pi = \frac{1}{n} \sum_{i=1}^n 1_{y_i = 1}
$$

(I.e. count the number of $1$s in the training set)

This estimator is low bias/variance.

. . .

\
*As we will soon see, it turns out we won't have to use this estimator.*

<!-- * We then looked at what happens if we **assume** $Pr(X \given Y=y)$ is Normally distributed. -->

<!-- We then used this distribution and the class prior $\pi$ to find the __posterior__ $Pr(Y=1 \given X=x)$. -->


## Estimating $p_0$ and $p_1$ is much harder

$\P(X=x \mid Y = 1)$ and $\P(X=x \mid Y = 0)$ are $p$-dimensional distributions.\
[Remember the *curse of dimensionality*?]{.small}

\
[Can we simplify our estimation problem?]{.secondary}


## Sidestepping the hard estimation problem

Rather than estimating $\P(X=x \mid Y = 1)$ and $\P(X=x \mid Y = 0)$,
I claim we can instead estimate the simpler ratio

$$
\frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)}
$$
Why?

. . .

\
Recall that $g_*(X)$ ony depends on the *ratio* $\P(X \mid Y = 1) / \P(X \mid Y = 0)$.

. . .

$$
\begin{align*}
  \frac{\P(X=x \mid Y = 1)}{\P(X=x \mid Y = 0)}
  &=
  \frac{
    \tfrac{\P(Y = 1 \mid X=x) \P(X=x)}{\P(Y = 1)}
  }{
    \tfrac{\P(Y = 0 \mid X=x) \P(X=x)}{\P(Y = 0)}
  }
  =
  \frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)}
  \underbrace{\left(\frac{1-\pi}{\pi}\right)}_{\text{Easy to estimate with } \hat \pi}
\end{align*}
$$

## Direct model

[As with regression, we'll start with a simple model.]{.small}\
Assume our data can be modelled by a distribution of the form

$$
\log\left( \frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)} \right) = \beta_0 + \beta^\top x
$$

[Why does it make sense to model the *log ratio* rather than the *ratio*?]{.secondary}

. . .


From this eq., we can recover an estimate of the ratio we need for the [Bayes classifier]{.secondary}:

$$
\begin{align*}
\log\left( \frac{\P(X=x \mid Y = 1)}{\P(X=x \mid Y = 0)} \right) 
&=
\log\left( \frac{\tfrac{\P(X=x)}{\P(Y = 1)}}{\tfrac{\P(X=x)}{\P(Y = 0)}}  \right) +
\log\left( \frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)} \right)
\\
&= \underbrace{\left( \tfrac{1 - \pi}{\pi} + \beta_0 \right)}_{\beta_0'} + \beta^\top x
\end{align*}
$$

## Recovering class probabilities

$$
\text{Our model:}\qquad
\log\left( \frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)} \right) = \beta_0 + \beta^\top x
$$

. . .

\
We know that $\P(Y = 1 \mid X=x) + \P(Y = 0 \mid X=x) = 1$. So...

$$
\frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)}
=
\frac{\P(Y = 1 \mid X=x)}{1 - \P(Y = 1 \mid X=x)}
=
\exp\left( \beta_0 + \beta^\top x \right),
$$

---

$$
\frac{\P(Y = 1 \mid X=x)}{\P(Y = 0 \mid X=x)}
=
\frac{\P(Y = 1 \mid X=x)}{1 - \P(Y = 1 \mid X=x)}
=
\exp\left( \beta_0 + \beta^\top x \right),
$$

[After algebra...]{.small}
$$
\begin{aligned}
\P(Y = 1 \given X=x) &= \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}},
\\\
\P(Y = 0 | X=x) &= \frac{1}{1 + \exp\{\beta_0 + \beta^{\top}x\}}
\end{aligned}
$$

This is logistic regression.


## Logistic Regression

$$\P(Y = 1 \given X=x) = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}
= h\left( \beta_0 + \beta^\top x \right),$$

where $h(z) = (1 + \exp(-z))^{-1} =  \exp(z) / (1+\exp(z))$
is the [logistic function]{.secondary}.

\

::: flex
::: w-60

### The "logistic" function is nice.

* It's symmetric: $1 - h(z) = h(-z)$

* Has a nice derivative: $h'(z) = \frac{\exp(z)}{(1 + \exp(z))^2} = h(z)(1-h(z))$.

* It's the inverse of the "log-odds" (logit): $\log(p / (1-p))$.

:::
::: w-35
```{r echo=FALSE}
#| fig-width: 6
#| fig-height: 4
library(tidyverse)
n <- 100
a <- 2
x <- runif(n, -5, 5)
y <- 1 / (1 + exp(-x))
df <- tibble(x, y)
ggplot(df, aes(x, y)) +
  geom_line() +
  labs(x = "x", y = "h(x)")
```
:::
:::


## Logistic regression is a Linear Classifier

<!-- Like LDA, -->
Logistic regression is a [linear classifier]{.secondary}

<!-- The _logit_ (i.e.: log odds) transformation -->
<!-- gives a linear decision boundary -->
$$\log\left( \frac{\P(Y = 1 \given X=x)}{\P(Y = 0 \given X=x) } \right) = \beta_0 + \beta^{\top} x$$

* If the log-odds are $>0$, classify as 1\
  [($Y=1$ is more likely)]{.small}

* If the log-odds are $<0$, classify as a 0\
  [($Y=0$ is more likely)]{.small}

\
The [decision boundary]{.secondary} is the hyperplane
$\{x : \beta_0 + \beta^{\top} x = 0\}$


## Visualizing the classification boundary

```{r plot-d1}
#| code-fold: true
#| fig-width: 8
#| fig-height: 5
library(mvtnorm)
library(MASS)
generate_lda_2d <- function(
    n, p = c(.5, .5),
    mu = matrix(c(0, 0, 1, 1), 2),
    Sigma = diag(2)) {
  X <- rmvnorm(n, sigma = Sigma)
  tibble(
    y = which(rmultinom(n, 1, p) == 1, TRUE)[, 1],
    x1 = X[, 1] + mu[1, y],
    x2 = X[, 2] + mu[2, y]
  )
}

dat1 <- generate_lda_2d(100, Sigma = .5 * diag(2))
logit <- glm(y ~ ., dat1 |> mutate(y = y - 1), family = "binomial")

gr <- expand_grid(x1 = seq(-2.5, 3, length.out = 100), 
                  x2 = seq(-2.5, 3, length.out = 100))
pts <- predict(logit, gr)
g0 <- ggplot(dat1, aes(x1, x2)) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = tibble(gr, disc = pts), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_steps2(n.breaks = 6, name = "log odds") 
g0
```


## Linear classifier $\ne$ linear smoother

* While logistic regression produces linear decision boundaries, it is **not** a linear smoother

* AIC/BIC/Cp work if you use the likelihood correctly and count degrees-of-freedom correctly

  * $\mathrm{df} = p + 1$ ([Why?]{.secondary})

* Most people use CV


## Logistic regression in R

```{r eval=FALSE}
logistic <- glm(y ~ ., dat, family = "binomial")
```

Or we can use lasso or ridge regression or a GAM as before

```{r eval=FALSE}
lasso_logit <- cv.glmnet(x, y, family = "binomial")
ridge_logit <- cv.glmnet(x, y, alpha = 0, family = "binomial")
gam_logit <- gam(y ~ s(x), data = dat, family = "binomial")
```


::: aside
glm means [generalized linear model]{.secondary}
:::


<!-- ## Baby example (continued from last time) -->

<!-- ```{r simple-lda, echo=FALSE} -->
<!-- library(mvtnorm) -->
<!-- library(MASS) -->
<!-- generate_lda_2d <- function( -->
<!--     n, p = c(.5, .5), -->
<!--     mu = matrix(c(0, 0, 1, 1), 2), -->
<!--     Sigma = diag(2)) { -->
<!--   X <- rmvnorm(n, sigma = Sigma) -->
<!--   tibble( -->
<!--     y = which(rmultinom(n, 1, p) == 1, TRUE)[, 1], -->
<!--     x1 = X[, 1] + mu[1, y], -->
<!--     x2 = X[, 2] + mu[2, y] -->
<!--   ) -->
<!-- } -->

<!-- dat1 <- generate_lda_2d(100, Sigma = .5 * diag(2)) -->
<!-- logit <- glm(y ~ ., dat1 |> mutate(y = y - 1), family = "binomial") -->
<!-- summary(logit) -->
<!-- ``` -->



<!-- ## Calculation -->

<!-- While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA. -->


<!-- Logistic regression for two classes simplifies to a likelihood: -->

<!-- Write $p_i(\beta) = \P(Y_i = 1 | X = x_i,\beta)$ -->

<!-- * $P(Y_i = y_i \given X = x_i, \beta) = p_i^{y_i}(1-p_i)^{1-y_i}$ (...Bernoulli distribution) -->

<!-- * $P(\mathbf{Y} \given \mathbf{X}, \beta) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$.  -->


<!-- ## Calculation -->


<!-- Write $p_i(\beta) = \P(Y_i = 1 | X = x_i,\beta)$ -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \ell(\beta)  -->
<!-- & = \log \left( \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i} \right)\\ -->
<!-- &=\sum_{i=1}^n \left( y_i\log(p_i(\beta)) + (1-y_i)\log(1-p_i(\beta))\right) \\ -->
<!-- & =  -->
<!-- \sum_{i=1}^n \left( y_i\log(e^{\beta^{\top}x_i}/(1+e^{\beta^{\top}x_i})) - (1-y_i)\log(1+e^{\beta^{\top}x_i})\right) \\ -->
<!-- & =  -->
<!-- \sum_{i=1}^n \left( y_i\beta^{\top}x_i -\log(1 + e^{\beta^{\top} x_i})\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- This gets optimized via Newton-Raphson updates and iteratively reweighed -->
<!-- least squares. -->


<!-- ## IRWLS for logistic regression (skip for now) -->

<!-- (This is preparation for Neural Networks.) -->

<!-- ```{r} -->
<!-- logit_irwls <- function(y, x, maxit = 100, tol = 1e-6) { -->
<!--   p <- ncol(x) -->
<!--   beta <- double(p) # initialize coefficients -->
<!--   beta0 <- 0 -->
<!--   conv <- FALSE # hasn't converged -->
<!--   iter <- 1 # first iteration -->
<!--   while (!conv && (iter < maxit)) { # check loops -->
<!--     iter <- iter + 1 # update first thing (so as not to forget) -->
<!--     eta <- beta0 + x %*% beta -->
<!--     mu <- 1 / (1 + exp(-eta)) -->
<!--     gp <- 1 / (mu * (1 - mu)) # inverse of derivative of logistic -->
<!--     z <- eta + (y - mu) * gp # effective transformed response -->
<!--     beta_new <- coef(lm(z ~ x, weights = 1 / gp)) # do Weighted Least Squares -->
<!--     conv <- mean(abs(c(beta0, beta) - betaNew)) < tol # check if the betas are "moving" -->
<!--     beta0 <- betaNew[1] # update betas -->
<!--     beta <- betaNew[-1] -->
<!--   } -->
<!--   return(c(beta0, beta)) -->
<!-- } -->
<!-- ``` -->


<!-- ## Comparing LDA and Logistic regression -->

<!-- Both decision boundaries are linear in $x$:   -->

<!-- - LDA $\longrightarrow \alpha_0 + \alpha_1^\top x$  -->
<!-- - Logit $\longrightarrow \beta_0 + \beta_1^\top x$. -->

<!-- But the parameters are estimated differently. -->

<!-- ## Comparing LDA and Logistic regression -->

<!-- Examine the joint distribution of $(X,\ Y)$ [(not the posterior)]{.f3}:   -->

<!-- - LDA: $f(X_i,\ Y_i) = \underbrace{ f(X_i \given Y_i)}_{\textrm{Gaussian}}\underbrace{ f(Y_i)}_{\textrm{Bernoulli}}$ -->
<!-- - Logistic Regression: $f(X_i,Y_i) = \underbrace{ f(Y_i\given X_i)}_{\textrm{Logistic}}\underbrace{ f(X_i)}_{\textrm{Ignored}}$ -->

<!-- * LDA estimates the joint, but Logistic estimates only the conditional (posterior) distribution. [But this is really all we need.]{.hand} -->

<!-- * So logistic requires fewer assumptions. -->

<!-- * But if the two classes are perfectly separable, logistic crashes (and the MLE is undefined, too many solutions) -->

<!-- * LDA "works" even if the conditional isn't normal, but works very poorly if any X is qualitative -->


<!-- ## Comparing with QDA (2 classes) -->


<!-- * Recall: this gives a "quadratic" decision boundary (it's a curve). -->

<!-- * If we have $p$ columns in $X$ -->
<!--     - Logistic estimates $p+1$ parameters -->
<!--     - LDA estimates $2p + p(p+1)/2 + 1$ -->
<!--     - QDA estimates $2p + p(p+1) + 1$ -->

<!-- * If $p=50$, -->
<!--     - Logistic: 51 -->
<!--     - LDA: 1376 -->
<!--     - QDA: 2651 -->

<!-- * QDA doesn't get used much: there are better nonlinear versions with way fewer parameters -->

<!-- ## Bad parameter counting -->

<!-- I've motivated LDA as needing $\Sigma$, $\pi$ and $\mu_0$, $\mu_1$ -->

<!-- In fact, we don't _need_ all of this to get the decision boundary. -->

<!-- So the "degrees of freedom" is much lower if we only want the _classes_ and not -->
<!-- the _probabilities_. -->

<!-- The decision boundary only really depends on -->

<!-- * $\Sigma^{-1}(\mu_1-\mu_0)$  -->
<!-- * $(\mu_1+\mu_0)$,  -->
<!-- * so appropriate algorithms estimate $<2p$ parameters. -->


## Estimating $\beta_0$, $\beta$

For regression...

$$
\text{Model:} \qquad
\P(Y=y \mid X=x) = \mathcal N( y; \:\: \beta^\top x, \:\:\sigma^2)
$$

... recall that we motivated OLS with
the [principle of maximum likelihood]{.secondary}

$$
\begin{align*}
\hat \beta_\mathrm{OLS}
&= \argmax_{\beta} \prod_{i=1}^n \P(Y_i = y_i \mid X_i = x_i)
\\
&= \argmin_{\beta} \sum_{i=1}^n -\log\P(Y_i = y_i \mid X_i = x_i)
\\
\\
&= \ldots (\text{because regression is nice})
\\
&= \textstyle \left( \sum_{i=1}^n x_i x_i^\top \right)^{-1}\left( \sum_{i=1}^n y_i x_i \right)
\end{align*}
$$

## Estimating $\beta_0$, $\beta$

For classification with logistic regression...

$$
\begin{align*}
\text{Model:} &\qquad
\tfrac{\P(Y=1 \mid X=x)}{\P(Y=0 \mid X=x)} = \exp\left( \beta_0 +\beta^\top x \right)
\\
\text{Or alternatively:} &\qquad \P(Y=1 \mid X=x) = h\left(\beta_0 + \beta^\top x \right)
\\
&\qquad \P(Y=0 \mid X=x) = h\left(-(\beta_0 + \beta^\top x)\right)
\end{align*}
$$
... we can also apply
the [principle of maximum likelihood]{.secondary}

$$
\begin{align*}
\hat \beta_{0}, \hat \beta
&= \argmax_{\beta_0, \beta} \prod_{i=1}^n \P(Y_i \mid X_i)
\\
&= \argmin_{\beta_0,\beta} \sum_{i=1}^n -\log\P(Y_i \mid X_i)
\end{align*}
$$

. . .

Unfortunately that's as far as we can get with algebra alone.

<!-- ## Calculation -->

<!-- While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA. -->


<!-- Logistic regression for two classes simplifies to a likelihood: -->

<!-- Write $p_i(\beta) = \P(Y_i = 1 | X = x_i,\beta)$ -->

<!-- * $P(Y_i = y_i \given X = x_i, \beta) = p_i^{y_i}(1-p_i)^{1-y_i}$ (...Bernoulli distribution) -->

<!-- * $P(\mathbf{Y} \given \mathbf{X}, \beta) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$.  -->

# Next time:

The workhorse algorithm for obtaining $\hat \beta_0$, $\hat \beta$
