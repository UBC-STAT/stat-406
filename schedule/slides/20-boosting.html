<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>20 Boosting</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="materials/libs/fabric/fabric.min.js"></script>
    <link href="materials/libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#e98a15"],"pen_size":3,"eraser_size":30,"palette":["#2c365e","#e98a15","#0a8754","#a8201a","#E41A1C","#377EB8","#4DAF4A","#984EA3","#FF7F00","#FFFF33"]}) })</script>
    <link href="materials/libs/panelset/panelset.css" rel="stylesheet" />
    <script src="materials/libs/panelset/panelset.js"></script>
    <script src="materials/libs/clipboard/clipboard.min.js"></script>
    <link href="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="materials/libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="materials/libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 20 Boosting
]
.author[
### STAT 406
]
.author[
### Daniel J. McDonald
]
.date[
### Last modified - 2022-11-02
]

---


## Last time






<style>.panelset{--panel-tab-active-foreground: #2c365e;--panel-tab-hover-foreground: #e98a15;}</style>


We learned about bagging, for averaging .secondary[low-bias] / .primary[high-variance] estimators.

Today, we examine it's opposite: __Boosting__.

__Boosting__ also combines estimators, but it combines __high-bias__ / low-variance estimators.

Boosting has a number of flavours. And if you Google descriptions, most are wrong.

For a deep (and accurate) treatment, see [ESL] Chapter 10


--

We'll discuss 2 flavours: AdaBoost and Gradient Boosting

Neither requires a tree, but that's the typical usage.

Boosting needs a "weak learner", so small trees (called stumps) are natural.

`$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$`


---


## AdaBoost intuition

At each iteration, we weight the __observations__.

Observations that are currently misclassified, get __higher__ weights.

So on the next iteration, we'll try harder to correctly classify our mistakes.

The number of iterations must be chosen.

---

## AdaBoost (Freund and Schapire)

Let `\(G(x, \theta)\)` be a weak learner (say a tree with one split)

.emphasis[

__Algorithm (AdaBoost):__

1. Set observation weights `\(w_i=1/n\)`.

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Estimate the classifier `\(G(x,\theta_m)\)` using weights `\(w_i\)`
    
    b. Calculate it's weighted error `\(\textrm{err}_m = \sum_{i=1}^n w_i I(y_i \neq G(x_i, \theta_m)) / \sum w_i\)`
    
    c. Set `\(\alpha_m = \log((1-\textrm{err}_m)/\text{err}_m)\)`
    
    d. Update `\(w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G(x_i,\theta_m)))\)`

3. Final classifier is `\(G(x) = \textrm{sign}\left( \sum_{m=1}^M \alpha_m G(x, \theta_m)\right)\)`
]

---

## Using mobility data again




```r
library(gbm)
train_boost &lt;- train %&gt;% mutate(mobile = as.integer(mobile) - 1) # needs {0, 1} responses
test_boost &lt;- test %&gt;% mutate(mobile = as.integer(mobile) - 1)
adab &lt;- gbm(mobile ~ ., data = train_boost, n.trees = 500, distribution = "adaboost")
preds$adab = as.numeric(predict(adab, test_boost) &gt; 0)
par(mar = c(5,15,0,1))
summary(adab, las = 1)
```

&lt;img src="rmd_gfx/20-boosting/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

```
##                                                 var    rel.inf
## Single_mothers                       Single_mothers 14.0404924
## Local_tax_rate                       Local_tax_rate  6.7989919
## Religious                                 Religious  6.6286695
## Commute                                     Commute  6.4179702
## Test_scores                             Test_scores  5.9974412
## Black                                         Black  4.8716771
## Manufacturing                         Manufacturing  4.7307967
## Gini_99                                     Gini_99  4.2246181
## Chinese_imports                     Chinese_imports  3.9555660
## Latitude                                   Latitude  3.8442520
## Foreign_born                           Foreign_born  2.8028885
## Middle_class                           Middle_class  2.7339875
## Progressivity                         Progressivity  2.6475539
## Graduation                               Graduation  2.4408142
## Share01                                     Share01  2.4262649
## Tuition                                     Tuition  2.3563082
## Longitude                                 Longitude  2.0708840
## Student_teacher_ratio         Student_teacher_ratio  2.0645286
## School_spending                     School_spending  1.9700753
## Migration_out                         Migration_out  1.8248691
## Married                                     Married  1.8081761
## HS_dropout                               HS_dropout  1.6722032
## Colleges                                   Colleges  1.5849022
## Local_gov_spending               Local_gov_spending  1.2733602
## Migration_in                           Migration_in  1.2450674
## Social_capital                       Social_capital  1.2442027
## Seg_racial                               Seg_racial  0.9630583
## Teenage_labor                         Teenage_labor  0.7700715
## Labor_force_participation Labor_force_participation  0.6406051
## Income                                       Income  0.6310008
## Divorced                                   Divorced  0.5880420
## Seg_poverty                             Seg_poverty  0.5662602
## Violent_crime                         Violent_crime  0.5654377
## Population                               Population  0.4550882
## EITC                                           EITC  0.4385737
## Seg_affluence                         Seg_affluence  0.3075372
## Seg_income                               Seg_income  0.2414435
## Gini                                           Gini  0.1563206
## Urban                                         Urban  0.0000000
```

---

## Forward stagewise additive modeling

Generic for regression or classification, any weak learner `\(G(x,\ \theta)\)`

.emphasis[

__Algorithm:__

1. Set initial predictor `\(f_0(x)=0\)`

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Compute `$$(\beta_m, \theta_m) = \arg\min_{\beta, \theta} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i,\ \theta)\right)$$`
    
    b. Set `\(f_m(x) = f_{m-1}(x) + \beta_m G(x,\ \theta_m)\)`
    
3. Final classifier is `\(G(x, \theta_M) = \textrm{sign}\left( f_M(x) \right)\)`
]

Here, `\(L\)` is a loss function that measures prediction accuracy

If `\(L(y,\ f(x))= \exp(-y f(x))\)`, `\(G\)` is a classifier, and `\(y \in \{-1, 1\}\)` then this is equivalent to AdaBoost. Proven 5 years later (Friedman, Hastie, and Tibshirani 2000).

---

## So what?

It turns out that "exponential loss" `\(L(y,\ f(x))= \exp(-y f(x))\)` is not very robust.

Here are some other loss functions for 2-class classification

&lt;img src="rmd_gfx/20-boosting/loss-funs-1.svg" style="display: block; margin: auto;" /&gt;

--

Want losses which penalize negative margin, but not positive margins.

Robust means .hand[don't over-penalize large negatives]

---

## Gradient boosting


In the forward stagewise algorithm, we solved a minimization and then made an update 

`\(f_m(x) = f_{m-1}(x) + \beta_m G(x, \theta_m)\)`.

For most loss functions, `\(\arg\min_{\beta, \theta} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i, \theta)\right)\)` cannot be solved

Instead, if we take one gradient step toward the minimum, we get

`\(f_m(x) = f_{m-1}(x) -\gamma_m \nabla L(y,f_{m-1}(x)) = f_{m-1}(x) +\gamma_m \left(-\nabla L(y,f_{m-1}(x))\right)\)`

This is called __Gradient boosting__

Notice how similar the update steps look.

Gradient boosting goes only part of the way toward the minimum at each `\(m\)`. This has two implications:

1. Since we're not fitting `\(\beta, \theta\)` to the data as "hard", the learner is weaker.

2. This procedure is computationally much simpler.


---

## Gradient boosting

.emphasis[

__Algorithm:__

1. Set initial predictor `\(f_0(x)=\overline{\y}\)`

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Compute pseudo-residuals (what is the gradient of `\(L(y,f)=(y-f(x))^2\)`?)
    `$$r_i = -\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{m-1}(x_i)}$$`
    
    b. Estimate weak learner, `\(G(x, \theta_m)\)`, with the training set `\(\{r_i, x_i\}\)`.
    
    c. Find the step size `\(\gamma_m = \arg\min_\gamma \sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \gamma G(x_i, \theta_m))\)`
    
    b. Set `\(f_m(x) = f_{m-1}(x) + \gamma_m G(x, \theta_m)\)`
    
3. Final predictor is `\(f_M(x)\)`.
]


```r
grad_boost &lt;- gbm(mobile ~ ., data = train_boost, n.trees = 500, distribution = "bernoulli")
```

---

## Gradient boosting modifications

* Typically done with "small" trees, not stumps because of the gradient. You can specify the size. Usually 4-8 terminal nodes is recommended (more gives more interactions between predictors)

* Usually modify the gradient step to `\(f_m(x) = f_{m-1}(x) + \gamma_m \alpha G(x,\theta_m)\)` with `\(0&lt;\alpha&lt;1\)`. Helps to keep from fitting too hard.

* Often combined with Bagging so that each step is fit using a bootstrap resample of the data. Gives us out-of-bag options.

* There are many other extensions, notably XGBoost.

&lt;img src="rmd_gfx/20-boosting/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---

## Major takeaways

* Two flavours of Boosting (1) AdaBoost (the original) and (2) gradient boosting (easier and more computationally friendly)

* The connection is "Forward stagewise additive modelling" (AdaBoost is a special case)

* But that special case "isn't robust because it uses exponential loss" (squared error is even worse)

* Gradient boosting is a computationally easier version of FSAM

* All use **weak learners** (compare to Bagging)

* Think about the Bias-Variance implications






---
class: middle, inverse, center

# Next time...

Neural networks and deep learning, the beginning
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
