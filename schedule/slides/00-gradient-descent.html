<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>00 Gradient descent</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="materials/libs/fabric/fabric.min.js"></script>
    <link href="materials/libs/xaringanExtra-scribble/scribble.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-scribble/scribble.js"></script>
    <script>document.addEventListener('DOMContentLoaded', function() { window.xeScribble = new Scribble({"pen_color":["#e98a15"],"pen_size":3,"eraser_size":30,"palette":["#2c365e","#e98a15","#0a8754","#a8201a","#E41A1C","#377EB8","#4DAF4A","#984EA3","#FF7F00","#FFFF33"]}) })</script>
    <link href="materials/libs/panelset/panelset.css" rel="stylesheet" />
    <script src="materials/libs/panelset/panelset.js"></script>
    <script src="materials/libs/clipboard/clipboard.min.js"></script>
    <link href="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="materials/libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"<i class=\"fa fa-clipboard\"><\/i>","success":"<i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Press Ctrl+C to Copy"})</script>
    <link href="materials/libs/font-awesome/css/all.css" rel="stylesheet" />
    <link href="materials/libs/font-awesome/css/v4-shims.css" rel="stylesheet" />
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# 00 Gradient descent
]
.author[
### STAT 406
]
.author[
### Daniel J. McDonald
]
.date[
### Last modified - 2022-10-27
]

---


## Simple optimization techniques

`$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$`





<style>.panelset{--panel-tab-active-foreground: #2c365e;--panel-tab-hover-foreground: #e98a15;}</style>


We'll see "gradient descent" a few times: 

1. solves logistic regression (simple version of IRWLS)

2. gradient boosting

3. Neural networks

This seems like a good time to explain it.

So what is it and how does it work?

---

## Very basic example

.pull-left-wide[
Suppose I want to minimize `\(f(x)=(x-6)^2\)` numerically.

I start at a point (say `\(x_1=23\)`)

I want to "go" in the negative direction of the gradient.

The gradient (at `\(x_1=23\)`) is  `\(f'(23)=2(23-6)=34\)`.

Move current value toward current value - 34.

`\(x_2 = x_1 - \gamma 34\)`, for `\(\gamma\)` small.

In general, `\(x_{n+1} = x_n -\gamma f'(x_n)\)`.


```r
niter &lt;- 10
gam &lt;- 0.1
x &lt;- double(niter)
x[1] &lt;- 23
grad &lt;- function(x) 2 * (x - 6)
for (i in 2:niter) x[i] = x[i - 1] - gam * grad(x[i - 1])
```


]
  
.pull-right-narrow[  
  

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

]

---

## Why does this work?

__Heuristic interpretation:__

* Gradient tells me the slope.

* negative gradient points toward the minimum

* go that way, but not too far (or we'll miss it)

--

__More rigorous interpretation:__

- Taylor expansion
$$
f(x) \approx f(x_0) + \nabla f(x_0)^{\top}(x-x_0) + \frac{1}{2}(x-x_0)^\top H(x_0) (x-x_0)
$$
- replace `\(H\)` with `\(\gamma^{-1} I\)`

- minimize the quadratic approximation in `\(x\)`:
$$
0\overset{\textrm{set}}{=}\nabla f(x_0) + \frac{1}{\gamma}(x-x_0) \Longrightarrow x = x_0 - \gamma \nabla f(x_0)
$$

---
layout: true

## Visually

---

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-4-1.svg" style="display: block; margin: auto;" /&gt;


---
layout: false

## What `\(\gamma\)`? (more details than we have time for)

What to use for `\(\gamma_k\)`? 


__Fixed__

- Only works if `\(\gamma\)` is exactly right 
- Usually does not work

__Decay on a schedule__ 

`$$\gamma_{k+1} = \frac{\gamma_k}{1+ck} \quad \textrm{or} \quad \gamma_{k} = \gamma_0 b^k$$`



__Exact line search__

- Tells you exactly how far to go.
- At each `\(k\)`, solve
`\(\gamma_k = \arg\min_{s \geq 0} f( x^{(k)} - s f(x^{(k-1)}))\)`
- Usually can't solve this.



---
layout: true

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$


```r
x &lt;- matrix(0, 40, 2)
x[1, ] &lt;- c(1, 1)
grad &lt;- function(x) c(2, 1) * x
```

---

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-6-1.svg" style="display: block; margin: auto;" /&gt;

---



```r
gamma &lt;- .1
for (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ])
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-8-1.svg" style="display: block; margin: auto;" /&gt;

---




```r
gamma &lt;- .9 # bigger gamma
for (k in 2:40) x[k,] &lt;- x[k - 1,] - gamma * grad(x[k-1,])
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-11-1.svg" style="display: block; margin: auto;" /&gt;

---





```r
gamma &lt;- .9 # big, but decrease it on schedule
for (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * .9^k * grad(x[k - 1, ]) 
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-14-1.svg" style="display: block; margin: auto;" /&gt;

---





```r
gamma &lt;- .5 # theoretically optimal
for (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ]) 
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-17-1.svg" style="display: block; margin: auto;" /&gt;



---
layout: false

## When do we stop?

For `\(\epsilon&gt;0\)`, small


Check any / all of

1. `\(|f'(x)| &lt; \epsilon\)`

2. `\(|x^{(k)} - x^{(k-1)}| &lt; \epsilon\)`

3. `\(|f(x^{(k)}) - f(x^{(k-1)})| &lt; \epsilon\)`

---

## Stochastic gradient descent

Suppose `\(f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x)\)`

Like if `\(f(\beta) = \frac{1}{n}\sum_{i=1}^n (y_i - x^\top_i\beta)^2\)`.

Then `\(f'(\beta) = \frac{1}{n}\sum_{i=1}^n f'_i(\beta) = \frac{1}{n} \sum_{i=1}^n -2x_i^\top(y_i - x^\top_i\beta)\)` 

If `\(n\)` is really big, it may take a long time to compute `\(f'\)`

So, just sample some partition our data into mini-batches `\(\mathcal{M}_j\)`

And approximate (imagine the Law of Large Numbers, use a sample to approximate the population)
`$$f'(x) = \frac{1}{n}\sum_{i=1}^n f'_i(x) \approx \frac{1}{m}\sum_{i\in\mathcal{M}_j}f'_{i}(x)$$`

Usually cycle through "mini-batches":
* Use a different mini-batch at each iteration of GD
* Cycle through until we see all the data

--

This is the workhorse for neural network optimization

---
class: center, middle, inverse

# Practice with GD and Logistic regression

---

## Gradient descent for Logistic regression

Suppose `\(Y=1\)` with probability `\(p(x)\)` and `\(Y=0\)` with probability `\(1-p(x)\)`, `\(x \in \R\)`.  

I want to model `\(P(Y=1| X=x)\)`. 

I'll assume that `\(\log\left(\frac{p(x)}{1-p(x)}\right) = ax\)` for some scalar `\(a\)`. This means that
`\(p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}\)`


--

.pull-left[

```r
n &lt;- 100
a &lt;- 2
x &lt;- runif(n, -5, 5)
logit &lt;- function(x) 1 / (1 + exp(-x))
p &lt;- logit(a * x)
y &lt;- rbinom(n, 1, p)
df &lt;- tibble(x, y)
g &lt;- ggplot(df, aes(x, y)) +
  geom_point(color="cornflowerblue") +
  stat_function(fun = ~ logit(a * .x))
```
]


.pull-right[
![](rmd_gfx/00-gradient-descent/unnamed-chunk-18-1.svg)&lt;!-- --&gt;
]

---
layout: true


## Reminder: the likelihood

`$$L(y | a, x) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$`

Remember, we defined `\(p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}\)`.

---

The log likelihood is therefore

$$
`\begin{aligned}
\ell(y | a, x) &amp;= \log \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i} \\
&amp;= \sum_{i=1}^n y_i\log p(x_i) + (1-y_i)\log(1-p(x_i))\\
&amp;= \sum_{i=1}^n\log(1-p(x_i)) + y_i\log\left(\frac{p(x_i)}{1-p(x_i)}\right)\\
&amp;=\sum_{i=1}^n ax_i y_i + \log\left(1-p(x_i)\right)\\
&amp;=\sum_{i=1}^n ax_i y_i + \log\left(\frac{1}{1+\exp(ax_i)}\right)
\end{aligned}`
$$

---

Now, we want the negative of this. Why? 

We would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood.


`$$-\ell(y | a, x) = \sum_{i=1}^n -ax_i y_i - \log\left(\frac{1}{1+\exp(ax_i)}\right)$$`

---

This is, in the notation of our slides `\(f(a)\)`. 

We want to minimize it in `\(a\)` by gradient descent. 

So we need the derivative with respect to `\(a\)`: `\(f'(a)\)`. 

Now, conveniently, this simplifies a lot. 


`$$\frac{d}{d a} f(a) = \sum_{i=1}^n -x_i y_i - \left(-\frac{x_i \exp(ax_i)}{1+\exp(ax_i)}\right) = 
\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \sum_{i=1}^n -x_i(y_i-p(x_i)).$$`

---

(Simple) gradient descent to minimize `\(-\ell(a)\)` or maximize `\(L(y|a,x)\)` is:

1. Input `\(a_1,\ \gamma&gt;0,\ j_\max,\ \epsilon&gt;0,\ \frac{d}{da} -\ell(a)\)`.
2. For `\(j=1,\ 2,\ \ldots,\ j_\max\)`,
`$$a_j = a_{j-1} - \gamma \frac{d}{da} (-\ell(a_{j-1}))$$`
3. Stop if `\(\epsilon &gt; |a_j - a_{j-1}|\)` or `\(|d / da\  \ell(a)| &lt; \epsilon\)`.

---


```r
amle &lt;- function(x, y, a0, gam = 0.5, jmax = 50, eps = 1e-6){
  a &lt;- double(jmax) # place to hold stuff (always preallocate space)
  a[1] &lt;- a0 # starting value
  for (j in 2:jmax) { # avoid possibly infinite while loops
    px &lt;- logit(a[j - 1] * x) 
    grad &lt;- sum( -x * (y - px) ) 
    a[j] &lt;- a[j - 1] - gam * grad
    if (abs(grad) &lt; eps || abs(a[j] - a[j-1]) &lt; eps) break
  }
  a[1:j]
}
```

---
layout: false

## Try it:


```r
round(too_big &lt;- amle(x, y, 5, .5), 3)
```

```
##  [1] 5.000 3.360 2.019 1.815 2.059 1.782 2.113 1.746 2.180 1.711 2.250 1.684
## [13] 2.309 1.669 2.344 1.663 2.359 1.661 2.364 1.660 2.365 1.660 2.366 1.660
## [25] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660
## [37] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660
## [49] 2.366 1.660
```

```r
round(too_small &lt;- amle(x, y, 5, .01), 3)
```

```
##  [1] 5.000 4.967 4.934 4.902 4.869 4.837 4.804 4.772 4.739 4.707 4.675 4.643
## [13] 4.611 4.579 4.547 4.515 4.483 4.451 4.420 4.388 4.357 4.326 4.294 4.263
## [25] 4.232 4.201 4.170 4.140 4.109 4.078 4.048 4.018 3.988 3.957 3.927 3.898
## [37] 3.868 3.838 3.809 3.779 3.750 3.721 3.692 3.663 3.635 3.606 3.578 3.550
## [49] 3.522 3.494
```

```r
round(just_right &lt;- amle(x, y, 5, .1), 3)
```

```
##  [1] 5.000 4.672 4.351 4.038 3.735 3.445 3.171 2.917 2.688 2.488 2.322 2.191
## [13] 2.094 2.027 1.983 1.956 1.940 1.930 1.925 1.922 1.920 1.919 1.918 1.918
## [25] 1.918 1.918 1.918 1.917 1.917 1.917 1.917 1.917 1.917 1.917 1.917
```

---

## Visual


.pull-left[

```r
negll &lt;- function(a) {
  -a * sum(x*y) - 
    rowSums(log(1 / (1 + exp(outer(a, x)))))
}
tb &lt;- tibble(a = too_big, negll = negll(a))
ts &lt;- tibble(a = too_small, negll = negll(a))
jr &lt;- tibble(a = just_right, negll = negll(a))
ppp &lt;- bind_rows(too_big = tb, 
                 too_small = ts, 
                 just_right = jr,
                 .id = "gamma")
g &lt;- ggplot(ppp, aes(a, negll)) +
  geom_point(aes(color = gamma)) + 
  facet_wrap(~gamma, ncol = 1) +
  stat_function(fun = negll, xlim = c(-2.5,5)) +
  scale_y_log10() + 
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") 
summary(glm(y ~ x - 1, family = "binomial"))
```

```
## 
## Call:
## glm(formula = y ~ x - 1, family = "binomial")
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -1.26228  -0.06652   0.01671   0.15655   2.26728  
## 
## Coefficients:
##   Estimate Std. Error z value Pr(&gt;|z|)    
## x   1.9174     0.4785   4.008 6.13e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 138.629  on 100  degrees of freedom
## Residual deviance:  32.335  on  99  degrees of freedom
## AIC: 34.335
## 
## Number of Fisher Scoring iterations: 7
```
]

.pull-right[

```r
g
```

![](rmd_gfx/00-gradient-descent/unnamed-chunk-20-1.svg)&lt;!-- --&gt;
]

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
