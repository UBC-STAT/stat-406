---
title: "15 LDA and QDA"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

```{r css-extras, file="css-extras.R", echo=FALSE}
```


## Last time

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$

We showed that with two classes, the __Bayes' classifier__ is

$$g_*(X) = \begin{cases}
1 & \textrm{ if } \frac{p_1(X)}{p_0(X)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

where $p_1(X) = Pr(X \given Y=1)$, $p_0(X) = Pr(X \given Y=0)$ and $\pi = Pr(Y=1)$

--

For more than two classes.

$$g_*(X) = 
\argmax_k \frac{\pi_k p_k(X)}{\sum_k \pi_k p_k(X)}$$

where $p_k(X) = Pr(X \given Y=k)$ and $\pi_k = P(Y=k)$

--
 
Let's make some assumptions:

1. $Pr(X\given Y=k) = \mbox{N}(\mu_k,\Sigma_k)$
2. $\Sigma_k = \Sigma_{k'} = \Sigma$

--

This leads to __Linear Discriminant Analysis__ (LDA), one of the oldest classifiers


---
layout: true

## LDA

.emphasis[
1. Split your training data into $K$ subsets based on $y_i=k$.
2. In each subset, estimate the mean of $X$: $\hat\mu_k = \overline{X}_k$
3. Estimate the pooled variance: $$\hat\Sigma = \frac{1}{n-K} \sum_{k \in \mathcal{K}} \sum_{i \in k} (x_i - \overline{X}_k) (x_i - \overline{X}_k)^{\top}$$
4. Estimate the class proportion: $\hat\pi_k = n_k/n$
]

---

Assume just $K = 2$ so $k \in \{0,\ 1\}$

We predict $\hat{y} = 1$ if

$$\hat{p_1}(x) / \hat{p_0}(x) > \hat{\pi_0} / \hat{\pi_1}$$ 

Plug in the density estimates:

$$\hat{p_k}(x) = N(x - \hat{\mu}_k,\ \hat\Sigma)$$

---


Now we take $\log$ and simplify $(K=2)$:

$$
\begin{aligned}
&\Rightarrow \log(\hat{p_1}(x)\times\hat{\pi_1}) - \log(\hat{p_0}(x)\times\hat{\pi_0})
= \cdots = \cdots\\
&= \underbrace{\left(x^\top\hat\Sigma^{-1}\overline X_1-\frac{1}{2}\overline X_1^\top \hat\Sigma^{-1}\overline X_1 + \log \hat\pi_1\right)}_{\delta_1(x)} -  \underbrace{\left(x^\top\hat\Sigma^{-1}\overline X_0-\frac{1}{2}\overline X_0^\top \hat\Sigma^{-1}\overline X_0 + \log \hat\pi_0\right)}_{\delta_0(x)}\\
&= \delta_1(x) - \delta_0(x)
\end{aligned}
$$


**If $\delta_1(x) > \delta_0(x)$, we set $\hat g(x)=1$**

---
layout: false

## What is linear?

Look closely at the equation for $\delta_1(x)$:

$$\delta_1(x)=x^\top\hat\Sigma^{-1}\overline X_1-\frac{1}{2}\overline X_1^\top \hat\Sigma^{-1}\overline X_1 + \log \hat\pi_1$$

We can write this as $\delta_1(x) = x^\top a_1 + b_1$ with $a_1 = \hat\Sigma^{-1}\overline X_1$ and $b_1=-\frac{1}{2}\overline X_1^\top \hat\Sigma^{-1}\overline X_1 + \log \hat\pi_1$.

We can do the same for $\delta_0(x)$ (in terms of $a_0$ and $b_0$)

Therefore, 

$$\delta_1(x)-\delta_0(x) = x^\top(a_1-a_0) + (b_1-b_0)$$

This is how we discriminate between the classes.

We just calculate $(a_1 - a_0)$ (a vector in $\R^p$), and $b_1 - b_0$ (a scalar)

---

## Baby example

.pull-left[
```{r simple-lda}
library(mvtnorm)
library(MASS)
generate_lda <- function(
  n, p = c(.5, .5), 
  mu_mat = matrix(c(0, 0, 1, 1), 2),
  Sigma = diag(2)) {
  
  X <- rmvnorm(n, sigma = Sigma)
  tibble(
    y = apply(rmultinom(n, 1, p) > 0, 2, which) - 1,
    x1 = X[ ,1] + mu_mat[1, y + 1],
    x2 = X[ ,2] + mu_mat[2, y + 1]
  )
}
dat1 <- generate_lda(100, Sigma = .5 * diag(2))
lda_fit <- lda(y ~ ., dat1)
```
]

.pull-right[

```{r plot-d1, fig.align='center', fig.width=7, fig.height=7, dev='png',dvi=300,echo=FALSE}
gr <- expand_grid(
  x1 = seq(-2.5, 3, length.out = 100),
  x2 = seq(-2.5, 3, length.out = 100))
pts <- predict(lda_fit, gr)
g0 <- ggplot(dat1, aes(x1, x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data = tibble(gr, disc = c(pts$x)), aes(x1, x2, fill=disc)) +
  theme_bw(base_size = 24) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_steps2(n.breaks = 6,
                       name = bquote(delta[1]-delta[0])) +
  theme(legend.position = "bottom", legend.key.width=unit(3,"cm"))
g0
```
]

---

## Multiple classes

```{r 3class-lda}
moreclasses <- generate_lda(150, c(.2, .3, .5), matrix(c(0, 0, 1, 1, 1, 0), 2), .5 * diag(2))
separateclasses <- generate_lda(150, c(.2, .3, .5), matrix(c(-1, -1, 2, 2, 2, -1), 2), .1 * diag(2))
```

```{r 3class-plot,echo=FALSE,fig.align='center',fig.height=6,fig.width=12,dev='png',dvi=300}
library(cowplot)
lda_3fit = lda(y~., moreclasses)
lda_separate = lda(y~., separateclasses)
pts3 = predict(lda_3fit, gr)
ptss = predict(lda_separate,gr)
g1 = ggplot(moreclasses, aes(x1,x2)) + 
  scale_shape_manual(values=levels(pts3$class), guide="none") +
  geom_raster(data=tibble(gr,disc=pts3$class), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_viridis_d(alpha=.5,name=bquote(hat(g)(x))) +
  theme_bw(base_size = 24) +
  theme(legend.position = "bottom")
g2 = ggplot(separateclasses, aes(x1,x2)) + 
  scale_shape_manual(values=levels(ptss$class), guide="none") +
  geom_raster(data=tibble(gr,disc=ptss$class), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  theme_bw(base_size = 24) +
  scale_fill_viridis_d(alpha=.5,name=bquote(hat(g)(x))) +
  theme(legend.position = "bottom")
plot_grid(g1, g2)
```

---

## QDA

Just like LDA, but $\Sigma_k$ is separate for each class.

Produces __Quadratic__ decision boundary.

Everything else is the same.

```{r fit-qda}
qda_fit = qda(y ~ ., dat1)
qda_3fit = qda(y ~ ., moreclasses)
```

```{r qda-vs-lda-2class,echo=FALSE,fig.align='center',fig.height=5,fig.width=12,dev='png',dvi=300}
pts_qda = predict(qda_fit, gr)
pts_qda3 = predict(qda_3fit, gr)
z = apply(pts_qda$posterior,1, function(x) log(x[2]/x[1]))
gq0 = ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=z), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  theme_bw(base_size = 24) +
  scale_fill_steps2(n.breaks=8,name=bquote(delta[1]-delta[0])) +
  theme(legend.position = "bottom", legend.key.width=unit(3,"cm"))
plot_grid(g0,gq0)
```

---

## 3 class comparison

```{r 3class-comparison,echo=FALSE,fig.align='center',fig.height=6,fig.width=12,dev='png',dvi=300}
gq1 = ggplot(moreclasses, aes(x1,x2)) + 
  scale_shape_manual(values=levels(pts_qda3$class), guide="none") +
  geom_raster(data=tibble(gr,disc=pts_qda3$class), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  theme_bw(base_size = 24) +
  scale_fill_viridis_d(alpha=.5,name=bquote(hat(g)(x))) +
  theme(legend.position = "bottom")
plot_grid(g1,gq1)
```

---

## Notes

* LDA is a linear classifier. It is not a linear smoother.
  - It is derived from Bayes rule.
  - Assume each class-conditional density in Gaussian
  - It assumes the classes have different mean vectors, but the same (common) covariance matrix.
  - It estimates densities and probabilities and "plugs in" 

QDA is not a linear classifier. It depends on quadratic functions of the data.
  - It is derived from Bayes rule.
  - Assume each class-conditional density in Gaussian
  - It assumes the classes have different mean vectors and different covariance matrices.
  - It estimates densities and probabilities and "plugs in" 

.hand[It is hard (maybe impossible) to come up with reasonable classifiers that are linear smoothers. Many "look" like a linear smoother, but then apply a nonlinear transformation.]

---
class: middle, inverse, center

# Next time...

One more linear classifier and transformations