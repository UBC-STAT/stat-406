---
lecture: "22 Neural nets - optimization"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## This lecture

1. How do we train (optimize) NN?
2. Challenges with NN optimization (and solutions)


# How do we train (optimize) NN?

## Neural network review (L hidden layers)


::: flex
::: w-40

$$
\begin{aligned}
&\boldsymbol a^{(0)} = \boldsymbol x \\
&\text{For }  t \in 1 \ldots L:\\
&\qquad \boldsymbol p^{(t)} = \boldsymbol W^{(t)} a^{(t-1)} \\ 
&\qquad \boldsymbol a^{(t)} = \begin{bmatrix} g(p^{(t)}_1) & \ldots & g(p^{(t)}_D) \end{bmatrix}^\top \\ 
&\hat f_\mathrm{NN}(x) = \boldsymbol \beta^\top \boldsymbol a^{(L)}
\end{aligned}
$$

### Terms

* Weights ( $\boldsymbol \beta \in \R^{K_T}, \boldsymbol{W}^{(t)} \in \R^{D \times D}$ )

* Preactivations ( $\boldsymbol p^{(t)} \in \R^D$ )

* (Hidden) activations ( $\boldsymbol a^{(t)} \in \R^D$ )

:::
::: w-55
![](gfx/two-layer-net.png){fig-align="center" width=600}

### Predictions:
- Regression: $\hat Y_j = \hat f_\mathrm{NN}(x)$
- Binary classification: $\hat Y_j = I[\hat f_\mathrm{NN}(x) \geq 0]$

:::
:::


## Training neural networks: the procedure



1. Choose the architecture:\
   [How many layers, units per layer, activation function $g$?]{.small}

2. Choose the **loss** function:
   [Measures "goodness" of NN predictions]{.small}
  
   * Regression: $\ell(\hat f_\mathrm{NN}(x_i), y_i) = \frac{1}{2}(y_i - \hat f_\mathrm{NN}(x_i))^2$\
   [(the 1/2 just makes the derivative nice)]{.small}
  
   * Classification : $\ell(\hat f_\mathrm{NN}(x_i), y_i) = \log\left( 1 + \exp\left( (1 - y_i) \hat f_\mathrm{NN}(x_i) \right) \right)$

3. Choose paramteters $\boldsymbol W^{(t)}$, $\beta$ to minimize the loss over training data

   * $\mathrm{argmin}_{\boldsymbol W^{(t)}, \beta} \sum_{i=1}^n \ell(\hat f_\mathrm{NN}(x_i), y_i)$
   * We will solve this optimization through *stochastic gradient descent*.


## Training neural networks with Stochastic Gradient Descent

$$ \text{Goal:} \quad \argmin_{w_{\ell,k}^{(t)}, \beta_{m,\ell}} \sum_{i=1}^n \underbrace{\ell(\hat f_\mathrm{NN}(x_i), y_i)}_{\ell_i} $$

**SGD update rule:** for a random subset $\mathcal M \subset \{1, \ldots, n \}$

$$
\begin{align}
  \boldsymbol W^{(t)} &\leftarrow \boldsymbol W^{(t)} - \gamma \: \frac{n}{\left| \mathcal M \right|} \sum_{i \in \mathcal M} \frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}} \\
  \beta &\leftarrow \beta - \gamma \: \frac{n}{\left| \mathcal M \right|} \sum_{i \in \mathcal M} \frac{\partial \ell_i}{\partial \boldsymbol \beta}
\end{align}
$$

[Recall that $\frac{\partial }{\partial \boldsymbol W^{(t)}}\sum_{i=1}^n \ell_i \approx \frac{N}{\left| \mathcal M \right|} \sum_{i \in \mathcal M} \frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}}$ ]{.small}

## Training neural networks with Stochastic Gradient Descent

$$ \text{Goal:} \quad \argmin_{w_{\ell,k}^{(t)}, \beta_{m,\ell}} \sum_{i=1}^n \underbrace{\ell(\hat f_\mathrm{NN}(x_i), y_i)}_{\ell_i} $$

$$ \text{Need:} \qquad \frac{\partial \ell_i}{\partial \boldsymbol \beta}, \quad \frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}} $$

**Solution**: Compute $\frac{\partial \ell_i}{\partial \boldsymbol \beta}$ and $\frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}}$ via the *chain rule* \
[In NN parlance, "chain rule" = "backpropagation"]{.secondary}


## Backpropagation (Automated Chain Rule)
 
Consider the *computation graph* corresponding to the 1-hidden layer NN

[ $$
\begin{aligned}
\boldsymbol p^{(1)} = \boldsymbol W^{(1)} \boldsymbol x,
\qquad
\boldsymbol a^{(1)} = \begin{bmatrix} g(p^{(1)}_1) & \ldots & g(p^{(1)}_D) \end{bmatrix}^\top,
\qquad
f_\mathrm{NN}(x) = \boldsymbol \beta^\top \boldsymbol a^{(1)}
\end{aligned}
$$ ]{.small}

![](gfx/comp_graph_forward.png){fig-align="center" width=1000 fig-caption="Forward computation graph"}

* Each node in the computation graph is computed from a simple and differentiable function.

* So we can compute $\partial \ell_i / \partial \boldsymbol \beta$ through *recursive* application of the chain rule.


## Backpropagation (Automated Chain Rule)

![](gfx/comp_graph_forward.png){fig-align="center" width=1000 fig-caption="Forward computation graph"}

$$
\begin{aligned}
  \partial \ell_i / \partial \boldsymbol W^{(1)}
  &=
  \underbrace{\left( \partial \ell_i / \partial \boldsymbol p^{(1)} \right)}_{\text{compute recursively}}
  \underbrace{\Bigl( \partial \overbrace{\boldsymbol W^{(1)} \boldsymbol x}^{\boldsymbol p^{(1)}} / \partial \boldsymbol W^{(1)} \Bigr)^\top}_{\text{easy!}}
  \\
  \partial \ell_i / \partial p^{(1)}_j
  &=
  \underbrace{\left( \partial \ell_i / \partial a^{(1)}_j \right)}_{\text{compute recursively}}
  \underbrace{\Bigl( \partial \overbrace{\boldsymbol g(p^{(1)}_j)}^{a_j^{(1)}} / \partial p_j^{(1)} \Bigr)^\top}_{\text{easy!}}
  \\
  &\vdots
\end{aligned}
$$

## Backpropagation (Automated Chain Rule)

![](gfx/comp_graph_forward.png){fig-align="center" width=400 fig-caption="Forward computation graph"}

Applying these rules recursively, we end up with the *reverse computation graph*

![](gfx/comp_graph_backward.png){fig-align="center" width=1000 fig-caption="Backward computation graph"}

## Backpropagation (Automated Chain Rule)

![](gfx/comp_graph_backward.png){fig-align="center" width=1000 fig-caption="Backward computation graph"}

### Observations

* If this process looks automatable, it is!\
  [Modern *autodifferentiation* frameworks automatically perform these recursive  gradients computations for any function that can be written as a *computation graph* (i.e. the composition of simple differentiable primative functions).]{.small}
  
* We do not need to compute $\partial \ell_i / \partial \boldsymbol x$ in order to perform gradient descent. [Why? Can you generalize this rule?]{.secondary}

* However, we do need to compute $\partial \ell_i / \partial \boldsymbol a^{(1)}$. [Why? Can you generalize this rule?]{.secondary}

<!-- ## Chain rule {.smaller} -->


<!-- We want $\frac{\partial}{\partial B} \hat{R}_i$ and $\frac{\partial}{\partial W_{t}}\hat{R}_i$ for all $t$. -->

<!-- [Regression:]{.secondary} $\hat{R}_i = \frac{1}{2}(y_i - \hat{y}_i)^2$ -->


<!-- $$\begin{aligned} -->
<!-- \frac{\partial\hat{R}_i}{\partial B} &= -(y_i - \hat{y}_i)\frac{\partial \hat{y_i}}{\partial B} =\underbrace{-(y_i - \hat{y}_i)}_{-r_i}  \boldsymbol{A}^{(T)}\\ -->
<!-- \frac{\partial}{\partial \boldsymbol{W}_T} \hat{R}_i &= -(y_i - \hat{y}_i)\frac{\partial\hat{y_i}}{\partial \boldsymbol{W}_T} = -r_i \frac{\partial \hat{y}_i}{\partial \boldsymbol{A}^{(T)}} \frac{\partial \boldsymbol{A}^{(T)}}{\partial \boldsymbol{W}_T}\\  -->
<!-- &= -\left(r_i  B \odot g'(\boldsymbol{W}_T \boldsymbol{A}^{(T)}) \right) \left(\boldsymbol{A}^{(T-1)}\right)^\top\\ -->
<!-- \frac{\partial}{\partial \boldsymbol{W}_{T-1}} \hat{R}_i &= -(y_i - \hat{y}_i)\frac{\partial\hat{y_i}}{\partial \boldsymbol{W}_{T-1}} = -r_i \frac{\partial \hat{y}_i}{\partial \boldsymbol{A}^{(T)}} \frac{\partial \boldsymbol{A}^{(T)}}{\partial \boldsymbol{W}_{T-1}}\\ -->
<!-- &= -r_i \frac{\partial \hat{y}_i}{\partial \boldsymbol{A}^{(T)}} \frac{\partial \boldsymbol{A}^{(T)}}{\partial \boldsymbol{W}_{T}}\frac{\partial \boldsymbol{W}_{T}}{\partial \boldsymbol{A}^{(T-1)}}\frac{\partial \boldsymbol{A}^{(T-1)}}{\partial \boldsymbol{W}_{T-1}}\\ -->
<!-- \cdots &= \cdots -->
<!-- \end{aligned}$$ -->



<!-- ## Mapping it out {.smaller} -->

<!-- Given current $\boldsymbol{W}_t, B$, we want to get new, $\widetilde{\boldsymbol{W}}_t,\ \widetilde B$ for $t=1,\ldots,T$ -->

<!-- * Squared error for regression, cross-entropy for classification -->

<!-- ::: flex -->
<!-- ::: w-50 -->

<!-- [Feed forward]{.tertiary} `r fa("arrow-down", fill=green)` -->

<!-- $$\boldsymbol{A}^{(0)} = \boldsymbol{X}  \in \R^{n\times p}$$ -->

<!-- Repeat, $t= 1,\ldots, T$ -->

<!-- 1. $\boldsymbol{Z}_{t} = \boldsymbol{A}^{(t-1)}\boldsymbol{W}_t \in \R^{n\times K_t}$ -->
<!-- 1. $\boldsymbol{A}^{(t)} = g(\boldsymbol{Z}_{t})$ (component wise) -->
<!-- 1. $\dot{\boldsymbol{A}}^{(t)} = g'(\boldsymbol{Z}_t)$ -->

<!-- $$\begin{cases} -->
<!-- \hat{\boldsymbol{y}} =\boldsymbol{A}^{(T)} B \in \R^n \\ -->
<!-- \hat{\Pi} = \left(1 + \exp\left(-\boldsymbol{A}^{(T)}\boldsymbol{B}\right)\right)^{-1} \in \R^{n \times M}\end{cases}$$ -->

<!-- ::: -->

<!-- ::: w-50 -->


<!-- [Back propogate]{.secondary} `r fa("arrow-up", fill=orange)` -->

<!-- $$-r = \begin{cases} -->
<!-- -\left(\boldsymbol{y} - \widehat{\boldsymbol{y}}\right) \\ -->
<!-- -\left(1 - \widehat{\Pi}\right)[y]\end{cases}$$ -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \frac{\partial}{\partial \boldsymbol{B}} \widehat{R} &= \left(\boldsymbol{A}^{(T)}\right)^\top \boldsymbol{r}\\ -->
<!-- -\boldsymbol{\Gamma} &\leftarrow -\boldsymbol{r}\\ -->
<!-- \boldsymbol{W}_{T+1} &\leftarrow \boldsymbol{B} -->
<!-- \end{aligned} -->
<!-- $$ -->


<!-- Repeat, $t = T,...,1$, -->

<!-- 1. $-\boldsymbol{\Gamma} \leftarrow -\left(\boldsymbol{\Gamma} \boldsymbol{W}_{t+1}\right) \odot\dot{\boldsymbol{A}}^{(t)}$ -->
<!-- 1. $\frac{\partial R}{\partial \boldsymbol{W}_t} = -\left(\boldsymbol{A}^{(t)}\right)^\top \Gamma$ -->

<!-- ::: -->
<!-- ::: -->



<!-- ## Simple example -->

<!-- ```{r} -->
<!-- #| eval: false -->
<!-- n <- 200 -->
<!-- df <- tibble( -->
<!--   x = seq(.05, 1, length = n), -->
<!--   y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function -->
<!-- ) -->
<!-- testdata <- matrix(seq(.05, 1, length.out = 1e3), ncol = 1) -->
<!-- library(neuralnet) -->
<!-- nn_out <- neuralnet(y ~ x, data = df, hidden = c(10, 5, 15), threshold = 0.01, rep = 3) -->
<!-- nn_preds <- map(1:3, ~ compute(nn_out, testdata, .x)$net.result) -->
<!-- yhat <- nn_preds |> bind_cols() |> rowMeans() # average over the runs -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| eval: false -->
<!-- #| code-fold: true -->
<!-- # This code will reproduce the analysis, takes some time -->
<!-- set.seed(406406406) -->
<!-- n <- 200 -->
<!-- df <- tibble( -->
<!--   x = seq(.05, 1, length = n), -->
<!--   y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function -->
<!-- ) -->
<!-- testx <- matrix(seq(.05, 1, length.out = 1e3), ncol = 1) -->
<!-- library(neuralnet) -->
<!-- library(splines) -->
<!-- fstar <- sin(1 / testx) -->
<!-- spline_test_err <- function(k) { -->
<!--   fit <- lm(y ~ bs(x, df = k), data = df) -->
<!--   yhat <- predict(fit, newdata = tibble(x = testx)) -->
<!--   mean((yhat - fstar)^2) -->
<!-- } -->
<!-- Ks <- 1:15 * 10 -->
<!-- SplineErr <- map_dbl(Ks, ~ spline_test_err(.x)) -->

<!-- Jgrid <- c(5, 10, 15) -->
<!-- NNerr <- double(length(Jgrid)^3) -->
<!-- NNplot <- character(length(Jgrid)^3) -->
<!-- sweep <- 0 -->
<!-- for (J1 in Jgrid) { -->
<!--   for (J2 in Jgrid) { -->
<!--     for (J3 in Jgrid) { -->
<!--       sweep <- sweep + 1 -->
<!--       NNplot[sweep] <- paste(J1, J2, J3, sep = " ") -->
<!--       nn_out <- neuralnet(y ~ x, df, -->
<!--         hidden = c(J1, J2, J3), -->
<!--         threshold = 0.01, rep = 3 -->
<!--       ) -->
<!--       nn_results <- sapply(1:3, function(x) { -->
<!--         compute(nn_out, testx, x)$net.result -->
<!--       }) -->
<!--       # Run them through the neural network -->
<!--       Yhat <- rowMeans(nn_results) -->
<!--       NNerr[sweep] <- mean((Yhat - fstar)^2) -->
<!--     } -->
<!--   } -->
<!-- } -->

<!-- bestK <- Ks[which.min(SplineErr)] -->
<!-- bestspline <- predict(lm(y ~ bs(x, bestK), data = df), newdata = tibble(x = testx)) -->
<!-- besthidden <- as.numeric(unlist(strsplit(NNplot[which.min(NNerr)], " "))) -->
<!-- nn_out <- neuralnet(y ~ x, df, hidden = besthidden, threshold = 0.01, rep = 3) -->
<!-- nn_results <- sapply(1:3, function(x) compute(nn_out, testdata, x)$net.result) -->
<!-- # Run them through the neural network -->
<!-- bestnn <- rowMeans(nn_results) -->
<!-- plotd <- data.frame( -->
<!--   x = testdata, spline = bestspline, nnet = bestnn, truth = fstar -->
<!-- ) -->
<!-- save.image(file = "data/nnet-example.Rdata") -->
<!-- ``` -->

<!-- ```{r fun-nnet-spline, echo=FALSE, fig.align='center', fig.width=10, fig.height=4} -->
<!-- load("data/nnet-example.Rdata") -->
<!-- plotd |> -->
<!--   pivot_longer(-x) |> -->
<!--   ggplot(aes(x, value, color = name)) + -->
<!--   geom_line(linewidth = 1.5) + -->
<!--   ylab("y") + -->
<!--   scale_color_manual(values = c(red, orange, blue)) + -->
<!--   theme(legend.title = element_blank()) + -->
<!--   geom_point( -->
<!--     data = df, mapping = aes(x, y), -->
<!--     color = "black", alpha = .4, shape = 16 -->
<!--   ) -->
<!-- ``` -->


<!-- ## Different architectures -->

<!-- ```{r nnet-vs-spline-plots, echo=FALSE, fig.align='center',fig.height=6,fig.width=12} -->
<!-- library(cowplot) -->
<!-- doppler_nnet <- data.frame(x = NNplot, err = NNerr) -->
<!-- spl <- data.frame(x = Ks, err = SplineErr) -->
<!-- best <- c(min(NNerr), min(SplineErr)) -->
<!-- rel <- function(x) abs(x) / .01 -->
<!-- g1 <- ggplot(doppler_nnet, aes(x, rel(err), group = 1)) + -->
<!--   ggtitle("Neural Nets") + -->
<!--   xlab("architecture") + -->
<!--   theme(axis.text.x = element_text(angle = 90, vjust = .5)) + -->
<!--   geom_line(color = orange, linewidth = 1.5) + -->
<!--   ylab("Increase in error over f*") + -->
<!--   scale_y_continuous(labels = scales::percent_format()) + -->
<!--   geom_hline(yintercept = rel(best[1]), color = red, linewidth = 1.5) + -->
<!--   geom_hline(yintercept = rel(best[2]), color = green, linewidth = 1.5) -->
<!-- g2 <- ggplot(spl, aes(x, rel(err))) + -->
<!--   ggtitle("Splines") + -->
<!--   xlab("degrees of freedom") + -->
<!--   geom_line(color = orange, linewidth = 1.5) + -->
<!--   ylab("Increase in error over f*") + -->
<!--   scale_y_continuous(labels = scales::percent_format()) + -->
<!--   geom_hline(yintercept = rel(best[1]), color = red, linewidth = 1.5) + -->
<!--   geom_hline(yintercept = rel(best[2]), color = green, linewidth = 1.5) -->
<!-- plot_grid(g1, g2) -->
<!-- ``` -->

## NN training: zooming back out

* NN parameters solve the optimization problem\
  $$ \argmin_{\boldsymbol W^{(t)}, \boldsymbol \beta} \sum_{i=1}^n \ell( \hat f_\mathrm{NN}(\boldsymbol x_i), y_i) $$
  
* We solve the optimization problem with *stochastic* gradient descent\
  [ $$
  \begin{align}
    \boldsymbol W^{(t)} \leftarrow \boldsymbol W^{(t)} - \gamma \: \frac{n}{\left| \mathcal M \right|} \sum_{i \in \mathcal M} \frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}}, \qquad
    \beta \leftarrow \beta - \gamma \: \frac{n}{\left| \mathcal M \right|} \sum_{i \in \mathcal M} \frac{\partial \ell_i}{\partial \boldsymbol \beta}
  \end{align}
  $$ ]{.small}
  
* We compute the gradients $\frac{\partial \ell_i}{\partial \boldsymbol \beta}$ and $\frac{\partial \ell_i}{\partial \boldsymbol W^{(t)}}$ using the chain rule (i.e. backpropagation)

  * Procedure (intuitivly): reformulate $\hat f_\mathrm{NN}(\boldsymbol x_i)$ as a computation graph, and then "reverse" the computation graph.
  
  * Modern NN libraries perform this automatically.
  
  
# Challenges with NN training
(and why they're not actually challenges)

## Challenges with NN training

### Major Challenges

1. Computing $\sum_{i=1}^n \partial \ell_i / \partial \boldsymbol W^{(t)}$ requires *a lot* of computation!

1. $\ell_i$ is a non-convex function of $\boldsymbol W^{(t)}$ and $\boldsymbol \beta$!\
   [Extra credit: convince yourself that this is true for any neural network with any activation function.]

   * Gradient descent might get stuck in sub-optimal local minima!
   
   * Gradient descent might be unstable / sensitive to initial conditions.

### Minor Challenges

* How do we choose the step size of gradient descent?


## Solution: Stochastic Gradient Descent(?!?!)*
[*It doesn't solve the problem of choosing the step size. That remains a dark art.]{.small}

1. We have already seen how [SGD]{.secondary} can reduce the computational burden of gradient descent by approximating $\sum_{i=1}^n \partial \ell_i / \partial \boldsymbol W^{(t)}$ with the *randomized* partial sum $\frac{n}{|\mathcal M|} \sum_{i \in \mathcal M} \partial \ell_i / \partial \boldsymbol W^{(t)}$

2. I claim that SGD also helps (but doesn't guarantee) convergence to "good" minima.

   * [I can't theoretically prove this to be true. No one can. (Though some statisticians have made progress.)]{.small}
   
   * [Empirically, SGD-trained NN outperforms non-stochasticly trained NN.]{.small}
   
   * [I can offer some hand-wavy intuition...]{.small}
  

## Intuition: why SGD helps optimization

[The stochastisity (noise) in SGD turns out to be a *feature, not a bug*.]{.secondary}\
Consider the loss as a function of $w^{(i)}_{ij}$

::: flex
::: w-65
* Imagine the loss (as a function of $w^{(i)}_{ij}$) has a "narrow" minimum and a "wide" minimum

* The steps made by SGD are too imprecise to converge to the "narrow" minimum.\
  [A more precise optimization method, like non-stochastic gradient descent, would have no problem converging to these.]{.small}
  
* The "wider" minima that SGD converges to usually yield better predictors.\
  [Intuitively, a "wide" minimum means that the loss is robust to small perturbations in $w^{(i)}_{ij}$. A robust set of parameters is likely less dependent on any one training set.]{.small}
:::

::: w-30
![](gfx/sgd_vs_gd_toy_1d.png){fig-align="center" fig-caption="Toy depiction of SGD vs GD converging to 'narrow' vs 'wide' optima."}

![](gfx/sgd_vs_gd_toy_2d.png){fig-align="center" fig-caption="Toy depiction of SGD vs GD converging to 'narrow' vs 'wide' optima."}
:::
:::
  
<!-- ## Estimation procedures (training) -->


<!-- Back-propagation -->

<!-- [Advantages:]{.secondary} -->

<!-- -   It's updates only depend on local -->
<!--     information in the sense that if objects in the hierarchical model -->
<!--     are unrelated to each other, the updates aren't affected -->

<!--     (This helps in many ways, most notably in parallel architectures) -->

<!-- -   It doesn't require second-derivative information -->

<!-- -   As the updates are only in terms of $\hat{R}_i$, the algorithm can -->
<!--     be run in either batch  or online  mode -->

<!-- [Down sides:]{.tertiary} -->

<!-- -   It can be very slow -->

<!-- -   Need to choose the learning rate -->
<!--     $\gamma_t$ -->

## Other Optimization Tricks

* Don't initialize the neural network parameters to be $0$.\
  [In general, initialize the parameters to be i.i.d. Gaussian with a reasonable stddev. Deep learning libraries do this automatically.]{.small}
  
* Use [batch normalization]()

* Scale input features to be zero mean and unit variance

* Use small minibatches.\
  [Small minibatches = noisier (higher variance) gradients = convergence to "better" minima]{.secondary}
  
* Reduce step size during optimization.

* Use a slightly fancier optimization algorithm than SGD:
   * SGD with momentum
   * Adam (SGD with adaptively tuned learning rates)




# Next time...

Why do NN work so well?
