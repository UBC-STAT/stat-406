---
lecture: "17 Nonlinear classifiers"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}

## Two lectures ago


We discussed logistic regression

$$\begin{aligned}
Pr(Y = 1 \given X=x)  & = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}} \\
Pr(Y = 0 \given X=x) & = \frac{1}{1 + \exp\{\beta_0 + \beta^{\top}x\}}=1-\frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}\end{aligned}$$

## Make it nonlinear

We can make logistic regression have non-linear decision boundaries by mapping the features to a higher dimension (just like with linear regression)

Say:

__Polynomials__

$(x_1, x_2) \mapsto \left(1,\ x_1,\ x_1^2,\ x_2,\ x_2^2,\ x_1 x_2\right)$

```{r simple-lda, echo=FALSE}
library(mvtnorm)
library(MASS)
generate_lda_2d <- function(
    n, p = c(.5, .5),
    mu = matrix(c(0, 0, 1, 1), 2),
    Sigma = diag(2)) {
  X <- rmvnorm(n, sigma = Sigma)
  tibble(
    y = which(rmultinom(n, 1, p) == 1, TRUE)[, 1],
    x1 = X[, 1] + mu[1, y],
    x2 = X[, 2] + mu[2, y]
  )
}
```

```{r}
dat1 <- generate_lda_2d(100, Sigma = .5 * diag(2)) |> mutate(y = as.factor(y))
logit_poly <- glm(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1, family = "binomial")
```



## Visualizing the classification boundary

```{r plot-d1}
#| code-fold: true
#| fig-width: 5
#| fig-height: 5
library(cowplot)
gr <- expand_grid(x1 = seq(-2.5, 3, length.out = 100), x2 = seq(-2.5, 3, length.out = 100))
pts_logit <- predict(logit_poly, gr)
g0 <- ggplot(dat1, aes(x1, x2)) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = tibble(gr, disc = pts_logit), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_viridis_b(n.breaks = 6, alpha = .5, name = "log odds") +
  ggtitle("Polynomial logit") +
  theme(legend.position = "bottom", legend.key.width = unit(1.5, "cm"))
plot_grid(g0)
```

A linear decision boundary in the higher-dimensional space corresponds to a non-linear decision boundary in low dimensions.


## KNN classifiers

<<<<<<< HEAD
=======
* We saw $k$-nearest neighbors in the last module.

```{r}
library(class)
knn3 <- knn(dat1[, -1], gr, dat1$y, k = 3)
```

```{r}
#| code-fold: true
#| fig-width: 8
#| fig-height: 4
gr$nn03 <- knn3
ggplot(dat1, aes(x1, x2)) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = tibble(gr, disc = knn3), aes(x1, x2, fill = disc), alpha = .5) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_manual(values = c(orange, blue), labels = c("0", "1")) +
  theme(
    legend.position = "bottom", legend.title = element_blank(),
    legend.key.width = unit(2, "cm")
  )
```


## Choosing $k$ is very important


```{r}
#| code-fold: true
#| fig-width: 16
#| fig-height: 5
set.seed(406406406)
ks <- c(1, 2, 5, 10, 20)
nn <- map(ks, ~ as_tibble(knn(dat1[, -1], gr[, 1:2], dat1$y, .x)) |> 
  set_names(sprintf("k = %02s", .x))) |>
  list_cbind() |>
  bind_cols(gr)
pg <- pivot_longer(nn, starts_with("k ="), names_to = "k", values_to = "knn")

ggplot(pg, aes(x1, x2)) +
  geom_raster(aes(fill = knn), alpha = .6) +
  facet_wrap(~ k) +
  scale_fill_manual(values = c(orange, green), labels = c("0", "1")) +
  geom_point(data = dat1, mapping = aes(x1, x2, shape = as.factor(y)), size = 4) +
  theme_bw(base_size = 18) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  theme(
    legend.title = element_blank(),
    legend.key.height = unit(3, "cm")
  )
```

* How should we choose $k$?

* Scaling is also very important. "Nearness" is determined by distance, so better to standardize your data first.

* If there are ties, break randomly. So even $k$ is strange.


## `knn.cv()` (leave one out)

```{r}
kmax <- 20
err <- map_dbl(1:kmax, ~ mean(knn.cv(dat1[, -1], dat1$y, k = .x) != dat1$y))
```

```{r}
#| echo: false
ggplot(data.frame(k = 1:kmax, error = err), aes(k, error)) +
  geom_point(color = orange) +
  geom_line(color = orange)
```

I would use the _largest_ (odd) `k` that is close to the minimum.  
This produces simpler, smoother, decision boundaries.



## Final version


::: flex
::: w-50

```{r}
#| code-fold: true
#| fig-height: 6
#| fig-width: 6
kopt <- max(which(err == min(err)))
kopt <- kopt + 1 * (kopt %% 2 == 0)
gr$opt <- knn(dat1[, -1], gr[, 1:2], dat1$y, k = kopt)
tt <- table(knn(dat1[, -1], dat1[, -1], dat1$y, k = kopt), dat1$y, dnn = c("predicted", "truth"))
ggplot(dat1, aes(x1, x2)) +
  theme_bw(base_size = 24) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = gr, aes(x1, x2, fill = opt), alpha = .6) +
  geom_point(aes(shape = y), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_manual(values = c(orange, green), labels = c("0", "1")) +
  theme(
    legend.position = "bottom", legend.title = element_blank(),
    legend.key.width = unit(2, "cm")
  )
```

:::

::: w-50

* Best $k$: `r kopt`

* Misclassification error: `r 1-sum(diag(tt))/sum(tt)`

* Confusion matrix:

```{r echo=FALSE}
tt
```

:::
:::



>>>>>>> 1c36b39 (Update last of classification slides)
## Trees

::: flex

::: w-50
We saw regression trees last module

Classification trees are 

- More natural
- Slightly different computationally

Everything else is pretty much the same
:::

::: w-50
![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg)
:::
:::



## Axis-parallel splits

Like with regression trees, classification trees operate by greedily splitting the predictor space

```{r bake-it, echo=FALSE}
data("bakeoff_train", package = "Stat406")
bakeoff <- bakeoff_train[complete.cases(bakeoff_train), ]
library(tree)
library(maptree)
```

::: flex
::: w-50
```{r glimpse-bakers, R.options = list(width = 50)}
names(bakeoff)
```

```{r our-partition}
smalltree <- tree(
  winners ~ technical_median + percent_star,
  data = bakeoff
)
```

:::


::: w-50

```{r plot-partition}
#| code-fold: true
#| fig-width: 6
#| fig-height: 6
par(mar = c(5, 5, 0, 0) + .1)
plot(bakeoff$technical_median, bakeoff$percent_star,
  pch = c("-", "+")[bakeoff$winners + 1], cex = 2, bty = "n", las = 1,
  ylab = "% star baker", xlab = "times above median in technical",
  col = orange, cex.axis = 2, cex.lab = 2
)
partition.tree(smalltree,
  add = TRUE, col = blue,
  ordvars = c("technical_median", "percent_star")
)
```
:::
:::


## When do trees do well?

::: flex
::: w-50
![](gfx/8.7.png)
:::

::: w-50

[2D example]{.hand}

[Top Row:]{.primary} 

true decision boundary is linear

🍎 linear classifier 

👎 tree with axis-parallel splits

[Bottom Row:]{.primary}

true decision boundary is non-linear

🤮 A linear classifier can't capture the true decision boundary

🍎 decision tree is successful.
:::
:::




## How do we build a tree?


1. Divide the predictor space into
$J$ non-overlapping regions $R_1, \ldots, R_J$ 

  > this is done via greedy, recursive binary splitting

2. Every observation that falls into a given region $R_j$ is given the same prediction

  > determined by majority (or plurality) vote in that region.



[Important:]{.hand}

* Trees can only make rectangular regions that are aligned with the coordinate axis.

* We use a *greedy* (not optimal) algorithm to fit the tree


## Flashback: Constructing Trees for Regression

* While ($\mathtt{depth} \ne \mathtt{max.depth}$):
    * For each existing region $R_k$
        * For a given *splitting variable* $j$ and *split value* $s$,
          define
          $$
          \begin{align}
          R_k^> &= \{x \in R_k : x^{(j)} > s\} \\
          R_k^< &= \{x \in R_k : x^{(j)} > s\}
          \end{align}
          $$
        * Choose $j$ and $s$ 
          to *maximize quality of fit*; i.e.
          $$\min |R_k^>| \cdot \widehat{Var}(R_k^>) + |R_k^<| \cdot  \widehat{Var}(R_k^<)$$

. . .

[We have to change this last line for classification]{.secondary}





## How do we measure quality of fit?


Let $p_{mk}$ be the proportion of training observations in the $m^{th}$
region that are from the $k^{th}$ class.

| |  |
|---|---|
| __classification error rate:__ | $E = 1 - \max_k (\widehat{p}_{mk})$|
| __Gini index:__   | $G = \sum_k \widehat{p}_{mk}(1-\widehat{p}_{mk})$ |
| __cross-entropy:__ | $D = -\sum_k \widehat{p}_{mk}\log(\widehat{p}_{mk})$|


Both Gini and cross-entropy measure the purity of the classifier (small if all $p_{mk}$ are near zero or 1).  

Classification error is hard to optimize.

We build a classifier by growing a tree that minimizes $G$ or $D$.


<!--
## Pruning the tree


* Cross-validation can be used to directly prune the tree, 

* But it is computationally expensive (combinatorial complexity).

* Instead, we use _weakest link pruning_, (Gini version)

$$\sum_{m=1}^{|T|} \sum_{k \in R_m} \widehat{p}_{mk}(1-\widehat{p}_{mk}) + \alpha |T|$$

* $|T|$ is the number of terminal nodes.  

* Essentially, we are trading training fit (first term) with model complexity (second) term (compare to lasso).

* Now, cross-validation can be used to pick $\alpha$.
-->



## Advantages and disadvantages of trees (again)

🎉 Trees are very easy to explain (much easier than even linear regression).  

🎉 Some people believe that decision trees mirror human decision.  

🎉 Trees can easily be displayed graphically no matter the dimension of the data.

🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.

💩 Trees aren't very good at prediction.

💩 Trees are highly variable. Small changes in training data $\Longrightarrow$ big changes in the tree.

To fix these last two, we can try to grow many trees and average their performance. 

. . .

We do this next module
