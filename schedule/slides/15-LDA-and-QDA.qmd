---
lecture: "15 LDA and QDA"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Last time


We showed that with two classes, the [Bayes' classifier]{.secondary} is

$$g_*(x) = \begin{cases}
1 & \textrm{ if } \frac{p_1(x)}{p_0(x)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

where $p_1(x) = \Pr(X=x \given Y=1)$, $p_0(x) = \Pr(X=x \given Y=0)$ and $\pi = \Pr(Y=1)$

. . .

For more than two classes:

$$g_*(x) = 
\argmax_k \frac{\pi_k p_k(x)}{\sum_k \pi_k p_k(x)}$$

where $p_k(x) = \Pr(X=x \given Y=k)$ and $\pi_k = P(Y=k)$


## Estimating these
 
Let's make some assumptions:

1. $\Pr(X=x\given Y=k) = \mbox{N}(x; \mu_k,\Sigma_k)$
2. $\Sigma_k = \Sigma_{k'} = \Sigma$

This leads to [Linear Discriminant Analysis]{.secondary} (LDA), one of the oldest classifiers

## LDA


1. Split your training data into $K$ subsets based on $y_i=k$.
2. In each subset, estimate the mean of $X$: $\widehat\mu_k = \overline{X}_k$
3. Estimate the pooled variance: $$\widehat\Sigma = \frac{1}{n-K} \sum_{k \in \mathcal{K}} \sum_{i \in k} (x_i - \overline{X}_k) (x_i - \overline{X}_k)^{\top}$$
4. Estimate the class proportion: $\widehat\pi_k = n_k/n$

## LDA

Assume just $K = 2$ so $k \in \{0,\ 1\}$

We predict $\widehat{y} = 1$ if

$$\widehat{p_1}(x) / \widehat{p_0}(x) > \widehat{\pi_0} / \widehat{\pi_1}$$ 

Plug in the density estimates:

$$\widehat{p_k}(x) = N(x - \widehat{\mu}_k,\ \widehat\Sigma)$$


## LDA


Now we take $\log$ and simplify $(K=2)$:

$$
\begin{aligned}
&\Rightarrow \log(\widehat{p_1}(x)\times\widehat{\pi_1}) - \log(\widehat{p_0}(x)\times\widehat{\pi_0})
= \cdots = \cdots\\
&= \underbrace{\left(x^\top\widehat\Sigma^{-1}\overline X_1-\frac{1}{2}\overline X_1^\top \widehat\Sigma^{-1}\overline X_1 + \log \widehat\pi_1\right)}_{\delta_1(x)} -  \underbrace{\left(x^\top\widehat\Sigma^{-1}\overline X_0-\frac{1}{2}\overline X_0^\top \widehat\Sigma^{-1}\overline X_0 + \log \widehat\pi_0\right)}_{\delta_0(x)}\\
&= \delta_1(x) - \delta_0(x)
\end{aligned}
$$


[If $\delta_1(x) > \delta_0(x)$, we set $\widehat g(x)=1$]{.secondary}

## One dimensional intuition

```{r}
set.seed(406406406)
n <- 100
pi <- .6
mu0 <- -1
mu1 <- 2
sigma <- 2
tib <- tibble(
  y = rbinom(n, 1, pi),
  x = rnorm(n, mu0, sigma) * (y == 0) + rnorm(n, mu1, sigma) * (y == 1)
)
```

```{r}
#| code-fold: true
gg <- ggplot(tib, aes(x, y)) +
  geom_point(colour = blue) +
  stat_function(fun = ~ 6 * (1 - pi) * dnorm(.x, mu0, sigma), colour = orange) +
  stat_function(fun = ~ 6 * pi * dnorm(.x, mu1, sigma), colour = orange) +
  annotate("label",
    x = c(-3, 4.5), y = c(.5, 2 / 3),
    label = c("(1-pi)*p[0](x)", "pi*p[1](x)"), parse = TRUE
  )
gg
```



## What is linear?

Look closely at the equation for $\delta_1(x)$:

$$\delta_1(x)=x^\top\widehat\Sigma^{-1}\overline X_1-\frac{1}{2}\overline X_1^\top \widehat\Sigma^{-1}\overline X_1 + \log \widehat\pi_1$$

We can write this as $\delta_1(x) = x^\top a_1 + b_1$ with $a_1 = \widehat\Sigma^{-1}\overline X_1$ and $b_1=-\frac{1}{2}\overline X_1^\top \widehat\Sigma^{-1}\overline X_1 + \log \widehat\pi_1$.

We can do the same for $\delta_0(x)$ (in terms of $a_0$ and $b_0$)

Therefore, 

$$\delta_1(x)-\delta_0(x) = x^\top(a_1-a_0) + (b_1-b_0)$$

This is how we discriminate between the classes.

We just calculate $(a_1 - a_0)$ (a vector in $\R^p$), and $b_1 - b_0$ (a scalar)


## Baby example

::: flex
::: w-50

```{r simple-lda}
library(mvtnorm)
library(MASS)
generate_lda_2d <- function(
    n, p = c(.5, .5), 
    mu = matrix(c(0, 0, 1, 1), 2),
    Sigma = diag(2)) {
  X <- rmvnorm(n, sigma = Sigma)
  tibble(
    y = which(rmultinom(n, 1, p) == 1, TRUE)[,1],
    x1 = X[, 1] + mu[1, y],
    x2 = X[, 2] + mu[2, y]
  )
}
dat1 <- generate_lda_2d(100, Sigma = .5 * diag(2))
lda_fit <- lda(y ~ ., dat1)
```

:::
::: w-50

```{r plot-d1, fig.align='center', fig.width=7, fig.height=7, dev='png', dvi=300, echo=FALSE}
gr <- expand_grid(
  x1 = seq(-2.5, 3, length.out = 100),
  x2 = seq(-2.5, 3, length.out = 100)
)
pts <- predict(lda_fit, gr)
g0 <- ggplot(dat1, aes(x1, x2)) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = tibble(gr, disc = c(pts$x)), aes(x1, x2, fill = disc)) +
  theme_bw(base_size = 24) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_steps2(
    n.breaks = 6,
    name = bquote(delta[1] - delta[0])
  ) +
  theme(legend.position = "bottom", legend.key.width = unit(3, "cm"))
g0
```

:::

:::


## Multiple classes

```{r 3class-lda}
moreclasses <- generate_lda_2d(150, c(.2, .3, .5), matrix(c(0, 0, 1, 1, 1, 0), 2), .5 * diag(2))
separateclasses <- generate_lda_2d(150, c(.2, .3, .5), matrix(c(-1, -1, 2, 2, 2, -1), 2), .1 * diag(2))
```

```{r 3class-plot,echo=FALSE,fig.align='center',fig.height=6,fig.width=12,dev='png',dvi=300}
library(cowplot)
lda_3fit <- lda(y ~ ., moreclasses)
lda_separate <- lda(y ~ ., separateclasses)
pts3 <- predict(lda_3fit, gr)
ptss <- predict(lda_separate, gr)
g1 <- ggplot(moreclasses, aes(x1, x2)) +
  scale_shape_manual(values = levels(pts3$class), guide = "none") +
  geom_raster(data = tibble(gr, disc = pts3$class), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  scale_fill_viridis_d(alpha = .5, name = bquote(hat(g)(x))) +
  theme_bw(base_size = 24) +
  theme(legend.position = "bottom")
g2 <- ggplot(separateclasses, aes(x1, x2)) +
  scale_shape_manual(values = levels(ptss$class), guide = "none") +
  geom_raster(data = tibble(gr, disc = ptss$class), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  theme_bw(base_size = 24) +
  scale_fill_viridis_d(alpha = .5, name = bquote(hat(g)(x))) +
  theme(legend.position = "bottom")
plot_grid(g1, g2)
```



## QDA

Just like LDA, but $\Sigma_k$ is separate for each class.

Produces [Quadratic]{.secondary} decision boundary.

Everything else is the same.

```{r fit-qda}
qda_fit <- qda(y ~ ., dat1)
qda_3fit <- qda(y ~ ., moreclasses)
```

```{r qda-vs-lda-2class,echo=FALSE,fig.align='center',fig.height=5,fig.width=12,dev='png',dvi=300}
pts_qda <- predict(qda_fit, gr)
pts_qda3 <- predict(qda_3fit, gr)
z <- apply(pts_qda$posterior, 1, function(x) log(x[2] / x[1]))
gq0 <- ggplot(dat1, aes(x1, x2)) +
  scale_shape_manual(values = c("0", "1"), guide = "none") +
  geom_raster(data = tibble(gr, disc = z), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  theme_bw(base_size = 24) +
  scale_fill_steps2(n.breaks = 8, name = bquote(delta[1] - delta[0])) +
  theme(legend.position = "bottom", legend.key.width = unit(3, "cm"))
plot_grid(g0, gq0)
```


## 3 class comparison

```{r 3class-comparison,echo=FALSE,fig.align='center',fig.height=6,fig.width=12,dev='png',dvi=300}
gq1 <- ggplot(moreclasses, aes(x1, x2)) +
  scale_shape_manual(values = levels(pts_qda3$class), guide = "none") +
  geom_raster(data = tibble(gr, disc = pts_qda3$class), aes(x1, x2, fill = disc)) +
  geom_point(aes(shape = as.factor(y)), size = 4) +
  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +
  theme_bw(base_size = 24) +
  scale_fill_viridis_d(alpha = .5, name = bquote(hat(g)(x))) +
  theme(legend.position = "bottom")
plot_grid(g1, gq1)
```


## Notes

* LDA is a linear classifier. It is not a linear smoother.
  - It is derived from Bayes rule.
  - Assume each class-conditional density in Gaussian
  - It assumes the classes have different mean vectors, but the same (common) covariance matrix.
  - It estimates densities and probabilities and "plugs in" 

* QDA is not a linear classifier. It depends on quadratic functions of the data.
  - It is derived from Bayes rule.
  - Assume each class-conditional density in Gaussian
  - It assumes the classes have different mean vectors and different covariance matrices.
  - It estimates densities and probabilities and "plugs in" 
  
##

[It is hard (maybe impossible) to come up with reasonable classifiers that are linear smoothers. Many "look" like a linear smoother, but then apply a nonlinear transformation.]{.hand}

## NaÃ¯ve Bayes

Assume that $\Pr(X=x | Y = k) = \Pr(X_1=x_1 | Y = k)\cdots \Pr(X_p=x_p | Y = k)$.

That is, conditional on the class, the feature distribution is independent.

. . .

If we further assume that $\Pr(X_j=x_j | Y = k)$ is Gaussian,

This is the same as QDA but with $\Sigma_k$ Diagonal.

. . .

Don't have to assume Gaussian. Could do lots of stuff. 


# Next time...

Another linear classifier and transformations
