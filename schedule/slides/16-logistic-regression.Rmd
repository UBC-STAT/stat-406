---
title: "16 Logistic regression"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

```{r css-extras, file="css-extras.R", echo=FALSE}
```


## Last time

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$

* We showed that with two classes, the __Bayes' classifier__ is

$$g_*(X) = \begin{cases}
1 & \textrm{ if } \frac{p_1(X)}{p_0(X)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

where $p_1(X) = Pr(X \given Y=1)$ and $p_0(X) = Pr(X \given Y=0)$

* We then looked at what happens if we **assume** $Pr(X \given Y=y)$ is Normally distributed.

We then used this distribution and the class prior $\pi$ to find the __posterior__ $Pr(Y=1 \given X=x)$.

--

Instead, let's directly model the posterior

$$\begin{aligned}
Pr(Y = 1 \given X=x)  & = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}} \\
\P(Y = 0 | X=x) & = \frac{1}{1 + \exp\{\beta_0 + \beta^{\top}x\}}=1-\frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}\end{aligned}$$

This is logistic regression.

---

## Why this?

$$Pr(Y = 1 \given X=x) = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}$$

* There are lots of ways to map $\R \mapsto [0,1]$.

* The "logistic" function $z\mapsto \exp(z) / (1+\exp(z)) =:h(z)$ is nice.

* It's symmetric: $1 - h(z) = h(-z)$

* Has a nice derivative: $h'(z) = \frac{\exp(z)}{(1 + \exp(z))^2} = h(z)(1-h(z))$.

* It's the inverse of the "log-odds": $\log(p / (1-p))$.


---

## Another linear classifier

Like LDA, logistic regression is a linear classifier

The _logit_ (i.e.: log odds) transformation
gives a linear decision boundary
$$\log\left( \frac{\P(Y = 1 \given X=x)}{\P(Y = 0 \given X=x) } \right) = \beta_0 + \beta^{\top} x$$
The decision boundary is the hyperplane
$\{x : \beta_0 + \beta^{\top} x = 0\}$

If the log-odds are below 0, classify as 0, above 0 classify as a 1.

--

Logistic regression is also easy in R

```{r eval=FALSE}
logistic <- glm(y ~ ., dat, family="binomial")
```

Or we can use lasso or ridge regression or a GAM as before

```{r eval=FALSE}
lasso_logit <- cv.glmnet(x, y, family = "binomial")
ridge_logit <- cv.glmnet(x, y, alpha = 0, family = "binomial")
gam_logit <- gam(y~s(x), data = dat, family = "binomial")
```


_Aside: glm means generalized linear model_

---

## Baby example (continued from last time)

```{r simple-lda, echo=FALSE}
library(mvtnorm)
library(MASS)
generate_lda <- function(
  n, p=c(.5,.5), 
  mumat=matrix(c(0,0,1,1),2),
  Sigma=diag(2)){
  X = rmvnorm(n, sigma=Sigma)
  tibble(
    y=apply(rmultinom(n,1,p) > 0, 2, which)-1,
    x1 = X[,1] + mumat[1,y+1],
    x2 = X[,2] + mumat[2,y+1])
}
```

```{r}
dat1 <- generate_lda(100, Sigma = .5*diag(2))
logit <- glm(y~., dat1, family="binomial")
summary(logit)
```

---

## Visualizing the classification boundary

```{r plot-d1, fig.align='center', fig.width=7, fig.height=7, dev='png',dvi=300,echo=FALSE}
gr = expand_grid(x1=seq(-2.5,3,length.out = 100),x2=seq(-2.5,3,length.out=100))
pts = predict(logit, gr)
g0=ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=pts), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_steps2(n.breaks=6, name="log odds") +
  theme_bw(base_size = 24) +
  theme(legend.position = "bottom", legend.key.width=unit(3,"cm"))
g0
```

---

## Calculation

While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA.


Logistic regression for two classes simplifies to a likelihood:

Write $p_i(\beta) = \P(Y_i = 1 | X = x_i,\beta)$

* $P(Y_i = y_i \given X = x_i, \beta) = p_i^{y_i}(1-p_i)^{1-y_i}$ (...Bernoulli distribution)

* $P(\mathbf{Y} \given \mathbf{X}, \beta) = \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$. 
$$\begin{aligned}
\ell(\beta) 
& = \log \left( \prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i} \right)\\
&=\sum_{i=1}^n \left( y_i\log(p_i(\beta)) + (1-y_i)\log(1-p_i(\beta))\right) \\
& = 
\sum_{i=1}^n \left( y_i\log(e^{\beta^{\top}x_i}/(1+e^{\beta^{\top}x_i})) - (1-y_i)\log(1+e^{\beta^{\top}x_i})\right) \\
& = 
\sum_{i=1}^n \left( y_i\beta^{\top}x_i -\log(1 + e^{\beta^{\top} x_i})\right)\end{aligned}$$

This gets optimized via Newton-Raphson updates and iteratively reweighed
least squares.

---

## IRWLS for logistic regression (skip for now)

(This is preparation for Neural Networks.)

```{r}
logit_irwls <- function(y, x, maxit = 100, tol=1e-6){
  p <- ncol(x)
  beta <- double(p) # initialize coefficients 
  beta0 <- 0
  conv <- FALSE # hasn't converged
  iter <- 1 # first iteration
  while (!conv && (iter < maxit)) { # check loops
    iter <- iter + 1 # update first thing (so as not to forget) 
    eta <- beta0 + x %*% beta 
    mu <- exp(eta) / (1 + exp(eta))
    gp <- 1 / (mu * (1 - mu)) # inverse of derivative of logistic
    z <- eta + (y - mu) * gp # effective transformed response
    betaNew <- coef(lm(z ~ x, weights = 1/gp)) # do weighted Least Squares
    conv <- mean((c(beta0, beta) - betaNew)^2) < tol # check if the betas are "moving" 
    beta0 <- betaNew[1] # update betas
    beta <- betaNew[-1]
  }
  return(c(beta0, beta))
}
```

---

## Comparing LDA and Logistic regression

Both decision boundaries are linear in $x$:  

- LDA $\longrightarrow \alpha_0 + \alpha_1^\top x$ 
- Logit $\longrightarrow \beta_0 + \beta_1^\top x$.

But the parameters are estimated differently.

Examine the joint distribution of $(X,\ Y)$ .tiny[(not the posterior)]:  

- LDA: $f(X_i,\ Y_i) = \underbrace{ f(X_i \given Y_i)}_{\textrm{Gaussian}}\underbrace{ f(Y_i)}_{\textrm{Bernoulli}}$
- Logistic Regression: $f(X_i,Y_i) = \underbrace{ f(Y_i\given X_i)}_{\textrm{Logistic}}\underbrace{ f(X_i)}_{\textrm{Ignored}}$
  
* LDA estimates the joint, but Logistic estimates only the conditional (posterior) distribution. .hand[But this is really all we need.]

* So logistic requires fewer assumptions.

* But if the two classes are perfectly separable, logistic crashes (and the MLE is undefined, too many solutions)

* LDA "works" even if the conditional isn't normal, but works very poorly if any X is qualitative

---

## Comparing with QDA


* Recall: this gives a "quadratic" decision boundary (it's a curve).

* If we have $p$ columns in $X$
    - Logistic estimates $p+1$ parameters
    - LDA estimates $2p + p(p+1)/2 + 1$
    - QDA estimates $2p + p(p+1) + 1$
  
* If $p=50$,
    - Logistic: 51
    - LDA: 1376
    - QDA: 2651
  
* QDA doesn't get used much: there are better nonlinear versions with way fewer parameters

* LDA only really depends on $\Sigma^{-1}(\mu_1-\mu_0)$ and $(\mu_1+\mu_0)$, so appropriate algorithms use $<2p$ parameters.

--

__Note again:__ while logistic regression and LDA produce linear decision boundaries, they are **not** linear smoothers

AIC/BIC/Cp work if you use the likelihood correctly and count degrees-of-freedom correctly

Must people use either test set or CV

---
class: middle, center, inverse

# Next time:

Nonlinear classifiers