---
lecture: "27 K-means clustering"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Clustering

So far, we've looked at ways of reducing the dimension.

Either linearly or nonlinearly, 

> The goal is visualization/exploration or possibly for an input to supervised learning.

Now we try to find groups or clusters in our data.

Think of [clustering]{.secondary} as classification without the labels.




## K-means (ideally) {.smaller}


1.  Select a number of clusters $K$.

2.  Let $C_1,\ldots,C_K$ partition $\{1,2,3,\ldots,n\}$ such that
    - All observations belong to some set $C_k$.
    - No observation belongs to more than one set.

3.  Make __within-cluster
    variation__, $W(C_k)$, as small as
    possible. $$\min_{C_1,\ldots,C_K} \sum_{k=1}^K W(C_k).$$

4.  Define $W$ as $$W(C_k) =  \frac{1}{2|C_k|} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2.$$
That is, the average (Euclidean) distance between all cluster
members.

. . .

To work, K-means needs [distance to a center]{.secondary} and [notion of center]{.secondary}


## Why this formula? {background-color="#97D4E9" .smaller}

Let $\overline{x}_k = \frac{1}{|C_k|} \sum_{i\in C_k} x_i$

$$
\begin{aligned}
\sum_{k=1}^K W(C_k) 
&= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2
= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i\neq i' \in C_k} \norm{x_i - x_{i'}}_2^2 \\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i \neq i' \in C_k} \norm{x_i -\overline{x}_k + \overline{x}_k - x_{i'}}_2^2\\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \left[\sum_{i \neq i' \in C_k} \left(\norm{x_i - \overline{x}_k}_2^2 + 
\norm{x_{i'} - \overline{x}_k}_2^2\right) + \sum_{i \neq i' \in C_k} 2 (x_i-\overline{x}_k)^\top(\overline{x}_k - x_{i'})\right]\\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \left[2(|C_k|-1)\sum_{i \in C_k} \norm{x_i - \overline{x}_k}_2^2  + 2\sum_{i \in C_k} \norm{x_i - \overline{x}_k}_2^2 \right]\\
&= \sum_{k=1}^K \sum_{x \in C_k} \norm{x-\overline{x}_k}^2_2
\end{aligned}
$$

If you wanted (equivalently) to minimize $\sum_{k=1}^K \frac{1}{|C_k|} \sum_{x \in C_k} \norm{x-\overline{x}_k}^2_2$, then you'd use $\sum_{k=1}^K \frac{1}{\binom{C_k}{2}} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2$


## K-means (in reality)

This is too computationally challenging ( $K^n$ partions! )
$$\min_{C_1,\ldots,C_K} \sum_{k=1}^K W(C_k).$$
So, we make a greedy approximation:

1.  Randomly assign observations to the $K$ clusters
2.  Iterate the following:
    -   For each cluster, compute the $p$-length
        vector of the means in that cluster.
    -   Assign each observation to the cluster whose centroid is closest
        (in Euclidean distance).

This procedure is guaranteed to decrease $\sum_{k=1}^K W(C_k)$ at each step.


## Best practices

To fit K-means, you need to

1.  Pick $K$ (inherent in the method)

2.  Convince yourself you have found a good solution (due to the
    randomized / greedy algorithm).

For 2., run
K-means many times with different starting points. Pick the solution
that has the smallest value for
$$\sum_{k=1}^K W(C_k)$$


It turns out that [1.]{.secondary} is difficult to do in a
principled way.


## Choosing the Number of Clusters

Why is it important?

-   It might make a big difference (concluding there are $K = 2$ cancer
    sub-types versus $K = 3$).

-   One of the major goals of statistical learning is automatic
    inference. A good way of choosing $K$ is certainly a part of this.


## Withinness and betweenness

$$W(K) = \sum_{k=1}^K W(C_k) = \sum_{k=1}^K \sum_{x \in C_k} \norm{x-\overline{x}_k}^2_2,$$



Within-cluster variation measures how [tightly grouped]{.secondary} the clusters are. 

. . .

It's opposite is [Between-cluster variation]{.secondary}: How spread apart are the clusters?

$$B(K) = \sum_{k=1}^K |C_k| \norm{\overline{x}_k - \overline{x} }_2^2,$$


where $|C_k|$ is the number of points in $C_k$, and $\overline{x}$ is
the grand mean

::: flex
::: w-30
$W$ `r fa("arrow-down",fill=green)` when $K$ `r fa("arrow-up",fill=blue)`
:::
::: w-30
$B$ `r fa("arrow-up",fill=blue)` when $K$ `r fa("arrow-up",fill=blue)`
:::
::: w-30
$B/K$ `r fa("arrow-down",fill=blue)` when $K$ `r fa("arrow-up",fill=blue)`
:::

:::



## CH index  

> Want small $W$, big $B/K$

$$\textrm{CH}(K) = \frac{B(K)/(K-1)}{W(K)/(n-K)}$$ 

To choose $K$, pick some
maximum number of clusters to be considered, $K_{\max} = 20$, for
example


$$\widehat K = \argmax_{K \in \{ 2,\ldots, K_{\max} \}} CH(K).$$


::: callout-note
* CH is undefined for $K = 1$. 
* The divisors $(K-1)$ and $(n-K)$ scale $B$ and $W$ appropriately.
:::



## Dumb example


```{r}
library(mvtnorm)
set.seed(406406406)
X1 <- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))
X2 <- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))
X3 <- rmvnorm(40, c(4, 4))
```

```{r plotting-dumb-clusts, echo=FALSE, fig.align="center", fig.width=10,fig.height=5}
clust_raw <- rbind(X1, X2, X3)
clust <- tibble(
  x1 = clust_raw[, 1], x2 = clust_raw[, 2],
  true = as.factor(rep(1:3, times = c(50, 40, 40)))
)
clust |> ggplot(aes(x = x1, y = x2, color = true)) +
  geom_point(size = 2) +
  scale_colour_manual(values = c(blue, orange, green)) +
  theme(legend.position = "none")
```


## Dumb example

* We would [maximize]{.secondary} CH

```{r}
#| code-fold: true
#| fig-width: 10
#| fig-height: 4
K <- 2:40
N <- nrow(clust_raw)
all_clusters <- map(K, ~ kmeans(clust_raw, .x, nstart = 20))
all_assignments <- map_dfc(all_clusters, "cluster")
names(all_assignments) <- paste0("K = ", K)
summaries <- map_dfr(all_clusters, `[`, c("tot.withinss", "betweenss")) |>
  rename(W = tot.withinss, B = betweenss) |>
  mutate(
    K = K,
    `W / (N - K)` = W / (N - K),
    `B / K` = B / (K - 1), 
    `CH index` = `B / K` / `W / (N - K)`
  )
summaries |>
  pivot_longer(-K) |>
  ggplot(aes(K, value)) +
  geom_line(color = blue, linewidth = 2) +
  ylab("") +
  coord_cartesian(c(1, 20)) +
  facet_wrap(~name, ncol = 3, scales = "free_y")
```


## Dumb example

```{r, echo=FALSE,fig.align='center',fig.width=10,fig.height=6}
small_assignments <- all_assignments[, c(1, 2, 3, 4, 9, 14, 19, 29)]
nums <- as.character(1:30)
bind_cols(small_assignments, clust) |>
  mutate(across(starts_with("K = "), factor)) |>
  pivot_longer(-starts_with("x")) |>
  mutate(value = fct_relevel(
    value, "1", "30", "29", "28", "27", "26", "10",
    "5", "25", "24", "23", "22", "21", "9",
    "3", "20", "19", "18", "17", "16", "8",
    "4", "15", "14", "13", "12", "11", "7",
    "2", "6"
  )) |>
  ggplot(aes(x1, x2, color = value)) +
  geom_point() +
  facet_wrap(~ factor(
    name, 
    levels = c("true", paste0("K = ", c(2:5, 10, 15, 20, 30)))
  )) +
  theme(legend.position = "none") +
  scale_color_viridis_d()
```



## Dumb example

* $K = 3$
```{r, echo = TRUE, message = FALSE}
km <- kmeans(clust_raw, 3, nstart = 20)
names(km)
centers <- as_tibble(km$centers, .name_repair = "unique")
```

```{r, echo=FALSE, fig.align='center',fig.width=10,fig.height=3}
names(centers) <- c("x1", "x2")
bind_cols(clust, est = factor(km$cluster)) |>
  ggplot(aes(x1, x2)) +
  geom_point(aes(color = est), size = 2) +
  geom_point(data = centers, size = 10, pch = "ðŸ˜€") +
  theme(legend.position = "none") +
  scale_color_manual(values = c(blue, orange, green))
```


# Next time...

Hierarchical clustering
