---
lecture: "07 Greedy selection"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}

## Recap

Model Selection means [select a family of distributions for your data]{.secondary}.

Ideally, we'd do this by comparing the $R_n$ for one family with that for
another.

We'd use whichever has smaller $R_n$.

But $R_n$ depends on the truth, so we estimate it with $\widehat{R}$.

Then we use whichever has smaller $\widehat{R}$.

## Example

The truth:
```{r}
dat <- tibble(
  x1 = rnorm(100), 
  x2 = rnorm(100),
  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)
)
```

Model 1: `y ~ x1 + x2`

Model 2: `y ~ x1 + x2 + x1*x2`

Model 3: `y ~ x2 + sin(x1 * x2)`

. . .

(What are the families for each of these?)


## Fit each model and estimate $R_n$

```{r}
forms <- list("y ~ x1 + x2", "y ~ x1 * x2", "y ~ x2 + sin(x1*x2)") |> 
  map(as.formula)
fits <- map(forms, ~ lm(.x, data = dat))
map(fits, ~ tibble(
  R2 = summary(.x)$r.sq,
  training_error = mean(residuals(.x)^2),
  loocv = mean( (residuals(.x) / (1 - hatvalues(.x)))^2 ),
  AIC = AIC(.x),
  BIC = BIC(.x)
)) |> list_rbind()
```

## Model Selection vs. Variable Selection

Model selection is very comprehensive

You choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.

Variable selection is a subset of this. It means 

> choosing which predictors to include in a predictive model

Eliminating a predictor, means removing it from the model.

Some [procedures]{.hand} automatically search predictors, and eliminate some.

We call this variable selection. But the procedure is implicitly selecting a model
as well.

. . .

Making this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them.

## Selecting variables / predictors with linear methods

::: flex
::: w-50

Suppose we have a pile of predictors.

We estimate models with different subsets of predictors and use CV / Cp / AIC 
/ BIC to decide which is preferred.

Sometimes you might have a few plausible subsets. Easy enough to choose with our criterion.

Sometimes you might just have a bunch of predictors, then what do you do?
:::

::: w-50
All subsets
: estimate model based on every possible subset of size $|\mathcal{S}| \leq \min\{n, p\}$, use one with 
lowest risk estimate

Forward selection
: start with $\mathcal{S}=\varnothing$, add predictors greedily

Backward selection
: start with $\mathcal{S}=\{1,\ldots,p\}$, remove greedily

Hybrid
: combine forward and backward smartly
:::
:::

# Note:

[Within each procedure, we're comparing _nested_ models.]{.secondary}


## Costs and benefits


All subsets
: üëç estimates each subset  
üí£ takes $2^p$ model fits when $p<n$. If $p=50$, this is about $10^{15}$ models. 

Forward selection
: üëç computationally feasible  
üí£ ignores some models, correlated predictors means bad performance

Backward selection
: üëç computationally feasible  
üí£ ignores some models, correlated predictors means bad performance  
üí£ doesn't work if $p>n$

Hybrid
: üëç visits more models than forward/backward  
üí£ slower


## Synthetic example

```{r data-setup}
set.seed(123)
n <- 406
df <- tibble( # like data.frame, but columns can be functions of preceding
  x1 = rnorm(n),
  x2 = rnorm(n, mean = 2, sd = 1),
  x3 = rexp(n, rate = 1),
  x4 = x2 + rnorm(n, sd = .1), # correlated with x2
  x5 = x1 + rnorm(n, sd = .1), # correlated with x1
  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)
  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)
  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only
)
```

. . .

$\mathbf{x}_1$ and $\mathbf{x}_2$ are the true predictors

But the rest are correlated with them


## Full model

```{r full-model}
full <- lm(y ~ ., data = df)
summary(full)
```


## True model

```{r true-model}
truth <- lm(y ~ x1 + x2, data = df)
summary(truth)
```


## All subsets

```{r try-them-all}
library(leaps)
trythemall <- regsubsets(y ~ ., data = df)
summary(trythemall)
```


## BIC and Cp

```{r more-all-subsets1}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(trythemall)$bic, 
  Cp = summary(trythemall)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~name, scales = "free_y") + 
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange), 
    guide = "none"
  )
```


## Forward stepwise

```{r step-forward}
stepup <- regsubsets(y ~ ., data = df, method = "forward")
summary(stepup)
```


## BIC and Cp

```{r more-step-forward}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(stepup)$bic,
  Cp = summary(stepup)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange),
    guide = "none"
  )
```


## Backward selection

```{r step-backward}
stepdown <- regsubsets(y ~ ., data = df, method = "backward")
summary(stepdown)
```


## BIC and Cp

```{r more-step-backward}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(stepdown)$bic,
  Cp = summary(stepdown)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange), 
    guide = "none"
  )
```



##

<br><br><br><br>

::: r-stack
[somehow, for this seed, everything is the same]{.secondary .large}
:::

## Randomness and prediction error

All of that was for one data set.

Doesn't say which [procedure]{.secondary} is better [generally]{.secondary}.

If we want to know how they compare [generally]{.secondary}, we should repeat many times
  
  1. Generate training data
  1. Estimate with different algorithms
  1. Predict held-out set data
  1. Examine prediction MSE (on held-out set)
  
. . .

I'm not going to do all subsets, just the truth, forward selection, backward, and the full model

For forward/backward selection, I'll use Cp to choose the final size


#  ‚ò†Ô∏è‚ò†Ô∏è Danger ‚ò†Ô∏è‚ò†Ô∏è

You cannot compare the Cp scores between forward selection and backward selection
to decide which to use.

Why not?



## Code for simulation

... Annoyingly, no predict method for `regsubsets`, so we make one.

```{r predict-regsubsets}
#| code-line-numbers: "|2"
predict.regsubsets <- function(object, newdata, risk_estimate = c("cp", "bic"), ...) {
  risk_estimate <- match.arg(risk_estimate)
  chosen <- coef(object, which.min(summary(object)[[risk_estimate]]))
  predictors <- names(chosen)
  if (object$intercept) predictors <- predictors[-1]
  X <- newdata[, predictors]
  if (object$intercept) X <- cbind2(1, X)
  drop(as.matrix(X) %*% chosen)
}
```

##

```{r replication-exercise}
simulate_and_estimate_them_all <- function(n = 406) {
  N <- 2 * n # generate 2x the amount of data (half train, half test)
  df <- tibble( # generate data
    x1 = rnorm(N), 
    x2 = rnorm(N, mean = 2), 
    x3 = rexp(N),
    x4 = x2 + rnorm(N, sd = .1), 
    x5 = x1 + rnorm(N, sd = .1),
    x6 = x1 - x2 + rnorm(N, sd = .1), 
    x7 = x1 + x3 + rnorm(N, sd = .1),
    y = x1 * 3 + x2 / 3 + rnorm(N, sd = 2.2)
  )
  train <- df[1:n, ] # half the data for training
  test <- df[(n + 1):N, ] # half the data for evaluation
  
  oracle <- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs
  full <- lm(y ~ ., data = train)
  stepup <- regsubsets(y ~ ., data = train, method = "forward")
  stepdown <- regsubsets(y ~ ., data = train, method = "backward")
  
  tibble(
    y = test$y,
    oracle = predict(oracle, newdata = test),
    full = predict(full, newdata = test),
    stepup = predict(stepup, newdata = test),
    stepdown = predict(stepdown, newdata = test),
    truth = drop(as.matrix(test[, c("x1", "x2")]) %*% c(3, 1/3))
  )
}

set.seed(12345)
our_sim <- map(1:50, ~ simulate_and_estimate_them_all(406)) |>
  list_rbind(names_to = "sim")
```

## What is "Oracle"

::: flex
::: w-70
<a title="Helen Simonsson, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Delfi_Apollons_tempel.jpg"><img width="800" alt="Delfi Apollons tempel" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Delfi_Apollons_tempel.jpg/512px-Delfi_Apollons_tempel.jpg"></a>
:::

::: w-30
![](https://www.worldhistory.org/img/r/p/750x750/186.jpg.webp?v=1628028003)
:::
:::


## Results


```{r synth-results}
#| output-location: column
#| fig-height: 6
#| fig-width: 6
our_sim |> 
  group_by(sim) %>%
  summarise(
    across(oracle:truth, ~ mean((y - .)^2)), 
    .groups = "drop"
  ) %>%
  transmute(across(oracle:stepdown, ~ . / truth - 1)) |> 
  pivot_longer(
    everything(), 
    names_to = "method", 
    values_to = "mse"
  ) |> 
  ggplot(aes(method, mse, fill = method)) +
  geom_boxplot(notch = TRUE) +
  geom_hline(yintercept = 0, linewidth = 2) +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  scale_y_continuous(
    labels = scales::label_percent()
  ) +
  ylab("% increase in mse relative\n to the truth")
```






# Next time...

[Module 2]{.large}

[regularization, constraints, and nonparametrics]{.secondary}
