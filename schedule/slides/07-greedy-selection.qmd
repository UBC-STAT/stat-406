---
lecture: "07 practical model/variable selection"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}

## Model Selection

Model Selection means [select the best distributions to describe your data]{.secondary}.\
[(I.e. the model with the smallest risk $R_n$.)]{.small}

. . .

### The procedure

1. Generate a list of (comparable) candidate models ($\mathcal M = \{ \mathcal P_1, \mathcal P_2, \ldots \}$)
2. Choose a procedure for estimating risk (e.g. $C_p$)
3. Train each model and estimate its risk
4. Choose the model with the lowest risk (e.g. $\argmin_{\mathcal P \in \mathcal M} C_p(\mathcal P)$)


## Example

The truth:
```{r}
dat <- tibble(
  x1 = rnorm(100), 
  x2 = rnorm(100),
  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)
)
```

Model 1: `y ~ x1 + x2`

Model 2: `y ~ x1 + x2 + x1*x2`

Model 3: `y ~ x2 + sin(x1 * x2)`

. . .

[The models above are written in short hand. In full statistical glory...]{.secondary}

$$
\text{Model 1} = \left\{ Y|X  \sim \mathcal N\left( \:( \beta_0 + \beta_1 X^{(1)} + \beta_2X^{(2)}), \: \sigma^2 \right) \quad \text{for some } \beta_0, \beta_1, \beta_2, \sigma \right\}
$$


## Fit each model and estimate $R_n$

```{r}
forms <- list("y ~ x1 + x2", "y ~ x1 * x2", "y ~ x2 + sin(x1*x2)") |> 
  map(as.formula)
fits <- map(forms, ~ lm(.x, data = dat))
map(fits, ~ tibble(
  R2 = summary(.x)$r.sq,
  training_error = mean(residuals(.x)^2),
  loocv = mean( (residuals(.x) / (1 - hatvalues(.x)))^2 ),
  AIC = AIC(.x),
  BIC = BIC(.x)
)) |> list_rbind()
```

## Model Selection vs. Variable Selection

Variable selection is a subset of model selection.

> Assume we have 2 predictors (`x1`, `x2`) and we're trying to choose which to include in a linear regressor:
>
> Model 1: `y ~ x1` [ (i.e. $\left\{ Y|X  \sim \mathcal N\left( \:( \beta_0 + \beta_1 X^{(1)} ), \: \sigma^2 \right) \right\}$) ]{.small} \
> Model 2: `y ~ x2` [ (i.e. $\left\{ Y|X  \sim \mathcal N\left( \:( \beta_0 + \beta_1X^{(2)}), \: \sigma^2 \right)  \right\}$) ]{.small} \
> Model 3: `y ~ x1 + x2` [ (i.e. $\left\{ Y|X  \sim \mathcal N\left( \:( \beta_0 + \beta_1 X^{(1)} + \beta_2X^{(2)}), \: \sigma^2 \right)  \right\}$) ]{.small}

*Choosing which predictors to include is implicitly selecting a model.*

. . .

::: callout-note
Note that $\mathrm{Model 1}, \mathrm{Model 2} \subset \mathrm{Model 3}$ \
We say that these models are **nested**.
:::

## Selecting variables / predictors with linear methods

::: flex
::: w-40

Suppose we have a set of predictors.

We estimate models with different subsets of predictors and use CV / Cp / AIC 
/ BIC to decide which is preferred.

How do we choose which variable subsets to consider?
:::

::: w-60
All subsets
: estimate model based on every possible subset of size $|\mathcal{S}| \leq \min\{n, p\}$, use one with 
lowest risk estimate

Forward selection
: start with $\mathcal{S}=\varnothing$, add predictors greedily

Backward selection
: start with $\mathcal{S}=\{1,\ldots,p\}$, remove greedily

Hybrid
: combine forward and backward smartly
:::
:::

. . .

::: callout-caution
Within each procedure, we're comparing _nested_ models.
This is important.
:::


## Costs and benefits


All subsets
: üëç estimates each subset  
üí£ takes $2^p$ model fits when $p<n$. If $p=50$, this is about $10^{15}$ models. 

. . .

Forward selection
: üëç computationally feasible  
üí£ ignores some models, correlated predictors means bad performance

Backward selection
: üëç computationally feasible  
üí£ ignores some models, correlated predictors means bad performance  
üí£ doesn't work if $p>n$

. . .

Hybrid
: üëç visits more models than forward/backward  
üí£ slower


## Synthetic example

```{r data-setup}
set.seed(123)
n <- 406
df <- tibble( # like data.frame, but columns can be functions of preceding
  x1 = rnorm(n),
  x2 = rnorm(n, mean = 2, sd = 1),
  x3 = rexp(n, rate = 1),
  x4 = x2 + rnorm(n, sd = .1), # correlated with x2
  x5 = x1 + rnorm(n, sd = .1), # correlated with x1
  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)
  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)
  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only
)
```

. . .

$\mathbf{x}_1$ and $\mathbf{x}_2$ are the true predictors

But the rest are correlated with them


## Full model

```{r full-model}
full <- lm(y ~ ., data = df)
summary(full)
```


## True model

```{r true-model}
truth <- lm(y ~ x1 + x2, data = df)
summary(truth)
```


## All subsets

```{r try-them-all}
library(leaps)
trythemall <- regsubsets(y ~ ., data = df)
summary(trythemall)
```


## BIC and Cp

```{r more-all-subsets1}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(trythemall)$bic, 
  Cp = summary(trythemall)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~name, scales = "free_y") + 
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange), 
    guide = "none"
  )
```


## Forward stepwise

```{r step-forward}
stepup <- regsubsets(y ~ ., data = df, method = "forward")
summary(stepup)
```


## BIC and Cp

```{r more-step-forward}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(stepup)$bic,
  Cp = summary(stepup)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange),
    guide = "none"
  )
```


## Backward selection

```{r step-backward}
stepdown <- regsubsets(y ~ ., data = df, method = "backward")
summary(stepdown)
```


## BIC and Cp

```{r more-step-backward}
#| output-location: column
#| fig-height: 6
#| fig-width: 8
tibble(
  BIC = summary(stepdown)$bic,
  Cp = summary(stepdown)$cp,
  size = 1:7
) |>
  pivot_longer(-size) |>
  ggplot(aes(size, value, colour = name)) +
  geom_point() +
  geom_line() +
  facet_wrap(~name, scales = "free_y") +
  ylab("") +
  scale_colour_manual(
    values = c(blue, orange), 
    guide = "none"
  )
```



##

<br><br><br><br>

::: r-stack
[for this dataset, everything is the same]{.secondary .large}
:::

## What algorithm should you use in practice?

Each algorithm (forward selection, backward selection, etc.) produces a series of models. \
For a given algorithm, we know how to choose amongst the models in a principled way (model selection!)

[How do we choose which algorithm to use?]{.secondary}

. . .

### As a practicioner

Determine how big your computational budget is, make an educated guess.

### As a researcher

We can systematically compare the different algorithms by simulating multiple data sets
and comparing the prediction error of the models produced by each algorithm.


## Comparing algorithms through simulation

::: callout-note
- For each algorithm (forward selection, backward selection, all subsets), do:
   - For $i \in [1, 100]$, do
        1. Generate a samples of training data
        1. Selected a model (e.g. based on $C_p$) generated by the algorithm
        1. Make predictions on held-out set data
        1. Examine prediction MSE (on held-out set)

Compare the average MSE (across the 100 simulations) for each algorithm.
:::

. . .

Why are we using held-out MSE to compare [forward selection vs. backward selection vs. all subsets]{.secondary}?
Why not use $C_p$ of the selected model?




## Code for simulation

... Annoyingly, no predict method for `regsubsets`, so we make one.

```{r predict-regsubsets}
#| code-line-numbers: "|2"
predict.regsubsets <- function(object, newdata, risk_estimate = c("cp", "bic"), ...) {
  risk_estimate <- match.arg(risk_estimate)
  chosen <- coef(object, which.min(summary(object)[[risk_estimate]]))
  predictors <- names(chosen)
  if (object$intercept) predictors <- predictors[-1]
  X <- newdata[, predictors]
  if (object$intercept) X <- cbind2(1, X)
  drop(as.matrix(X) %*% chosen)
}
```

##

```{r replication-exercise}
simulate_and_estimate_them_all <- function(n = 406) {
  N <- 2 * n # generate 2x the amount of data (half train, half test)
  df <- tibble( # generate data
    x1 = rnorm(N), 
    x2 = rnorm(N, mean = 2), 
    x3 = rexp(N),
    x4 = x2 + rnorm(N, sd = .1), 
    x5 = x1 + rnorm(N, sd = .1),
    x6 = x1 - x2 + rnorm(N, sd = .1), 
    x7 = x1 + x3 + rnorm(N, sd = .1),
    y = x1 * 3 + x2 / 3 + rnorm(N, sd = 2.2)
  )
  train <- df[1:n, ] # half the data for training
  test <- df[(n + 1):N, ] # half the data for evaluation
  
  oracle <- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs
  full <- lm(y ~ ., data = train)
  stepup <- regsubsets(y ~ ., data = train, method = "forward")
  stepdown <- regsubsets(y ~ ., data = train, method = "backward")
  
  tibble(
    y = test$y,
    oracle = predict(oracle, newdata = test),
    full = predict(full, newdata = test),
    stepup = predict(stepup, newdata = test),
    stepdown = predict(stepdown, newdata = test),
    truth = drop(as.matrix(test[, c("x1", "x2")]) %*% c(3, 1/3))
  )
}

set.seed(12345)
our_sim <- map(1:50, ~ simulate_and_estimate_them_all(406)) |>
  list_rbind(names_to = "sim")
```

<!--
## What is "Oracle"

::: flex
::: w-70
<a title="Helen Simonsson, CC BY-SA 3.0 &lt;https://creativecommons.org/licenses/by-sa/3.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:Delfi_Apollons_tempel.jpg"><img width="800" alt="Delfi Apollons tempel" src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/7b/Delfi_Apollons_tempel.jpg/512px-Delfi_Apollons_tempel.jpg"></a>
:::

::: w-30
![](https://www.worldhistory.org/img/r/p/750x750/186.jpg.webp?v=1628028003)
:::
:::
-->

## Results


```{r synth-results}
#| output-location: column
#| fig-height: 6
#| fig-width: 6
our_sim |> 
  group_by(sim) %>%
  summarise(
    across(oracle:truth, ~ mean((y - .)^2)), 
    .groups = "drop"
  ) %>%
  transmute(across(oracle:stepdown, ~ . / truth - 1)) |> 
  pivot_longer(
    everything(), 
    names_to = "method", 
    values_to = "mse"
  ) |> 
  ggplot(aes(method, mse, fill = method)) +
  geom_boxplot(notch = TRUE) +
  geom_hline(yintercept = 0, linewidth = 2) +
  scale_fill_viridis_d() +
  theme(legend.position = "none") +
  scale_y_continuous(
    labels = scales::label_percent()
  ) +
  ylab("% increase in mse relative\n to the truth")
```






# Next time...

[Module 2]{.large}

[regularization, constraints, and nonparametrics]{.secondary}
