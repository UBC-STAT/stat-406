---
lecture: "12 To(o) smooth or not to(o) smooth?"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## Smooting vs Linear Models

We've been discussing nonlinear methods in 1-dimension:

$$\Expect{Y\given X=x} = f(x),\quad x\in\R$$

1. Basis expansions, e.g.:

$$\hat f_\mathrm{basis}(x) = \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_k x^k$$ 

2. Local methods, e.g.:

$$\hat f_\mathrm{local}(x_i) = s_i^\top \y$$

Which should we choose? \
[Of course, we can do model selection. But can we analyze the risk mathematically?]{.small}


## Risk Decomposition

$$
R_n = \mathrm{Bias}^2 + \mathrm{Var} + \sigma^2
$$

How does $R_n^{(\mathrm{basis})}$ compare to $R_n^{(\mathrm{local})}$ as we change $n$?\

::: fragment
### Variance

- Basis: variance decreases as $n$ increases
- Local: variance decreases as $n$ increases\
  [But at what rate?]{.small}

:::

::: fragment
### Bias

- Basis: bias is *fixed*\
  [Assuming num. basis features is fixed]{.small}
- Local: bias depends on choice of bandwidth $\sigma$. 

:::


## Risk Decomposition


::: flex

::: w-60
### Basis

$$
R_n^{(\mathrm{basis})} =
  \underbrace{C_1^{(\mathrm{basis})}}_{\mathrm{bias}^2} +
  \underbrace{\frac{C_2^{(\mathrm{basis})}}{n}}_{\mathrm{var}} +
  \sigma^2
$$

### Local

*With the optimal bandwidth* ($\propto n^{-1/5}$), we have

$$
R_n^{(\mathrm{local})} =
  \underbrace{\frac{C_1^{(\mathrm{local})}}{n^{4/5}}}_{\mathrm{bias}^2} +
  \underbrace{\frac{C_2^{(\mathrm{local})}}{n^{4/5}}}_{\mathrm{var}} +
  \sigma^2
$$ 
:::

::: w-40
::: callout-important

_you don't need to memorize these formulas_ but you should know the intuition

_The constants_ don't matter for the intuition, but they matter for a particular data set. You have to estimate them.

:::

### What do you notice?
::: fragment
- As $n$ increases, the optimal bandwidth $\sigma$ decreases
- $R_n^{(\mathrm{basis})} \overset{n \to \infty}{\longrightarrow} C_1^{(\mathrm{basis})} + \sigma^2$
- $R_n^{(\mathrm{local})} \overset{n \to \infty}{\longrightarrow} \sigma^2$
:::

:::
:::

<!-- . . . -->

<!-- What if $x \in \R^p$ and $p>1$? -->

<!-- ::: aside -->
<!-- Note that $p$ means the dimension of $x$, not the dimension of the space of the polynomial basis or something else. That's why I put $k$ above. -->
<!-- ::: -->


## Takeaway

1. Local methods are *consistent universal approximators* (bias and variance go to 0 as $n \to \infty$)
2. Fixed basis expansions are *biased* but have lower variance when $n$ is relatively small.\
   [$\underbrace{O(1/n)}_{\text{basis var.}} < \underbrace{O(1/n^{4/5})}_{\text{local var.}}$]{.small}


# The Curse of Dimensionality

How do local methods perform when $p > 1$?


## Intuitively

*Parametric* multivariate regressors (e.g. basis expansions) require you to specify nonlinear interaction terms\
[e.g. $x^{(1)} x^{(2)}$, $\cos( x^{(1)} + x^{(2)})$, etc.]{.small}

\
*Nonparametric* multivariate regressors (e.g. KNN, local methods)
automatically handle interactions.\
[The distance function (e.g. $d(x,x') = \Vert x - x' \Vert_2$) used by kernels implicitly defines *infinitely many* interactions!]{.small}


\
[This extra complexity (automatically including interactions, as well as other things) comes with a tradeoff.]{.secondary}



## Mathematically

::: flex

::: w-70
Consider $x_1, x_2, \ldots, x_n$ distributed *uniformly* within
a $p$-dimensional ball of radius 1.
For a test point $x$ at the center of the ball,
how far away are its $k = n/10$ nearest neighbours?

[(The picture on the right makes sense in 2D. However, it gives the wrong intuition for higher dimensions!)]{.small}
:::

::: w-30
```{r fig.height=3, fig.width=3, echo=FALSE}
# Load necessary library
library(ggplot2)
set.seed(10000)

# Function to generate random points uniformly in a circle
generate_uniform_points_in_circle <- function(n, r = 1) {
  theta <- runif(n, 0, 2 * pi)  # Random angles
  rho <- sqrt(runif(n, 0, 1)) * r  # Random radii adjusted for uniform distribution
  x <- rho * cos(theta)
  y <- rho * sin(theta)
  return(data.frame(x = x, y = y))
}

# Circle drawing data
circle <- data.frame(
  x = cos(seq(0, 2 * pi, length.out = 100)),
  y = sin(seq(0, 2 * pi, length.out = 100))
)

# Generate 10 uniformly distributed points
points <- generate_uniform_points_in_circle(20)

# Plot circle and points
ggplot() +
  geom_path(data = circle, aes(x = x, y = y), color = "blue") +  # Circle
  geom_point(data = points, aes(x = x, y = y), color = "red", size = 3) +  # Points
  geom_point(aes(x = 0, y = 0), color = "black", size = 4) +  # Point at center
  geom_text(aes(x = 0, y = 0, label = "x"), vjust = -1, color = "black") +  # Label at center
  coord_equal() +
  theme_minimal() + 
  labs(x = NULL, y = NULL)
```
:::

:::

. . . 

::: flex
::: w-60
Let $r$ the the average distance between $x$ and its $k^\mathrm{th}$ nearest neighbour.

- When $p=2$, $r = (0.1)^{1/2} \approx 0.316$
- When $p=10$, $r = (0.1)^{1/10} \approx 0.794$(!)
- When $p=100$, $r = (0.1)^{1/100} \approx 0.977$(!!)
- When $p=1000$, $r = (0.1)^{1/1000} \approx 0.999$(!!!)
:::

::: w-35
::: fragment
### Why is this problematic?

- All points are maximally far apart
- Can't distinguish between "similar" and "different" inputs
:::
:::
:::

## Curse of Dimensionality

Distance becomes (exponentially) meaningless in high dimensions.*\
[*(Unless our data has "low dimensional structure.")]{.small}

. . .

### Risk decomposition ($p > 1$)
[Assuming optimal bandwidth of $n^{-1/(4+p)}$...]{.small}

$$
R_n^{(\mathrm{OLS})} =
  \underbrace{C_1^{(\mathrm{OLS})}}_{\mathrm{bias}^2} +
  \underbrace{\tfrac{C_2^{(\mathrm{OLS})}}{n/p}}_{\mathrm{var}} +
  \sigma^2,
\qquad
R_n^{(\mathrm{local})} =
  \underbrace{\tfrac{C_1^{(\mathrm{local})}}{n^{4/(4+p)}}}_{\mathrm{bias}^2} +
  \underbrace{\tfrac{C_2^{(\mathrm{local})}}{n^{4/(4+p)}}}_{\mathrm{var}} +
  \sigma^2.
$$

::: fragment
### Observations

- $(C_1^{(\mathrm{local})} + C_2^{(\mathrm{local})}) / n^{4/(4+p)}$ is relatively big, but $C_2^{(\mathrm{OLS})} / (n/p)$ is relatively small.
- So unless $C_1^{(\mathrm{OLS})}$ is big, we should use the linear model.*\
:::

## In practice

[The previous math assumes that our data are "densely" distributed throughout $\R^p$.]{.small}

However, if our data lie on a low-dimensional manifold within $\R^p$, then local methods can work well!

[We generally won't know the "intrinsic dimensinality" of our data though...]{.small}

:::fragment
### How to decide between basis expansions versus local kernel smoothers:
1. Model selection
2. Using a [very, very]{.secondary} questionable rule of thumb: if $p>\log(n)$, don't do smoothing.
:::

# ☠️☠️ Danger ☠️☠️

You can't just compare the GCV/CV/etc. scores for basis models versus local kernel smoothers.

You used GCV/CV/etc. to select the tuning parameter, so we're back to the usual problem of using the data twice. You have to do [another]{.hand} CV to estimate the risk of the kernel version once you have used GCV/CV/etc. to select the bandwidth.



# Next time...

Compromises if _p_ is big

Additive models and trees
