---
lecture: "00 Gradient descent"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}

## Motivation: maximum likelihood estimation as optimization

By the principle of maximum likelihood, we have that

$$
\begin{align*}
\hat \beta
&= \argmax_{\beta} \prod_{i=1}^n \P(Y_i \mid X_i)
\\
&= \argmin_{\beta} \sum_{i=1}^n -\log\P(Y_i \mid X_i)
\end{align*}
$$

Under the model we use for logistic regression...
$$
\begin{gathered}
\P(Y=1 \mid X=x) = h(\beta^\top x), \qquad \P(Y=0 \mid X=x) = h(-\beta^\top x),
\\
h(z) = \tfrac{1}{1-e^{-z}}
\end{gathered}
$$

... we can't simply find the argmin with algebra.

## Gradient descent: the workhorse optimization algorithm

We'll see "gradient descent" a few times: 

1. solves logistic regression
1. gradient boosting
1. Neural networks

This seems like a good time to explain it.

So what is it and how does it work?


## Very basic example

::: flex
::: w-65

Suppose I want to minimize $f(x)=(x-6)^2$ numerically.

I start at a point (say $x_1=23$)

I want to "go" in the negative direction of the gradient.

The gradient (at $x_1=23$) is  $f'(23)=2(23-6)=34$.

Move current value toward current value - 34.

$x_2 = x_1 - \gamma 34$, for $\gamma$ small.

In general, $x_{n+1} = x_n -\gamma f'(x_n)$.

```{r}
niter <- 10
gam <- 0.1
x <- double(niter)
x[1] <- 23
grad <- function(x) 2 * (x - 6)
for (i in 2:niter) x[i] <- x[i - 1] - gam * grad(x[i - 1])
```

:::

::: w-35


```{r echo=FALSE}
#| fig-width: 5
#| fig-height: 8
ggplot(data.frame(x = x, y = (x - 6)^2)) +
  geom_path(aes(x, y)) +
  geom_point(aes(x, y)) +
  coord_cartesian(xlim = c(6 - 24, 24), ylim = c(0, 300)) +
  geom_vline(xintercept = 6, colour = red, linetype = "dotted") +
  geom_hline(yintercept = 0, colour = red, linetype = "dotted") +
  stat_function(data = data.frame(x = c(6 - 24, 24)), aes(x), 
                fun = function(x) (x - 6)^2, colour = blue, alpha = .4) +
  ylab(bquote(f(x)))
```

:::
:::


## Why does this work?


[Heuristic interpretation:]{.secondary}

* Gradient tells me the slope.

* negative gradient points toward the minimum

* go that way, but not too far (or we'll miss it)

## Why does this work?

[More rigorous interpretation:]{.secondary}

- Taylor expansion
$$
f(x) \approx f(x_0) + \nabla f(x_0)^{\top}(x-x_0) + \frac{1}{2}(x-x_0)^\top H(x_0) (x-x_0)
$$
- replace $H$ with $\gamma^{-1} I$

- minimize this quadratic approximation in $x$:
$$
0\overset{\textrm{set}}{=}\nabla f(x_0) + \frac{1}{\gamma}(x-x_0) \Longrightarrow x = x_0 - \gamma \nabla f(x_0)
$$


## Visually

```{r}
#| echo: false
f <- function(x) (x - 1)^2 * (x > 1) + log(1 + exp(-2 * x))
fp <- function(x) 2 * (x - 1) * (x > 1) - 2 / (1 + exp(2 * x))
quad <- function(x, x0, gam = .1) f(x0) + fp(x0) * (x - x0) + 1 / (2 * gam) * (x - x0)^2
x <- c(-1.75, -1, -.5)

ggplot(data.frame(x = c(-2, 3)), aes(x)) +
  stat_function(fun = f, colour = blue) +
  geom_point(data = data.frame(x = x, y = f(x)), aes(x, y), colour = red) +
  stat_function(fun = quad, args = list(x0 = -1.75), colour = red) +
  stat_function(fun = quad, args = list(x0 = -1), colour = red) +
  stat_function(fun = quad, args = list(x0 = -.5), colour = red) +
  coord_cartesian(ylim = c(0, 4)) +
  ggtitle("gamma = 0.1")
```

## Visually

```{r}
#| echo: false
ggplot(data.frame(x = c(-2, 3)), aes(x)) +
  stat_function(fun = f, colour = blue) +
  geom_point(data = data.frame(x = x, y = f(x)), aes(x, y), colour = red) +
  stat_function(fun = quad, args = list(x0 = -1.75, gam = .25), colour = red) +
  stat_function(fun = quad, args = list(x0 = -1, gam = .25), colour = red) +
  stat_function(fun = quad, args = list(x0 = -.5, gam = .25), colour = red) +
  coord_cartesian(ylim = c(0, 4)) +
  ggtitle("gamma = 0.25")
```


## What $\gamma$? (more details than we have time for)

What to use for $\gamma_k$? 


[Fixed]{.secondary}

- Only works if $\gamma$ is exactly right 
- Usually does not work

[Decay on a schedule]{.secondary}

$\gamma_{n+1} = \frac{\gamma_n}{1+cn}$ or $\gamma_{n} = \gamma_0 b^n$

[Exact line search]{.secondary}

- Tells you exactly how far to go.
- At each iteration $n$, solve
$\gamma_n = \arg\min_{s \geq 0} f( x^{(n)} - s f(x^{(n-1)}))$
- Usually can't solve this.



##

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$

```{r, message=FALSE, echo=TRUE}
x <- matrix(0, 40, 2); x[1, ] <- c(1, 1)
grad <- function(x) c(2, 1) * x
```


```{r, message=FALSE, echo=FALSE}
#| fig-width: 8
#| fig-height: 4
df <- expand.grid(b1 = seq(-1, 1, length.out = 100), b2 = seq(-1, 1, length.out = 100))
df <- df %>% mutate(f = b1^2 + b2^2 / 2)
g <- ggplot(df, aes(b1, b2)) +
  geom_raster(aes(fill = log10(f))) +
  scale_fill_viridis_c() +
  xlab("x1") +
  ylab("x2") +
  coord_cartesian(expand = FALSE, ylim = c(-1, 1), xlim = c(-1, 1))
g
```

##

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$

```{r, message=FALSE, echo=TRUE}
gamma <- .1
for (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])
```

```{r, echo=FALSE}
#| fig-width: 8
#| fig-height: 4
b <- tibble(b1 = x[, 1], b2 = x[, 2])
g + geom_path(data = b, aes(b1, b2)) +
  geom_point(data = b, aes(b1, b2), size = 2)
```

##

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$

```{r, message=FALSE, echo=FALSE}
x <- matrix(0, 40, 2)
x[1, ] <- c(1, 1)
```

```{r, echo=TRUE}
gamma <- .9 # bigger gamma
for (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])
```

```{r, echo=FALSE}
#| fig-width: 8
#| fig-height: 4
b <- tibble(b1 = x[, 1], b2 = x[, 2])
g + geom_path(data = b, aes(b1, b2)) +
  geom_point(data = b, aes(b1, b2), size = 2)
```

##

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$

```{r, message=FALSE, echo=FALSE, fig.align='center'}
x <- matrix(0, 40, 2)
x[1, ] <- c(1, 1)
```

```{r, echo=TRUE}
gamma <- .9 # big, but decrease it on schedule
for (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * .9^k * grad(x[k - 1, ])
```

```{r, echo=FALSE}
#| fig-width: 8
#| fig-height: 4
b <- tibble(b1 = x[, 1], b2 = x[, 2])
g + geom_path(data = b, aes(b1, b2)) + geom_point(data = b, aes(b1, b2), size = 2)
```

##

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$

```{r, message=FALSE, echo=FALSE, fig.align='center'}
x <- matrix(0, 40, 2)
x[1, ] <- c(1, 1)
```

```{r, echo=TRUE}
gamma <- .5 # theoretically optimal
for (k in 2:40) x[k, ] <- x[k - 1, ] - gamma * grad(x[k - 1, ])
```

```{r, echo=FALSE}
#| fig-width: 8
#| fig-height: 4
b <- tibble(b1 = x[, 1], b2 = x[, 2])
g + geom_path(data = b, aes(b1, b2)) + geom_point(data = b, aes(b1, b2), size = 2)
```




## When do we stop?

For $\epsilon>0$, small


Check any / all of

1. $|f'(x)| < \epsilon$
1. $|x^{(k)} - x^{(k-1)}| < \epsilon$
1. $|f(x^{(k)}) - f(x^{(k-1)})| < \epsilon$


## Stochastic gradient descent (SGD)

If optimizing
$\argmin_\beta \sum_{i=1}^n -\log P_\beta(Y_i \mid X_i)$
then derivative also additive:
$$
\sum_{i=1}^n \frac{\partial}{\partial \beta} \left[-\log P_\beta(Y_i \mid X_i) \right]
$$

If $n$ is really big, it may take a long time to compute this

So, just sample a subset of data $\mathcal{M} \subset \{ (X_i, Y_i)\}_{i=1}^n$
and approximate:
$$\sum_{i=1}^n \frac{\partial}{\partial \beta} \left[-\log P_\beta(Y_i \mid X_i) \right] \approx \frac{n}{\vert \mathcal M \vert}\sum_{i\in\mathcal{M}} \left[-\log P_\beta(Y_i \mid X_i) \right]$$

For SGD need:

* the gradient estimates to be unbiased (are they?)
* decaying step size $\gamma$  (why?)

## SGD

$$
\begin{aligned}
f'(\beta) &= \frac{1}{n}\sum_{i=1}^n f'_i(\beta) \approx \frac{1}{|\mathcal{M}_j|}\sum_{i\in\mathcal{M}_j}f'_{i}(\beta)
\end{aligned}
$$

Instead of drawing samples independently, better to:

* Randomly order the whole dataset (N points), then iterate:
    * grab the next M points
    * compute a gradient estimate based on those points, take a step
    * once you exhaust all the data, that's an "epoch"; start from the beginning again

Gradient estimates are still marginally unbiased (why?)

This is the workhorse for neural network optimization

## When do we stop SGD?

For $\epsilon>0$, small

**Can we** check any / all of

1. $|f'(x)| < \epsilon$ ?
1. $|x^{(k)} - x^{(k-1)}| < \epsilon$ ?
1. $|f(x^{(k)}) - f(x^{(k-1)})| < \epsilon$ ?

. . .

None of this works due to the stochasticity. Knowing when to terminate SGD is *hard*.

# Practice with GD and Logistic regression


## Gradient descent for Logistic regression

$$
\begin{gathered}
\P(Y=1 \mid X=x) = h(\beta^\top x), \qquad \P(Y=0 \mid X=x) = h(-\beta^\top x),
\\
\\
h(z) = \tfrac{1}{1+e^{-z}}
\end{gathered}
$$

<!-- I want to model $P(Y=1| X=x)$.  -->

<!-- I'll assume that $\log\left(\frac{p(x)}{1-p(x)}\right) = ax$ for some scalar $a$. This means that -->
<!-- $p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}$ -->

. . .

```{r generate-data}
#| output-location: column
#| fig-width: 6
#| fig-height: 4
n <- 100
beta <- 2
x <- runif(n, -5, 5)
logit <- function(x) 1 / (1 + exp(-x))
p <- logit(beta * x)
y <- rbinom(n, 1, p)
df <- tibble(x, y)
ggplot(df, aes(x, y)) +
  geom_point(colour = "cornflowerblue") +
  stat_function(fun = ~ logit(beta * .x))
```



---

$$
\P(Y=1 \mid X=x) = h(\beta^\top x), \qquad \P(Y=0 \mid X=x) = h(-\beta^\top x)
$$

Under maximum likelihood

$$
\hat \beta = \argmin_{\beta} \underbrace{
  \textstyle{\sum_{i=1}^n - \log( \P_\beta(Y_i=y_i \mid X_i=x_i) )}
}_{:= -\ell(\beta)}
$$

---

$$
\begin{align*}
\P_\beta(Y_i=y_i \mid X_i=X_i) &= h\left( [-1]^{1 - y_i} \beta^\top x_i \right)
\\
\\
-\ell(\beta) &= \sum_{i=1}^n -\log\left( \P_\beta(Y_i=y_i \mid X_i=X_i) \right)
\\
&= \sum_{i=1}^n \log\left( 1 + \exp\left( [-1]^{y_i} \beta^\top x_i \right) \right)
\\
\\
-\frac{\partial \ell}{\partial \beta}
&= \sum_{i=1}^n x_i[-1]^{y_i} \frac{\exp\left( [-1]^{y_i} \beta^\top x_i \right)}{1 + \exp\left( [-1]^{y_i} \beta^\top x_i \right)}
\\
%&= \sum_{i=1}^n x_i \left( y_i - \P_\beta(Y_i=y_i \mid X_i=X_i) \right)
\end{align*}
$$

<!-- $$ -->
<!-- L(y | a, x) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\textrm{ and } -->
<!-- p(x) = \frac{1}{1+\exp(-ax)} -->
<!-- $$ -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \ell(y | a, x) &= \log \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}  -->
<!-- = \sum_{i=1}^n y_i\log p(x_i) + (1-y_i)\log(1-p(x_i))\\ -->
<!-- &= \sum_{i=1}^n\log(1-p(x_i)) + y_i\log\left(\frac{p(x_i)}{1-p(x_i)}\right)\\ -->
<!-- &=\sum_{i=1}^n ax_i y_i + \log\left(1-p(x_i)\right)\\ -->
<!-- &=\sum_{i=1}^n ax_i y_i + \log\left(\frac{1}{1+\exp(ax_i)}\right) -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ## Reminder: the likelihood -->

<!-- $$ -->
<!-- L(y | a, x) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\textrm{ and } -->
<!-- p(x) = \frac{1}{1+\exp(-ax)} -->
<!-- $$ -->

<!-- Now, we want the negative of this. Why?  -->

<!-- We would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood (and scale by $1/n$) -->


<!-- $$-\ell(y | a, x) = \frac{1}{n}\sum_{i=1}^n -ax_i y_i - \log\left(\frac{1}{1+\exp(ax_i)}\right)$$ -->

<!-- ## Reminder: the likelihood -->

<!-- $$ -->
<!-- \frac{1}{n}L(y | a, x) = \frac{1}{n}\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\textrm{ and } -->
<!-- p(x) = \frac{1}{1+\exp(-ax)} -->
<!-- $$ -->

<!-- This is, in the notation of our slides $f(a)$.  -->

<!-- We want to minimize it in $a$ by gradient descent.  -->

<!-- So we need the derivative with respect to $a$: $f'(a)$.  -->

<!-- Now, conveniently, this simplifies a lot.  -->


<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \frac{d}{d a} f(a) &= \frac{1}{n}\sum_{i=1}^n -x_i y_i - \left(-\frac{x_i \exp(ax_i)}{1+\exp(ax_i)}\right)\\ -->
<!-- &=\frac{1}{n}\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \frac{1}{n}\sum_{i=1}^n -x_i(y_i-p(x_i)). -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ## Reminder: the likelihood -->

<!-- $$ -->
<!-- \frac{1}{n}L(y | a, x) = \frac{1}{n}\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\textrm{ and } -->
<!-- p(x) = \frac{1}{1+\exp(-ax)} -->
<!-- $$ -->

---

## Finding $\hat\beta = \argmin_{\beta} -\ell(\beta)$ with gradient descent:

1. Input $\beta_0,\ \gamma>0,\ \epsilon>0,\ \tfrac{d \ell}{d\beta}$.
2. For $j=1,\ 2,\ \ldots$,
$$\beta_j = \beta_{j-1} - \gamma \tfrac{d}{d\beta}\left(-\!\ell(\beta_{j-1}) \right)$$
3. Stop if $|\beta_j - \beta_{j-1}| < \epsilon$ or $|d\ell / d\beta\ | < \epsilon$.


```{r betamle}
beta.mle <- function(x, y, beta0, gamma = 0.5, jmax = 50, eps = 1e-6) {
  beta <- double(jmax) # place to hold stuff (always preallocate space)
  beta[1] <- beta0 # starting value
  for (j in 2:jmax) { # avoid possibly infinite while loops
    px <- logit(beta[j - 1] * x)
    grad <- mean(-x * (y - px))
    beta[j] <- beta[j - 1] - gamma * grad
    if (abs(grad) < eps || abs(beta[j] - beta[j - 1]) < eps) break
  }
  beta[1:j]
}
```



## Try it:

```{r ourmle1}
too_big <- beta.mle(x, y, beta0 = 5, gamma = 50)
too_small <- beta.mle(x, y, beta0 = 5, gamma = 1)
just_right <- beta.mle(x, y, beta0 = 5, gamma = 10)
```

```{r}
#| output-location: column
#| fig-width: 7
#| fig-height: 7
negll <- function(beta) {
  -beta * mean(x * y) -
    rowMeans(log(1 / (1 + exp(outer(beta, x)))))
}
blah <- list_rbind(
  map(
    rlang::dots_list(
      too_big, too_small, just_right, .named = TRUE
    ), 
    as_tibble),
  names_to = "gamma"
) |> mutate(negll = negll(value))
ggplot(blah, aes(value, negll)) +
  geom_point(aes(colour = gamma)) +
  facet_wrap(~gamma, ncol = 1) +
  stat_function(fun = negll, xlim = c(-2.5, 5)) +
  scale_y_log10() + 
  xlab("beta") + 
  ylab("negative log likelihood") +
  geom_vline(xintercept = tail(just_right, 1)) +
  scale_colour_brewer(palette = "Set1") +
  theme(legend.position = "none")
```

## Check vs. `glm()`

```{r}
summary(glm(y ~ x - 1, family = "binomial"))
```
