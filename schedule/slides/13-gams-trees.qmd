---
lecture: "13 GAMs and Trees"
format: revealjs
metadata-files: 
  - _metadata.yml
---

{{< include _titleslide.qmd >}}


## GAMs

Last time we discussed smoothing in multiple dimensions.


Here we introduce the concept of GAMs ([G]{.secondary}eneralized [A]{.secondary}dditive [M]{.secondary}odels)

The basic idea is to imagine that the response is the sum of some functions of the predictors:

$$\Expect{Y \given X=x} = \beta_0 + f_1(x_{1})+\cdots+f_p(x_{p}).$$


Note that OLS [is]{.secondary} a GAM (take $f_j(x_{j})=\beta_j x_{j}$):

$$\Expect{Y \given X=x} = \beta_0 + \beta_1 x_{1}+\cdots+\beta_p x_{p}.$$

## Gams

These work by estimating each $f_i$ using basis expansions in predictor $i$

The algorithm for fitting these things is called "backfitting" (very similar to the CD intuition for lasso):


1. Center $\y$ and $\X$.
2. Hold $f_k$ for all $k\neq j$ fixed, and regress $\X_j$ on $(\y - \widehat{\y}_{-j})$ using your favorite smoother.
3. Repeat for $1\leq j\leq p$.
4. Repeat steps 2 and 3 until the estimated functions "stop moving" (iterate)
5. Return the results.



## Very small example


```{r}
#| message: false
library(mgcv)
set.seed(12345)
n <- 500
simple <- tibble(
  x1 = runif(n, 0, 2*pi),
  x2 = runif(n),
  y = 5 + 2 * sin(x1) + 8 * sqrt(x2) + rnorm(n, sd = .25)
)

pivot_longer(simple, -y, names_to = "predictor", values_to = "x") |>
  ggplot(aes(x, y)) +
  geom_point(col = blue) +
  facet_wrap(~predictor, scales = "free_x")
```

## Very small example

Smooth each coordinate independently

```{r gam-mod}
#| fig-width: 8
ex_smooth <- gam(y ~ s(x1) + s(x2), data = simple)
# s(z) means "smooth" z, uses spline basis for each with ridge penalty, GCV
plot(ex_smooth, pages = 1, scale = 0, shade = TRUE, 
     resid = TRUE, se = 2, las = 1)
head(coef(ex_smooth))
ex_smooth$gcv.ubre
```


## Wherefore GAMs?


If 

$\Expect{Y \given X=x} = \beta_0 + f_1(x_{1})+\cdots+f_p(x_{p}),$

then

$$
R_n^{(\mathrm{GAM})} =
  \underbrace{\frac{C_1^{(\mathrm{GAM})}}{n^{4/5}}}_{\mathrm{bias}^2} +
  \underbrace{\frac{C_2^{(\mathrm{GAM})}}{n^{4/5}}}_{\mathrm{var}} +
  \sigma^2.
$$
Compare with OLS and non-additive local smoothers:

$$
R_n^{(\mathrm{OLS})} =
  \underbrace{C_1^{(\mathrm{OLS})}}_{\mathrm{bias}^2} +
  \underbrace{\tfrac{C_2^{(\mathrm{OLS})}}{n/p}}_{\mathrm{var}} +
  \sigma^2,
\qquad
R_n^{(\mathrm{local})} =
  \underbrace{\tfrac{C_1^{(\mathrm{local})}}{n^{4/(4+p)}}}_{\mathrm{bias}^2} +
  \underbrace{\tfrac{C_2^{(\mathrm{local})}}{n^{4/(4+p)}}}_{\mathrm{var}} +
  \sigma^2.
$$

---

* We no longer have an exponential dependence on $p$!

* But our predictor is restrictive to functions that decompose additively.
  (This is a big limitation.)

* You could also use the same methods to include "some" interactions like

$$\begin{aligned}&\Expect{Y \given X=x}\\ &= \beta_0 + f_{12}(x_{1},\ x_{2})+f_3(x_3)+\cdots+f_p(x_{p}),\end{aligned}$$

## Very small example

Smooth two coordinates together

```{r}
#| fig-width: 8
ex_smooth2 <- gam(y ~ s(x1, x2), data = simple)
plot(ex_smooth2,
  scheme = 2, scale = 0, shade = TRUE,
  resid = TRUE, se = 2, las = 1
)
```



## Regression trees

* Trees involve stratifying or segmenting the predictor space into a number of simple regions.
* Trees are simple and useful for interpretation.  
* Basic trees are not great at prediction. 
* Modern methods that use trees are much better (Module 4)


## Example with mobility data

::: flex
::: w-50

"Small" tree
```{r}
#| code-fold: true
#| fig-width: 8
data("mobility", package = "Stat406")
library(tree)
library(maptree)
mob <- mobility[complete.cases(mobility), ] %>% dplyr::select(-ID, -Name)
set.seed(12345)
par(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0))
bigtree <- tree(Mobility ~ ., data = mob)
smalltree <- prune.tree(bigtree, k = .09)
draw.tree(smalltree, digits = 2)
```
:::

::: w-50
"Big" tree
```{r big-tree, echo=FALSE}
#| fig-width: 8
#| fig-height: 5
draw.tree(bigtree, digits = 2)
```
:::
:::

[Terminology]{.secondary}

* We call each split or end point a *node*.
* Each terminal node is referred to as a *leaf*.

## Example with mobility data

```{r partition-view}
#| code-fold: true
#| fig-width: 10
mob$preds <- predict(smalltree)
par(mfrow = c(1, 2), mar = c(5, 3, 0, 0))
draw.tree(smalltree, digits = 2)
cols <- viridisLite::viridis(20, direction = -1)[cut(log(mob$Mobility), 20)]
plot(mob$Black, mob$Commute,
  pch = 19, cex = .4, bty = "n", las = 1, col = cols,
  ylab = "Commute time", xlab = "% Black"
)
partition.tree(smalltree, add = TRUE, ordvars = c("Black", "Commute"))
```


[(The three regions correspond to the leaves of the tree.)]{.small}
\

* Trees are *piecewise constant functions*.\
  [We predict all observations in a region with the same value.]{.small}
* Prediction regions are axis-parallel rectangles $R_1,\ldots,R_K$ based on $\X$



<!-- ## -->


<!-- ![](https://www.aafp.org/dam/AAFP/images/journals/blogs/inpractice/covid_dx_algorithm4.png) -->


<!-- ## Dendrogram view -->

<!-- ```{r} -->
<!-- #| code-fold: true -->
<!-- #| fig-width: 8 -->
<!-- data("mobility", package = "Stat406") -->
<!-- library(tree) -->
<!-- library(maptree) -->
<!-- mob <- mobility[complete.cases(mobility), ] %>% dplyr::select(-ID, -Name) -->
<!-- set.seed(12345) -->
<!-- par(mar = c(0, 0, 0, 0), oma = c(0, 0, 0, 0)) -->
<!-- smalltree <- prune.tree(bigtree, k = .09) -->
<!-- draw.tree(smalltree, digits = 2) -->
<!-- ``` -->

<!-- This is called the [dendrogram]{.secondary} -->


<!-- ## Partition view -->

<!-- ```{r partition-view} -->
<!-- #| code-fold: true -->
<!-- #| fig-width: 10 -->
<!-- mob$preds <- predict(smalltree) -->
<!-- par(mfrow = c(1, 2), mar = c(5, 3, 0, 0)) -->
<!-- draw.tree(smalltree, digits = 2) -->
<!-- cols <- viridisLite::viridis(20, direction = -1)[cut(log(mob$Mobility), 20)] -->
<!-- plot(mob$Black, mob$Commute, -->
<!--   pch = 19, cex = .4, bty = "n", las = 1, col = cols, -->
<!--   ylab = "Commute time", xlab = "% Black" -->
<!-- ) -->
<!-- partition.tree(smalltree, add = TRUE, ordvars = c("Black", "Commute")) -->
<!-- ``` -->



## Constructing Trees

::: flex
::: w-60

Iterative algorithm:

* While ($\mathtt{depth} \ne \mathtt{max.depth}$):
    * For each existing region $R_k$
        * For a given *splitting variable* $j$ and *split value* $s$,
          define
          $$
          \begin{align}
          R_k^> &= \{x \in R_k : x^{(j)} > s\} \\
          R_k^< &= \{x \in R_k : x^{(j)} > s\}
          \end{align}
          $$
        * Choose $j$ and $s$ 
          to minimize
          $$|R_k^>| \cdot \widehat{Var}(R_k^>) + |R_k^<| \cdot  \widehat{Var}(R_k^<)$$

:::

::: w-35
```{r echo=FALSE}
#| fig-width: 5
#| fig-height: 4
plot(mob$Black, mob$Commute,
  pch = 19, cex = .4, bty = "n", las = 1, col = cols,
  ylab = "Commute time", xlab = "% Black"
)
partition.tree(smalltree, add = TRUE, ordvars = c("Black", "Commute"))
```
::: fragment
This algorithm is *greedy*, so it doesn't find the optimal tree\
[(But it works well!)]{.small}

:::
:::
:::


## Advantages and disadvantages of trees

ðŸŽ‰ Trees are very easy to explain (much easier than even linear regression).  

ðŸŽ‰ Some people believe that decision trees mirror human decision.  

ðŸŽ‰ Trees can easily be displayed graphically no matter the dimension of the data.

ðŸŽ‰ Trees can easily handle categorical predictors without the need to create one-hot encodings.

ðŸŽ‰ *Trees are GREAT for missing data!!!*

ðŸ’© Trees aren't very good at prediction.

ðŸ’© Big trees badly overfit, so we "prune" them using CV 

. . .

[We'll talk more about trees next module for Classification.]{.hand}

# Next time ... {background-image="gfx/proforhobo.png" background-opacity=.3}


Module 3

Classification

