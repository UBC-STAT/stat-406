---
title: "05 Estimating test MSE"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---


```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```


```{r css-extras, file="css-extras.R", echo=FALSE}
```


## Estimating prediction risk

Last time, we saw

$R_n(\widehat{f}) = E[(Y-\widehat{f}(X))^2]$


prediction risk  =  $\textrm{bias}^2$  +  variance  +  irreducible error 


We argued that we want procedures that produce $\widehat{f}$ with small $R_n$.

.emphasis[
How do we estimate $R_n$?
]

$$
\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
$$



---
class: inverse, midi

**Claim:**

$E[(Y-\widehat{f}(X))^2] = \textrm{bias}^2  +  \Var{\widehat{f}}  +  \textrm{irr. error}$

**Proof:**

Recall that: 
1. $\textrm{irr. error} = \Var{Y} = \Expect{(Y-f(X))^2} =: \sigma^2$.
2. $\Expect{Y} = f(X)$ ( $f(X)$ is _constant_).
3. $Y$ and $\widehat{f}$ are independent. $X$ is fixed.


\begin{aligned}
\Expect{(Y-\widehat{f}(X))^2} 
&= \Expect{(Y- f(X) + f(X) - \widehat{f}(X))^2}\\
&= \Expect{(Y-f(X))^2 + (f(X) - \widehat{f}(X))^2 + 2(Y-f(X))(f(X) - \widehat{f}(X)))}\\
&= \Expect{(Y-f(X))^2} + \Expect{(f(X) - \widehat{f}(X))^2} + 2\Expect{(Y-f(X))(f(X) - \widehat{f}(X)))}\\
&= \sigma^2 + \Expect{\left(f(X) - \Expect{\widehat{f}(X)} + \Expect{\widehat{f}(X)} - \widehat{f}(X)\right)^2} + 2\Expect{Y-f(X)}\Expect{f(X) - \widehat{f}(X)}\\
&= \sigma^2 + \Expect{\left(f(X) - \Expect{\widehat{f}(X)}\right)^2} + \Expect{\left(\Expect{\widehat{f}(X)} - \widehat{f}(X)\right)^2} +\\
&\quad\quad 2\Expect{\left(f(X) - \Expect{\widehat{f}(X)}\right) \left(\Expect{\widehat{f}(X)} - \widehat{f}(X)\right)} + 0\\
& = \sigma^2 + \left(f(X) - \Expect{\widehat{f}(X)}\right)^2 + \Var{\widehat{f}(X)} + 0\\
& = \sigma^2 + \textrm{bias}^2 + \Var{\widehat{f}(X)}
\end{aligned}


---

## Don't use training error


The training error in regression is

$$\widehat{R}_n(\widehat{f}) = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{f}(x_i))^2$$

Here, the $n$ is doubly used (annoying, but simple): $n$ observations to create $\widehat{f}$ and $n$ terms in the sum.

.emphasis[
Training error is a bad estimator for $R_n(\widehat{f})$.
]

`lab00-rsq` demonstrates this with $R^2$ ( $R^2$ roughly is the same as "training error", recall its definition).

So we should __never__ use it.

--

 ---

You should not use `anova` or the $p$-values from the `lm` output for this purpose.

Why? These things are to determine whether those __parameters__ are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.

Not the same as __"are they useful for prediction = do they help me get smaller out-of-sample MSE"__

---

## Held out sets

One option is to have a separate "held out" or "validation set".


`r pro` Estimates the test error

`r pro` Fast computationally

`r con` Estimate is random

`r con` Estimate has high variance

`r con` Estimate has some bias because we only used some of the data

--

 ---

Sometimes also called a "validation set". This is not the same as "new data". It's data I have, that I'm not using to fit my model.


---

## Intuition for CV


One reason that $\widehat{R}_n(\widehat{f})$ is bad is that we are using the same data to pick $\widehat{f}$ __AND__ to estimate $R_n$.

"Validation set" fixes this, but holds out a particular, fixed block of data we treat as "new"

Of course, we don't have actually have new data.

--

...or do we?

What if we set aside one observation, say the first one $(y_1, x_1)$.

We estimate $\widehat{f}^{(1)}$ without using the first observation.

Then we test our prediction:


$$\widetilde{R}_1(\widehat{f}^{(1)}) = (y_1 -\widehat{f}^{(1)}(x_1))^2.$$


(why the notation $\widetilde{R}_1$? Because we're estimating the risk with 1 observation. )

---

## Keep going

But that was only one data point $(y_1, x_1)$. Why stop there?

Do the same with $(y_2, x_2)$! Get an estimate $\widehat{f}^{(2)}$ 
without using it, then

$$\widetilde{R}_1(\widehat{f}^{(2)}) = (y_2 -\widehat{f}^{(2)}(x_2))^2.$$

We can keep doing this until we try it for every data point.

And then average them! (Averages are good)


$$\mbox{LOO-CV} = \frac{1}{n}\sum_{i=1}^n \widetilde{R}_1(\widehat{f}^{(i)}) = \frac{1}{n}\sum_{i=1}^n 
(y_i - \widehat{f}^{(i)}(x_i))^2$$

--

This is __leave-one-out cross validation__

---

## Problems with LOO-CV

`r con` Each held out set is small $(n=1)$. Therefore, the variance of the Squared Error of each prediction is high.

`r con` The training sets overlap. This is bad. 
  * Usually, averaging reduces variance: $\Var{\overline{X}} = \frac{1}{n^2}\sum_{i=1}^n \Var{X_i} = \frac{1}{n}\Var{X_1}.$
  * But only if the variables are independent. If not, then
    $\Var{\overline{X}} = \frac{1}{n^2}\Var{ \sum_{i=1}^n X_i} = \frac{1}{n}\Var{X_1} + \frac{1}{n^2}\sum_{i\neq j} \Cov{X_i}{X_j}.$
  * Since the training sets overlap a lot, that covariance can be pretty big.
    
`r con` We have to estimate this model $n$ times.

---
  
## K-fold CV

.pull-left[
To alleviate some of these problems, people usually use $K$-fold cross validation.

.emphasis[The idea of $K$-fold is 
  1. Divide the data into $K$ groups. 
  2. Leave a group out and estimate with the rest.
  3. Test on the held-out group. Calculate an average risk over these $\sim n/K$ data.
  4. Repeat for all $K$ groups.
  5. Average the average risks.
]
]

--

.pull-right[  
`r pro` Less overlap, smaller covariance.

`r pro` Larger hold-out sets, smaller variance.

`r pro` Less computations (only need to estimate $K$ times)

`r con` LOO-CV is (nearly) unbiased for $R_n$

`r con` K-fold CV is unbiased for $R_{n(1-1/K)}$

The risk depends on how much data you use to estimate the model. $R_n$ depends on $n$.

]

---

## A picture

```{r, fig.align='center',echo=FALSE,fig.height=6, fig.width=10}
par(mar=c(0,0,0,0))
plot(NA, NA, ylim=c(0,5), xlim=c(0,10), bty='n', yaxt='n', xaxt='n')
rect(0,.1+c(0,2,3,4),10,.9+c(0,2,3,4), col=blue, density=10)
rect(c(0,1,2,9),rev(.1+c(0,2,3,4)),c(1,2,3,10),rev(.9+c(0,2,3,4)),col=red, density=10)
points(c(5,5,5),1+1:3/4,pch=19)
text(.5+c(0,1,2,9),.5+c(4,3,2,0),c("1","2","3","K"), cex=3, col=red)
text(6,4.5,'Training data',cex=3, col=blue)
text(2,1.5,'Testing data',cex=3,col=red)
```

---

## Code


```{r}
#' @param data The full data set
#' @param estimator Function. Has 1 argument (some data) and fits a model. 
#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions
#' @param error_fun Function. Has one arg: the test data, with fits added.
#' @param kfolds Integer. The number of folds.
kfold_cv <- function(data, estimator, predictor, 
                     error_fun, kfolds = 5) {
  n <- nrow(data)
  fold_labels <- sample(rep(1:kfolds, length.out = n))
  errors <- double(kfolds)
  for (fold in seq_len(kfolds)) {
    test_rows <- fold_labels == fold
    train <- data[!test_rows, ]
    test <- data[test_rows, ]
    current_model <- estimator(train)
    test$.preds <- predictor(current_model, test)
    errors[fold] <- error_fun(test)
  }
  mean(errors)
}
```

```{r}
somedata <- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))
est <- function(dataset) lm(z ~ ., data = dataset)
pred <- function(mod, dataset) predict(mod, newdata = dataset)
error_fun <- function(testdata) mutate(testdata, errs = (z - .preds)^2) %>% pull(errs) %>% mean()
kfold_cv(somedata, est, pred, error_fun, 5)
```

---

## Trick

.emphasis[ 
__For nice models__, one can show 

(after pages of tedious algebra which I wouldn't wish on my worst enemy, but might, in a fit of rage assign as homework to beligerant students) 

if $\widehat{y}_i = \widehat{f}(x_i) = h_i^\top \mathbf{y}$, for some vector $h_i(\mathbf{X})$
]

$$\mbox{LOO-CV} = \frac{1}{n} \sum_{i=1}^n \frac{(y_i -\widehat{y}_i)^2}{(1-h_{ii})^2} = \frac{1}{n} \sum_{i=1}^n \frac{\widehat{e}_i^2}{(1-h_{ii})^2}.$$

* This trick means that you only have to fit the model once rather than $n$ times!

* You still have to calculate this for each model!

```{r}
cv_nice <- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )
```

---
class: middle, inverse, center

# Next time...

More tricks and what's up with the name "hatvalues"
