---
title: "{{< fa calendar-alt >}} Schedule"
format:
  html:
    toc: true
---

Required readings are listed below for each module.
Readings from [ISLR](https://www.statlearning.com/) are required, while those from [ESL](https://hastie.su.domains/ElemStatLearn/) (in parentheses) are optional and supplemental.

::: {.callout-note appearance="minimal"}
All lecture notes as `.qmd` files are available [here](https://github.com/UBC-STAT/stat-406/tree/main/schedule/lectures/).
`R` code for all lectures is available [here](https://github.com/UBC-STAT/stat-406/tree/main/schedule/Rcode/).
:::

<div class="mt-5"></div>

## 1 The Learning Procedure - Models, Fitting, Model Selection

**Topics:** Learning through statistician and algorithmic lenses, model selection; cross validation

**Learning Objectives:**

1. Formulate learning problems in terms of statistical models, estimators, and model selection
2. Identify criteria for good statistical models, estimators, and model selection metrics

**Handouts and Resources:**

- Programming in `R` [`.Rmd`](handouts/00-programming.Rmd), [`.pdf`](handouts/00-programming.pdf)
- Using RMarkdown [`.Rmd`](handouts/00-rmarkdown.Rmd), [`.pdf`](handouts/00-rmarkdown.pdf)
- Overview of `R` and Tidyverse ([Slides from last term](slides/00-r-review.qmd))
- Overview of `git` and version control ([Slides from last term](slides/00-version-control.qmd))


| Date      | Topic                                        | Readings                      | Deadlines |
|:----------|:---------------------------------------------|:------------------------------|:----------|
| Sep 2  | (no class, Imagine UBC)              |                               |           |
| Sep 4  | Class Overview ([slides](slides/00-intro.qmd)), Probability Review ([notes](lectures/lecture_01_probability.qmd))  |                               |           |
| Sep 9  | Introduction to Learning, Regression ([notes](lectures/lecture_02_learning_procedure_regression.qmd)) | ISLR 2.1 (ESL 2.4, 2.6)      |           |
| Sep 11 | Learning (cont.), Classification ([notes](lectures/lecture_03_learning_procedure_classification.qmd))   | ISLR 4.3 (ESL 4.4)           |           |
| Sep 16 | Model Selection and Cross Validation | ISLR 5.1 (ESL 2.9, 7.10)     |           |




## 2 Bias-Variance Tradeoff, Linear Methods

**Topics:** bias/variance tradeoff; regularized regression (ridge and lasso); non-linearities via basis functions; advanced model selection and analysis

**Learning Objectives:**

1. Decompose prediction error into bias and variance components
2. Implement regularized versions of linear regression (ridge, lasso) and understand their impact on bias and variance
3. Implement basis expansions for linear regression and understand their impact on bias and variance
4. Apply closed-form selection techniques to linear methods, and identify factors in the formula that affect bias and variance

| Date      | Topic                                        | Readings                      | Deadlines |
|:----------|:---------------------------------------------|:------------------------------|:----------|
| Sep 18 | Bias-Variance Tradeoff               | ISLR 2.2 (ESL 7.1-7.3)       | HW 1 due |
| Sep 23 | Ridge Regression                     | ISLR 6.2.1 (ESL 3.4.0-3.4.1) |                   |
| Sep 25 | Lasso Regression, Optimization       | ISLR 6.2.2-6.2.3 (ESL 3.4.2-3.4.3) |                   |
| Sep 30 | (no class, Truth and Reconciliation) |                               |                   |
| Oct 2  | Basis Functions                      | ISLR 7.1, 7.4 (ESL 5.1-5.3)  |                   |
| Oct 7  | Model Selection for Linear Methods   | (ESL 7.6-7.7)                |           |





## 3 Nonparametric Methods, Curse of Dimensionality

**Topics:** kNN, kernel machines, curse of dimensionality

**Learning Objectives:**

1. Analyze how dimensionality affects the performance of parametric vs nonparametric methods
2. Implement nonparametric methods (kNN, kernel smoothing, kernel machines) and analyze their properties
3. Write the parametric version of nonparametric methods (e.g. kernel ridge regression) and vice versa

| Date      | Topic                                        | Readings                      | Deadlines |
|:----------|:---------------------------------------------|:------------------------------|:----------|
| Oct 9  | kNN                              | ISLR 3.5 (ESL 2.3.2, 5.4.1)  | HW 2 due |
| Oct 14 | Kernel Machines                  |                               |           |
| Oct 16 | The Curse of Dimensionality      | ISLR 6.4 (ESL 2.5)           |           |
| Oct 21 | Trees, Review                    | ISLR 8.1 (ESL 9.2)           |           |




## Midterm Exam

| Date      | Topic |
|:----------|:-----------------------------------------------------------------------------------------|
| Oct 23 | **MIDTERM EXAM** (In Class)      |

* In person attendance is required (per Faculty of Science guidelines)
* You must bring your computer as the exam will be given through Canvas
* Please [arrange to borrow](https://services.library.ubc.ca/computers-technology/technology-borrowing/) one from the library if you do not have your own. Let me know ASAP if this may pose a problem.
* You may bring 2 sheets of front/back 8.5 × 11 inch paper with _handwritten_ notes you want to use. No other materials will be allowed.
* There will be no required coding, but I may show code or output and ask questions about it.
* It will be entirely multiple choice / True-False / matching, etc. Delivered on Canvas.


## 4 Ensemble Methods

**Topics:** trees; ensembles; bootstrap; bagging; boosting; random forests

**Learning Objectives:**

1. Implement ensembling methods of base learners and reason through computational tradeoffs
2. Differentiate ensemble methods that reduce bias or variance
3. Utilize "hidden advantages" of ensembles around feature importance, uncertainty quantification, etc.
4. Identify inductive bias of decision trees and conditions that yield high bias or high variance

| Date      | Topic                                        | Readings                      | Deadlines |
|:----------|:---------------------------------------------|:------------------------------|:----------|
| Oct 28 | The Bootstrap                    | ISLR 5.2 (ESL 7.11, 8.2)     |           |
| Oct 30 | Bagging and Random Forests       | ISLR 8.2.0-8.2.2 (ESL 8.7, 15.1-15.3) |           |
| Nov 4  | Boosting                         | ISLR 8.2.3 (ESL 10.1-10.5, 10.9) |           |




## 5 Unsupervised Learning, Generative vs Discriminative Modelling

**Topics:** dimension reduction and clustering; generative vs discriminative modelling

**Learning Objectives:**

1. Differentiate between generative and discriminative modelling approaches and identify when each is most appropriate
2. Implement dimensionality reduction techniques (PCA, kernel PCA) and analyze their impact on data representation
3. Apply clustering algorithms (k-means, Gaussian mixture models) and evaluate their performance using appropriate metrics
4. Connect unsupervised learning methods to their generative/discriminative modelling framework

| Date      | Topic                                        | Readings                      | Deadlines          |
|:----------|:---------------------------------------------|:------------------------------|:-------------------|
| Nov 6  | Generative vs Discriminative Modelling | ISLR 4.2.0, 12.1             | HW 3 due  |
| Nov 11 | (no class, Midterm Break)          |                               |                    |
| Nov 13 | Dimensionality Reduction           | ISLR 12.2 (ESL 14.5.1, 14.5.4) |                    |
| Nov 18 | Clustering 1                       | ISLR 12.4.1 (ESL 14.3)       |                    |
| Nov 20 | Clustering 2                       |                               |           |





## 6 Deep Learning

**Topics:** neural networks; deep learning architectures; generative AI

**Learning Objectives:**

1. Construct a basic neural network architecture from simple mathematical building blocks
2. Articulate the effects of depth and width on the representational capacity and generalization of neural networks
3. Connect neural networks to other methods covered in the course (basis functions, kernel methods, boosting methods)
4. Derive the backpropagation algorithm
5. Evaluate modern neural network architectures for different problem types

| Date      | Topic                                        | Readings                      | Deadlines          |
|:----------|:---------------------------------------------|:------------------------------|:-------------------|
| Nov 25 | Introduction to Neural Networks    | ISLR 10.1-10.2 (ESL 11.1, 11.3) |                    |
| Nov 27 | Neural Network Optimization        | ISLR 10.7-10.8 (ESL 11.4)    | HW 4 due |
| Dec 2  | Neural Net Architectures, Generative AI |                               |                  |
| Dec 4  | Review                             |                               |                    |




## Final Exam

::: {.callout-important}
Do not make any plans to leave Vancouver before the final exam date is announced.
:::

* In person attendance is required (per Faculty of Science guidelines)
* You may bring 2 sheets of front/back 8.5 × 11 inch paper with _handwritten_ notes you want to use. No other materials will be allowed.
