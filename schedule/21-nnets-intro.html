<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>21 Neural nets</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 21 Neural nets
### STAT 406
### Daniel J. McDonald
### Last modified - 2021-11-16

---





## Overview

Neural networks are models for supervised
learning

 
Linear combinations of features  are passed
through a non-linear transformation in successive layers

 
At the top layer, the resulting latent
factors are fed into an algorithm for
predictions

(Most commonly via least squares or logistic regression)

 

---

## Background

.pull-left[
Neural networks have come about in 3 "waves" 

The first was an attempt in the 1950s to model the mechanics of the human brain

Through psychological and anatomical experimentation, it appeared the
brain worked by

-   taking atomic units known as __neurons__ ,
    which can either be "on" or "off"

-   putting them in __networks__  with each
    other, where the __signal__  is given by
    which neurons are "on" at a given time

 
Crucially, a neuron itself interprets the status of other neurons

There weren't really computers, so we couldn't estimate these things
]

.pull-right[
![](https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2015/05/neuralnets-678x381.jpg)
]

---

## Background

After the development of parallel, distributed computation in the 1980s,
this "artificial intelligence" view was diminished

And neural networks gained popularity 

But, the growing popularity of SVMs and boosting/bagging in the late
1990s, neural networks again fell out of favor

This was due to many of the problems we'll discuss (non convexity being
the main one)

--

In the mid 2000's, new approaches for
__initializing__  neural networks became
available

 
These approaches are collectively known as __deep
learning__

 
Together, some state-of-the-art performance on various classification
tasks have been accomplished via neural networks

Today, Neural Networks/Deep Learning are the hottest...


`$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}$$`

---


## High level overview

.center[
![:scale 50%](gfx/single-layer-net.svg)
]

---

## Recall nonparametric regression

Suppose `\(Y \in \mathbb{R}\)` and we are trying estimate
the regression function `$$\Expect{Y\given X} = f_*(X)$$`

 
Earlier, we discussed basis expansion, 



1.  We know `\(f_*(x) =\sum_{k=1}^\infty \beta_k h_k(x)\)` some basis `\(h_1,h_2,\ldots\)` (using `\(h\)` instead of `\(\phi\)` to match ISLR)

2.  Truncate this expansion at `\(K\)`: 
    `\(f_*^K(x) \approx \sum_{k=1}^K \beta_k h_k(x)\)`

3.  Estimate `\(\beta_k\)` with least squares

--

The weaknesses of this approach are:

-   The basis is fixed and independent of the data

-   If `\(p\)` is large, then nonparametrics doesn't work well at all (recall the Curse of Dimensionality)

-   If the basis doesn't "agree" with `\(f_*\)`, then `\(K\)` will have to be
    large to capture the structure

-   What if parts of `\(f_*\)` have substantially different structure? Say `\(f_*(x)\)` really wiggly for `\(x \in [-1,3]\)` but smooth elsewhere

An alternative would be to have the data
__tell__ us what kind of basis to use (Module 5)

---

## 1-layer for Regression

.pull-left[

`$$f_*(x) = \Expect{Y \given X = x}$$`

A single layer neural network model is
$$
`\begin{aligned}
f(x) &amp;= \beta_0 + \sum_{k=1}^K \beta_k h_k(x) \\
&amp;= \beta_0 + \sum_{k=1}^K \beta_k \ g(w_{k0} + w_k^{\top}x)\\
&amp;= \beta_0 + \sum_{k=1}^K \beta_k \ A_k\\
\end{aligned}`
$$

__Compare:__  
A nonparametric regression would have the form
`$$f(x) = \beta_0 + \sum_{k=1}^K \beta_k {\phi_k(x)}$$`

]

.pull-right[


![:scale 200%](gfx/single-layer-net.svg)

]

---

## Terminology

`$$f(x) = {\beta_0} + \sum_{k=1}^{{K}} {\beta_k} {g(w_{k0} + w_k^{\top}x)}$$`
The main components are

-   The derived features `\({A_k = g(w_{k0} + w_k^{\top}x)}\)` and are called the __hidden units__ or __activations__

-   The function `\(g\)` is called the __activation function__  

      
-   The parameters
`\({\beta_0},{\beta_k},{w_{k0}},{w_k},k = 1,\ldots,K\)` are estimated from the data.

-   The number of hidden units `\({K}\)` is a tuning
    parameter
    
- `\(\beta_0\)` and `\(w_{k0}\)` are usually called __biases__ (I'm not going to include them in future formulas, just for space)    

--

Notes (no biases):
&gt; `\(\beta \in \R^k\)`.  
`\(w_k \in \R^p,\ k = 1,\ldots,K\)`  
`\(\mathbf{W} \in \R^{K\times p}\)`



---

## What about classification (10 classes, 2 layers)


.pull-left[

$$
`\begin{aligned}
A_k^{(1)} &amp;= g\left(\sum_{j=1}^p w^{(1)}_{k,j} x_j\right)\\
A_\ell^{(2)} &amp;= g\left(\sum_{k=1}^{K_1} w^{(2)}_{\ell,k} A_k^{(1)} \right)\\
z_m &amp;= \sum_{\ell=1}^{K_2} \beta_{m,\ell} A_\ell^{(2)}\\
f_m(x) &amp;= Pr\left(Y = m \given X=x\right) = \frac{1}{1 + \exp(-z_m)}\\
\hat{Y} &amp;= \argmax_{m = 0,\ldots,9} f_m(x)
\end{aligned}`
$$

Notes:
&gt; `\(B \in \R^{M\times K_2}\)` (here `\(M=10\)`).  
`\(\mathbf{W}_2 \in \R^{K_2\times K_1}\)`  
`\(\mathbf{W}_1 \in \R^{K_1\times p}\)`

Final predicted class uses __Softmax__

]

.pull-right[

![:scale 200%](gfx/two-layer-net.svg)
]


---
layout: true

## Two observations


---
 

1. The `\(g\)` function generates a __feature map__

We start with `\(p\)` covariates and we generate `\(K\)` features (1-layer)


.pull-left[

__Logistic/Least-squares with a polynomial transformation__

$$
`\begin{aligned}
&amp;\Phi(x) \\
&amp; = 
(1, x_1, \ldots, x_p, x_1^2,\ldots,x_p^2, x_1x_2, \ldots, x_{p-1}x_p) \\
&amp; =
(\phi_1(x),\ldots,\phi_{K_2}(x))
\end{aligned}`
$$

 
Before feature map: 

`\(f(x) = \sum_{j=1}^p \beta_j x_j\)`

After feature map:

`\(f(x) =  \sum_{k=1}^{K_2} \beta_k \phi_k(x) = \beta^\top \Phi(x)\)`

]

.pull-right[

__Neural network__

`\(A_k = g\left( \sum_{j=1}^p w_{kj}x_j\right) = g\left( w_{k}^{\top}x\right)\)`

This gives

`\(\Phi(x) = (z_1, \ldots,z_K)^{\top} \in \mathbb{R}^{K}\)`

and

`$$\begin{aligned}f(x) &amp;=\beta^{\top} \Phi(x)=\beta^\top A\\ 
&amp;=  \sum_{k=1}^K \beta_k g\left( \sum_{j=1}^p w_{kj}x_j\right)\end{aligned}$$`

]

---

* If `\(g(u) = u\)`, then neural networks reduce to classic least squares (try to show this)

* ReLU is the current fashion

&lt;img src="rmd_gfx/21-nnets-intro/sigmoid-1.svg" style="display: block; margin: auto;" /&gt;

---
layout: false
class: middle,inverse,center

# Next time...

How do we estimate these monsters?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
