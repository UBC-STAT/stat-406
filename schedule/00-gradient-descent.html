<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>00 Gradient descent</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 00 Gradient descent
### STAT 406
### Daniel J. McDonald
### Last modified - 2021-10-28

---


## Simple optimization techniques

`$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$`




We'll see "gradient descent" a few times: 

1. solves logistic regression (simple version of IRWLS)

2. gradient boosting

3. Neural networks

This seems like a good time to explain it.

So what is it and how does it work?

---

## Very basic example

.pull-left-wide[
Suppose I want to minimize `\(f(x)=(x-6)^2\)` numerically.

I start at a point (say `\(x_1=23\)`)

I want to "go" in the negative direction of the gradient.

The gradient (at `\(x_1=23\)`) is  `\(f'(23)=2(23-6)=34\)`.

Move current value toward current value - 34.

`\(x_2 = x_1 - \gamma 34\)`, for `\(\gamma\)` small.

In general, `\(x_{n+1} = x_n -\gamma f'(x_n)\)`.


```r
niter &lt;- 10
gam &lt;- 0.1
x &lt;- double(niter)
x[1] &lt;- 23
grad &lt;- function(x) 2 * (x - 6)
for (i in 2:niter) x[i] = x[i-1] - gam * grad(x[i - 1])
```


]
  
.pull-right-narrow[  
  

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

]

---

## Why does this work?

__Heuristic interpretation:__

* Gradient tells me the slope.

* negative gradient points toward the minimum

* go that way, but not too far (or we'll miss it)

__More rigorous interpretation:__

- Taylor expansion
$$
f(x) \approx f(x_0) + \nabla f(x_0)^{\top}(x-x_0) + \frac{1}{2}(x-x_0)^\top H(x_0) (x-x_0)
$$
- replace `\(H\)` with `\(\gamma^{-1} I\)`

- minimize the quadratic approximation in `\(x\)`:
$$
0\overset{\textrm{set}}{=}\nabla f(x_0) + \frac{1}{\gamma}(x-x_0) \Longrightarrow x = x_0 - \gamma \nabla f(x_0)
$$

---

## Visually

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---

## What `\(\gamma\)`? (more details than we have time for)

What to use for `\(\gamma_k\)`? 


__Fixed__

- Only works if `\(\gamma\)` is exactly right 
- Usually does not work

__Sequence__ 

`$$\gamma_k \quad s.t.\quad\sum_{k=1}^{\infty} \gamma_k = \infty ,\quad
              \sum_{k=1}^{\infty} \gamma^{2}_k &lt; \infty$$`



__Exact line search__

- Tells you exactly how far to go.
- At each `\(k\)`, solve
`\(\gamma_k = \arg\min_{s \geq 0} f( x^{(k)} - s f(x^{(k-1)}))\)`
- Usually can't solve this.



---
layout: true

$$ f(x_1,x_2) = x_1^2 + 0.5x_2^2$$


```r
x &lt;- matrix(0, 40, 2)
x[1, ] &lt;- c(1, 1)
grad &lt;- function(x) c(2, 1) * x
```

---

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-5-1.svg" style="display: block; margin: auto;" /&gt;

---



```r
gamma &lt;- .1
for (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ])
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-7-1.svg" style="display: block; margin: auto;" /&gt;

---




```r
gamma &lt;- .9 # bigger gamma
for (k in 2:40) x[k,] &lt;- x[k - 1,] - gamma * grad(x[k-1,])
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-10-1.svg" style="display: block; margin: auto;" /&gt;

---





```r
gamma &lt;- .5 # medium, but decrease it
for (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma / sqrt(k) * grad(x[k - 1, ]) 
```

&lt;img src="rmd_gfx/00-gradient-descent/unnamed-chunk-13-1.svg" style="display: block; margin: auto;" /&gt;




---
layout: false

## When do we stop?

For `\(\epsilon&gt;0\)`, small


Check any / all of

1. `\(|f'(x)| &lt; \epsilon\)`

2. `\(|x^{(k)} - x^{(k-1)}| &lt; \epsilon\)`

3. `\(|f(x^{(k)}) - f(x^{(k-1)})| &lt; \epsilon\)`

---

## Stochastic gradient descent

Suppose `\(f(x) = \frac{1}{n}\sum_{i=1}^n f_i(x)\)`

Like if `\(f(\beta) = \frac{1}{n}\sum_{i=1}^n (y_i - x^\top_i\beta)^2\)`.

Then `\(f'(x) = \frac{1}{n}\sum_{i=1}^n f'_i(x)\)`

If `\(n\)` is really big, it may take a long time to compute `\(f'\)`

So, just sample some `\(\mathcal{M} = \{i_1,i_2,\ldots,i_m\}\)`

And approximate
`$$f'(x) = \frac{1}{n}\sum_{i=1}^n f_i(x) \approx \frac{1}{m}\sum_{i\in\mathcal{M}}f'_{i}(x)$$`

Usually cycle through "mini-batches" `\(\mathcal{M}\)` until we've seen all `\(n\)` points.

--

This is the workhorse for neural network optimization

---
class: center, middle, inverse

# Practice with GD and Logistic regression

---

## Gradient descent for Logistic regression

Suppose `\(Y=1\)` with probability `\(p(x)\)` and `\(Y=0\)` with probability `\(1-p(x)\)`, `\(x \in \R\)`.  

I want to model `\(P(Y=1| X=x)\)`. 

I'll assume that `\(\log\left(\frac{p(x)}{1-p(x)}\right) = ax\)` for some scalar `\(a\)`. This means that
`\(p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}\)`


--

.pull-left[

```r
n &lt;- 100
a &lt;- 2
x &lt;- runif(n, -5, 5)
logit &lt;- function(x) 1 / (1 + exp(-x))
p &lt;- logit(a * x)
y &lt;- rbinom(n, 1, p)
df &lt;- tibble(x, y)
g &lt;- ggplot(df, aes(x, y)) +
  geom_point(color="cornflowerblue") +
  stat_function(fun = ~ logit(a * .x))
```
]


.pull-right[
![](rmd_gfx/00-gradient-descent/unnamed-chunk-14-1.svg)&lt;!-- --&gt;
]

---
layout: true


## Reminder: the likelihood

`$$L(y | a, x) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}$$`

Remember, we defined `\(p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}\)`.

---

The log likelihood is therefore

$$
`\begin{aligned}
\ell(y | a, x) &amp;= \log \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i} \\
&amp;= \sum_{i=1}^n y_i\log p(x_i) + (1-y_i)\log(1-p(x_i))\\
&amp;= \sum_{i=1}^n\log(1-p(x_i)) + y_i\log\left(\frac{p(x_i)}{1-p(x_i)}\right)\\
&amp;=\sum_{i=1}^n ax_i y_i + \log\left(1-p(x_i)\right)\\
&amp;=\sum_{i=1}^n ax_i y_i + \log\left(\frac{1}{1+\exp(ax_i)}\right)
\end{aligned}`
$$

---

Now, we want the negative of this. Why? 

We would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood.


`$$-\ell(y | a, x) = \sum_{i=1}^n -ax_i y_i - \log\left(\frac{1}{1+\exp(ax_i)}\right)$$`

---

This is, in the notation of our slides `\(f(a)\)`. 

We want to minimize it in `\(a\)` by gradient descent. 

So we need the derivative with respect to `\(a\)`: `\(f'(a)\)`. 

Now, conveniently, this simplifies a lot. 


`$$\frac{d}{d a} f(a) = \sum_{i=1}^n -x_i y_i - \left(-\frac{x_i \exp(ax_i)}{1+\exp(ax_i)}\right) = 
\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \sum_{i=1}^n -x_i(y_i-p(x_i)).$$`

---

(Simple) gradient descent to minimize `\(-\ell(a)\)` or maximize `\(L(y|a,x)\)` is:

1. Input `\(a_1, \gamma&gt;0, j_\max, \epsilon&gt;0, \frac{d}{da} -\ell(a)\)`.
2. For `\(j=1,2,\ldots,j_\max\)`,
`$$a_j = a_{j-1} - \gamma \frac{d}{da} (-\ell(a_{j-1}))$$`
3. Stop if `\(\epsilon &gt; |a_j - a_{j-1}|\)`.

---


```r
amle &lt;- function(x, y, a0, gam = 0.5, jmax = 50, eps = 1e-6){
  a &lt;- double(jmax) # place to hold stuff (always preallocate space)
  a[1] &lt;- a0 # starting value
  for (j in 2:jmax) { # avoid possibly infinite while loops
    px &lt;- logit(a[j - 1] * x) 
    grad &lt;- sum( -x * (y - px) ) 
    a[j] &lt;- a[j - 1] - gam * grad
    if (abs(grad) &lt; eps || abs(a[j] - a[j-1]) &lt; eps) break
  }
  a[1:j]
}
```

---
layout: false

## Try it:


```r
(too_big &lt;- amle(x, y, 5, .5))
```

```
##  [1] 5.000000 3.359839 2.018854 1.814590 2.059427 1.782188 2.113405 1.745887
##  [9] 2.179818 1.710849 2.250337 1.683968 2.309035 1.668789 2.344048 1.662555
## [17] 2.358831 1.660516 2.363718 1.659917 2.365158 1.659748 2.365566 1.659700
## [25] 2.365681 1.659687 2.365713 1.659683 2.365721 1.659682 2.365724 1.659682
## [33] 2.365725 1.659682 2.365725 1.659682 2.365725 1.659682 2.365725 1.659682
## [41] 2.365725 1.659682 2.365725 1.659682 2.365725 1.659682 2.365725 1.659682
## [49] 2.365725 1.659682
```

```r
(too_small &lt;- amle(x, y, 5, .01))
```

```
##  [1] 5.000000 4.967197 4.934454 4.901773 4.869155 4.836601 4.804113 4.771692
##  [9] 4.739338 4.707055 4.674842 4.642702 4.610636 4.578645 4.546731 4.514896
## [17] 4.483142 4.451469 4.419880 4.388377 4.356961 4.325634 4.294399 4.263257
## [25] 4.232210 4.201260 4.170409 4.139660 4.109015 4.078475 4.048044 4.017724
## [33] 3.987517 3.957425 3.927451 3.897599 3.867869 3.838266 3.808791 3.779449
## [41] 3.750241 3.721170 3.692240 3.663454 3.634814 3.606325 3.577988 3.549809
## [49] 3.521789 3.493932
```

```r
(just_right &lt;- amle(x, y, 5, .1))
```

```
##  [1] 5.000000 4.671968 4.350631 4.037548 3.734732 3.444772 3.170931 2.917187
##  [9] 2.688100 2.488370 2.321949 2.190810 2.093847 2.026714 1.982962 1.955817
## [17] 1.939571 1.930079 1.924617 1.921501 1.919733 1.918733 1.918169 1.917850
## [25] 1.917671 1.917570 1.917513 1.917481 1.917463 1.917452 1.917447 1.917443
## [33] 1.917442 1.917441 1.917440
```

---

## Visual


.pull-left[

```r
negll &lt;- function(a) {
  -a * sum(x*y) - 
    rowSums(log(1 / (1 + exp(outer(a, x)))))
}
tb &lt;- tibble(a = too_big, negll = negll(a))
ts &lt;- tibble(a = too_small, negll = negll(a))
jr &lt;- tibble(a = just_right, negll = negll(a))
ppp &lt;- bind_rows(too_big = tb, 
                 too_small = ts, 
                 just_right = jr,
                 .id = "gamma")
g &lt;- ggplot(ppp, aes(a, negll)) +
  geom_point(aes(color = gamma)) + 
  facet_wrap(~gamma, ncol = 1) +
  stat_function(fun = negll, xlim = c(-2.5,5)) +
  scale_y_log10() + 
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") 
coef(glm(y ~ x - 1, family = "binomial"))
```

```
##        x 
## 1.917439
```
]

.pull-right[

```r
g
```

![](rmd_gfx/00-gradient-descent/unnamed-chunk-16-1.svg)&lt;!-- --&gt;
]

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
