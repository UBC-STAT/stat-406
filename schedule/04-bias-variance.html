<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>04 Bias and variance</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="materials/libs/header-attrs/header-attrs.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="materials/slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 04 Bias and variance
### STAT 406
### Daniel J. McDonald
### Last modified - 2021-09-15

---







## What is bias?

We need to be more specific about what we mean when we say __bias__.

Bias is neither good nor bad in and of itself.

A very simple example: let `\(Z_1,\ldots,Z_n \sim N(\mu, 1)\)`.
  - We don't know `\(\mu\)`, so we try to use the data (the `\(Z_i\)`'s) to estimate it.
  
  - I propose 3 estimators: 
      1. `\(\widehat{\mu}_1 = 12\)`, 
  
      2. `\(\widehat{\mu}_2=Z_6\)`, 
  
      3. `\(\widehat{\mu}_3=\overline{Z}\)`.
  
  - The __bias__ (by definition) of my estimator is `\(E\left[\widehat{\mu}\right]-\mu\)`.
  
--

Calculate the bias and variance of each estimator.

$$
\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
$$
  
---


## Regression in general

If I want to predict `\(Y\)` from `\(X\)`, it is almost always the case that

$$
\mu(x) = \Expect{Y\given X=x} \neq x^{\top}\beta
$$

So the __bias__ of using a linear model is __not__ zero.

We can include as many predictors as we like, 

but this doesn't change the fact that the world is __non-linear__.

---

## Predicting new Y's

.emphasis[
Suppose we know that we want to predict a quantity `\(Y\)`, 

where `\(E[Y]= \mu \in \mathbb{R}\)` and `\(\textrm{Var}[Y] = 1\)`.  


Our data is `\(\{y_1,\ldots,y_n\}\)`

We want to estimate `\(\mu\)` 
]

--

Let's try one more: `\(\widehat Y_a = a\overline{Y}_n\)` for some `\(a \in (0,1]\)`.
  
$$
  R_n(\widehat Y_a) = \Expect{(\widehat Y_a-Y)^2} = (1 - a)^2\mu^2 +
  \frac{a^2}{n} +1 
$$
  
We can minimize this in `\(a\)` to get the best possible prediction risk for an estimator of the form `\(\widehat Y_a\)`: 
  
$$
\arg\min_{a} R_n(\widehat Y_a) = \left(\frac{\mu^2}{\mu^2 + 1/n} \right)
$$

--

What happens if `\(\mu \ll 1\)`?
  
---
class: middle
  
.alert[
Wait a minute! You're saying there is a __better__ estimator than `\(\overline{Y}_n\)`?
]



---

## Bias-variance tradeoff: Estimating the mean

$$
R_n(\widehat Y_a) = (a - 1)^2\mu^2 +  \frac{a^2}{n} + \sigma^2
$$


```r
mu = 1; n = 5; sig2 = 1
```

&lt;img src="rmd_gfx/04-bias-variance/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

---

## To restate

If `\(\mu=\)` 1 and `\(n=\)` 5 

then it is better to predict with 0.83 `\(\overline{Y}_5\)` 

than with `\(\overline{Y}_5\)` itself.  

--

For this `\(a =\)` 0.83 and `\(n=5\)`

1. `\(R_5(\widehat{Y}_a) =\)` 1.17

2. `\(R_5(\overline{Y}_5)=\)` 1.2

---

## Prediction risk


$$
R_n(f) = \Expect{(Y - f(X))^2}
$$
  
Why care about `\(R_n(f)\)`? 


<svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0076A5;overflow:visible;position:relative;"><path d="M416 208H272V64c0-17.67-14.33-32-32-32h-32c-17.67 0-32 14.33-32 32v144H32c-17.67 0-32 14.33-32 32v32c0 17.67 14.33 32 32 32h144v144c0 17.67 14.33 32 32 32h32c17.67 0 32-14.33 32-32V304h144c17.67 0 32-14.33 32-32v-32c0-17.67-14.33-32-32-32z"/></svg> Measures predictive accuracy on average.

<svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0076A5;overflow:visible;position:relative;"><path d="M416 208H272V64c0-17.67-14.33-32-32-32h-32c-17.67 0-32 14.33-32 32v144H32c-17.67 0-32 14.33-32 32v32c0 17.67 14.33 32 32 32h144v144c0 17.67 14.33 32 32 32h32c17.67 0 32-14.33 32-32V304h144c17.67 0 32-14.33 32-32v-32c0-17.67-14.33-32-32-32z"/></svg> How much confidence should you have in `\(f\)`'s predictions.

<svg aria-hidden="true" role="img" viewBox="0 0 448 512" style="height:1em;width:0.88em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#0076A5;overflow:visible;position:relative;"><path d="M416 208H272V64c0-17.67-14.33-32-32-32h-32c-17.67 0-32 14.33-32 32v144H32c-17.67 0-32 14.33-32 32v32c0 17.67 14.33 32 32 32h144v144c0 17.67 14.33 32 32 32h32c17.67 0 32-14.33 32-32V304h144c17.67 0 32-14.33 32-32v-32c0-17.67-14.33-32-32-32z"/></svg> Compare with other models.

<svg aria-hidden="true" role="img" viewBox="0 0 512 512" style="height:1em;width:1em;vertical-align:-0.125em;margin-left:auto;margin-right:auto;font-size:inherit;fill:#DB0B5B;overflow:visible;position:relative;"><path d="M440.5 88.5l-52 52L415 167c9.4 9.4 9.4 24.6 0 33.9l-17.4 17.4c11.8 26.1 18.4 55.1 18.4 85.6 0 114.9-93.1 208-208 208S0 418.9 0 304 93.1 96 208 96c30.5 0 59.5 6.6 85.6 18.4L311 97c9.4-9.4 24.6-9.4 33.9 0l26.5 26.5 52-52 17.1 17zM500 60h-24c-6.6 0-12 5.4-12 12s5.4 12 12 12h24c6.6 0 12-5.4 12-12s-5.4-12-12-12zM440 0c-6.6 0-12 5.4-12 12v24c0 6.6 5.4 12 12 12s12-5.4 12-12V12c0-6.6-5.4-12-12-12zm33.9 55l17-17c4.7-4.7 4.7-12.3 0-17-4.7-4.7-12.3-4.7-17 0l-17 17c-4.7 4.7-4.7 12.3 0 17 4.8 4.7 12.4 4.7 17 0zm-67.8 0c4.7 4.7 12.3 4.7 17 0 4.7-4.7 4.7-12.3 0-17l-17-17c-4.7-4.7-12.3-4.7-17 0-4.7 4.7-4.7 12.3 0 17l17 17zm67.8 34c-4.7-4.7-12.3-4.7-17 0-4.7 4.7-4.7 12.3 0 17l17 17c4.7 4.7 12.3 4.7 17 0 4.7-4.7 4.7-12.3 0-17l-17-17zM112 272c0-35.3 28.7-64 64-64 8.8 0 16-7.2 16-16s-7.2-16-16-16c-52.9 0-96 43.1-96 96 0 8.8 7.2 16 16 16s16-7.2 16-16z"/></svg> __This is hard:__  Don't know the distribution of the data (if I knew the truth, this would be easy)

  
---

## Bias-variance decomposition


So,

1. prediction risk  =  `\(\textrm{bias}^2\)`  +  variance  +  irreducible error 

2. estimation risk  =  `\(\textrm{bias}^2\)`  +  variance
    

What is `\(R_n(\widehat{Y}_a)\)` for our estimator `\(\widehat{Y}_a=a\overline{Y}_n\)`?


`\begin{aligned}
\textrm{bias}(\widehat{Y}_a) &amp;= \Expect{a\overline{Y}_n} - \mu=(a-1)\mu\\
\textrm{var}(\widehat f(x)) &amp;= \Expect{ \left(a\overline{Y}_n - \Expect{a\overline{Y}_n}\right)^2}
=a^2\Expect{\left(\overline{Y}_n-\mu\right)^2}=\frac{a^2}{n} \\
\sigma^2 &amp;= \Expect{(Y-\mu)^2}=1
\end{aligned}`

--

That is: `\(R_n(\widehat{Y}_a)=(a - 1)^2\mu^2 + \frac{a^2}{n} + 1\)`


---

## Bias-variance decomposition

.alert[  
__Important implication:__ prediction risk is proportional to estimation risk.  However, defining estimation risk requires stronger assumptions.
]

--

.emphasis[
In order to make good predictions, we want our prediction risk to be small.  This means that we want to "balance" the bias and variance.
]

---
  


&lt;img src="rmd_gfx/04-bias-variance/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---

## Bias-variance tradeoff: Overview

 __bias:__ how well does `\(\widehat{f}(x)\)` approximate the truth `\(\Expect{Y\given X=x}\)`

* If we allow more complicated possible `\(f\)`, lower bias. Flexibility `\(\Rightarrow\)` Expressivity

* But, more flexibility `\(\Rightarrow\)` larger variance

* Complicated models are hard to estimate precisely for fixed `\(n\)`

* Irreducible error

--

Since we can't take expectations...

---
class: inverse, center, middle

# Next time...


Estimating risk
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
