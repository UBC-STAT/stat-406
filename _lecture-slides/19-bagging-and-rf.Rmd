---
title: "19 Bagging and random forests"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      beforeInit: materials/macros.js
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
library(randomForest)
```



## Bagging

Many methods (trees, nonparametric smoothers) tend to have low bias but high variance.  

Especially fully grown trees (that's why we prune them)


High-variance means if we split the training data into two parts at random and fit a decision tree to each part, the results will be quite different.


In contrast, a low variance estimator would yield similar results if applied it to distinct data sets (consider $\widehat{f} = 0$).


__Bagging__, short for __bootstrap aggregation__, is a general purpose procedure for reducing variance. 

We'll use it specifically in the context of trees, but it can be applied much more broadly.

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$


---

## Bagging: The main idea


Suppose we have $n$ uncorrelated observations $Z_1, \ldots, Z_n$, each with variance $\sigma^2$.


What is the variance of

$$\overline{Z} = \frac{1}{n} \sum_{i=1}^n Z_i?$$



More generally, if we have $B$ separate (uncorrelated) training sets, $1, \ldots, B$, 

we can form $B$ separate model fits, 
$\widehat{f}^1(x), \ldots, \widehat{f}^B(x)$, and then average them:

$$\widehat{f}_{B}(x) = \frac{1}{B} \sum_{b=1}^B \widehat{f}^b(x)$$


---

## Bagging: The bootstrap part

Of course, this isn't practical: we don't have many training sets.  

We therefore turn to the bootstrap to simulate having many training sets.

Suppose we have data $Z_1, \ldots, Z_n$ 

1. Choose some large number of samples, $B$. 
2. For each $b = 1,\ldots,B$, draw a resample from $Z_1, \ldots, Z_n$, call it 
$\widetilde{Z}_1, \ldots, \widetilde{Z}_n$. 
3. Compute $\widehat{f}^b = \widehat{f}(\widetilde{Z}_1, \ldots, \widetilde{Z}_n)$.

Then average them:

$$\widehat{f}_{\textrm{bag}}(x) = \frac{1}{B} \sum_{b=1}^B \widehat{f}^b(x)$$

This process is known as Bagging

---

## Bagging trees

.pull-left[.center[
![](gfx/bagtree.jpg)
]]

.pull-right[

The procedure for trees is the following


1. Choose a large number $B$.
2. For each $b = 1,\ldots, B$, grow an unpruned tree on the $b^{th}$ bootstrap draw from the data.
3. Average all these trees together.



Each tree, since it is unpruned, will have 

.primary[low]/.secondary[high] variance

.primary[low]/.secondary[high] bias



Therefore averaging many trees results in an estimator that has .primary[lower]/.secondary[higher] variance and .primary[low]/.secondary[high] bias.

]

---

## Bagging trees: Variable importance measures


Bagging can improve predictive performance of trees 

We sacrificed some .hand[interpretability]. 

We no longer have that nice diagram that shows the segmentation of the predictor space 

(more accurately, we have $B$ of them).  

To recover some information, we can do the following:

.emphasis[

1. For each of the $b$ trees and each of the $p$ variables, we record the amount that the Gini index is reduced
by the addition of that variable 
2. Report the average reduction over all $B$ trees.
]

---


## Random Forest

Random Forest is an extension of Bagging, in which the bootstrap trees are __decorrelated__.  


The idea is, we draw a bootstrap sample and start to build a tree. 

* At each split, we randomly select
$m$ of the possible $p$ predictors as candidates for the split. 
* A new sample of size $m$ of the predictors is taken at each split. 
   


Usually, we use about $m = \sqrt{p}$ 



In other words, at each split, we .hand-blue[aren't even allowed to consider the majority of possible predictors!]

---

## What is going on here?


Suppose there is 1 really strong predictor and many mediocre ones. 


* Then each tree will have this one predictor in it,

* Therefore, each tree will look very similar (i.e. highly correlated).  

* Averaging highly correlated things leads to much less variance reduction than if they were uncorrelated.

If we don't allow some trees/splits to use this important variable, each of the trees will be much less similar and
hence much less correlated.


Bagging Trees is Random Forest when $m = p$, that is, when we can consider all the variables at each split.

---

## Example with Mobility data

.pull-left[
```{r mobility-rf}
data(mobility, package="UBCstat406labs")
mob = mobility %>% 
  mutate(mobile=as.factor(Mobility>.1)) %>%
  dplyr::select(-ID,-Name,-Mobility,-State) %>% 
  drop_na()
n = nrow(mob)
trainidx = sample.int(n, floor(n*.75))
testidx = setdiff(1:n, trainidx)
train = mob[trainidx,]; test=mob[testidx,]
rf = randomForest(mobile~., data=train)
bag = randomForest(mobile~., data=train,
  mtry=ncol(mob)-1)
preds = tibble(
  truth=test$mobile,
  rf = predict(rf, test),
  bag = predict(bag, test))
cbind(table(preds$truth, preds$rf), 
      table(preds$truth, preds$bag))
```
]

.pull-right[
```{r mobility-results}
varImpPlot(rf)
```
]


---

## One last thing...

On average, drawing $n$ samples from $n$ observations with replacement (bootstrapping) results in .hand-blue[~ 2/3] of the observations being selected. (Can you show this?)


The remaining ~ 1/3 of the observations not used on that tree.

These are referred to as __out-of-bag (OOB)__.


We can think of it as a for-free cross-validation.  


Each time a tree is grown, we can get its prediction error on the unused observations.  

We average this over all bootstrap samples.


---

## Out-of-bag error estimation for bagging

```{r}
tab = table(predict(bag), train$mobile)
tab
1-sum(diag(tab))/sum(tab) ## misclassification error
```


---
class: inverse, middle, center

# Next time...

Boosting