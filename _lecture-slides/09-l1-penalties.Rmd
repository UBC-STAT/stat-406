---
title: "09 L1 penalties"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Last time

$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\E}{\mathbb{E}}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\brt}{\widehat{\beta}^R_{s}}
\newcommand{\brl}{\widehat{\beta}^R_{\lambda}}
\newcommand{\bls}{\widehat{\beta}_{ols}}
\newcommand{\blt}{\widehat{\beta}^L_{s}}
\newcommand{\bll}{\widehat{\beta}^L_{\lambda}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$



Ridge regression: $\min ||\y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_2^2 \leq s$ 

Best linear regression model of size $t$: $\min ||\y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_0 \leq t$

$||\beta||_0$ is the number of nonzero elements in $\beta$

Finding the best linear model (of size $s$, among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)

Ridge regression is convex (easy to solve), but doesn't do model selection

Can we somehow "interpolate" to get both?



---

## Geometry of convexity

```{r plotting-functions, echo=FALSE}
library(mvtnorm)
normBall <- function(q=1, len=1000){
  tg = seq(0,2*pi, length=len)
  out = data.frame(x = cos(tg)) %>%
    mutate(b=(1-abs(x)^q)^(1/q), bm=-b) %>%
    gather(key='lab', value='y',-x)
  out$lab = paste0('"||" * beta * "||"', '[',signif(q,2),']')
  return(out)
}

ellipseData <- function(n=100,xlim=c(-2,3),ylim=c(-2,3), 
                        mean=c(1,1), Sigma=matrix(c(1,0,0,.5),2)){
  df = expand.grid(x=seq(xlim[1],xlim[2],length.out = n),
                   y=seq(ylim[1],ylim[2],length.out = n))
  df$z = dmvnorm(df, mean, Sigma)
  df
}
lballmax <- function(ed,q=1,tol=1e-6){
  ed = filter(ed, x>0,y>0)
  for(i in 1:20){
    ff = abs((ed$x^q+ed$y^q)^(1/q)-1)<tol
    if(sum(ff)>0) break
    tol = 2*tol
  }
  best = ed[ff,]
  best[which.max(best$z),]
}
```


```{r convexity,echo=FALSE, dev="svg", fig.height=6,fig.width=11,fig.align="center"}
nbs = list()
nbs[[1]] = normBall(0,1)
qs = c(.5,.75,1,1.5,2)
for(ii in 2:6) nbs[[ii]] = normBall(qs[ii-1])
nbs = bind_rows(nbs)
nbs$lab = factor(nbs$lab, levels = unique(nbs$lab))
seg = data.frame(lab=levels(nbs$lab)[1],
                 x0=c(-1,0),x1=c(1,0),y0=c(0,-1),y1=c(0,1))
levels(seg$lab) = levels(nbs$lab)
ggplot(nbs, aes(x,y)) + geom_path(size=1.2) + 
  facet_wrap(~lab,labeller = label_parsed) + 
  geom_segment(data=seg,aes(x=x0,xend=x1,y=y0,yend=y1),size=1.2) + 
  coord_equal() + geom_vline(xintercept = 0,size=.5) + 
  geom_hline(yintercept = 0,size=.5) +
  theme_cowplot() + xlab("") + ylab("")
```

---

## The best of both worlds


```{r, echo=FALSE, warning=FALSE, dev="svg", fig.height=4,fig.width=4,fig.align="center"}
nb = normBall(1)
ed = ellipseData()
bols = data.frame(x=1,y=1)
bhat = lballmax(ed, 1)
ggplot(nb,aes(x,y)) + xlim(-2,2) + ylim(-2,2) + geom_path(color=red) + 
  geom_contour(mapping=aes(z=z), color=blue, data=ed, bins=7) +
  geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + 
  geom_point(data=bols) + coord_equal() +
  geom_label(data=bols, mapping=aes(label=bquote('hat(beta)[ols]')), parse=TRUE,
             nudge_x = .3, nudge_y = .3) +
  geom_point(data=bhat) + xlab(expression(beta[1])) + ylab(expression(beta[2])) + 
  geom_label(data=bhat, mapping=aes(label=bquote('hat(beta)[s]^L')), parse=TRUE,
             nudge_x = -.4, nudge_y = -.4) +
  theme_cowplot()
```

This regularization set...

* ... is convex (computationally efficient)
* ... has corners (performs variable selection)

---

## $\ell_1$-regularized regression

Known as 

* "lasso"
* "basis pursuit"

The estimator satisfies

$$\blt = \arg\min_{ ||\beta||_1 \leq s}  \frac{1}{n}||\y-\X\beta||_2^2$$


In its corresponding Lagrangian dual form:

$$\bll = \arg\min_{\beta} \frac{1}{n}||\y-\X\beta||_2^2 + \lambda ||\beta||_1$$

---

## Lasso

While the ridge solution can be easily computed 

$$\brl = \arg\min_{\beta} ||\y-\X\beta||_2^2 + \lambda ||\beta||_2^2 = (\X^{\top}\X + \lambda \mathbf{I})^{-1} \X^{\top}\y$$


the lasso solution


$$\bll = \arg\min_{\beta} ||\y-\X\beta||_2^2 + \lambda ||\beta||_1 = \; ??$$

doesn't have a closed form solution.


However, because the optimization problem is convex, there exist efficient algorithms for computing it

---

## Coefficient path: ridge vs lasso


```{r ridge-v-lasso,echo=TRUE,dev="svg",fig.align="center", fig.width=11, fig.height=4}
library(glmnet)
data(prostate, package = "ElemStatLearn")
X <- prostate %>% dplyr::select(-train,-lpsa) %>% as.matrix()
Y <- prostate$lpsa
lasso <- glmnet(x = X, y = Y) # alpha = 1 by default
ridge <- glmnet(x = X, y = Y, alpha = 0)
op <- par()
par(mfrow = c(1, 2), mar = c(5,3,3,.1))
plot(lasso, main = "Lasso")
plot(ridge, main = "Ridge")
par(op)
```

```{r tidy-glmnet, include = FALSE, eval = FALSE}
df = data.frame(as.matrix(t(ridge$beta)))
df1 = data.frame(as.matrix(t(lasso$beta)))
df$l1norm = colSums(abs(ridge$beta))
df1$l1norm = colSums(abs(lasso$beta))
df$method = 'ridge'
df1$method = 'lasso'
bind_rows(df,df1) %>%
  pivot_longer(names_to='predictor',values_to ='coefficient',
               cols=-c(l1norm,method)) %>%
  ggplot(aes(x=l1norm, y=coefficient, color=predictor)) + geom_path() + 
  facet_wrap(~method,scales = 'free_x') + 
  geom_hline(color="black",linetype="dotted",yintercept = 0) +
  scale_color_brewer(palette = 'Set1') + theme_cowplot() 
```

---

## Packages

There are two main `R` implementations for finding lasso


`{glmnet}`: `lasso.out = glmnet(X, Y, alpha=1)`.  

* Setting `alpha=0` gives ridge regression (as does `lm.ridge` in the `MASS` package)
* Setting `alpha` $\in (0,1)$ gives a method called the "elastic net" which combines ridge regression and lasso, more on that next lecture.
* If you don't specify `alpha`, it does lasso

`{lars}`: `lars.out = lars(X, Y)`
* `lars` also does other things called "Least angle" and "forward stagewise" in addition to "forward stepwise" regression

---

## (lots of others, but these are the biggies)

1. `lars` (this one came first)
2. `glmnet` (this one is faster)

Use different algorithms, but both compute the solution for a range of $\lambda$.

`lars` starts with an empty model and adds coefficients until saturated. The sequence of $\lambda$'s comes from the nature of the optimization problem.

`glmnet` starts with an empty model and examines each value of $\lambda$ using previous values as "warm starts". It is generally much faster than `lars` and uses lots of other tricks (as well as compiled code) for extra speed.

The path returned by `lars` is more useful than that returned by `glmnet`.

--

But you should use `glmnet`.




---



## Choosing the lambda

You have to choose $\lambda$ in lasso or in ridge regression

lasso selects variables (by setting coefficients to zero), but the value of $\lambda$ determines how many/which.

All of these packages come with CV built in.

However, the way to do it differs from package to package

--

<p align="center"><iframe src="https://giphy.com/embed/fYfeQAOD8pSjN7M0jY" width="480" height="270" frameBorder="0" class="giphy-embed"></iframe></p>

---


## `glmnet` version (lasso or ridge)

.pull-left[
```{r}
# 1. Estimate cv and model at once
# no formula version
lasso.glmnet = cv.glmnet(X, Y) 
# NEVER use glmnet() itself
# 2. Look at the CV curve
# 3. If the dashed lines are at the 
# boundaries, redo with better lambda
best.lam = lasso.glmnet$lambda.min 
# the value, not the location 
# (or use lasso$lambda.1se)
# 4. Return the coefs/predictions 
# for the best model
coefs.glmnet = coefficients(
  lasso.glmnet, s = "lambda.min")
preds.glmnet = predict(
  lasso.glmnet, newx = X, s = "lambda.1se") 
# must supply `newx`
```
]

.pull-right[
```{r, dev="svg", fig.align="center",fig.width=4, fig.height=5}
par(mfrow=c(2,1), mar=c(5, 3, 3, 0))
plot(lasso.glmnet) # a plot method for the cv fit
plot(lasso.glmnet$glmnet.fit) # the glmnet.fit == glmnet(X,Y)
```

```{r, include=FALSE}
par(op)
```
]

---

## Paths with chosen lambda

```{r, fig.width=11,fig.align="center",dev="svg",fig.height=4}
ridge.glmnet = cv.glmnet(X, Y, alpha = 0, lambda.min.ratio = 1e-10) # added to get a minimum
par(mfrow = c(1, 4))
plot(ridge.glmnet)
plot(lasso.glmnet)
plot(ridge.glmnet$glmnet.fit, main = 'Ridge')
abline(v = sum(abs(coef(ridge.glmnet)))) # defaults to `lambda.1se`
plot(lasso.glmnet$glmnet.fit, main = 'Lasso')
abline(v = sum(abs(coef(lasso.glmnet)))) # again, `lambda.1se` unless told otherwise
```

---
class: middle, inverse, center

# Next time...

What happens when we're tired of all this linearity.