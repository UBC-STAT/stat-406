---
title: "22 Neural nets - estimation"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      beforeInit: materials/macros.js
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Neural Network terms again

.pull-left[.center[
![:scale 75%](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1280px-Colored_neural_network.svg.png)]
]

.pull-right[

$$
\begin{aligned}
z_k   &= \sigma(\alpha_k^{\top}x) \quad  ( k = 1, \ldots K)\\
K &= \textrm{# hidden units}\\
w_g  &= \beta_g^{\top}z \quad  ( g = 1, \ldots G)\\
G &= \textrm{# of outputs}\\
\mu_g(x) &= L^{-1}(w_g)
\end{aligned}
$$


-   .hand[Regression]:  The link
    function is $L(u) = u$ (here, $G=1$)

-   .hand[Classification]:  With $G$
    classes, we are modeling $\pi_g = P(Y = g\ \vert\ X=x)$ and
    $L = \textrm{logit}$:
    
    $\hat{\pi}_g(x) = \frac{e^{w_g}}{\sum_{g'=1}^G e^{w_{g'}}}$
    
    $\hat{y} = \widehat{g}(x) = \arg\max_g \hat{\pi}_g(x)$
    
    This is called the __softmax__  function for
    historical reasons

]

---

## Training neural networks

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfx}{\mathbf{x}}$$

Neural networks have _many_
( __MANY__ ) unknown parameters

They are usually called __weights__ in this dialect and __biases__

 
For our simple network (1 hidden layer) these are

-   $\alpha_{k0}, \alpha_k \textrm{ for } k = 1,\ldots,K$ (total of
    $K(p+1)$ parameters)

-   $\beta_{g0}, \beta_g \textrm{ for } g = 1,\ldots,G$ (total of
    $G(K+1)$ parameters)

 
__Total parameters:__
$\asymp Kp + GK = K(p + G)$

---

## Common loss functions


__Regression:__ 
    $$\hat{R} = \sum_{i = 1}^n (y_i - \hat{y}_i)^2$$

__Classification:__ 
    Cross-entropy
    $$\hat{R} = -\sum_{i = 1}^n \sum_{g=1}^G y_{ig} \log( \hat{\pi}_g(x_i))$$

-   Here, $y_{ig}$ is an indicator variable for the $g^{th}$ class.
    In other words $Y_i \in \{0,1\}^G$

- Neural networks seamlessly incorporate multivariate response variables, even in regression

- With the softmax $+$ cross-entropy, a neural network is a linear multinomial logistic regression model in the hidden units

---

## Training neural networks

The simplest approach to minimizing $\hat{R}$ is via gradient
descent

 
In this dialect, we call it __back propagation__

 
Derivatives can be formed using the chain
rule and then computed via a forward and backward sweep

All the $\sigma$'s that get used have $\sigma'(u)$ "nice".

.pull-left[

__Our predictor__

$\mu(x) = \beta_0 + \sum_{g=1}^G \sum_{k=1}^{K}\beta_{gk} \sigma\left(\alpha_{k0} + \alpha^\top_{k}x\right)$

__Ignore the "biases"__ (for simplicity)

$\mu(x) = \sum_{g=1}^k\sum_{k=1}^K \beta_{gk} \sigma\left(\alpha^\top_{k}x\right)$
]

--

.pull-right[
 
__Derivatives__
 
$\frac{\partial \mu}{\partial \beta_{gk}} = \sigma(\alpha_k^{\top}x)  =: z_k$

$\frac{\partial \mu}{\partial \alpha_{k}} = \sum_{g=1}^G \beta_{gk}\sigma'( \alpha_k^{\top}x)x$

]

---

## The rest of the derivatives

For squared error, let $\hat{R}_i = (y_i - \hat{y}_i)^2$

$$\begin{aligned}
\frac{\partial \hat{R}_i}{\partial \beta_{gk}} 
& = -2(y_i - \hat{y}_i)z_{ik} \\
\frac{\partial \hat{R}_i}{\partial \alpha_{kj}}  
& = -2(y_i - \hat{y}_i)\beta_{gk} \sigma'(\alpha_k^{\top} x_i)x_{ij}\end{aligned}$$
Given these derivatives, a gradient descent update can be found
$$\begin{aligned}
\hat{\beta}_{gk}^{t+1} 
& = 
\hat{\beta}_{gk}^{t} - \gamma_t \sum_{i=1}^n \left. \frac{\partial \hat{R}_i}{\partial \vec\beta_{gk}} \right|_{\hat{\beta}_k^{t},\ \hat{\alpha}_k^t} \\
\hat{\alpha}_{kj}^{t+1} 
& = 
\hat{\alpha}_{kj}^{t}  - \gamma_t \sum_{i=1}^n \left. \frac{\partial \hat{R}_i}{\partial \alpha_{kj}} \right|_{\hat{\beta}^t_k,\ \hat{\alpha}_{k}^{t}} \end{aligned}$$

---

## Mapping it out (regression)

Given current $\beta,\ {\alpha}$, we want to get new, $\tilde\beta,\ \tilde\alpha$ 


.pull-left[
__Feed forward__ `r fa("arrow-down", fill=green)`

$\mathbf{a}_k = \X \alpha_k \in \R^n$

$\mathbf{z}_k = \sigma(\mathbf{a}_k)$ (component wise)

$\mathbf{z}'_k = \sigma'(\mathbf{a}_k)$

$w_g = \beta_g^\top \mathbf{z} \in \R^n$

$\hat{\mathbf{y}} = \sum_{g=1}^G w_g$

]

.pull-right[
__Back propogate__ `r fa("arrow-up", fill=orange)`

$\mathbf{r} = (\mathbf{y} - \widehat{\mathbf{y}})$

$\frac{\partial R}{\partial \beta_k} = \mathbf{r}^\top \mathbf{z}_k \in \R$

$\frac{\partial R}{\partial \alpha_{kj}} = -2\beta_k \mathbf{r}^\top (\mathbf{x}_j \odot \mathbf{z}'_k) \in \R$

$\tilde\beta_k = \beta_k - \gamma \frac{\partial R}{\partial \beta_k}$

$\tilde\alpha_{kj} = \alpha_{kj} - \gamma \frac{\partial R}{\partial \alpha_{kj}}$

]

Not much has to change if we alter $\sigma$ or $L$.

---

## Deep nets

Generalizing to multi-layer neural networks, we can specify any number
of hidden units:

$$\begin{aligned}
\textrm{0 Layer} & = \sigma( \alpha_{\textrm{lowest}}^{\top}x) \\
\textrm{1 Layer} &  = \sigma( \alpha_{\textrm{lowest} + 1}^{\top}(\textrm{0 Layer})) \\
&\quad \vdots  \\
\textrm{Top Layer} &  = \sigma( \alpha_{\textrm{Top} }^{\top}(\textrm{Top - 1 Layer})) \\
L(\mu_g(x)) & = \beta_{g0} + \beta_g^{\top}(\textrm{Top Layer}) \quad  ( g = 1, \ldots G)\end{aligned}$$


Some comments on adding layers:

-   It has been shown that one hidden layer is sufficient to approximate
    any bounded piecewise continuous function

- However, this may take a huge number of hidden units (i.e. $K \gg 1$). 

- This is what people mean when they say that NNets are "universal approximators"
    
-   By including multiple layers, we can have fewer hidden units per
    layer. 
    
- Also, we can encode (in)dependencies that can speed computations

---

## Simple example

```{r eval=FALSE}
n = 200
df = tibble(x = seq(.05, 1, length=n),
 y = sin(1/x) + rnorm(n, 0, .1) ## Doppler function
)
testdata = matrix(seq(.05, 1, length.out = 1e3), ncol=1)
library(neuralnet)
nn_out = neuralnet(y~x, data=df, hidden=c(10,5,15), threshold = 0.01, rep=3)
nn_preds = sapply(1:3, function(x) compute(nn_out, testdata, x)$net.result)
yhat = rowMeans(nn_results) # average over the runs
```

```{r eval=FALSE, cache=TRUE, echo=FALSE}
## This code will reproduce the analysis, takes some time
n = 200
df = tibble(x = seq(.05, 1, length=n),
 y = sin(1/x) + rnorm(n, 0, .1) ## Doppler function
)
testdata = matrix(seq(.05, 1, length.out = 1e3), ncol=1)
library(neuralnet)
library(splines)
fstar = sin(1/testdata)
fun <- function(k){
  X = bs(df$x, k)
  Xtest = bs(testdata, k)
  yhat = predict(lm(df$y~.,data=X), newdata=Xtest)
  mean((yhat-fstar)^2)
}
Ks = 1:15*10
SplineErr = sapply(Ks, fun) 

Jgrid = c(5,10,15)
NNerr = double(length(Jgrid)^3)
NNplot = character(length(Jgrid)^3)
sweep = 0
for(J1 in Jgrid){
  for(J2 in Jgrid){
    for(J3 in Jgrid){
      sweep = sweep + 1
      NNplot[sweep] = paste(J1,J2,J3,sep=' ')
      nn_out = neuralnet(y~x, df, hidden=c(J1,J2,J3), 
                         threshold=0.01,rep=3)      
      nn_results = sapply(1:3, function(x) 
        compute(nn_out, testdata, x)$net.result) 
      # Run them through the neural network  
      Yhat = rowMeans(nn_results)
      NNerr[sweep] = mean((Yhat - fstar)^2)  
    }
  }
}

bestK = Ks[which.min(SplineErr)]
X = bs(df$x, bestK)
Xtest = bs(testdata, bestK)
bestspline = predict(lm(df$y~.,data=X),newdata=Xtest)
besthidden = as.numeric(unlist(strsplit(NNplot[which.min(NNerr)],' ')))
nn_out = neuralnet(y~x, df, hidden=besthidden, threshold=0.01,rep=3)      
nn_results = sapply(1:3, function(x) 
  compute(nn_out, testdata, x)$net.result) 
      # Run them through the neural network  
bestnn = rowMeans(nn_results)
plotd = data.frame(x = testdata, spline = bestspline, nnet=bestnn,
                   truth=fstar)
```

```{r fun-nnet-spline, echo=FALSE, fig.align='center', fig.width=10, fig.height=4}
load("data-and-big-results/nnet-example.Rdata")
plotd %>%
  pivot_longer(-x) %>%
  ggplot(aes(x,value,color=name)) + geom_line() + 
  scale_color_manual(values=c(red, green, blue)) + theme_cowplot() +
  theme(legend.position = c(.75,.25), legend.title = element_blank()) +
  geom_point(data=df,mapping=aes(x,y),color='black', alpha=.2) 
```

---

## Different architectures

```{r nnet-vs-spline-plots, echo=FALSE, fig.align='center',fig.height=6,fig.width=12}
doppler_nnet = data.frame(x=NNplot,err=NNerr)
spl = data.frame(x=Ks,err=SplineErr)
best = c(min(NNerr),min(SplineErr))
g1 <- ggplot(doppler_nnet, aes(x,err,group=1)) +  
  ggtitle('Neural Nets') + xlab('architecture') + theme_cowplot() + 
  theme(axis.text.x = element_text(angle = 90,vjust=.5)) + 
  geom_line(color=orange) + geom_hline(yintercept = best[1], color=red) +
  geom_hline(yintercept = best[2], color=green) 
g2 <- ggplot(spl, aes(x,err)) + ggtitle('Splines') + xlab('degrees of freedom') +
  geom_line(color=orange) + geom_hline(yintercept = best[1], color=red) +
  geom_hline(yintercept = best[2], color=green) + theme_cowplot()
plot_grid(g1,g2)
```

---
class: middle,center,inverse

# Next time...

Other considerations
