---
title: "22 Neural nets - estimation"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Neural Network terms again (T hidden layers, regression)

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\bfy}{\mathbf{y}}
\newcommand{\bfx}{\mathbf{x}}$$


.pull-left[

$$\begin{aligned}
A_{k}^{(1)} &= g\left(\sum_{j=1}^p w^{(1)}_{k,j} x_j\right)\\
A_{\ell}^{(t)} &= g\left(\sum_{k=1}^{K_{t-1}} w^{(t)}_{\ell,k} A_{k}^{(t-1)} \right)\\
z_m &= \sum_{\ell=1}^{K_T} \beta_{m,\ell} A_{\ell}^{(T)}\\
\hat{Y} &= z_m \ \ (M=1)
\end{aligned}$$

Notes:
> $B \in \R^{M\times K_T}$. $M=1$ for regression  
$\mathbf{W}_t \in \R^{K_2\times K_1}$ $t=1,\ldots,T$  


]

.pull-right[

![:scale 200%](gfx/single-layer-net.svg)
]

---

## Training neural networks, first, choices



* Choose the architecture: how many layers, units per layer, what connections?

* Choose the loss: common choices (for each data point $i$)


__Regression:__  $\hat{R}_i = \frac{1}{2}(y_i - \hat{y}_i)^2$ (the 1/2 just makes the derivative nice)

__Classification:__ $\hat{R}_i = I(y_i = m)\log( 1 + \exp(-z_{im}))$

* Choose the activation function $g$


---

## Training neural networks


We need to estimate $B$, $\mathbf{W}_t$ $t=1,\ldots,T$

We want to minimize $\hat{R} = \sum_{i=1}^n \hat{R}_i$ as a function of all this.


We use gradient descent, but in this dialect, we call it __back propagation__

 
Derivatives can be formed using the chain
rule and then computed via a forward and backward sweep

All the $g(u)$'s that get used have $g'(u)$ "nice".

--

If $g$ is ReLu: $g(u) = xI(x>0)$, $g'(u) = I(x>0)$

Get derivatives from backprop, then

$$\begin{align}
\widetilde{B} &\leftarrow B - \gamma \frac{\partial \widehat{R}}{\partial B}\\
\widetilde{\mathbf{W}_t} &\leftarrow \mathbf{W}_t - \gamma \frac{\partial \widehat{R}}{\partial \mathbf{W}_t}
\end{align}$$

---

## Chain rule


We want $\frac{\partial}{\partial B} \hat{R}_i$ and $\frac{\partial}{\partial W_{t}}\hat{R}_i$ for all $t$.

__Regression__: $\hat{R}_i = \frac{1}{2}(y_i - \hat{y}_i)^2$

$$\begin{aligned}
\frac{\partial\hat{R}_i}{\partial B} &= -(y_i - \hat{y}_i)\frac{\partial \hat{y_i}}{\partial B} =\underbrace{-(y_i - \hat{y}_i)}_{-r_i}  \mathbf{A}^{(T)}\\
\frac{\partial}{\partial \mathbf{W}_T} \hat{R}_i &= -(y_i - \hat{y}_i)\frac{\partial\hat{y_i}}{\partial \mathbf{W}_T} = -r_i \frac{\partial \hat{y}_i}{\partial \mathbf{A}^{(T)}} \frac{\partial \mathbf{A}^{(T)}}{\partial \mathbf{W}_T}\\ 
&= -\left(r_i  B \odot g'(\mathbf{W}_T \mathbf{A}^{(T)}) \right) \left(\mathbf{A}^{(T-1)}\right)^\top\\
\frac{\partial}{\partial \mathbf{W}_{T-1}} \hat{R}_i &= -(y_i - \hat{y}_i)\frac{\partial\hat{y_i}}{\partial \mathbf{W}_{T-1}} = -r_i \frac{\partial \hat{y}_i}{\partial \mathbf{A}^{(T)}} \frac{\partial \mathbf{A}^{(T)}}{\partial \mathbf{W}_{T-1}}\\
&= -r_i \frac{\partial \hat{y}_i}{\partial \mathbf{A}^{(T)}} \frac{\partial \mathbf{A}^{(T)}}{\partial \mathbf{W}_{T}}\frac{\partial \mathbf{W}_{T}}{\partial \mathbf{A}^{(T-1)}}\frac{\partial \mathbf{A}^{(T-1)}}{\partial \mathbf{W}_{T-1}}\\
\cdots &= \cdots
\end{aligned}$$

---

## Mapping it out 

Given current $\mathbf{W}_t, B$, we want to get new, $\widetilde{\mathbf{W}}_t,\ \widetilde B$ for $t=1,\ldots,T$

* Squared error for regression, cross-entropy for classification

.pull-left[
__Feed forward__ `r fa("arrow-down", fill=green)`

$$\mathbf{A}^{(0)} = \mathbf{X}  \in \R^{n\times p}$$

Repeat, $t= 1,\ldots, T$
1. $\mathbf{Z}_{t} = \mathbf{A}^{(t-1)}\mathbf{W}_t \in \R^{n\times K_t}$
1. $\mathbf{A}^{(t)} = g(\mathbf{Z}_{t})$ (component wise)
1. $\dot{\mathbf{A}}^{(t)} = g'(\mathbf{Z}_t)$

$$\begin{cases}
\hat{\mathbf{y}} =\mathbf{A}^{(T)} B \in \R^n \\
\hat{\Pi} = \left(1 + \exp\left(-\mathbf{A}^{(T)}\mathbf{B}\right)\right)^{-1} \in \R^{n \times M}\end{cases}$$

]

.pull-right[
__Back propogate__ `r fa("arrow-up", fill=orange)`



$$\mathbf{r} = 
\begin{cases}
-\left(\mathbf{y} - \widehat{\mathbf{y}}\right)\\
- \left(1 - \widehat{\Pi}\right)[y]
\end{cases}$$

$$\begin{aligned}
\frac{\partial}{\partial \mathbf{B}} \widehat{R} &= \left(\mathbf{A}^{(T)}\right)^\top \mathbf{r}\\
\boldsymbol{\Gamma} &\leftarrow \mathbf{r}\\
\mathbf{W}_{T+1} &\leftarrow \mathbf{B}
\end{aligned}$$


Repeat, $t = T,...,1$,
1. $\boldsymbol{\Gamma} \leftarrow \left(\boldsymbol{\Gamma} \mathbf{W}_{t+1}\right) \odot\dot{\mathbf{A}}^{(t)}$
1. $\frac{\partial R}{\partial \mathbf{W}_t} =\left(\mathbf{A}^{(t)}\right)^\top \Gamma$

]


---

## Deep nets


Some comments on adding layers:

-   It has been shown that one hidden layer is sufficient to approximate
    any bounded piecewise continuous function

- However, this may take a huge number of hidden units (i.e. $K_1 \gg 1$). 

- This is what people mean when they say that NNets are "universal approximators"
    
-   By including multiple layers, we can have fewer hidden units per
    layer. 
    
- Also, we can encode (in)dependencies that can speed computations

- We don't have to connect everything the way we have been

---

## Simple example

```{r eval=FALSE}
n = 200
df = tibble(x = seq(.05, 1, length=n),
 y = sin(1/x) + rnorm(n, 0, .1) ## Doppler function
)
testdata = matrix(seq(.05, 1, length.out = 1e3), ncol=1)
library(neuralnet)
nn_out = neuralnet(y~x, data=df, hidden=c(10,5,15), threshold = 0.01, rep=3)
nn_preds = sapply(1:3, function(x) compute(nn_out, testdata, x)$net.result)
yhat = rowMeans(nn_results) # average over the runs
```

```{r eval=FALSE, cache=TRUE, echo=FALSE}
## This code will reproduce the analysis, takes some time
n = 200
df = tibble(x = seq(.05, 1, length=n),
 y = sin(1/x) + rnorm(n, 0, .1) ## Doppler function
)
testdata = matrix(seq(.05, 1, length.out = 1e3), ncol=1)
library(neuralnet)
library(splines)
fstar = sin(1/testdata)
fun <- function(k){
  X = bs(df$x, k)
  Xtest = bs(testdata, k)
  yhat = predict(lm(df$y~.,data=X), newdata=Xtest)
  mean((yhat-fstar)^2)
}
Ks = 1:15*10
SplineErr = sapply(Ks, fun) 

Jgrid = c(5,10,15)
NNerr = double(length(Jgrid)^3)
NNplot = character(length(Jgrid)^3)
sweep = 0
for(J1 in Jgrid){
  for(J2 in Jgrid){
    for(J3 in Jgrid){
      sweep = sweep + 1
      NNplot[sweep] = paste(J1,J2,J3,sep=' ')
      nn_out = neuralnet(y~x, df, hidden=c(J1,J2,J3), 
                         threshold=0.01,rep=3)      
      nn_results = sapply(1:3, function(x) 
        compute(nn_out, testdata, x)$net.result) 
      # Run them through the neural network  
      Yhat = rowMeans(nn_results)
      NNerr[sweep] = mean((Yhat - fstar)^2)  
    }
  }
}

bestK = Ks[which.min(SplineErr)]
X = bs(df$x, bestK)
Xtest = bs(testdata, bestK)
bestspline = predict(lm(df$y~.,data=X),newdata=Xtest)
besthidden = as.numeric(unlist(strsplit(NNplot[which.min(NNerr)],' ')))
nn_out = neuralnet(y~x, df, hidden=besthidden, threshold=0.01,rep=3)      
nn_results = sapply(1:3, function(x) 
  compute(nn_out, testdata, x)$net.result) 
      # Run them through the neural network  
bestnn = rowMeans(nn_results)
plotd = data.frame(x = testdata, spline = bestspline, nnet=bestnn,
                   truth=fstar)
```

```{r fun-nnet-spline, echo=FALSE, fig.align='center', fig.width=10, fig.height=4}
load("data/nnet-example.Rdata")
plotd %>%
  pivot_longer(-x) %>%
  ggplot(aes(x, value, color=name)) + geom_line(size=1.5) + ylab("y") +
  scale_color_manual(values=c(red, orange, blue)) + 
  theme(legend.title = element_blank()) +
  geom_point(data=df, mapping=aes(x,y),
             color='black', alpha=.4, shape = 16) 
```

---

## Different architectures

```{r nnet-vs-spline-plots, echo=FALSE, fig.align='center',fig.height=6,fig.width=12}
doppler_nnet = data.frame(x=NNplot,err=NNerr)
spl = data.frame(x=Ks,err=SplineErr)
best = c(min(NNerr),min(SplineErr))
rel <- function(x) abs(x)/.01
g1 <- ggplot(doppler_nnet, aes(x,rel(err),group=1)) +  
  ggtitle('Neural Nets') + xlab('architecture') + 
  theme(axis.text.x = element_text(angle = 90,vjust=.5)) + 
  geom_line(color=orange) + ylab("Increase in error over f*") +
  scale_y_continuous(labels = scales::percent_format()) +
  geom_hline(yintercept = rel(best[1]), color=red) +
  geom_hline(yintercept = rel(best[2]), color=green) 
g2 <- ggplot(spl, aes(x, rel(err))) + ggtitle('Splines') +
  xlab('degrees of freedom') +
  geom_line(color=orange) + ylab("Increase in error over f*") +
  scale_y_continuous(labels = scales::percent_format()) +
  geom_hline(yintercept = rel(best[1]), color=red) +
  geom_hline(yintercept = rel(best[2]), color=green) 
plot_grid(g1,g2)
```

---
class: middle,center,inverse

# Next time...

Other considerations
