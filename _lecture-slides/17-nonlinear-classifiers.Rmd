---
title: "17 Nonlinear classifiers"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Last time

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$

We reviewed logistic regression

$$\begin{aligned}
\P(Y = 1 \given X=x)  & = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}} \\
\P(Y = 0 \given X=x) & = \frac{1}{1 + \exp\{\beta_0 + \beta^{\top}x\}}=1-\frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}\end{aligned}$$

--

We can make LDA or logistic regression have non-linear decision boundaries by mapping the features to a higher dimension (just like with regular regression)

Say:

__Polynomials__

$(x_1, x_2) \mapsto \left(1,\ x_1,\ x_1^2,\ x_2,\ x_2^2,\ x_1 x_2\right)$

```{r simple-lda, echo=FALSE}
library(mvtnorm)
library(MASS)
generate_lda <- function(
  n, p=c(.5,.5), 
  mumat=matrix(c(0,0,1,1),2),
  Sigma=diag(2)){
  X = rmvnorm(n, sigma=Sigma)
  tibble(
    y=apply(rmultinom(n,1,p) > 0, 2, which)-1,
    x1 = X[,1] + mumat[1,y+1],
    x2 = X[,2] + mumat[2,y+1])
}
```

```{r}
dat1 <- generate_lda(100, Sigma = .5*diag(2))
logit_poly <- glm(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1, family = "binomial")
lda_poly <- lda(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1)
```

---

## Visualizing the classification boundary

```{r plot-d1, fig.align='center', fig.width=11, fig.height=7, dev='png',dvi=300,echo=FALSE}
gr = expand_grid(x1=seq(-2.5,3,length.out = 100),x2=seq(-2.5,3,length.out=100))
pts_logit = predict(logit_poly, gr)
pts_lda = predict(lda_poly, gr)
g0=ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=pts_logit), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_viridis_b(n.breaks=6,alpha=.5,name="log odds") +
  ggtitle("Polynomial logit") +
  theme(legend.position = "bottom", legend.key.width=unit(2,"cm"))
g1=ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=pts_lda$x), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_viridis_b(n.breaks=6,alpha=.5,name=bquote(delta[1]-delta[0])) +
  ggtitle("Polynomial lda") +
  theme(legend.position = "bottom", legend.key.width=unit(2,"cm"))
plot_grid(g0,g1)
```

A linear decision boundary in the higher-dimensional space corresponds to a non-linear decision boundary in low dimensions.

---

## Trees (reforestation)

.pull-left[
We saw regression trees last module

Classification trees are 
- More natural
- Slightly different computationally

Everything else is pretty much the same
]

.pull-right[
![](https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg)
]

---

## Axis-parallel splits

Like with regression trees, classification trees operate by greedily splitting the predictor space

```{r bake-it, echo=FALSE}
gbbakeoff <- readRDS("data/gbbakeoff.RDS")
gbbakeoff <- gbbakeoff[complete.cases(gbbakeoff),]
library(tree)
library(maptree)
```

.pull-left[
```{r glimpse-bakers, R.options = list(width = 50)}
gbbakeoff[FALSE,]
```

```{r our-partition}
smalltree <- tree(
  winners ~ technical_median + percent_star, 
  data = gbbakeoff)
```

]

.pull-right[

```{r plot-partition, echo=FALSE, fig.align="center", fig.height=5, fig.width=5}
plot(gbbakeoff$technical_median, gbbakeoff$percent_star, 
     pch=c("-","+")[gbbakeoff$winners+1], cex=1, bty='n',las=1,
     ylab="% star baker",xlab="times above median in technical",
     col=orange)
partition.tree(smalltree, add=TRUE, col=blue,
               ordvars = c("technical_median","percent_star"))
```
]

---

## When do trees do well?

.pull-left[
![:scale 100%](gfx/8.7.png)
]


.pull-right[

.hand[2D example]

__Top Row:__ 

true decision boundary is linear

`r pro` linear classifier 

`r con` tree with axis-parallel splits

__Bottom Row:__ 

true decision boundary is non-linear

`r con` A linear classifier can't capture the true decision boundary

`r pro` decision tree is successful.
]

---



## How do we build a tree?


1. Divide the predictor space into
$J$ non-overlapping regions $R_1, \ldots, R_J$ 

  > this is done via greedy, recursive binary splitting

2. Every observation that falls into a given region $R_j$ is given the same prediction

  > determined by majority (or plurality) vote in that region.



.hand[Important:]

* Trees can only make rectangular regions that are aligned with the coordinate axis.
* The fit is __greedy__, which means that after a split is made, all further decisions are conditional on that split.

---




## How do we measure quality of fit?


Let $p_{mk}$ be the proportion of training observations in the $m^{th}$
region that are from the $k^{th}$ class.

| |  |
|---|---|
| __classification error rate:__ | $E = 1 - \max_k (\widehat{p}_{mk})$|
| __Gini index:__   | $G = \sum_k \widehat{p}_{mk}(1-\widehat{p}_{mk})$ |
| __cross-entropy:__ | $D = -\sum_k \widehat{p}_{mk}\log(\widehat{p}_{mk})$|


Both Gini and cross-entropy measure the purity of the classifier (small if all $p_{mk}$ are near zero or 1).  

These are preferred over the classification error rate. 

Classification error is hard to optimize.

We build a classifier by growing a tree that minimizes $G$ or $D$.

---

## Pruning the tree


* Cross-validation can be used to directly prune the tree, 

* But it is computationally expensive (combinatorial complexity).

* Instead, we use __weakest link pruning__, (Gini version)

$$\sum_{m=1}^{|T|} \sum_{k \in R_m} \widehat{p}_{mk}(1-\widehat{p}_{mk}) + \alpha |T|$$

* $|T|$ is the number of terminal nodes.  

* Essentially, we are trading training fit (first term) with model complexity (second) term (compare to lasso).

* Now, cross-validation can be used to pick $\alpha$.


---


## Advantages and disadvantages of trees (again)

`r pro` Trees are very easy to explain (much easier than even linear regression).  

`r pro` Some people believe that decision trees mirror human decision.  

`r pro`  Trees can easily be displayed graphically no matter the dimension of the data.

`r pro` Trees can easily handle qualitative predictors without the need to create dummy variables.

`r con` Trees aren't very good at prediction.

`r con` Trees are highly variable. Small changes in training data $\Longrightarrow$ big changes in the tree.

To fix these last two, we can try to grow many trees and average their performance. 

--

We do this next module

---

## KNN classifiers

* We saw $k$-nearest neighbors in the last module.

```{r}
library(class)
knn3 <- knn(dat1[,-1], gr, dat1$y, k = 3)
```

```{r, fig.align="center", fig.width=10, fig.height=6, dev='png',dvi=300,echo=FALSE}
gr$nn03 = knn3
ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=knn3), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_manual(values=c(orange,green)) +
  theme(legend.position = "bottom", legend.title=element_blank(),
        legend.key.width=unit(2,"cm"))
```

---

## Choosing $k$ is very important


```{r, fig.align="center",fig.width=10, fig.height=6, dev='png',dvi=300,echo=FALSE}
gr$nn01 = knn(dat1[,-1], gr[,1:2], dat1$y, k=1)
gr$nn02 = knn(dat1[,-1], gr[,1:2], dat1$y, k=2)
gr$nn05 = knn(dat1[,-1], gr[,1:2], dat1$y, k=5)
gr$nn10 = knn(dat1[,-1], gr[,1:2], dat1$y, k=10)
gr$nn20 = knn(dat1[,-1], gr[,1:2], dat1$y, k=20)
pg = pivot_longer(gr, names_to='k',values_to = 'knn',-c(x1,x2))

ggplot(pg, aes(x1,x2)) + geom_raster(aes(fill=knn)) +
  facet_wrap(~k,labeller = label_both) + 
  scale_fill_manual(values=c(orange,green)) +
  geom_point(data=dat1,mapping=aes(x1,x2,shape=as.factor(y)), size=4) +
  scale_shape_manual(values=c("0","1"), guide="none") +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  theme(legend.position = "bottom", legend.title=element_blank(),
        legend.key.width=unit(2,"cm"))
```

* How should we choose $k$?

* Scaling is also very important. "Nearness" is determined bydistance, so better to standardize your data first.

* If there are ties, break randomly. So even $k$ is strange.

---

## `knn.cv()`

```{r}
kmax <- 20
err <- double(kmax)
for (ii in 1:kmax) {
  pk <- knn.cv(dat1[,-1], dat1$y, k = ii) # does leave one out CV
  err[ii] <- mean(pk != dat1$y)
}
```

```{r, fig.width=10, fig.height=4, fig.align="center", echo=FALSE}
ggplot(data.frame(k=1:kmax, error=err), aes(k,error)) + 
  geom_point(color=red) +
  geom_line(color=red)
```

* I would use the __largest__ `k` that is close to the minimum. This produces simpler, smoother, decision boundaries.

---

## Final version


.pull-left[
```{r fig.align="center",fig.width=5, fig.height=5, dev='png',dvi=300,echo=FALSE}
kkk = which.min(err)
gr$opt = knn(dat1[,-1], gr[,1:2], dat1$y, k=kkk)
ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=gr, aes(x1,x2,fill=opt)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_manual(values=c(orange,green)) +
  theme(legend.position = "bottom", legend.title=element_blank(),
        legend.key.width=unit(2,"cm"))
tt <- table(knn(dat1[,-1],dat1[,-1],dat1$y,k=kkk),dat1$y, dnn=c('predicted','truth'))
```
]

.pull-right[

* Best $k$: `r kkk`

* Misclassification error: `r 1-sum(diag(tt))/sum(tt)`

* Confusion matrix:

```{r echo=FALSE}
knitr::kable(tt)
```
]

---
class: middle, center
background-image: url("https://i1.wp.com/bdtechtalks.com/wp-content/uploads/2018/12/artificial-intelligence-deep-learning-neural-networks-ai.jpg?w=1392&ssl=1")
background-size: cover


.secondary[.larger[Next time...]]

.secondary[.larger[Module]] .huge-orange-number[4]

.secondary[.large[boosting, bagging, random forests, and neural nets]]


