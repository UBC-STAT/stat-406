---
title: "02 Linear model example"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE}
source("rmd_config.R")
```


## Economic mobility


```{r}
mobility <- readRDS("data/mobility.RDS")
library(tidyverse)
mobility
```

???

Note how many observations and predictors it has.

We'll use `mobility` as the response


---

## A linear model


$$\mbox{mobility}_i = \beta_0 + \beta_1 \, \mbox{State}_i + \beta_2 \, \mbox{Urban}_i + \cdots + \epsilon_i$$ 
    
or equivalently

$$E \left[ \biggl. \mbox{mobility} \, \biggr| \, \mbox{State}, \mbox{Urban}, 
    \ldots \right]  = \beta_0 + \beta_1 \, \mbox{State} + 
    \beta_2 \, \mbox{Urban} + \cdots$$

---

## Analysis


-   Randomly split into a training (say 3/4) and a test set (1/4)

-   Use training set to fit a model 

-   Fit the "full" model

-   "Look" at the fit

--

```{r}
set.seed(20210901)
mob <- mobility[complete.cases(mobility),]
n <- nrow(mob)
mob <- mob %>% select(-Name,-ID,-State)
set <- sample.int(n, floor(n * .75), FALSE)
train <- mob[set,]  #<<
test <- mob[setdiff(1:n, set),]
full <- lm(Mobility ~ ., data = train)
```

???

Why don't we include `Name` or `ID`?


---

## Results

```{r}
summary(full)
```

---

## Diagnostic plots

.pull-left[
```{r}
plot(full, 1)
```
]

.pull-right[
```{r}
plot(full, 2)
```

]

---

## Fit a reduced model

```{r}
reduced <- lm(
  Mobility ~ Commute + Gini_99 + Test_scores + HS_dropout +
    Manufacturing + Migration_in + Religious + Single_mothers, 
  data = train)
reduced %>% broom::tidy()
reduced %>% broom::glance()
```

---

## Diagnostic plots for reduced model

.pull-left[
```{r}
plot(reduced, 1)
```
]

.pull-right[
```{r}
plot(reduced, 2)
```

]


---

## How do we decide which model is better?

.pull-left[

* Goodness of fit versus prediction power

* Use both models to predict `Mobility` 

* Compare both sets of predictions


```{r}
test$full <- predict(full, newdata = test)
test$reduced <- predict(reduced, newdata = test)

mses <- function(preds, obs) mean( (obs - preds)^2 )

summarize(
  test, 
  across(full:reduced, ~ mses(.x, Mobility))
)
```
]

.pull-right[
```{r, echo=FALSE}
test %>% 
  select(Mobility, full, reduced) %>%
  pivot_longer(-Mobility) %>%
  ggplot(aes(Mobility,value)) + geom_point(color="orange") + 
  facet_wrap(~name, 2) +
  xlab('observed mobility') + 
  ylab('predicted mobility') +
  geom_abline(slope=1,intercept = 0,col="blue")
```

]

---
class: middle, center
background-image: url("https://upload.wikimedia.org/wikipedia/commons/6/6d/Activemarker2.PNG")
background-size: cover

.secondary[.larger[Next time...]]

.secondary[.larger[Module]] .huge-orange-number[1]

.secondary[.large[selecting models]]



