---
title: "16 Logistic regression"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Last time

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{Pr}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$

We showed that with two classes, the __Bayes' classifier__ is

$$g_*(X) = \begin{cases}
1 & \textrm{ if } \frac{f_1(X)}{f_0(X)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

where $f_1(X) = Pr(X \given Y=1)$ and $f_0(X) = Pr(X \given Y=0)$

We then looked at what happens if we assume $Pr(X \given Y=y)$ is Normally distributed.

We then used this distribution and the class prior $\pi$ to find the __posterior__ $Pr(Y=1 \given X=x)$.

Instead, let's directly model the posterior

$$\begin{aligned}
Pr(Y = 1 \given X=x)  & = \frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}} \\
\P(Y = 0 | X=x) & = \frac{1}{1 + \exp\{\beta_0 + \beta^{\top}x\}}=1-\frac{\exp\{\beta_0 + \beta^{\top}x\}}{1 + \exp\{\beta_0 + \beta^{\top}x\}}\end{aligned}$$

This is logistic regression.

---

## Another linear classifier

Like LDA, logistic regression is a linear classifier

The _logit_ (i.e.: log odds) transformation
gives a linear decision boundary
$$\log\left( \frac{\P(Y = 1 \given X=x)}{\P(Y = 0 \given X=x) } \right) = \beta_0 + \beta^{\top} x$$
The decision boundary is the hyperplane
$\{x : \beta_0 + \beta^{\top} x = 0\}$

If the log-odds are below 0, classify as 0, above 0 classify as a 1.

--

Logistic regression is also easy in R

```{r eval=FALSE}
logistic = glm(y~., dat, family="binomial")
```

Or we can use lasso or ridge regression as before

```{r eval=FALSE}
lasso_logit = cv.glmnet(x, y, family="binomial")
ridge_logit = cv.glmnet(x, y, alpha=0, family="binomial")
```

???

Aside: glm means generalized linear model

---

## Baby example (continued from last time)

```{r simple-lda, echo=FALSE}
library(mvtnorm)
library(MASS)
generate_lda <- function(
  n, p=c(.5,.5), 
  mumat=matrix(c(0,0,1,1),2),
  Sigma=diag(2)){
  X = rmvnorm(n, sigma=Sigma)
  tibble(
    y=apply(rmultinom(n,1,p) > 0, 2, which)-1,
    x1 = X[,1] + mumat[1,y+1],
    x2 = X[,2] + mumat[2,y+1])
}
```

```{r}
dat1 = generate_lda(100, Sigma = .5*diag(2))
logit = glm(y~., dat1, family="binomial")
summary(logit)
```

---

## Visualizing the classification boundary

```{r plot-d1, fig.align='center', fig.width=7, fig.height=7, dev='png',dvi=300,echo=FALSE}
gr = expand_grid(x1=seq(-2.5,3,length.out = 100),x2=seq(-2.5,3,length.out=100))
pts = predict(logit, gr)
g0=ggplot(dat1, aes(x1,x2)) + 
  scale_shape_manual(values=c("0","1"), guide="none") +
  geom_raster(data=tibble(gr,disc=pts), aes(x1,x2,fill=disc)) +
  geom_point(aes(shape=as.factor(y)), size=4) +
  coord_cartesian(c(-2.5,3),c(-2.5,3)) +
  scale_fill_viridis_b(n.breaks=6,alpha=.5,name="log odds") +
  theme_cowplot() + 
  theme(legend.position = "bottom", legend.key.width=unit(3,"cm"))
g0
```

---

## Calculation

While the `R` formula for logistic regression is straightforward, it's not as easy to compute as OLS or LDA or QDA.


Logistic regression for two classes simplifies to a likelihood:

(Using $\pi_i(\beta) = \P(Y_i = 1 | X = x_i,\beta)$)
$$\begin{aligned}
\ell(\beta) 
& = 
\sum_{i=1}^n \left( y_i\log(\pi_i(\beta)) + (1-y_i)\log(1-\pi_i(\beta))\right) \\
& = 
\sum_{i=1}^n \left( y_i\log(e^{\beta^{\top}x_i}/(1+e^{\beta^{\top}x_i})) - (1-y_i)\log(1+e^{\beta^{\top}x_i})\right) \\
& = 
\sum_{i=1}^n \left( y_i\beta^{\top}x_i -\log(1 + e^{\beta^{\top} x_i})\right)\end{aligned}$$

This gets optimized via Newton-Raphson updates and iteratively reweighed
least squares.

---

## IRWLS for logistic regression

(Hard stuff here, easy version in lab. This is preparation for Neural Networks.)

```{r}
logit_irwls <- function(y, x, maxit = 100, tol=1e-6){
  p = ncol(x)
  beta = double(p) # initialize coefficients 
  conv = FALSE # hasn't converged
  iter = 1 # first iteration
  while(!conv && (iter<maxit)){ # check loops
    iter = iter + 1 # update first thing (so as not to forget) 
    eta = x %*% beta 
    mu = exp(eta)/(1+exp(eta))
    gp = 1/(mu*(1-mu)) # evaluate g'(mu)
    z = eta + (y - mu) * gp # effective transformed response
    betaNew = coef(lm(z~x-1, weights=1/gp)) # do weighted regression
    conv = (mean((beta-betaNew)^2)<tol) # check if the betas are "moving" 
    beta = betaNew # update betas
  }
  return(beta) 
}
```

---

## Comparing LDA and Logistic regression

Both are linear in $x$:  

- LDA $\longrightarrow \alpha_0 + \alpha_1^\top x$ 
- Logit $\longrightarrow \beta_0 + \beta_1^\top x$.

But the parameters are estimated differently.

Examine the joint distribution of $(X,\ Y)$:  

- LDA $\prod_i f(X_i,\ Y_i) = \underbrace{\prod_i f(X_i \given Y_i)}_{\textrm{Gaussian}}\underbrace{\prod_i f(Y_i)}_{\textrm{Bernoulli}}$
- Logistic $\prod_i f(X_i,Y_i) = \underbrace{\prod_i f(Y_i\given X_i)}_{\textrm{Logistic}}\underbrace{\prod_i f(X_i)}_{\textrm{Ignored}}$
  
LDA estimates the joint, but Logistic estimates only the conditional distribution. But this is really all we need.

So logistic requires fewer assumptions.

But if the two classes are perfectly separable, logistic crashes (and the MLE is undefined)

LDA works even if the conditional isn't normal, but works poorly if any X is qualitative

---

## Comparing with QDA


* Recall: this gives a "quadratic" decision boundary (it's a curve).

* If we have many columns in $X$ $(p)$
    - Logistic estimates $p+1$ parameters
    - LDA estimates $2p + p(p+1)/2 + 1$
    - QDA estimates $2p + p(p+1) + 1$
  
* If $p=50$,
    - Logistic: 51
    - LDA: 1376
    - QDA: 2651
  
* QDA doesn't get used much: there are better nonlinear versions with way fewer parameters

* LDA only really depends on $\Sigma^{-1}(\mu_1-\mu_0)$ and $(\mu_1+\mu_0)$, so appropriate algorithms use $<2p$ parameters.

--

__Final note:__ while logistic regression and LDA produce linear decision boundaries, they are not "linear smoothers" (why not)

AIC/BIC/Cp work if you use the likelihood correctly and count parameters correctly

Must people use either test set or CV

---
class: middle, center, inverse

# Next time:

Nonlinear classifiers