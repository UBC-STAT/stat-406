---
title: "28 Hierarchical clustering"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## From $K$-means to hierarchical clustering


.pull-left[

__K-means__


1.  It fits exactly $K$ clusters.

2.  Final clustering assignments depend on the chosen initial cluster
    centers.

__Hierarchical clustering__

1.  No need to choose the number of clusters before hand.

2.  There is no random component (nor choice of starting point).


There is a catch: we need to choose a way to measure the distance
between clusters, called the __linkage__.

]

.pull-right[

```{r, fig.height=5,fig.width=5,fig.align='center', echo=FALSE}
library(mvtnorm)
set.seed(406406406)
X1 = rmvnorm(50,c(-1,2),sigma=matrix(c(1,.5,.5,1),2))
X2 = rmvnorm(40,c(2,-1),sigma=matrix(c(1.5,.5,.5,1.5),2))
```

```{r, fig.height=5,fig.width=5,fig.align='center'}
# same data as K-means "Dumb example"
egg <- heatmaply::heatmaply(
  as.matrix(dist(rbind(X1,X2))), 
  showticklabels=c(FALSE,FALSE), hide_colorbar = TRUE)
egg
```

]

---

## Hierarchical clustering

.pull-left[
```{r, echo=FALSE}
egg
```

]

.pull-right[
* Given the linkage, hierarchical clustering produces a sequence of
clustering assignments.


* At one end, all points are in their __own__
cluster.


* At the other, all points are in __one__
cluster.


* In the middle, there are __nontrivial__
solutions.

]

---

## Agglomeration


.pull-left[
```{r, fig.height=3,fig.width=3,echo=FALSE}
set.seed(1)
x1 = runif(7)
x1[1:3] = x1[1:3] + 1
x2 = runif(7,0,2)
tiny = tibble(x1=x1,x2=x2) %>% arrange(x2)
tiny$true = factor(c(2,2,2,1,1,1,1))
g <- ggplot(tiny, aes(x=x1,y=x2)) + 
  geom_point(aes(color=true), size=3) + 
  theme(legend.position='none') + 
  scale_color_manual(values=c(orange,blue))
g + geom_text(label=1:7,nudge_x =.1,nudge_y = .1)
```

* Given these data points, an agglomerative algorithm chooses a cluster sequence by combining the points into groups.

* We can also represent the sequence of clustering assignments as a
dendrogram

* Cutting the dendrogram horizontally partitions the data points
into clusters
]

.pull-right[

* Notation: Define $x_1,\ldots, x_n$ to be the data


* Let the dissimiliarities be $d_{ij}$ between
each pair $x_i, x_j$


* At any level, clustering assignments can be expressed by sets
$G = \{ i_1, i_2, \ldots, i_r\}$ giving the
indicies of points in this group. Define
$|G|$ to be the size of $G$.


__Linkage:__ The function $d(G,H)$ that takes
two groups $G,\ H$ and returns the linkage
distance between them.


Agglomerative clustering, given the linkage:

.emphasis[
1. Start with each point in its own group
2. Until there is only one cluster, repeatedly merge the two groups
$G,H$ that minimize $d(G,H)$.
]
]
    
---

## Single linkage

In __single linkage__ (a.k.a nearest-neighbor
linkage), the linkage distance between $G,\ H$ is the smallest
dissimilarity between two points in different groups:
$$d_{\textrm{single}}(G,H) = \min_{i \in G, \, j \in H} d_{ij}$$


```{r, fig.height=4, echo = FALSE, fig.align='center'}
seg = function(a,b){
  o = bind_cols(tiny[a,1:2],tiny[b,1:2])
  names(o) = c('x','y','xend','yend')
  o
}
g + geom_segment(aes(x=x,y=y,xend=xend,yend=yend),data = seg(3,4)) +
  geom_text(label=1:7,nudge_x =.02,nudge_y = .1)
```

---

## Complete linkage

In __complete linkage__ (i.e. farthest-neighbor
linkage), linkage distance between $G,H$ is the
largest dissimilarity between two points in
different clusters:
$$d_{\textrm{complete}}(G,H) = \max_{i \in G,\, j \in H} d_{ij}.$$


```{r, fig.height=4, echo = FALSE, fig.align='center'}
g + geom_segment(aes(x=x,y=y,xend=xend,yend=yend),data = seg(2,5))+
  geom_text(label=1:7,nudge_x =.02,nudge_y = .1)
```

---

## Average linkage

In __average linkage__, the linkage distance
between $G,H$ is the average dissimilarity
over all points in different clusters:
$$d_{\textrm{average}}(G,H) = \frac{1}{|G| \cdot |H| }\sum_{i \in G, \,j \in H} d_{ij}.$$


```{r, fig.height=4, echo=FALSE, fig.align='center'}
g + geom_segment(aes(x=x,y=y,xend=xend,yend=yend),
                 data = seg(rep(1:3,times=4),rep(4:7,each=3))) +
  geom_text(label=1:7,nudge_x =.02,nudge_y = .1)
```

---

## Common properties

__Single__, __complete__, and __average__ linkage share the following:

-   They all operate on the dissimilarities $d_{ij}$. 
    
    This means that
    the points we are clustering can be quite general (number of
    mutations on a genome, polygons, faces, whatever).

-   Running agglomerative clustering with any of these linkages produces
    a dendrogram with no inversions


- "No inversions" means that the linkage distance between merged clusters
only __increases__ as we run the algorithm.


In other words, we can draw a proper dendrogram, where the height of a
parent is always higher than the height of either daughter.

(We'll return to this again shortly)

---

## Centroid linkage

__Centroid linkage__ is 
relatively new. We need
$x_i \in \mathbb{R}^p$.



$\overline{x}_G$ and $\overline{x}_H$ are group averages

$d_{\textrm{centroid}} = ||\overline{x}_G - \overline{x}_H||_2^2$


```{r, fig.height=4, echo=FALSE, fig.align='center'}
tf = tiny %>% group_by(true) %>% summarize(mx = mean(x1), my = mean(x2)) 
tff = bind_cols(tf[1,2:3],tf[2,2:3])
names(tff) = c('x','y','xend','yend')
g + geom_segment(aes(x=x,y=y,xend=xend,yend=yend),data = tff) + 
  geom_point(data=tf, aes(x=mx,y=my,color=true), shape=1,size=5) +
  geom_text(label=1:7,nudge_x =.02,nudge_y = .1)
```

---

## Centroid linkage

.pull-left[
Centroid linkage is

-   ... quite intuitive

-   ... nicely analogous to $K$-means.

-   ... very related to average linkage (and much, much faster)

However, it may introduce inversions.

```{r, fig.height=4, echo=FALSE}
tt = seq(0,2*pi,len=50)
tt2 = seq(0,2*pi,len=75)
c1 = data.frame(x=cos(tt),y=sin(tt),grp=1)
c2 = data.frame(x=1.5*cos(tt2),y=1.5*sin(tt2),grp=2)
circles = bind_rows(c1,c2)
di = dist(circles[,1:2])
hc = hclust(di, method="centroid")
g1 = ggplot(circles,aes(x=x,y=y,color=as.factor(grp))) + geom_point() + 
  scale_color_manual(values=c(orange,blue)) +
  ylab('x2')+xlab('x1')+
  theme(legend.position = 'none')
```


```{r, fig.width=3,fig.height=3,echo=FALSE,fig.align='center'}
g1
```
]

.pull-right[

```{r, echo=TRUE, eval=FALSE}
tt <- seq(0, 2*pi, len=50)
tt2 <- seq(0, 2*pi, len=75)
c1 <- tibble(x = cos(tt), y = sin(tt))
c2 <- tibble(x = 1.5 * cos(tt2), y = 1.5 * sin(tt2))
circles <- bind_rows(c1, c2)
di <- dist(circles[,1:2])
hc <- hclust(di, method = "centroid")
```


```{r, fig.width=4, fig.height=4,echo=TRUE,fig.align='center'}
par(mar = c(.1,5,3,.1))
plot(hc, xlab="")
```
]


---

## Shortcomings of some linkages

.pull-left[
__Single__:  
    `r con` chaining:  a single pair of close points merges two clusters.  
    $\Rightarrow$ clusters can be too spread out, not compact

__Complete linkage:__  
  `r con` crowding: a point can be closer to points in other clusters than to points in its own cluster.  
  $\Rightarrow$ clusters are compact, not far enough apart.

__Average linkage:__  tries to strike a balance these  
    `r con` Unclear what properties the resulting clusters have when we
    cut an average linkage tree.  
    `r con` Results change with a monotone increasing transformation of the dissimilarities   
    
Neither of these afflict single or complete linkage.

__Centroid linkage:__  
    `r con` same monotonicity problem  
    `r con` and inversions
    
__All linkages:__ where do we cut?
]

.pull-right[    
__Distances__

Note how all the methods depend on the distance function

Can do lots of things besides Euclidean

This is very important

```{r fig.height=4, fig.width=4, echo=FALSE}
g1
```
]

---
class: middle, inverse, center

# Next time...



### No more slides.  All done. 

`r fontawesome::fa('grin-stars', height = "6em", fill = blue)`

--

### FINAL EXAM!! 

`r fontawesome::fa('skull-crossbones', height = "6em", red)`