---
title: "27 K-means clustering"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Clustering

So far, we've looked at ways of reducing the dimension.

Either linearly or nonlinearly, 

* The goal is visualization/exploration or possibly for an input to supervised learning.

Now we try to find groups or clusters in our data.

Think of __clustering__ as classification without the labels.

$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{P}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\V}{\mathbf{V}}$$


---

## K-means (ideally)


1.  Select a number of clusters $K$.

2.  Let $C_1,\ldots,C_K$ partition $\{1,2,3,\ldots,n\}$ such that
    - All observations belong to some set $C_k$.
    - No observation belongs to more than one set.

3.  Make __within-cluster
    variation__, $W(C_k)$, as small as
    possible. $$\min_{C_1,\ldots,C_K} \sum_{k=1}^K W(C_k).$$

4.  Define $W$ as $$W(C_k) =  \frac{1}{2|C_k|} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2.$$
That is, the average (Euclidean) distance between all cluster
members.

--

To work, K-means needs __distance to a center__ and __notion of center__


---
class: inverse

## Why this formula?

Let $\mu_k = \frac{1}{|C_k|} \sum_{i\in C_k} x_i$

$$
\begin{aligned}
\sum_{k=1}^K W(C_k) 
&= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2
= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i\neq i' \in C_k} \norm{x_i - x_{i'}}_2^2 \\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \sum_{i \neq i' \in C_k} \norm{x_i -\mu_k + \mu_k - x_{i'}}_2^2\\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \left[\sum_{i \neq i' \in C_k} \left(\norm{x_i - \mu_k}_2^2 + 
\norm{x_{i'} - \mu_k}_2^2\right) + \sum_{i \neq i' \in C_k} 2 (x_i-\mu_k)^\top(\mu_k - x_{i'})\right]\\
&= \sum_{k=1}^K \frac{1}{2|C_k|} \left[2(|C_k|-1)\sum_{i \in C_k} \norm{x_i - \mu_k}_2^2  + 2\sum_{i \in C_k} \norm{x_i - \mu_k}_2^2 \right]\\
&= \sum_{k=1}^K \sum_{x \in C_k} \norm{x-\mu_k}^2_2
\end{aligned}
$$

If you wanted (equivalently) to minimize $\sum_{k=1}^K \frac{1}{|C_k|} \sum_{x \in C_k} \norm{x-\mu_k}^2_2$, then you'd use $\sum_{k=1}^K \frac{1}{\binom{C_k}{2}} \sum_{i, i' \in C_k} \norm{x_i - x_{i'}}_2^2$

---

## K-means (in reality)

It turns out $$\min_{C_1,\ldots,C_K} \sum_{k=1}^K W(C_k).$$ is too challenging
computationally ( $K^n$ partitions! ).


So, we make a greedy approximation:

.emphasis[

1.  Randomly assign observations to the $K$ clusters

2.  Iterate the following:
    -   For each cluster, compute the $p$-length
        vector of the means in that cluster.
    -   Assign each observation to the cluster whose centroid is closest
        (in Euclidean distance).
]

This procedure is guaranteed to decrease $\sum_{k=1}^K W(C_k)$ at each step.

But being greedy, it finds a local, rather than a global optimum. 

---

## Best practices

To fit K-means, you need to

1.  Pick $K$ (inherent in the method)

2.  Convince yourself you have found a good solution (due to the
    randomized approach to the algorithm).

For 2., run
K-means many times with different starting points. Pick the solution
that has the smallest value for
$$\sum_{k=1}^K W(C_k)$$


It turns out that __1.__ is difficult to do in a
principled way.

---

## Choosing the Number of Clusters

Why is it important?

-   It might make a big difference (concluding there are $K = 2$ cancer
    sub-types versus $K = 3$).

-   One of the major goals of statistical learning is automatic
    inference. A good way of choosing $K$ is certainly a part of this.


$$W(K) = \sum_{k=1}^K  \sum_{i \in C_k}  \norm{x_i - \overline{x}_k}_2^2,$$



Within-cluster variation measures how __tightly grouped__ the clusters are. 

--

It's opposite is __Between-cluster variation__: How spread apart are the clusters?

$$B(K) = \sum_{k=1}^K |C_k| \norm{\overline{x}_k - \overline{x} }_2^2,$$


where $|C_k|$ is the number of points in $C_k$, and $\overline{x}$ is
the grand mean

.pull-left[.center[
$W$ `r fa("arrow-down",fill=green)` when $K$ `r fa("arrow-up",fill=blue)`  ]]

.pull-right[.center[
$B$ `r fa("arrow-up",fill=blue)` when $K$ `r fa("arrow-up",fill=blue)`]]


---

## CH index

.emphasis[

Want small $W$ and big $B$

]

--

__CH index__

$$\textrm{CH}(K) = \frac{B(K)/(K-1)}{W(K)/(n-K)}$$ 

To choose $K$, pick some
maximum number of clusters to be considered, $K_{\max} = 20$, for
example


$$\hat K = \arg\max_{K \in \{ 2,\ldots, K_{\max} \}} CH(K).$$

__Note:__ CH is undefined for $K =1$



---

## Dumb example


```{r}
library(mvtnorm)
set.seed(406406406)
X1 <- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))
X2 <- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))
```

```{r plotting-dumb-clusts, echo=FALSE, fig.align="center", fig.width=10,fig.height=5}
clust_raw = rbind(X1,X2)
clust = tibble(x1=clust_raw[,1], x2=clust_raw[,2],
               true = as.factor(rep(c(1,2), times=c(50,40))))
clust %>% ggplot(aes(x=x1,y=x2,color=true)) + geom_point() +
  scale_color_manual(values = c(orange,blue)) +
  theme(legend.position = "none")
```

---

## Dumb example

* We would __maximize__ CH

```{r, echo=FALSE, fig.align='center',fig.width=10,fig.height=5}
K <- 2:40
N <- nrow(clust_raw)
all_clusters <- map(K, ~ kmeans(clust_raw, .x, nstart = 20))
all_assignments <- map_dfc(all_clusters, "cluster")
names(all_assignments) <- paste0("K = ", K)
summaries <- map_dfr(all_clusters, `[`, c("tot.withinss", "betweenss")) %>%
  rename(W = tot.withinss, B = betweenss) %>%
  mutate(K = K, CHidx = (B / (K-1)) / (W / (N-K)))
summaries %>% 
  pivot_longer(-K) %>%
  ggplot(aes(K, value)) + geom_line(color = blue) + ylab('')+
  coord_cartesian(c(1, 20)) +
  facet_wrap(~name, ncol=3, scales='free_y')
```

---

## Dumb example

```{r, echo=FALSE,fig.align='center',fig.width=10,fig.height=6}
small_assignments <- all_assignments[, c(1,2,3,4,9,14,19,29)]
nums <- as.character(1:30)
bind_cols(small_assignments, clust) %>% 
  mutate(across(starts_with("K = "), factor)) %>%
  pivot_longer(-starts_with("x")) %>%
  mutate(value = fct_relevel(value, "1", "30", "29", "28","27","26","10",
                             "5", "25", "24", "23","22","21","9",
                             "3", "20", "19", "18", "17", "16","8",
                             '4', "15", "14", "13", "12", "11","7",
                             "2","6")) %>%
  ggplot(aes(x1, x2, color=value)) + geom_point() + 
  facet_wrap(~ factor(name, levels = c("true", paste0("K = ", c(2:5,10,15,20,30))))) +
  theme(legend.position = 'none') +
  scale_color_viridis_d()
```

---

## Dumb example

* $K = 2$
```{r, echo = TRUE, message = FALSE}
km <- kmeans(clust_raw, 2, nstart = 20)
names(km)
centers <- as_tibble(km$centers, .name_repair = "unique")
```

```{r, echo=FALSE, fig.align='center',fig.width=10,fig.height=3}
names(centers) <- c("x1", "x2")
bind_cols(clust, est = factor(km$cluster)) %>%
  ggplot(aes(x1, x2)) + geom_point(aes(color=est)) +
  geom_point(data = centers, color = orange, size=5) +
  theme(legend.position = 'none') +
  scale_color_viridis_d(begin = .2, end=.8)
```

---
class: middle, center, inverse

# Next time...

Hierarchical clustering

