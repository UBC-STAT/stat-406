---
title: "21 Neural nets"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      beforeInit: materials/macros.js
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```


## Overview

Neural networks are models for supervised
learning

 
Linear combinations of features  are passed
through a non-linear transformation in successive layers

 
At the top layer, the resulting latent
factors are fed into an algorithm for
predictions

(Most commonly via least squares or logistic regression)

 
Chapter 11 in [ESL] is a good introductory reference for neural networks

---

## Background

.pull-left[
Neural networks have come about in 3 "waves" 

The first was an attempt in the 1950s to model the mechanics of the human brain

Through psychological and anatomical experimentation, it appeared the
brain worked by

-   taking atomic units known as __neurons__ ,
    which can either be "on" or "off"

-   putting them in __networks__  with each
    other, where the __signal__  is given by
    which neurons are "on" at a given time

 
Crucially, a neuron itself interprets the status of other neurons

There weren't really computers, so we couldn't estimate these things
]

.pull-right[
![](https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2015/05/neuralnets-678x381.jpg)
]

---

## Background

After the development of parallel, distributed computation in the 1980s,
this "artificial intelligence" view was diminished

And neural networks gained popularity 

But, the growing popularity of SVMs and boosting/bagging in the late
1990s, neural networks again fell out of favor

This was due to many of the problems we'll discuss (non convexity being
the main one)

--

In the mid 2000's, new approaches for
__initializing__  neural networks became
available

 
These approaches are collectively known as __deep
learning__

 
Together, some state-of-the-art performance on various classification
tasks have been accomplished via neural networks

Today, Neural Networks/Deep Learning are the hottest...


$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$

---


## High level overview

.center[
![:scale 65%](https://upload.wikimedia.org/wikipedia/commons/8/82/FeedForwardNN.png)
]

---

## Recall nonparametric regression

Suppose $Y \in \mathbb{R}$ and we are trying estimate
the regression function $$\Expect{Y\given X} = f_*(X)$$

 
Earlier, we discussed basis expansion, 



1.  We know $f_*(X) =\sum_{k=1}^\infty \beta_k \phi_k(x)$ some basis $\phi_1,\phi_2,\ldots$

2.  Truncate this expansion at $K$: 
    $f_*^K(X) \approx \sum_{k=1}^K \beta_k \phi_k(x)$

3.  Estimate $\beta_k$ with least squares

--

The weaknesses of this approach are:

-   The basis is fixed and independent of the data

-   If $p$ is large, then nonparametrics doesn't work well at all (recall the Curse of Dimensionality)

-   If the basis doesn't "agree" with $f_*$, then $K$ will have to be
    large to capture the structure

-   What if parts of $f_*$ have substantially different structure?

An alternative would be to have the data
__tell__ us what kind of basis to use

---

## Specialize to logistic regression (2 classes)

Let $\mu(x) = \Expect{Y \given X=x}$

Write $L$ (the __link function__) as $L(\mu)=\log\left(\frac{\mu}{1-\mu}\right)$ 

A single layer neural network is
$$L(\mu(x)) = \beta_0 + \sum_{k=1}^K \beta_k \ \sigma(\alpha_{k0} + \alpha_k^{\top}x)$$

--

__Compare:__  
A nonparametric logistic regression would have the form
$$L(\mu(x)) = \beta_0 + \sum_{k=1}^K \beta_k {\phi_k(x)}$$


---

## Terminology

$$L(\mu(x)) = {\beta_0} + \sum_{k=1}^{{K}} {\beta_k} {\sigma(\alpha_{k0} + \alpha_k^{\top}x)}$$
The main components are

-   The derived features ${z_k = \sigma(\alpha_{k0} + \alpha_k^{\top}x)}$ and are called the __hidden units__

-   The function ${\sigma}$ is called the __activation function__  

      
-   The parameters
${\beta_0},{\beta_k},{\alpha_{k0}},{\alpha_k}$ are estimated from the data.

-   The number of hidden units ${K}$ is a tuning
    parameter
    
- $\beta_0$ and $\alpha_{k0}$ are usually called __biases__ (I'm not going to include them in future formulas, just for space)    

---

## Simplification for regression

__Example:__

To do rergression, then $L(\mu)=\mu$

$$L(\mu(x)) = \mu(x) = \sum_{k=1}^K \beta_k \sigma\left(\sum_{j=1}^p\alpha_{kj}x_j\right)$$
but in a transformed space

 
__Two observations:__

-   The $\sigma$ function generates a __feature map__

-   If additionally, $\sigma(u) = u$, then neural networks reduce to classic least squares

 
Let's discuss each of these $\ldots$

---

## Feature map

We start with $p$ covariates and we generate $K$ features


.pull-left[

__Logistic/Least-squares with a (feature)  transformation__
$$\begin{aligned}
&\Phi(x) \\
& = 
(1, x_1, \ldots, x_p, x_1^2,\ldots,x_p^2, x_1x_2, \ldots, x_{p-1}x_p) \\
& =
(\phi_1(x),\ldots,\phi_K(x))
\end{aligned}$$

 
Before feature map: 

$L(\mu(x)) = \sum_{j=1}^p \beta_j x_j$

After feature map:

$L(\mu(x)) =  \beta^{\top}\Phi(x) = \sum_{k=1}^K \beta_k \phi_k(x)$

]

.pull-right[

__Neural network__

$z_k = \sigma\left( \sum_{j=1}^p\alpha_{kj}x_j\right) = \sigma\left( \alpha_{k}^{\top}x\right)$

This gives

$\Phi(x) = (z_1, \ldots,z_K)^{\top} \in \mathbb{R}^{K}$

and

$$\begin{aligned}L(\mu(x)) &=\beta^{\top} \Phi(x)=\beta^\top z\\ 
&=  \sum_{k=1}^K \beta_k \sigma\left( \sum_{j=1}^p\alpha_{kj}x_j\right)\end{aligned}$$

]

---

## Activation functions

If $\sigma(u) = u$ is linear, then we recover a linear model (Try to show this)

ReLU is the current fashion

```{r sigmoid, echo=FALSE, fig.align='center', fig.height=5, fig.width=10}
activations = tibble(x = seq(-2,2,length.out = 100),
       identity = x,
       step = -1*(x<=0) + 1*(x>0),
       logistic = 2/(1+exp(-x))-1,
       tanh = tanh(x),
       ReLU = 0*(x<=0) + x*(x>0))
activations %>% pivot_longer(-x) %>%
  ggplot(aes(x, y=value, color=name)) + geom_line() + theme_cowplot() +
  #coord_cartesian(ylim=c(0,3)) + 
  theme(legend.title = element_blank(), legend.position = c(.65,.25)) + 
  scale_color_brewer(palette = "Set1") +
  ylab(bquote(sigma(u))) + xlab("u")
```

---

## Grabbing the output

.pull-left[.center[
![:scale 75%](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1280px-Colored_neural_network.svg.png)]
]

.pull-right[

$$
\begin{aligned}
z_k   &= \sigma(\alpha_k^{\top}x) \quad  ( k = 1, \ldots K)\\
K &= \textrm{# hidden units}\\
w_g  &= \beta_g^{\top}z \quad  ( g = 1, \ldots G)\\
G &= \textrm{# of outputs}\\
\mu_g(x) &= L^{-1}(w_g)
\end{aligned}
$$


-   .hand[Regression]:  The link
    function is $L(u) = u$ (here, $G=1$)

-   .hand[Classification]:  With $G$
    classes, we are modeling $\pi_g = P(Y = g\given X=x)$ and
    $L = \textrm{logit}$:
    
    $\hat{\pi}_g(x) = \frac{e^{w_g}}{\sum_{g'=1}^G e^{w_{g'}}}$
    
    $\hat{y} = \widehat{g}(x) = \argmax_g \hat{\pi}_g(x)$
    
    This is called the __softmax__  function for
    historical reasons

]

---
class: middle,inverse,center

# Next time...

How do we estimate these monsters?
