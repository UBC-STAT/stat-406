---
title: "20 Boosting"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      beforeInit: materials/macros.js
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

## Last time



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

We learned about bagging, for averaging low-bias estimators.

Today, we examine it's opposite: __Boosting__.

__Boosting__ also combines estimators, but it combines __high-bias__ (low-variance) estimators.

Boosting has a number of flavors. And if you Google descriptions, most are wrong.

For a deep (and accurate) treatment, see [ESL] Chapter 10


--

We'll discuss 2 flavors: AdaBoost and Gradient Boosting

Neither requires a tree, but that's the typical usage.

Boosting needs a "weak learner", so small trees (called stumps) are natural.

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$


---

## AdaBoost (Freund and Schapire)

Let $G_m(x)$ be a weak learner (say a tree with one split)

.emphasis[

__Algorithm (AdaBoost):__

1. Set observation weights $w_i=1/n$.

2. Until we quit ( $m<M$ iterations )
    
    a. Estimate a classifier $G_m(x)$ using weights $w_i$
    
    b. Calculate it's weighted error $\textrm{err}_m = \sum_{i=1}^n w_i I(y_i \neq G_m(x_i)) / \sum w_i$
    
    c. Set $\alpha_m = \log((1-\textrm{err}_m)/\text{err}_m)$
    
    d. Update $w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G_m(x_i)))$

3. Final classifier is $G(x) = \textrm{sign}\left( \sum_{m=1}^M \alpha_m G_m(x)\right)$
]

---

## AdaBoost intuition

At each iteration, we weight the __observations__.

Observations that are currently misclassified, get __higher__ weights.

So on the next iteration, we'll try harder to correctly classify our mistakes.

The number of iterations must be chosen.
---

## Using mobility data again

```{r echo=FALSE}
library(randomForest)
data(mobility, package="UBCstat406labs")
mob = mobility %>% 
  mutate(mobile=as.numeric(Mobility>.1)) %>%
  dplyr::select(-ID,-Name,-Mobility,-State) %>% 
  drop_na()
n = nrow(mob)
trainidx = sample.int(n, floor(n*.75))
testidx = setdiff(1:n, trainidx)
train = mob[trainidx,]; test=mob[testidx,]
rf = randomForest(as.factor(mobile)~., data=train)
bag = randomForest(as.factor(mobile)~., data=train,
  mtry=ncol(mob)-1)
preds = tibble(
  truth=test$mobile,
  rf = predict(rf, test),
  bag = predict(bag, test))
```

```{r, fig.align='center', fig.height=5, fig.width=10}
library(gbm)
adab = gbm(mobile~., data=train, n.trees = 500, distribution = "adaboost")
preds$adab = as.numeric(predict(adab, test)>0)
par(mar=c(5,15,0,1))
summary(adab, las=1)
```

---

## Forward stagewise additive modeling

Generic for regression or classification, any weak learner

.emphasis[

__Algorithm:__

1. Set initial predictor $f_0(x)=0$

2. Until we quit ( $m<M$ iterations )
    
    a. Compute $$(\beta_m, G_m) = \arg\min_{\beta, G} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i)\right)$$
    
    b. Set $f_m(x) = f_{m-1}(x) + \beta_m G_m(x)$
    
3. Final classifier is $G(x) = \textrm{sign}\left( f_M(x) \right)$
]

Here, $L$ is a loss function that measures how prediction accuracy

If $L(y,\ f(x))= \exp(-y f(x))$ and $G_m$ is a classifier, then this is equivalent to AdaBoost. Proven 5 years later (Friedman, Hastie, and Tibshirani 2000).

---

## So what?

It turns out that "exponential loss" $L(y,\ f(x))= \exp(-y f(x))$ is not very robust.

Here are some other loss functions for 0-1 classification

```{r loss-funs, fig.align="center", echo=FALSE, fig.height=4}
losses = tibble(x = seq(-2,2,length.out = 100),
       Misclassification = as.numeric(x<0),
       Exponential = exp(-x),
       Binomial_deviance = log2(1+exp(-2*x)),
       Squared_error = (x-1)^2,
       Support_vector = pmax((1-x),0))
losses %>% pivot_longer(-x) %>%
  ggplot(aes(x, y=value, color=name)) + geom_line() + theme_cowplot() +
  coord_cartesian(ylim=c(0,3)) + 
  theme(legend.title = element_blank(), legend.position = c(.65,.65)) + 
  scale_color_brewer(palette = "Set1") +
  ylab("Loss") + xlab(bquote(y~f(x)))
```

--

Want losses which penalize negative margin, but not positive margins.

Robust: don't over-penalize large negatives

---

## Gradient boosting


In the forward stagewise algorithm, we solved a minimization and then made an update 

$f_m(x) = f_{m-1}(x) + \beta_m G_m(x)$.

For most loss functions, $\arg\min_{\beta, G} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i)\right)$ cannot be solved

Instead, if we take one gradient step toward the minimum, we get

$f_m(x) = f_{m-1}(x) -\gamma_m \nabla L(y,f_{m-1}(x))$

Then do line search on $\gamma_m$

This is called __Gradient boosting__

Notice how similar the update steps look.

Gradient boosting goes only part of the way toward the minimum at each $m$. This has two implications:

1. Since we're not fitting $G$ to the data as hard, the "learner" is weaker.

2. This procedure is computationally much simpler.


---

## Gradient boosting

.emphasis[

__Algorithm:__

1. Set initial predictor $f_0(x)=\overline{\y}$

2. Until we quit ( $m<M$ iterations )
    
    a. Compute pseudo-residuals (what is the gradient of $L(y,f)=(y-f(x))^2$?)
    $$r_i = -\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{m-1}(x_i)}$$
    
    b. Estimate weak learner, $G_m$, with the training set $\{r_i, x_i\}$.
    
    c. Find the step size $\gamma_m = \arg\min_\gamma \sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \gamma G_m(x_i))$
    
    b. Set $f_m(x) = f_{m-1}(x) + \gamma_m G_m(x)$
    
3. Final predictor is $f_M(x)$.
]

```{r gbm}
grad_boost = gbm(mobile~., data=train, n.trees = 500, distribution = "bernoulli")
```

---

## Gradient boosting modifications

* Typically done with "small" trees, not stumps because of the gradient. You can specify the size. Usually 4-8 terminal nodes is recommended (more gives more interactions between predictors)

* Usually modify the gradient step to $f_m(x) = f_{m-1}(x) + \gamma_m \alpha G_m(x)$ with $0<\alpha<1$. Helps to keep from fitting too hard.

* Often combined with Bagging so that each step is fit using a bootstrap resample of the data. Gives us out-of-bag options.

* There are many other extensions, notably XGBoost.

```{r, fig.height=3.5, fig.width=10, fig.align='center', echo=FALSE}
boost_preds = tibble(
  adaboost = predict(adab, test),
  gbm = predict(grad_boost,test),
  truth = test$mobile)
g1 = ggplot(boost_preds, aes(adaboost, gbm, color=as.factor(truth))) +
  geom_text(aes(label=truth)) + geom_vline(xintercept = 0) + 
  geom_hline(yintercept = 0) +
  theme_cowplot() + theme(legend.position = 'none') +
  scale_color_manual(values=c('orange','blue')) +
  annotate("text", x=-4,y=5, color=green,
           label=round(with(boost_preds,mean((gbm>0)==truth)),2)) +
  annotate("text", x=4,y=-5, color=green, 
           label=round(with(boost_preds,mean((adaboost>0)==truth)),2))
boost_oob = tibble(adaboost=adab$oobag.improve, gbm=grad_boost$oobag.improve,
                   ntrees=1:500)
g2 = boost_oob %>% pivot_longer(-ntrees, values_to = "OOB_Error") %>% 
  ggplot(aes(x=ntrees, y=OOB_Error, color=name)) + geom_line() + 
  theme_cowplot() + scale_color_manual(values=c(orange,blue)) +
  theme(legend.title = element_blank())
plot_grid(g1,g2)
```

---
class: middle, inverse, center

# Next time...

Neural networks and deep learning, the beginning