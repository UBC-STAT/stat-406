---
title: "10 Basis expansions"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## What about nonlinear things

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$

So far we assume...

$$\Expect{Y \given X=x} = \sum_{j=1}^p x_j\beta_j$$

Now we relax this assumption of linearity:

$$\Expect{Y \given X=x} = f(x)$$

How do we estimate $f$?

--

For this lecture, we use $x \in \R$ (1 dimensional)

Higher dimensions are possible, but complexity grows __exponentially__.

We'll see some special techniques later this Module.

---

## Start simple

For any $f : \R \rightarrow [0,1]$

$$f(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2 + \frac{1}{3!}f'''(x_0)(x-x_0)^3 + R_3(x-x_0)$$

So we can linearly regress $y_i = f(x_i)$ on the polynomials.

The more terms we use, the smaller $R$.

```{r, echo=FALSE, fig.width=8, fig.height=4, fig.align="center", dev="svg", warning=FALSE}
data(lidar, package = "SemiPar")
lidar %>% 
  ggplot(aes(range,logratio)) + 
  geom_point(color=blue) +
  geom_smooth(color=orange, formula = y~poly(x,3), method="lm", se=FALSE)
```

---

## Same thing, different orders

```{r, fig.width=10, fig.height=4, fig.align="center", dev="svg", warning=FALSE}
lidar %>% 
  ggplot(aes(range, logratio)) + 
  geom_point(color=blue) + 
  geom_smooth(aes(color="a"), formula = y~poly(x, 4), method="lm", se=FALSE) +
  geom_smooth(aes(color="b"), formula = y~poly(x, 7), method="lm", se=FALSE) +
  geom_smooth(aes(color="c"), formula = y~poly(x, 13), method="lm", se=FALSE) +
  scale_color_manual(name="Taylor order",
    values=c(green,red,orange), labels=c("4 terms","7 terms","13 terms"))
```


---

## Still a "linear smoother"

.pull-left[
Really, this is still linear regression, just in a transformed space.

It's not linear in $x$, but it is linear in $(x,x^2,x^3)$ (for the 3rd-order case)

So, we're still doing OLS with

$$\X=\begin{bmatrix}1& x_1 & x_1^2 & x_1^3 \\ \vdots&&&\vdots\\1& x_n & x_n^2 & x_n^3\end{bmatrix}$$

So we can still use our nice formulas for LOO-CV, GCV, Cp, AIC, etc.


```{r}
nmods <- 20
loocv <- function(mdl) { 
  mean( residuals(mdl)^2 / 
         (1 - hatvalues(mdl))^2 ) }
cvscores <- double(nmods)
for (i in 1:nmods) {
  cvscores[i] <- loocv(
    lm(logratio ~ poly(range,i), data = lidar))}
```
]

.pull-right[

```{r, echo=FALSE, dev="svg"}
g1 = ggplot(data.frame(ord=1:nmods,cvscores=cvscores), aes(ord,cvscores)) +
  geom_point(color=blue) +
  geom_line(color=blue) + ylab('LOO CV') + xlab('polynomial order') +
  geom_vline(xintercept = which.min(cvscores), linetype="dotted") 
g2 = ggplot(lidar, aes(range,logratio)) + geom_point(color=blue) + 
  geom_smooth(color=orange, 
              formula = y~poly(x,which.min(cvscores)), method="lm", se=FALSE) + ggtitle("Best model")
plot_grid(g1, g2, nrow=2)
```
]

---

## Other bases

__Polynomials__

$x \mapsto \left(1,\ x,\ x^2, \ldots, x^p\right)$

__Linear splines__

$x \mapsto \bigg(1,\ x,\ (x-k_1)_+,\ (x-k_2)_+,\ldots, (x-k_p)_+\bigg)$ for some choices $\{k_1,\ldots,k_p\}$

__Cubic splines__

$x \mapsto \bigg(1,\ x,\ x^2,\ x^3,\ (x-k_1)^3_+,\ (x-k_2)^3_+,\ldots, (x-k_p)^3_+\bigg)$ for some choices $\{k_1,\ldots,k_p\}$

__Fourier series__

$x \mapsto \bigg(1,\ \cos(2\pi x),\ \sin(2\pi x),\ \cos(2\pi 2 x),\ \sin(2\pi 2 x), \ldots, \cos(2\pi p x),\ \sin(2\pi p x)\bigg)$

---

## How do you choose?

Procedure 1:

1. Pick your favorite basis. This is not as easy as it sounds. For instance, if $f$ is a step function, linear splines will do well with good knots, but polynomials will be terrible unless you have __lots__ of terms.
2. Perform OLS on different orders.
3. Use model selection criterion to choose the order.

Procedure 2:

1. Use a bunch of high-order bases, say Linear splines and Fourier series and whatever else you like.
2. Use Lasso or Ridge regression or elastic net.
3. Use model selection criterion to choose the tuning parameter.

---

## Try both procedures

1. Split Lidar into 75% training data and 25% testing data.
2. Estimate polynomials up to 20 as before and choose best order.
3. Do ridge, lasso and elastic net $\alpha=.5$ on 20th order polynomials, B splines with 20 knots, and Fourier series with $p=20$. Choose tuning parameter.
4. Repeat 1-3 10 times (different splits)

---


```{r simulation, cache = TRUE}
x <- (lidar$range - min(lidar$range) + .005) / (max(lidar$range) - min(lidar$range) + .01)
Xmat <- cbind(poly(x,20), splines::bs(x, df=20), 
             cos(2*pi*outer(x,1:20)), sin(2*pi*outer(x,1:20)))
y = lidar$logratio
library(glmnet)
mse <- function(x,y) mean( (x - y)^2 )
lidar_sim <- function(maxord = 20, train = 0.75){
  n <- nrow(lidar)
  train <- as.logical(rbinom(n, 1, .75))
  test <- !train # not precisely 75%, but on average
  polycv <- double(maxord) # preallocate
  for(i in 1:20) polycv[i] <- loocv( lm(y ~ Xmat[ , 1:i], subset = train))
  bpoly <- lm(y[train] ~ Xmat[train, 1:which.min(polycv)])
  lasso <- cv.glmnet(Xmat[train, ], y[train])
  ridge <- cv.glmnet(Xmat[train, ], y[train], alpha = 0)
  elnet <- cv.glmnet(Xmat[train, ], y[train], alpha = .5)
  out <- data.frame(
    methods = c("poly", "lasso", "ridge", "elnet"),
    mses = c(
      mse(y[test], cbind(1, Xmat[test, 1:which.min(polycv)]) %*% coef(bpoly)),
      mse(y[test], predict(lasso, Xmat[test,], s="lambda.min")),
      mse(y[test], predict(ridge, Xmat[test,], s="lambda.min")),
      mse(y[test], predict(elnet, Xmat[test,], s="lambda.min"))
    ),
    nvars = c(which.min(polycv),
              sum(abs(coef(lasso, s="lambda.min")) > 0),
              ncol(Xmat),
              sum(abs(coef(elnet, s="lambda.min")) > 0))
  )
  out
}
set.seed(12345)
sim_results <- lapply(1:20, lidar_sim) %>% bind_rows()
```

---

```{r sim-results, dev="svg", fig.height=5, fig.width=10, fig.align="center", echo=TRUE}
sim_results %>% 
  pivot_longer(-methods) %>%
  ggplot(aes(methods, value, fill=methods)) + geom_boxplot() +
  facet_wrap(~name, scales = "free_y") + ylab("") +
  theme(legend.position = "none") + xlab("") +
  scale_fill_viridis_d(begin = .2, end = 1)
```

---

## Common elements

In all these cases, we transformed $x$ to a __higher-dimensional space__

Used $p+1$ dimensions with polynomials

Used $p+4$ dimensions with cubic splines

Used $2p+1$ dimensions with Fourier basis

--

Each case applied a __feature map__ to $x$, call it $\Phi$

We used new "features" $\Phi(x) = \bigg(\phi_1(x),\ \phi_2(x),\ldots,\phi_k(x)\bigg)$

Neural networks (coming in module 4) use this idea

You've also probably seen it in earlier courses when you added interaction terms or other transformations.

--

Some methods (notably Support Vector Machines and Ridge regression) allow $k=\infty$

See [ISLR] 9.3.2 for baby overview or [ESL] 5.8 (note screamy face)

---
class: inverse, center, middle

# Next time...

Kernel regression and nearest neighbors