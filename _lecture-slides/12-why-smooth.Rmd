---
title: "12 To(o) smooth or not to(o) smooth?"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```


## Last time...

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$

We've been discussing smoothing methods in 1-dimension:

$$\Expect{Y\given X=x} = f(x),\quad x\in\R$$

We looked at basis expansions, e.g.:

$$f(x) \approx \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p$$ 

We looked at local methods, e.g.:

$$f(x_i) \approx  s_i^\top \y$$

--

What if $x \in \R^d$ and $d>1$?

---


## Kernels and interactions

In multivariate nonparametric regression, you estimate a __surface__ over the input variables.

This is trying essentially to find $\widehat{f}(x_1,\ldots,x_p)$.

Therefore, this function __by construction__ includes interactions, handles categorical data, etc. etc.

This is in contrast with __linear models__ which need you to specify these things.

This extra complexity (automatically including interactions, as well as other things) comes with tradeoffs.

---
layout: true

## Issue 1

More complicated functions (smooth Kernel regressions vs. linear models) tend to have __lower bias__ but __higher variance__.

For $p=1$, one can show that for kernels (with the correct bandwidth)

$$\textrm{MSE}(\hat{f}) = \frac{C_1}{n^{4/5}} + \frac{C_2}{n^{4/5}} + \sigma^2$$ 

---

_you don't need to memorize these formulas_

---

Recall, this decomposition is **squared bias** + **variance** + **irreducible error**

* It depends on the **choice** of $h$

$$\textrm{MSE}(\hat{f}) = C_1 h^4 + \frac{C_2}{nh} + \sigma^2$$ 

* Using $h= = cn^{-1/5}$ **balances** squared bias and variance, leads to the above rate.

---

### How does this compare to just using a linear model?

__Bias__  
    
1. The bias of using a linear model **if the truth nonlinear** is a number $b$ which doesn't depend on $n$.
2. The bias of using kernel regression is $C_1/n^{4/5}$. This goes to 0 as $n\rightarrow\infty$.
  
__Variance__

1. The variance of using a linear model is $C/n$ **no matter what**
2. The variance of using kernel regression is $C_2/n^{4/5}$.

---

### To conclude: 

* bias of kernels goes to zero (not for lines)

* but variance of lines goes to zero faster than for kernels.

If the linear model is **right**, you win. 

But if it's wrong, you (eventually) lose as $n$ grows.

How do you know if you have enough data? 

Compare of the kernel version with CV-selected tuning parameter with the estimate of the risk for the linear model.

---
layout: false

## Issue 2

For $p>1$, there is more trouble.

First, lets look again at 
$$\textrm{MSE}(\hat{f}) = \frac{C_1}{n^{4/5}} + \frac{C_2}{n^{4/5}} + \sigma^2$$ 

That is for $p=1$. It's not __that much__ slower than $C/n$, the variance for linear models.

If $d>1$ similar calculations show,

$$\textrm{MSE}(\hat f) = \frac{C_1+C_2}{n^{4/(4+p)}} + \sigma^2 \hspace{2em} \textrm{MSE}(\hat \beta)  = b + \frac{Cp}{n} + \sigma^2 .$$

--

What if $d$ is big (and $n$ is really big)?

1. Then $(C_1 + C_2) / n^{4/(4+p)}$ is still big.
2. But $Cp / n$ is small.
3. So unless $b$ is big, we should use the linear model.
  
How do you tell? Do model selection to decide.

A __very, very__ questionable rule of thumb: if $p>\log(n)$, don't do smoothing.

---
class: inverse, middle, center

# Next time...

Compromises if _p_ is big

Additive models and trees
