---
title: "12 To(o) smooth or not to(o) smooth?"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```


## Last time...

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$

We've been discussing smoothing methods in 1-dimension:

$$\Expect{Y\given X=x} = f(x),\quad x\in\R$$

We looked at basis expansions, e.g.:

$$f(x) \approx \beta_0 + \beta_1 x + \beta_2 x^2 + \cdots + \beta_p x^p$$ 

We looked at local methods, e.g.:

$$f(x_i) \approx  s_i^\top \y$$

--

What if $x \in \R^d$ and $d>1$?

---


## Kernels and interactions

In multivariate nonparametric regression, you estimate a __surface__ over the input variables.

This is trying essentially to find $\widehat{f}(x_1,\ldots,x_p)$.

Therefore, this function __by construction__ includes interactions, handles categorical data, etc. etc.

This is in contrast with __linear models__ which need you to specify these things.

This extra complexity (automatically including interactions, as well as other things) comes with tradeoffs.

---

## Issue 1

More complicated functions (smooth Kernel regressions vs. linear models) tend to have __lower bias__ but __higher variance__.

For $d=1$, one can show that for kernels (with the correct bandwidth)

$$\textrm{MSE}(\hat{f}) = \frac{C_1}{n^{4/5}} + \frac{C_2}{n^{4/5}} + \sigma^2$$ 

How does this compare to just using a linear model?

--

__Bias__  
    
1. The bias of using a linear model when it is wrong is a number $b(\widehat\beta)$ which doesn't depend on $n$.
2. The bias of using kernel regression is $C_1/n^{4/5}$. This goes to 0 as $n\rightarrow\infty$.
  
__Variance__

1. The variance of using a linear model is $C/n$
2. The variance of using kernel regression is $C_2/n^{4/5}$.

---

## Issue 1
  
To conclude: bias of kernels goes to zero (not for lines) but variance of lines goes to zero faster than for kernels.

If the linear model is right, you win. But if it's wrong, you (eventually) lose.

How do you know if you have enough data? 

Compare of the kernel version with CV-selected tuning parameter with the estimate of the risk for the linear model.

---

## Issue 2

For $d>1$, there is more trouble.

First, lets look again at 
$$\textrm{MSE}(\hat{f}) - \sigma^2 = \frac{C_1}{n^{4/5}} + \frac{C_2}{n^{4/5}}$$ 

That is for $d=1$. It's not __that much__ slower than $C/n$, the variance for linear models.

If $d>1$ similar calculations show,

$$\textrm{MSE}(\hat f)-\sigma^2 = \frac{C_1+C_2}{n^{4/(4+d)}} \hspace{2em} \textrm{MSE}(\hat \beta) -\sigma^2 = b(\hat\beta) + \frac{Cd}{n}.$$

--

What if $d$ is big (and $n$ is really big)?

1. Then $\frac{C}{n^{4/(4+d)}}$ is still big.
2. But $\frac{Cd}{n}$ is small.
3. So unless $b(\hat\beta)$ is big, we should use the linear model.
  
How do you tell? Do model selection to decide.

A __very, very__ questionable rule of thumb: if $d>\log(n)$, don't do smoothing.

---
class: inverse, middle, center

# Next time...

Compromises if _d_ is big

Additive models and trees
