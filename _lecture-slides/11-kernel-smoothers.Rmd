---
title: "11 Local methods"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
output:
  xaringan::moon_reader:
    includes:
      in_header: 
        - materials/load-fa.html
    lib_dir: libs
    css: ["materials/xaringan-themer.css","slides-style.css"]
    nature:
      highlightStyle: zenburn
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Last time...


.pull-left[
We looked at __feature maps__ as a way to do nonlinear regression:

We used new "features" $\Phi(x) = \bigg(\phi_1(x),\ \phi_2(x),\ldots,\phi_k(x)\bigg)$

Now we examine an alternative

Suppose I just look at the "neighbors" of some point (based on the $x$-values)

I just average the $y$'s at those locations together

Let's use 3 neighbors
]

--

.pull-right[
```{r load-lidar, echo=FALSE}
data(lidar, package="UBCstat406labs")
set.seed(12345)
lidar_unif = lidar[sample.int(nrow(lidar), 40),] %>% arrange(range)
pt = 15
nn = 3
neibs = sort.int(abs(lidar_unif$range-lidar_unif$range[pt]), 
                     index.return = TRUE)$ix[1:nn]
lidar_unif$neighbors = 1:40 %in% neibs
g1 = ggplot(lidar_unif, aes(range,logratio,color=neighbors)) + geom_point() +
  scale_color_manual(values = c(blue,red)) + theme_cowplot() +
  geom_vline(xintercept = lidar_unif$range[pt], color=red) + 
  annotate("rect", fill=red, alpha=.25, ymin=-Inf, ymax=Inf,
           xmin=min(lidar_unif$range[neibs]), xmax = max(lidar_unif$range[neibs])) +
  theme(legend.position = "none")
g2 = ggplot(lidar_unif, aes(range,logratio)) + geom_point(color=blue) + 
  theme_cowplot() + 
  geom_line(data=tibble(
    range=seq(min(lidar_unif$range),max(lidar_unif$range),length.out = 101),
    logratio=FNN::knn.reg(
      lidar_unif$range,matrix(range,ncol=1),y=lidar_unif$logratio)$pred), color=orange)
plot_grid(g1,g2,nrow=2)
```

]

---

## KNN 


.pull-left[
```{r small-lidar-again, echo=FALSE}
g2
```
]


.pull-right[
This method is $K$-nearest neighbors.

It's a __linear smoother__ just like in previous lectures: $\widehat{\mathbf{y}} = S \mathbf{y}$ for some matrix $S$.

You should imagine what $S$ looks like.

What is the effective degrees of freedom of KNN?

KNN averages the nearest points with equal weight.

But some sets of neighbors are "closer" than other sets.
]

---

## Local averages

Instead of choosing the number of neighbors to average, we can average any observations within a certain distance.

```{r, fig.height = 4, fig.align='center', fig.width=8, echo=FALSE,fig.height=4}
ggplot(lidar_unif, aes(range, logratio)) + geom_point(color=blue) + 
  coord_cartesian(ylim=c(-1,0)) + theme_cowplot() +
  geom_segment(aes(x=range[15],y=-1,xend=range[15],yend=logratio[15]),color=blue) + 
  geom_segment(aes(x=range[25],y=-1,xend=range[25],yend=logratio[25]),color=green) +
  geom_rect(aes(xmin=range[15]-15,xmax=range[15]+15,ymin=-1,ymax=-.7),fill=blue) +
  geom_rect(aes(xmin=range[25]-15,xmax=range[25]+15,ymin=-1,ymax=-.7),fill=green)
```

--

The boxes have width 30. 

---

## What is a "kernel" smoother?

* The mathematics:

> A kernel is any function $K$ such that for any $u$, $K(u) \geq 0$, $\int du K(u)=1$ and $\int uK(u)du=0$.

* The idea: a kernel is a nice way to take weighted averages. The kernel function gives the weights.

* The previous example is called the __boxcar__ kernel. It looks like this:

```{r boxcar, fig.width=8, fig.height=3, echo=FALSE, fig.align="center"}
testpts = seq(min(lidar_unif$range),max(lidar_unif$range),length.out = 101)
dmat = abs(outer(testpts, lidar_unif$range, "-"))
S = (dmat*(dmat<15))
S = S / rowSums(S)
boxcar = tibble(range=testpts, logratio=S %*% lidar_unif$logratio)
ggplot(lidar_unif, aes(range, logratio)) + geom_point(color=blue) + 
  theme_cowplot() +
  geom_line(data=boxcar,color=orange)
```

This one gives non-zero weight to points within $\pm 15$ range

---


## Other kernels

Most of the time, we don't use the boxcar because the weights are weird.

A more common one is the Gaussian kernel:

```{r, fig.height = 4, fig.align='center', fig.width=6, echo=FALSE}
gaussian_kernel = function(x) dnorm(x, mean=lidar_unif$range[15],sd=7.5)*3
ggplot(lidar_unif, aes(range, logratio+1)) + geom_point(color=blue) + 
  coord_cartesian(ylim=c(0,1)) + theme_cowplot() +
  geom_segment(aes(x=range[15],y=0,xend=range[15],yend=logratio[15]+1),color=orange)+
  stat_function(fun=gaussian_kernel, geom="area",fill=orange)
```

For the plot, I made $\sigma=7.5$. 

Now the weights "die away" for points farther from where we're predicting.

---

## Other kernels

What if I made $\sigma=15$?


```{r, fig.height = 4, fig.align='center', fig.width=6, echo=FALSE}
gaussian_kernel = function(x) dnorm(x, mean=lidar_unif$range[15],sd=15)*3
ggplot(lidar_unif, aes(range, logratio+1)) + geom_point(color=blue) + 
  coord_cartesian(ylim=c(0,1)) + theme_cowplot() +
  geom_segment(aes(x=range[15],y=0,xend=range[15],yend=logratio[15]+1),color=orange)+
  stat_function(fun=gaussian_kernel, geom="area",fill=orange)
x = lidar_unif$range
```

Before, points far from $x_{15}$ got very small weights, now they have more influence.

For the Gaussian kernel, $\sigma$ determines something like the "range" of the smoother.


---

## Many Gaussians

The following code gives $S$ for Gaussian kernel smoothers with different $\sigma$

```{r, eval=FALSE}
dmat = as.matrix(dist(x))
Sgauss <- function(sigma){
  gg = exp(-dmat^2/(2*sigma^2)) / (sigma * sqrt(2*pi))
  sweep(gg, 1, rowSums(gg),'/')
}
```

```{r, fig.height = 4, fig.align='center', fig.width=8, echo=FALSE}
Sgauss <- function(sigma){
  gg = exp(-dmat^2/(2*sigma^2)) / (sigma * sqrt(2*pi))
  sweep(gg, 1, rowSums(gg),'/')
}
boxcar$S15 = with(lidar_unif, Sgauss(15) %*% logratio)
boxcar$S08 = with(lidar_unif, Sgauss(8) %*% logratio)
boxcar$S30 = with(lidar_unif, Sgauss(30) %*% logratio)
bc = boxcar %>% select(range, S15,S08,S30) %>% 
  pivot_longer(-range, names_to = "Sigma")
ggplot(lidar_unif, aes(range, logratio)) + 
  geom_point(color=blue) + 
  geom_line(data=bc, aes(range, value, color=Sigma)) +
  scale_color_brewer(palette="Set1") +
  theme_cowplot()
```

---

## The bandwidth

* Choosing $\sigma$ is __very__ important.

* This "range" parameter is called the __bandwidth__.

* It is way more important than which kernel you use.

* The default kernel in `ksmooth()` is something called 'Epanechnikov':

```{r}
epan <- function(x) 3/4*(1-x^2)*(abs(x)<1)
```

```{r,fig.height=3, fig.width=8, fig.align='center', echo=FALSE}
ggplot(data.frame(x=c(-2,2)), aes(x)) + stat_function(fun=epan,color=green) +
  theme_cowplot()
```

---

## Choosing the bandwidth

As we have discussed, kernel smoothing (and KNN) are linear smoothers

$$\widehat{\mathbf{y}} = S\mathbf{y}$$

This has easy implications:

--

```{r} 
loocv <- function(y, S){
  yhat = S %*% y
  cv = mean( (y-yhat)^2 / (1 - diag(S))^2 )
  cv
}
``` 

--

The __effective degrees of freedom__ is $\textrm{tr}(S)$

Therefore we can use our model selection criteria from before 

---

## Smoothing the full Lidar data

```{r, echo=FALSE}
dmat = as.matrix(dist(lidar$range))
loocvs = double(100)
sigmas = exp(seq(log(300), log(.3), length=100))
for(i in 1:100){
  S = Sgauss(sigmas[i])
  loocvs[i] = loocv(lidar$logratio, S)
}
bests = which.min(loocvs)
lidar$smoothed = Sgauss(sigmas[bests]) %*% lidar$logratio
```

```{r smoothed-lidar, echo=FALSE, fig.width=10, fig.height=5,fig.align="center"}
g3 = ggplot(data.frame(sigma=sigmas, loocv=loocvs), aes(sigma,loocv)) + 
  geom_point(color=blue) + geom_vline(xintercept=sigmas[bests], color=red) +
  scale_x_log10() + theme_cowplot()
g4 = ggplot(lidar, aes(range, logratio)) + geom_point(color=blue) +
  geom_line(aes(y=smoothed), color=orange, size=2) + theme_cowplot()
plot_grid(g3,g4,nrow=1)
```

I considered $\sigma \in [0.3,\ 300]$ and used $`r round(sigmas[bests],2)`$.

---
class: middle, center, inverse

# Next time...

Why don't we just smooth everything all the time?