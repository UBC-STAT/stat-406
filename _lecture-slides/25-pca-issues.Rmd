---
title: "25 Principal components, the troubles"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---



```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## PCA

If we knew how to rotate our data, then we could more easily retain the structure.

__PCA__ gives us exactly this rotation



PCA works when the data can be represented (in a lower dimension) as .hand[lines] (or planes, or hyperplanes). 


So, in two dimensions:  


```{r good-pca, echo=FALSE, fig.align='center',fig.height=4,fig.width=5}
library(mvtnorm)
m = c(2,3)
covmat = matrix(c(1,.75,.75,1),2)
ee = eigen(covmat)
x = rmvnorm(200, m, covmat)
z = apply(x, 1, function(x) sqrt(sum(x^2)))
df = data.frame(x=x[,1], y=x[,2], z=z)
ss = sqrt(ee$values)
df_arrows = data.frame(x1 = m[1], y1 = m[2],
                       x2 = m[1] + 3*ee$vectors[1,]*ss/sum(ss),
                       y2 = m[2] + 3*ee$vectors[2,]*ss/sum(ss))
pgood <- ggplot(df) + geom_point(aes(x,y,color=z)) + 
  scale_color_viridis_c()+ coord_fixed() +
  theme(legend.position = 'none') +
  geom_segment(aes(x=x1,y=y1,xend=x2,yend=y2), data=df_arrows, arrow=arrow(), size=2, color=orange)
pgood
```

$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{P}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\V}{\mathbf{V}}$$


---

## PCA reduced
  
Here, we can capture a lot of the variation and underlying
structure with just 1 dimension, 

instead of the original 2 (the coloring is for visualizing). 

```{r pca-reduced, fig.align='center',fig.height=4,fig.width=10,echo=FALSE}
xx = scale(x, center=m, scale=FALSE) %*% ee$vectors[,1]
df2 = data.frame(x=xx + m[1], y=xx + m[2], z=z)
pred <- ggplot(df2) + geom_point(aes(x=x,y=y,col=z)) + 
  theme(legend.position = 'none') +
  coord_fixed() +
  scale_color_viridis_c()
cowplot::plot_grid(pgood, pred, align = "v")
```

---

## PCA bad

What about other data structures?  Again in two dimensions

```{r spiral, fig.align='center',fig.height=5,fig.width=6, echo=FALSE}
tt = seq(0,4*pi,length=100)
df_spiral = data.frame(x = 3/2*tt*sin(tt), y = 0.5*tt*cos(tt), z=tt) %>%
  mutate(x = x/sd(x), y=y/sd(y))
ee = eigen(crossprod(scale(as.matrix(df_spiral[,1:2]))))
ss = sqrt(ee$values)
df_arrows = data.frame(x1 = 0, y1 = 0,
                       x2 =  ee$vectors[1,]*ss/sum(ss)*2,
                       y2 =  ee$vectors[2,]*ss/sum(ss)*2)
gsp = ggplot(df_spiral) + geom_point(aes(x=x,y=y,col=z)) + 
  coord_fixed() +
  scale_color_viridis_c() + theme(legend.position = 'none') +
  geom_segment(aes(x=x1,y=y1,xend=x2,yend=y2), data=df_arrows, arrow=arrow(), size=2,color=orange)
gsp
```

---

## PCA bad

Here, we have failed miserably.  

There is actually only 1 dimension
to this data (imagine walking up the spiral going 
from purple to yellow).  

However, when we write it as 1 PCA dimension,
all the points are all "mixed up". 

```{r spiral-reduced, fig.align='center',fig.height=5,fig.width=10, echo=FALSE}
xx = as.matrix(df_spiral[,1:2]) %*% ee$vectors[,1]
df_spiral2 = data.frame(x=xx, y=xx, z=tt)
gspbad <- ggplot(df_spiral2) + geom_point(aes(x=x,y=y,col=z)) + 
  scale_color_viridis_c() + coord_fixed() +
  theme(legend.position = 'none')
plot_grid(gsp,gspbad)
```

---

## Explanation
  
.pull-left[  
PCA wants to minimize distances (equivalently maximize
    variance).  
    
This means it .hand[slices] through the data at the __meatiest__ point, and then the next one, and so on.  

If the data are .secondary[curved] this is going to induce artifacts.  

PCA also looks at things as being .hand[close] if they are near each other in a Euclidean sense.  


On the spiral, our intuition says that things are close only if the distance is constrained to go along the curve.  
    
In other words, purple and blue are close, blue and yellow are not. 
]    
    
.pull-right[
```{r, fig.align='center',fig.height=5,fig.width=6, echo=FALSE}
gsp
```

]

---

## Kernel PCA

Classical PCA comes from $\X=  \U\D\V^{\top}$, the SVD of the (centered) data


However, we can just as easily get it from the outer product
$\mathbf{K} = \X\X^{\top} =  \U\D^2\U^{\top}$


The intuition behind KPCA is that $\mathbf{K}$ is an expansion into a
.hand[kernel space], where
$$\mathbf{K}_{i,i'} = k(x_i,\ x_{i'}) = \langle x_i,x_{i'} \rangle$$

We saw this trick before with feature expansion.

--

.emphasis[

1.  Specify a kernel function $k$,  many people use $k(x,x') = \exp\left( -d(x, x')/\gamma\right)$ where $d(x,x') = \norm{x-x'}_2^2$

2.  Form $\mathbf{K}_{i,i'} = k(x_i,x_{i'})$

3. Double center $\mathbf{K} = \mathbf{PKP}$ where $\mathbf{P} = \mathbf{I}_n - \mathbf{11}^\top / n$

3.  Take eigendecomposition $\mathbf{K} = \U\D^2\U^{\top}$
]

The scores are still $\mathbf{Z} = \U_M\D_M$


__Note:__ We don't explicitly
generate the feature map $\longrightarrow$ there are NO loadings

---

## An alternate view

To get the first PC
in classical PCA, we solve
$$\max_\alpha \Var{\X\alpha} \quad \textrm{ subject to } \quad \left|\left| \alpha \right|\right|_2^2 = 1$$


In the kernel setting we solve
$$\max_{g \in \mathcal{H}_k} \Var{g(X)} \quad \textrm{ subject to } \quad\left|\left| g \right|\right|_{\mathcal{H}_k} = 1$$

Here $\mathcal{H}_k$ is a function space determined by $k(x,x')$.

__Examples:__

$k(x,x') = x^\top x'$ gives back regular PCA

$k(x,x') = (1+x^\top x')^d$ gives a function space which contains all $d^{th}$-order polynomials.

$k(x,x') = \exp(-\norm{x-x'}^2/\gamma)^d$ gives a function space spanned by the infinite Fourier basis

For more details see [ESL 14.5] 

---

## Kernel PCA on the spiral

```{r get-kpca, echo=FALSE}
n = nrow(df_spiral)
I_M = (diag(n) - tcrossprod(rep(1,n))/n)
kp = (tcrossprod(as.matrix(df_spiral[,1:2])) + 1)^2
Kp = I_M %*% kp %*% I_M
Ep = eigen(Kp, symmetric = TRUE)
polydf = tibble(
  x=Ep$vectors[,1]*Ep$values[1],
  y=Ep$vectors[,2]*Ep$values[2],
  z = df_spiral$z)
kg = exp(-as.matrix(dist(df_spiral[,1:2]))^2 / .1)
Kg = I_M %*% kg %*% I_M
Eg = eigen(Kg, symmetric = TRUE)
gaussdf = tibble(
  x=Eg$vectors[,1]*Eg$values[1],
  y=Eg$vectors[,2]*Eg$values[2],
  z = df_spiral$z)
dfkern = bind_rows(df_spiral, df_spiral2, polydf, gaussdf)
dfkern$method = rep(c('data','pca','kpoly (d=2)', 'kgauss (gamma=.1)'), each=n)
```

```{r plot-kpca, echo=FALSE, fig.align='center', fig.height=6, fig.width=10}
dfkern %>% 
  ggplot(aes(x=x,y=y,color=z)) + geom_point() +
  facet_wrap(~method, scales='free', nrow=2) +
  scale_color_viridis_c() + theme(legend.position = 'none') +
  ylab('x2') + xlab('x1')
```

---

## KPCA: summary

Kernel PCA seeks to generalize the notion of
similarity using a kernel map


This can be interpreted as finding smooth,
orthogonal directions in an RKHS


This can allow us to start picking up nonlinear (in the original feature
space) aspects of our data


This new representation can be passed to a
supervised method to form a semisupervised
learner

.emphasis[This kernel is different than kernel smoothing!!]

--

Basic semi-supervised (see [ISLR 6.3.1])

1. You get data $\{(x_1,y_1),\ldots,(x_n,y_n)\}$.

2. You do something unsupervised on $\X$ to create new features (like PCA).

3. You use the learned features to find a predictor $\hat{f}$ (say, do OLS on them)

---
class: middle, center, inverse

# Next time...

Other nonlinear dimension reduction
