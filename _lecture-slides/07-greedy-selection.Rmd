---
title: "07 Greedy selection"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

## Selecting predictors

.pull-left[

Suppose we have a pile of predictors.

We estimate models with different subsets of predictors and use CV/Cp/AIC/BIC to decide which is best.

Sometimes you might have a few plausible subsets. Easy enough to choose with our criterion.

Sometimes you might just have a bunch of predictors, then what do you do?
]

--

.pull-right[
__All subsets__: estimate model based on every possible subset, use one with lowest AIC 

__Forward selection__: start with $\mathcal{S}=\varnothing$, add predictors greedily

__Backward selection__: start with $\mathcal{S}=\{1,\ldots,p\}$, remove greedily

__Hybrid__: combine forward and backward smartly
]

---

## Costs and benefits


__All subsets__: 

`r pro` estimates each subset

`r con` takes $2^p$ model fits. If $p=50$, this is about $10^{15}$ models. 

__Forward selection__: 

`r pro` computationally feasible

`r con` ignores some models, correlated predictors means bad performance

__Backward selection__: 

`r pro` computationally feasible

`r con` ignores some models, correlated predictors means bad performance

`r con` doesn't work if $p>n$

__Hybrid__: 

`r pro` visits more models than forward/backward

`r con` slower

---

## Synthetic example

```{r data-setup}
set.seed(123)
n <- 406
df <- tibble( # like data.frame, but columns can be functions of preceding
  x1 = rnorm(n),
  x2 = rnorm(n, mean = 2, sd = 1),
  x3 = rexp(n, rate = 1),
  x4 = x2 + rnorm(n, sd = .1),
  x5 = x1 + rnorm(n, sd = .1),
  x6 = x1 - x2 + rnorm(n, sd = .1),
  x7 = x1 + x3 + rnorm(n, sd = .1),
  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2)
)
```

--

$\mathbf{x}_1$ and $\mathbf{x}_2$ are the true predictors

But the rest are correlated with them

---

## Full model

```{r full-model}
full <- lm(y ~ ., data = df)
```

```{r full-table, echo=FALSE}
summary(full)
```

---

## True model

```{r true-model}
truth <- lm(y ~ x1 + x2, data = df)
```

```{r true-table, echo=FALSE}
summary(truth)
```

---

## All subsets

```{r try-them-all}
library(leaps)
trythemall <- regsubsets(y ~ ., data = df)
summary(trythemall)
```

---

## BIC and Cp

.pull-left[
```{r more-all-subsets1}
infocrit <- tibble(
  BIC = summary(trythemall)$bic, 
  Cp = summary(trythemall)$cp,
  size = 1:7)
```

```{r more-all-subsets2, echo=TRUE, dev="svg"}
info_plot <- infocrit %>% 
  pivot_longer(-size, names_to="crit") %>%
  ggplot(aes(size, value, color=crit)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~crit, 2, scales = "free_y") + 
  scale_color_brewer(palette = "Set1") +
  theme(axis.title.y = element_blank(), 
        legend.position = "none")
```

]

.pull-right[
```{r more-all-subsets3, echo=FALSE, dev="svg"}
info_plot
```
]

---

## Forward stepwise

```{r step-forward}
stepup <- regsubsets(y ~ ., data = df, method = "forward")
summary(stepup)
```

---

## BIC and Cp

```{r more-step-forward, echo=FALSE, dev="svg", fig.width=10, fig.height=5, fig.align="center"}
infocrit2 = tibble(
  BIC = summary(stepup)$bic, 
  Cp = summary(stepup)$cp,
  size = 1:7)
infocrit2 %>% 
  pivot_longer(-size, names_to="crit") %>%
  ggplot(aes(size, value, color=crit)) + 
  geom_point() + 
  geom_line() + 
  facet_wrap(~crit, 1, scales = "free_y") +
  scale_color_brewer(palette = "Set1") +
  theme(axis.title.y=element_blank(), legend.position = "none")
```

---

## Randomness and prediction error

All of that was for one data set

If we want to know how they compare, we should repeat many times
  1. Generate data
  1. Estimate with different algorithms
  1. Predict new data
  1. Examine prediction MSE
  
--

I'm not going to do all subsets, just the truth, forward selection, and the full model

For forward selection, I'll use Cp to choose the final model

---

## Code for simulation

```{r replication-exercise, cache=TRUE}
fun <- function(n = 812) {
  df <- tibble( # generate data
    x1 = rnorm(n), 
    x2 = rnorm(n, mean=2, sd=1), 
    x3 = rexp(n, rate=1),
    x4 = x2 + rnorm(n, sd=.1), 
    x5 = x1 + rnorm(n, sd=.1),
    x6 = x1 - x2 + rnorm(n, sd=.1), 
    x7 = x1 + x3 + rnorm(n, sd=.1),
    y = x1*3 + x2/3 + rnorm(n, sd=2.2)
  )
  train <- df[1:ceiling(n / 2), ] # half the data for training
  test <- df[(ceiling(n / 2) + 1):n, ] # half the data for evaluation
  oracle <- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs
  full <- lm(y ~ ., data = train)
  stepup <- regsubsets(y ~ ., data = train, method = "forward")
  coefs <- double(ncol(df))
  names(coefs) <- names(coef(full))
  best <- which.min(summary(stepup)$cp) # choose best size using training data
  coefs[names(coef(stepup, best))] <- coef(stepup, best)
  mse <- function(x) mean( (test$y - x)^2)
  out <- c(
    oracle = mse(predict(oracle, newdata = test)),
    full = mse(predict(full, newdata = test)),
    step = mse(as.matrix(cbind(1,test[,-8])) %*% coefs),
    truth = mse(as.matrix(test[,1:2]) %*% c(3, 1/3)), # correct model and coefs
    nbest = best)
  out
}
set.seed(12345)
sim = t(replicate(50, fun())) # runs our simulation 50 times
```



---

## Results


```{r synth-results, dev="svg", fig.width=10, fig.height=5, echo=FALSE, fig.align="center"}
sim <- as.data.frame(sim)
mses <- sim %>% 
  dplyr::select(-nbest) %>% 
  mutate(full = (full - truth) / truth * 100,
         step = (step - truth) / truth * 100, 
         oracle = (oracle - truth) / truth * 100, truth=NULL)
g1 = mses %>% 
  pivot_longer(everything(), names_to = "method", values_to = "mse") %>%
  ggplot(aes(method, mse, fill = method)) + 
  geom_boxplot() +
  scale_fill_brewer(palette = "Set1") +
  theme(legend.position = "none") +
  ylab("% increase in mse relative to truth")
g2 = sim %>% 
  ggplot(aes(y = nbest)) + 
  geom_bar(aes(x = ..prop..), fill = orange) + 
  xlab("proportion of times") +
  ylim(c(0, 7.5)) + 
  ylab("# selected predictors")
plot_grid(g1, g2, rel_widths = c(.66, .34), axis = "b")
```

---
class: middle, center
background-image: url("https://disher.com/wp-content/uploads/2017/02/Product-Constraints.jpg")
background-size: cover

.primary[.larger[Next time...]]

.primary[.larger[Module]] .huge-blue-number[2]

.primary[regularization, constraints, and nonparametrics]
