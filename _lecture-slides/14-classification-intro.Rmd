---
title: "14 Classification"
author: 
  - "STAT 406"
  - "Daniel J. McDonald"
date: 'Last modified - `r Sys.Date()`'
---




```{r setup, include=FALSE, warning=FALSE, message=FALSE}
source("rmd_config.R")
```

class: middle, center
background-image: url("gfx/proforhobo.png")
background-size: cover



.hand[.secondary[.larger[Module]]] .huge-orange-number[3]



.fourth-color[.hand[.larger[
Professor or Hobo? 
]]]


---


## An Overview of Classification

$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$



* A person arrives at an emergency room with a set of symptoms that
could be 1 of 3 possible conditions. Which one is it?

* An online banking service must be able to determine whether each
transaction is fraudulent or not, using a customer's location, past
transaction history, etc.

* Given a set of individuals sequenced DNA, can we determine whether
various mutations are associated with different phenotypes?

--

These problems are __not__ regression
problems. They are __classification__ problems.

---

## The Set-up

It begins just like regression: suppose we have observations
$$\{(x_1,y_1),\ldots,(x_n,y_n)\}$$

Again, we want to estimate a function that maps $X$ to $Y$ to
predict as yet observed data.

(This function is known as a __classifier__)


The same constraints apply:

* We want a classifier that predicts test data, not just the training
data.

* Often, this comes with the introduction of some bias to get lower
variance and better predictions.

---
layout: true

## How do we measure quality?

Before in regression, we have $y_i \in \mathbb{R}$ and use squared error loss to measure accuracy: $(y - \hat{y})^2$
Instead, let $y \in \mathcal{K} = \{1,\ldots, K\}$

(This is arbitrary, sometimes other numbers, such as $\{-1,1\}$ will be
used)

We can always take "factors": $\{\textrm{cat},\textrm{dog}\}$ and convert to integers, which is what we assume.


We again make predictions $\hat{y}=k$ based on the data


* We get zero loss if we predict the right class
* We lose $\ell(k,k')$ on $(k\neq k')$ for incorrect predictions


---

Suppose you have a fever of 39ยบ C. You get a rapid test on campus.

| Loss | Test + | Test - |
|:---: | :---: | :---: |
| Are + | 0 | Infect others |
| Are - | Isolation | 0 |

---

Suppose you have a fever of 39ยบ C. You get a rapid test on campus.

| Loss | Test + | Test - |
|:---: | :---: | :---: |
| Are + | 0 | 1 |
| Are - | 1 | 0 |


---

**We're going to use $g(x)$ to be our classifier. It takes values in $\mathcal{K}$.**

---
layout: false

## How do we measure quality?

Again, we appeal to risk
$$R_n(g) = E [\ell(Y,g(X))]$$ If we use the law of
total probability, this can be written
$$R_n(g) = E_X \sum_{y=1}^K \ell(y,\; g(X)) Pr(Y = y \given X)$$
This can be minimized over $X$, to produce
$$g_*(X) = \arg\min_{g \in \mathcal{G}} \sum_{y=1}^K \ell(y,g(X)) Pr(Y = y \given X)$$


$g_*$ is named the __Bayes' classifier__. 

$R_n(g_*)$ is the called the __Bayes' limit__ or __Bayes' Risk__. 

It's the best we could hope to do if we knew the distribution of the data.

--

But we don't, so we'll try to do our best to estimate it.

---
layout: true

## Best classifier

If we make specific choices for $\ell$, we can find $g_*$ exactly (pretending we know the distribution)


As $Y$ takes only a few values, __zero-one__
prediction risk is natural
$$\ell(y,\ g(x)) = \begin{cases}0 & y=g(x)\\1 & y\neq g(x) \end{cases} \Longrightarrow R_n(g) = \Expect{\ell(Y,\ g(X))} = Pr(g(X) \neq Y),$$

---

| Loss | Test + | Test - |
|:---: | :---: | :---: |
| Are + | 0 | 1 |
| Are - | 1 | 0 |

---

This means we want to __label__ or
__classify__ a new observation $(x_0,y_0)$ such that
$g(x_0) = y_0$ as often as possible


Under this loss, we have
$$g_*(X) = \argmin_{g \in\mathcal{G}} Pr(g(X) \neq Y) = \argmin_{g \in \mathcal{G}} \left[ 1 - Pr(Y = g(x) | X=x)\right]  = \argmax_{g \in \mathcal{G}} Pr(Y = g(x) | X=x )$$

---
layout: false

## Best classifier

Suppose we encode a two-class response as $y \in \{0,1\}$

Zero-One loss is natural

--

**But just for fun** Let's try using __squared error loss__ instead:
$\ell(y,\ f(x)) = (y - f(x))^2$


Then, the Bayes' Classifier is
$$f_*(x) = E[ Y \given X = x] = Pr(Y = 1 \given X)$$ 
(note that $f_* \in [0,1]$ is the regression function)

Hence, we achieve the same Bayes' Risk with squared error
classification by discretizing the probability:

$$g_*(x) = \begin{cases}0 & f_*(x) < 1/2\\1 & \textrm{else}\end{cases}$$

--

Natural Classifier: 
1. Estimate $f_*$ using any method we've learned. 
2. Predict 0 if $\hat{f}(x)$ is less than 1/2, else predict 1.

---
layout: true


## Claim: Classification is easier than regression

---

.emphasis[
Let $\hat{f}$ be any estimate of $f_*$

Let $\widehat g (x) = \begin{cases}0 & \hat f(x) < 1/2\\1 & else\end{cases}$

"Do regression, threshold to classify"
]


.alert[
It can be shown that 
$$\begin{aligned}
  &Pr(Y \neq \hat{g}(X) \given X) - Pr(Y \neq g_*(X) \given X)  \\
  &= \cdots =\\
& =  2\left|f_*(X) - \frac{1}{2}\right|\mathbf{1}\left(g_*(X)\neq \hat{g}(X)\right) \end{aligned}$$
]


---

.alert[
Now
$$g_*(X)\neq \widehat{g}(X) \Rightarrow |\hat{f}(X) - f_*(X)| \geq |\hat{f}(X) - 1/2|$$
Therefore

$$\begin{aligned}
 &Pr(Y \neq \hat{g}(X)) - Pr(Y \neq g_*(X)) \\
& =  \int\left(Pr(Y \neq \hat{g}(X) \given X=x) - Pr(Y \neq g_*(X) \given X=x)\right)dx   \\
& =  \int 2\left|\hat{f}(x) - \frac{1}{2}\right|\mathbf{1}(g_*(x)\neq \hat{g}(x))dx  \\
& \leq  2\int |\hat{f}(x) - f_*(x)| \mathbf{1}(g_*(x)\neq \hat{g}(x))dx \\
& \leq  2\int |\hat{f}(x) - f_*(x)|dx \end{aligned}$$
]

.hand[Misclassification error < 2 x Regression error]


---
layout: false

## Bayes' Classifier and class densities

Using **Bayes' theorem**

$$\begin{aligned}
f_*(X) & = Pr(Y = 1 \given X) \\ 
&= \frac{Pr(X\given Y=1) Pr(Y=1)}{Pr(X)}\\
& =\frac{Pr(X\given Y = 1) Pr(Y = 1)}{\sum_{k \in \{0,1\}} Pr(X\given Y = k) Pr(Y = k)} \\ & = \frac{p_1(X) \pi}{ p_1(X)\pi + p_0(X)(1-\pi)}\end{aligned}$$

* We call $p_k(X)$ the __class (conditional) densities__

* $\pi$ is the __marginal probability__ $P(Y=1)$

The Bayes' Classifier (best classifier) can be rewritten 

$$g_*(X) = \begin{cases}
1 & \textrm{ if } \frac{p_1(X)}{p_0(X)} > \frac{1-\pi}{\pi} \\
0  &  \textrm{ otherwise}
\end{cases}$$

---

## How to find a classifier

.hand[Why did we go through that math?]

Each expression for $g_*$ suggests a way to find a classifier

* __Density estimation:__ Estimate $\pi$ and $p_g$

* __Regression:__ Find an
estimate $\hat{f}$ of $f^*$ and plug it in to the Bayes' rule (compare the predicted value to 1/2)

* __Empirical risk minimization:__ Choose a set
of classifiers $\mathcal{G}$ and find $g \in \mathcal{G}$ that minimizes
some estimate of $R_n(g)$
    
> (This can be quite challenging as, unlike in regression, the
training error is nonconvex)



--

Easiest classifier when $y\in \{0,\ 1\}$:

(stupidest version of the second case...)

```{r eval=FALSE}
ghat <- round(predict(lm(y~., trainingdata)))
```

Think about why this may not be very good. (At least 2 reasons I can think of.)

---
class: middle, inverse, center

# Next time:

Estimating the densities