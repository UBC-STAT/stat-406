<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>08 Ridge regression</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 08 Ridge regression
### STAT 406
### Daniel J. McDonald
### Last modified - 2020-09-21

---

class: middle, center
background-image: url("https://disher.com/wp-content/uploads/2017/02/Product-Constraints.jpg")
background-size: cover





.primary[.larger[Module]] .huge-blue-number[2]

.primary[regularization, constraints, and nonparametrics]

---


## Recap

So far, we thought of __model selection__ as

.hand[Decide which predictors we would like to use in our linear model]

To do this, we used a risk estimate, and chose the "model" with the lowest estimate

--

Moving forward, we need to generalize this to

.hand[Decide which of possibly infinite prediction functions] `\(f\in\mathcal{F}\)` .hand[to use]

Thankfully, this isn't really any different. We still use those same risk estimates.


__Remember:__ We were choosing models that balance bias and variance (and hence have low prediction risk).


`$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\E}{E}
\renewcommand{\P}{\mathbb{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\brt}{\widehat{\beta}^R_{s}}
\newcommand{\brl}{\widehat{\beta}^R_{\lambda}}
\newcommand{\bls}{\widehat{\beta}_{ols}}
\newcommand{\blt}{\widehat{\beta}^L_{s}}
\newcommand{\bll}{\widehat{\beta}^L_{\lambda}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$`

---

## Regularization

* Another way to control bias and variance is through __regularization__ or
__shrinkage__.  


* Rather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.

* I mean __ALL__.

* But, make your estimates of `\(\beta\)` "smaller"

---

## Brief aside on optimization

* An optimization problem has 2 components:

    1. The "Objective function": e.g. `\(\frac{1}{n}\sum_i (y_i-x^\top_i \beta)^2\)`.
    2. The "constraint": e.g. "fewer than 5 non-zero entries in `\(\beta\)`".
    
* A constrained minimization problem is written


`$$\min_\beta f(\beta) \mbox{ subject to } C(\beta)$$`

* `\(f(\beta)\)` is the objective function
* `\(C(\beta)\)` is the constraint

---

## Ridge regression (constrained version)

One way to do this for regression is to solve (say):

`\begin{aligned}
\min_\beta &amp;\ \ \frac{1}{n}\sum_i (y_i-x^\top_i \beta)^2 \\
\mbox{s.t.} &amp;\ \ \sum_j \beta^2_j &lt; s
\end{aligned}`

for some `\(s&gt;0\)`.

* This is called "ridge regression".

* The __minimizer__ of this problem is called `\(\brt\)`

--

Compare this to ordinary least squares:

`\begin{aligned}
\min_\beta &amp;\ \ \frac{1}{n}\sum_i (y_i-x^\top_i \beta)^2 \\
\mbox{s.t.} &amp;\ \ \beta \in \R^p
\end{aligned}`


---

## Geometry of ridge regression (2 dimensions)



&lt;img src="rmd_gfx/08-ridge-regression/unnamed-chunk-1-1.svg" style="display: block; margin: auto;" /&gt;

---

## Ridge regression

An equivalent way to write

`$$\brt = \arg \min_{ || \beta ||_2^2 \leq s} \frac{1}{n}\sum_i (y_i-x^\top_i \beta)^2$$`


is in the __Lagrangian__ form


`$$\brl = \arg \min_{ \beta} \frac{1}{n}\sum_i (y_i-x^\top_i \beta)^2 + \lambda || \beta ||_2^2.$$`


For every `\(\lambda\)` there is a unique `\(s\)` (and vice versa) that makes 

`$$\brt = \brl$$`

--

Observe:

* `\(\lambda = 0\)` (or `\(s = \infty\)`) makes `\(\brl = \bls\)`
* Any `\(\lambda &gt; 0\)` (or `\(s &lt;\infty\)`)  penalizes larger values of `\(\beta\)`, effectively shrinking them.


Note: `\(\lambda\)` and `\(s\)` are known as __tuning parameters__

---

## Example data

__prostate__ data from [ESL]


```r
prostate = as_tibble(
  read.table(
    "http://www.web.stanford.edu/~hastie/ElemStatLearn/datasets/prostate.data", 
    header = TRUE))
prostate
```

```
## # A tibble: 97 x 10
##    lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train
##     &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;lgl&gt;
##  1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE 
##  2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE 
##  3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE 
##  4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE 
##  5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE 
##  6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE 
##  7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE
##  8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE 
##  9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE
## 10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE
## # â€¦ with 87 more rows
```

???

Use lpsa as response.

---

## Ridge regression path

&lt;img src="rmd_gfx/08-ridge-regression/process-prostate-1.svg" style="display: block; margin: auto;" /&gt;

--

Model selection: means __choose__ some `\(\lambda=\)` vertical line on the path.

---

## Solving the minimization

* One nice thing about ridge regression is that it has a closed-form solution (like OLS)


`$$\brl = (\X^\top\X + \lambda I)^{-1}\X^\top \y$$`

* This is easy to calculate in `R` for any `\(\lambda\)`.

* However, computations and interpretation are simplified if we examine the Singular Value Decomposition of `\(\X = \mathbf{UDV}^\top\)`.

* Here `\(\mathbf{D}\)` is diagonal and `\(\mathbf{U}\)` and `\(\mathbf{V}\)` are orthonormal

* Then,


`$$\brl = (\X^\top \X + \lambda I)^{-1}\X^\top \y = (\mathbf{VD}^2\mathbf{V}^\top + \lambda \mathbf{I})^{-1}\mathbf{VDU}^\top \y
= \mathbf{V}(\mathbf{D}^2+\lambda \mathbf{I})^{-1} \mathbf{DU}^\top \y.$$`

* For computations, now we only need to invert `\(D\)`.

---

## Comparing with OLS


`$$\bls = (\X^\top\X)^{-1}\X^\top \y = (\mathbf{VD}^2\mathbf{V}^\top)^{-1}\mathbf{VDU}^\top \y = \mathbf{VD}^{-2}\mathbf{DU}^\top \y = \mathbf{VD}^{-1}\mathbf{U}^\top \y$$`

`$$\brl = (\X^\top \X + \lambda \mathbf{I})^{-1}\X^\top \y = \mathbf{V}(\mathbf{D}^2+\lambda \mathbf{I})^{-1} \mathbf{DU}^\top \y.$$`


* Notice that `\(\bls\)` depends on `\(d_j/d_j^2\)` while `\(\brl\)` depends on `\(d_j/(d_j^2 + \lambda)\)`.

* Ridge regression makes the coefficients smaller relative to OLS.

* But if `\(\X\)` has small singular values, ridge regression compensates with `\(\lambda\)` in the denominator.


---

## Ridge regression and multicollinearity

__Multicollinearity:__  a linear combination of predictor variables is nearly equal to another predictor variable. 

Some comments:

* A better phrase: `\(\X\)` is ill-conditioned
* AKA "(numerically) rank-deficient".
* `\(\X = \mathbf{U D V}^\top\)` ill-conditioned `\(\Longleftrightarrow\)` some elements of `\(D \approx 0\)`
* `\(\bls= \mathbf{V D}^{-1} \mathbf{U}^\top \y\)`, so small entries of `\(D\)` `\(\Longleftrightarrow\)` huge elements of `\(D^{-1}\)`
* Means huge variance: `\(\Var{\bls} =  \sigma^2(\X^\top \X)^{-1} = \sigma^2 \mathbf{V D}^{-2} \mathbf{V}^\top\)`


Ridge Regression fixes this problem by preventing the division by a near-zero number

__Conclusion:__ `\((\X^{\top}\X)^{-1}\)` can be really unstable, while `\((\X^{\top}\X + \lambda I)^{-1}\)` is not.

---

## Can we get the best of both worlds?

To recap:

* Deciding which predictors to include, adding quadratic terms, or interactions is __model selection__ (specifically __variable selection__).

* Ridge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.  


Ridge regression: `\(\min ||\y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_2^2 \leq t\)` 

Best linear regression model: `\(\min ||\y-\X\beta||_2^2 \textrm{ subject to } ||\beta||_0 \leq t\)`

--

`\(||\beta||_0\)` is the number of nonzero elements in `\(\beta\)`

Finding the best linear model is a nonconvex optimization problem (In fact, it is NP-hard)

Ridge regression is convex (easy to solve), but doesn't do __variable__ selection

Can we somehow "interpolate" to get both?


Note: selecting `\(\lambda\)` is still __model__ selection, but we've included __all__ the variables.

---
class: center, middle, inverse

# Next time...

The lasso, interpolating variable selection and model selection
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
