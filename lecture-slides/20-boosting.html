<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>20 Boosting</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 20 Boosting
### STAT 406
### Daniel J. McDonald
### Last modified - 2020-08-08

---


## Last time

`$$\newcommand{\Expect}[1]{\mathbb{E}\left[ #1 \right]}
\newcommand{\Var}[1]{\mathbb{V}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}$$`




We learned about bagging, for averaging low-bias estimators.

Today, we examine it's opposite: Boosting.

__Boosting__ also combines estimators, but it combines __high-bias__ (low-variance) estimators.

Boosting has a number of flavors. And if you Google descriptions, most are wrong.

For a deep (and accurate) treatment, see [ESL] Chapter 10

--

We'll discuss 2 flavors: AdaBoost and Gradient Boosting

Neither requires a tree, but that's the typical usage.

Boosting needs a "weak learner", so small trees (called stumps) are natural.

---

## AdaBoost (Freund and Schapire)

Let `\(G_m(x)\)` be a weak learner (say a tree with one split)

.emphasis[

__Algorithm (AdaBoost):__

1. Set observation weights `\(w_i=1/n\)`.

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Estimate a classifier `\(G_m(x)\)` using weights `\(w_i\)`
    
    b. Calculate it's weighted error `\(\textrm{err}_m = \sum_{i=1}^n w_i I(y_i \neq G_m(x_i)) / \sum w_i\)`
    
    c. Set `\(\alpha_m = \log((1-\textrm{err}_m)/\text{err}_m)\)`
    
    d. Update `\(w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G_m(x_i)))\)`

3. Final classifier is `\(G(x) = \textrm{sign}\left( \sum_{m=1}^M \alpha_m G_m(x)\right)\)`
]

---

## Using mobility data again




```r
library(gbm)
adab = gbm(mobile~., data=train, n.trees = 500, distribution = "adaboost")
preds$adab = as.numeric(predict(adab, test)&gt;0)
summary(adab)
```

&lt;img src="rmd_gfx/20-boosting/unnamed-chunk-2-1.svg" style="display: block; margin: auto;" /&gt;

```
##                                                 var    rel.inf
## Single_mothers                       Single_mothers 14.0404924
## Local_tax_rate                       Local_tax_rate  6.7989919
## Religious                                 Religious  6.6286695
## Commute                                     Commute  6.4179702
## Test_scores                             Test_scores  5.9974412
## Black                                         Black  4.8716771
## Manufacturing                         Manufacturing  4.7307967
## Gini_99                                     Gini_99  4.2246181
## Chinese_imports                     Chinese_imports  3.9555660
## Latitude                                   Latitude  3.8442520
## Foreign_born                           Foreign_born  2.8028885
## Middle_class                           Middle_class  2.7339875
## Progressivity                         Progressivity  2.6475539
## Graduation                               Graduation  2.4408142
## Share01                                     Share01  2.4262649
## Tuition                                     Tuition  2.3563082
## Longitude                                 Longitude  2.0708840
## Student_teacher_ratio         Student_teacher_ratio  2.0645286
## School_spending                     School_spending  1.9700753
## Migration_out                         Migration_out  1.8248691
## Married                                     Married  1.8081761
## HS_dropout                               HS_dropout  1.6722032
## Colleges                                   Colleges  1.5849022
## Local_gov_spending               Local_gov_spending  1.2733602
## Migration_in                           Migration_in  1.2450674
## Social_capital                       Social_capital  1.2442027
## Seg_racial                               Seg_racial  0.9630583
## Teenage_labor                         Teenage_labor  0.7700715
## Labor_force_participation Labor_force_participation  0.6406051
## Income                                       Income  0.6310008
## Divorced                                   Divorced  0.5880420
## Seg_poverty                             Seg_poverty  0.5662602
## Violent_crime                         Violent_crime  0.5654377
## Population                               Population  0.4550882
## EITC                                           EITC  0.4385737
## Seg_affluence                         Seg_affluence  0.3075372
## Seg_income                               Seg_income  0.2414435
## Gini                                           Gini  0.1563206
## Urban                                         Urban  0.0000000
```

---

## Forward stagewise additive modeling

Generic for regression or classification, any weak learner

.emphasis[

__Algorithm:__

1. Set initial predictor `\(f_0(x)=0\)`

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Compute `$$(\beta_m, G_m) = \arg\min_{\beta, G} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i)\right)$$`
    
    b. Set `\(f_m(x) = f_{m-1}(x) + \beta_m G_m(x)\)`
    
3. Final classifier is `\(G(x) = \textrm{sign}\left( f_M(x) \right)\)`
]

Here, `\(L\)` is a loss function that measures how prediction accuracy

If `\(L(y,\ f(x))= \exp(-y f(x))\)` and `\(G_m\)` is a classifier, then this is equivalent to AdaBoost. Proven 5 years later (Friedman, Hastie, and Tibshirani 2000).

---

## So what?

It turns out that "exponential loss" `\(L(y,\ f(x))= \exp(-y f(x))\)` is not very robust.

Here are some other loss functions for 0-1 classification

&lt;img src="rmd_gfx/20-boosting/loss-funs-1.svg" style="display: block; margin: auto;" /&gt;

???

Want losses which penalize negative margin a lot, but not positive margins.

---

## Gradient boosting


In the forward stagewise algorithm, we solved a minimization and then made an update 

`\(f_m(x) = f_{m-1}(x) + \beta_m G_m(x)\)`.

For most loss functions, `\(\arg\min_{\beta, G} \sum_{i=1}^n L\left(y_i,\ f_{m-1}(x_i) + \beta G(x_i)\right)\)` cannot be solved

Instead, if we take one gradient step toward the minimum, we get

`\(f_m(x) = f_{m-1}(x) -\gamma_m \nabla L(y,f_{m-1}(x))\)`

Then do line search on `\(\gamma_m\)`

This is called __Gradient boosting__

Notice how similar the update steps look.

Gradient boosting goes only part of the way toward the minimum at each `\(m\)`. This has two implications:

1. Since we're not fitting `\(G\)` to the data as hard, the "learner" is weaker.

2. This procedure is computationally much simpler.


---

## Gradient boosting

.emphasis[

__Algorithm:__

1. Set initial predictor `\(f_0(x)=\overline{y}\)`

2. Until we quit ( `\(m&lt;M\)` iterations )
    
    a. Compute pseudo-residuals (what is the gradient of `\(L(y,f)=(y-f(x))^2\)`?)
    `$$r_i = -\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}\bigg|_{f(x_i)=f_{m-1}(x_i)}$$`
    
    b. Estimate weak learner, `\(G_m\)`, with the training set `\(\{r_i, x_i\}\)`.
    
    c. Find the step size `\(\gamma_m = \arg\min_\gamma \sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \gamma G_m(x_i))\)`
    
    b. Set `\(f_m(x) = f_{m-1}(x) + \gamma_m G_m(x)\)`
    
3. Final predictor is `\(f_M(x)\)`.
]


```r
grad_boost = gbm(mobile~., data=train, n.trees = 500, distribution = "bernoulli")
```

---

## Gradient boosting modifications

* Typically done with "small" trees, not stumps because of the gradient. You can specify the size. Usually 4-8 terminal nodes is recommended (more gives more interactions between predictors)

* Usually modify the gradient step to `\(f_m(x) = f_{m-1}(x) + \gamma_m \alpha G_m(x)\)` with `\(0&lt;\alpha&lt;1\)`. Helps to keep from fitting too hard.

* Often combined with Bagging so that each step is fit using a bootstrap resample of the data. Gives us out-of-bag options.

* There are many other extensions, notably XGBoost.

&lt;img src="rmd_gfx/20-boosting/unnamed-chunk-3-1.svg" style="display: block; margin: auto;" /&gt;

---
class: middle, inverse, center

# Next time...

Neural networks and deep learning, the beginning
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
