<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>21 Neural nets</title>
    <meta charset="utf-8" />
    <meta name="author" content="STAT 406" />
    <meta name="author" content="Daniel J. McDonald" />
    <link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
    <script src="libs/anchor-sections/anchor-sections.js"></script>
    <script src="https://kit.fontawesome.com/ae71192e04.js" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="materials/xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="slides-style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# 21 Neural nets
### STAT 406
### Daniel J. McDonald
### Last modified - 2020-11-03

---





## Overview

Neural networks are models for supervised
learning

 
Linear combinations of features  are passed
through a non-linear transformation in successive layers

 
At the top layer, the resulting latent
factors are fed into an algorithm for
predictions

(Most commonly via least squares or logistic regression)

 
Chapter 11 in [ESL] is a good introductory reference for neural networks

---

## Background

.pull-left[
Neural networks have come about in 3 "waves" 

The first was an attempt in the 1950s to model the mechanics of the human brain

Through psychological and anatomical experimentation, it appeared the
brain worked by

-   taking atomic units known as __neurons__ ,
    which can either be "on" or "off"

-   putting them in __networks__  with each
    other, where the __signal__  is given by
    which neurons are "on" at a given time

 
Crucially, a neuron itself interprets the status of other neurons

There weren't really computers, so we couldn't estimate these things
]

.pull-right[
![](https://3s81si1s5ygj3mzby34dq6qf-wpengine.netdna-ssl.com/wp-content/uploads/2015/05/neuralnets-678x381.jpg)
]

---

## Background

After the development of parallel, distributed computation in the 1980s,
this "artificial intelligence" view was diminished

And neural networks gained popularity 

But, the growing popularity of SVMs and boosting/bagging in the late
1990s, neural networks again fell out of favor

This was due to many of the problems we'll discuss (non convexity being
the main one)

--

In the mid 2000's, new approaches for
__initializing__  neural networks became
available

 
These approaches are collectively known as __deep
learning__

 
Together, some state-of-the-art performance on various classification
tasks have been accomplished via neural networks

Today, Neural Networks/Deep Learning are the hottest...


`$$\newcommand{\Expect}[1]{E\left[ #1 \right]}
\newcommand{\Var}[1]{\mathrm{Var}\left[ #1 \right]}
\newcommand{\Cov}[2]{\mathrm{Cov}\left[#1,\ #2\right]}
\newcommand{\given}{\ \vert\ }
\newcommand{\argmin}{\arg\min}
\newcommand{\argmax}{\arg\max}
\newcommand{\R}{\mathbb{R}}
\newcommand{\P}{\mathbb{P}}
\renewcommand{\hat}{\widehat}
\newcommand{\tr}[1]{\mbox{tr}(#1)}
\newcommand{\X}{\mathbf{X}}
\newcommand{\y}{\mathbf{y}}$$`

---


## High level overview

.center[
![:scale 65%](https://upload.wikimedia.org/wikipedia/commons/8/82/FeedForwardNN.png)
]

---

## Recall nonparametric regression

Suppose `\(Y \in \mathbb{R}\)` and we are trying estimate
the regression function `$$\Expect{Y\given X} = f_*(X)$$`

 
Earlier, we discussed basis expansion, 



1.  We know `\(f_*(X) =\sum_{k=1}^\infty \beta_k \phi_k(x)\)` some basis `\(\phi_1,\phi_2,\ldots\)`

2.  Truncate this expansion at `\(K\)`: 
    `\(f_*^K(X) \approx \sum_{k=1}^K \beta_k \phi_k(x)\)`

3.  Estimate `\(\beta_k\)` with least squares

--

The weaknesses of this approach are:

-   The basis is fixed and independent of the data

-   If `\(p\)` is large, then nonparametrics doesn't work well at all (recall the Curse of Dimensionality)

-   If the basis doesn't "agree" with `\(f_*\)`, then `\(K\)` will have to be
    large to capture the structure

-   What if parts of `\(f_*\)` have substantially different structure?

An alternative would be to have the data
__tell__ us what kind of basis to use

---

## High level overview

Let `\(\mu(x) = \Expect{Y \given X=x}\)`

Write `\(L\)` as the __link function__, for example, `\(L(p)=\log\left(\frac{p}{1-p}\right)\)` 

A single layer neural network is
`$$L(\mu(x)) = \beta_0 + \sum_{k=1}^K \beta_k \ \sigma(\alpha_{k0} + \alpha_k^{\top}x)$$`

--

__Compare:__  
A nonparametric GLM would have the form
`$$L(\mu(x)) = \beta_0 + \sum_{k=1}^K \beta_k {\phi_k(x)}$$`

For example:

`$$\log\left(\frac{\mu(x)}{1-\mu(x)}\right)=\beta_0 + \sum_{k=1}^K \beta_k {\phi_k(x)}$$`

---

## Terminology

`$$L(\mu(x)) = {\beta_0} + \sum_{k=1}^{{K}} {\beta_k} {\sigma(\alpha_{k0} + \alpha_k^{\top}x)}$$`
The main components are

-   The derived features `\({z_k = \sigma(\alpha_{k0} + \alpha_k^{\top}x)}\)` and are called the __hidden units__

-   The function `\({\sigma}\)` is called the __activation function__  

      
-   The parameters
`\({\beta_0},{\beta_k},{\alpha_{k0}},{\alpha_k}\)` are estimated from the data.

-   The number of hidden units `\({K}\)` is a tuning
    parameter
    
- `\(\beta_0\)` and `\(\alpha_{k0}\)` are usually called __biases__ (I'm not going to include them in future formulas, just for space)    

---

## High level overview

__Example:__

If `\(L(\mu) = \mu\)`,
then we are doing regression:
`$$\mu(x) = \sum_{k=1}^K \beta_k \sigma\left(\sum_{j=1}^p\alpha_{kj}x_j\right)$$`
but in a transformed space

 
__Two observations:__

-   The `\(\sigma\)` function generates a __feature
    map__

-   If `\(\sigma(u) = u\)`, then neural networks reduce to classic least
    squares

 
Let's discuss each of these `\(\ldots\)`

---

## Feature map

We start with `\(p\)` covariates and we generate `\(K\)` features


.pull-left[

__GLMs with a (feature)  transformation__
`$$\begin{aligned}
&amp;\Phi(x) \\
&amp; = 
(1, x_1, \ldots, x_p, x_1^2,\ldots,x_p^2, x_1x_2, \ldots, x_{p-1}x_p) \\
&amp; =
(\phi_1(x),\ldots,\phi_K(x))
\end{aligned}$$`

 
Before feature map: 

`\(L(\mu(x)) = \sum_{j=1}^p \beta_j x_j\)`

After feature map:

`\(L(\mu(x)) =  \beta^{\top}\Phi(x) = \sum_{k=1}^K \beta_k \phi_k(x)\)`

]

.pull-right[

__Neural network__

`\(z_k = \sigma\left( \sum_{j=1}^p\alpha_{kj}x_j\right) = \sigma\left( \alpha_{k}^{\top}x\right)\)`

This gives

`\(\Phi(x) = (z_1, \ldots,z_K)^{\top} \in \mathbb{R}^{K}\)`

and

`$$\begin{aligned}L(\mu(x)) &amp;=\beta^{\top} \Phi(x)=\beta^\top z\\ 
&amp;=  \sum_{k=1}^K \beta_k \sigma\left( \sum_{j=1}^p\alpha_{kj}x_j\right)\end{aligned}$$`

]

---

## Activation functions

If `\(\sigma(u) = u\)` is linear, then we recover a linear model (Try to show this)

ReLU is the current fashion

&lt;img src="rmd_gfx/21-nnets-intro/sigmoid-1.svg" style="display: block; margin: auto;" /&gt;

---

## Hierarchical model

.pull-left[.center[
![:scale 75%](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1280px-Colored_neural_network.svg.png)]
]

.pull-right[


`\(z_k   = \sigma(\alpha_k^{\top}x) \quad  ( k = 1, \ldots K)\)`

`\(w_g  = \beta_g^{\top}z \quad  ( g = 1, \ldots G)\)`

`\(\mu_g(x) = L^{-1}(w_g)\)`


The output depends on the application. The goal is to map `\(w_g\)` to the appropriate space

-   .hand[Regression]:  The link
    function is `\(L(u) = u\)` (here, `\(G=1\)`)

-   .hand[Classification]:  With `\(G\)`
    classes, we are modeling `\(\pi_g = P(Y = g\given X=x)\)` and
    `\(L = \textrm{logit}\)`:
    
    `\(\hat{\pi}_g(x) = \frac{e^{w_g}}{\sum_{g'=1}^G e^{w_{g'}}}\)`
    
    `\(\hat{y} = \widehat{g}(x) = \argmax_g \hat{\pi}_g(x)\)`
    
    This is called the __softmax__  function for
    historical reasons

]

---
class: middle,inverse,center

# Next time...

How do we estimate these monsters?
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="materials/macros.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "zenburn",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
