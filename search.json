[
  {
    "objectID": "schedule/handouts/lab00-git.html",
    "href": "schedule/handouts/lab00-git.html",
    "title": "Lab 00 Git",
    "section": "",
    "text": "Check your Canvas profile settings to ensure the email associated with your Canvas account is correct.\nReview your Canvas notification settings and decide what you want to be notified about.\nVisit the Course website. In particular, as you might expect, this course requires computing. We will use R and RStudio as well as Git and GitHub. See the Computing tab.\nIf you have never used GitHub before, go to https://github.com/ create an account. You should be aware that this data is stored on US servers. Please exercise caution whenever using personal information. You may wish to use a pseudonym to protect your privacy if you have concerns. k_asking_for_help/).\n\nIf you haven’t already, visit https://ubc-stat.github.io/stat-406/computing/ and follow the instructions to set up your computer.\nNow we have to clone your labs-&lt;username&gt; repo.\n\nNavigate to the Course Github using the link at the top of the Course Website or from Canvas.\nThen go to your labs-&lt;username&gt;.\nClick the Green “Code” button, and copy the url by clicking the two overlapping squares.\nThen in RStudio, choose “New project” &gt; “Version Control” &gt; “Git” and paste the address.\nChoose a location on your machine where you want all your labs to be.\nSelect “Create Project”."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "href": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "title": "Lab 00 Git",
    "section": "Scenario 1. You do work on the wrong branch.",
    "text": "Scenario 1. You do work on the wrong branch.\nMake sure that you are on main. Remember that the actual submission is on the lab00-git branch.\nIn the R code chunk below, fit a linear model to the data and print the estimated coefficients, rounded to 2 decimal places.\n\nlibrary(tibble)\nset.seed(12345)\ndat &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = 2 + 3*x1 - x2 + rnorm(100)\n)\n\nNow, stage the .Rmd. Commit with the message “on the wrong branch” and push.\nYou likely see an error like:\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThat’s because you’re on main. Ugh! But I did some work, and now I need to be on a different branch!\nSo let’s fix it. We want the stuff we just did on main to be on lab00-git. Note that everything you did is saved! Here are the steps:\nGet our changes onto the correct branch\n\nUse the dropdown to switch branches to lab00-git.\nGo to the Terminal (next to console).\nType git merge main.\n\nThat should copy all your changes in the .Rmd that you made on main into the correct place. Did it?\nIf you do this and you ever see stuff like\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the stuff that is currently on this branch.\n=======\nThis is stuff that got added on the other branch.\nWhile someone else changed stuff on this branch!\nI (git) don't know which to keep!?\nYou have to decide for me.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict\nThis means that there were conflicts between the two versions. The stuff above ====== was in your current branch. The stuff below is what you’re trying to merge in. You decide what to keep, the top, the bottom, or both (or neither). Just be sure to delete the junk lines with &lt;, &gt;, or =.\nOnce you’ve resolved conflicts (and committed the conflict changes), double check the following:\n\nYou are on the correct branch (lab00-git)\nYou have no files with uncommitted changes in the “Git” tab\nYour changes to the R chunk above exist (on this branch)\n\nAnother way you can check for your changes is by running the git log commmand. You should see something like the following:\ncommit 1efefd8473c2cc81893dd2a5ded929978d9ee2aa (HEAD -&gt; lab00-git, main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 16:41:58 2024 -0700\n\n    on the wrong branch\n\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nThere’s a lot of information here, but you should (hopefully) see at the top your latest commit with the message “on the wrong branch.” The long string at the start of the commit (1efefd8473c2cc81893dd2a5ded929978d9ee2aa) is the hash. It is a unique identifier of the commit, which can be useful if you want to reference a specific commit with other commands.\nType q to exit the log viewer.\n\nOk. So now we have our changes in the right spot. Commit and Push the .Rmd (only). Let’s clean up main so we don’t have problems later. Switch back to main.\nUndo mistakes on the wrong branch.\nIn the terminal, type the following two commands:\ngit fetch\ngit reset --hard origin/main\nThere’s a lot to unpack in these two commands, but here’s the high level idea: we want to make sure that our main branch matches what’s on Github’s remote main branch. The second command resets our local main branch so that it has exactly the same commits as Github’s remote main branch. (The first command makes sure that our local computer knows about the latest changes on Github’s remote branches.)\nIf you now type git log, you should now see\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (HEAD -&gt; main, origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nSo our local main branch matches what’s on Github, and no longer contains the “on the wrong branch” commit. You can also verify that your changes to the R code on this branch are now gone.\nTo recap, now the work we want is in the right place (on the other branch), and the mess on main is cleaned up. Boom."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "href": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "title": "Lab 00 Git",
    "section": "Scenario 2. You did something you shouldn’t have",
    "text": "Scenario 2. You did something you shouldn’t have\nSwitch your branch back to lab00-git (or whatever you named it).\nOpen the file lab01.Rmd. Select everything after # Instructions and delete it. Save. Then Knit (producing a pdf). Commit both files with a message “did the wrong lab, and built a pdf”. Push your commits with the Green up arrow.\nTake a look at the PR on Github now. There’s a bunch of crud that shouldn’t be there.\nWe’ve done 3 things here that we shouldn’t have.\n\nWe built a pdf that we don’t want at all. It needs to go away.\nWe bollixed up the lab01.Rmd file. We don’t want that or it will screw up the lab next week.\nWe pushed it all into our submission for this week.\n\nThe first instinct is to Delete both files, commit, and push. This is VERY BAD. That will further screw up everything. Basically, you’re telling git “I don’t want these files at all” when you mean “I don’t want changes to these files in this branch”. The difference is subtle but important. Because you DO want these files (without the changes) at some point, but you don’t want them here.\nLet’s fix these issues.\n\nFirst, we want to “get rid of” the pdf. In the Terminal type\ngit reset HEAD^ -- lab01.pdf\nClick the little “Refresh” arrow in the Git panel. You should now see lab01.pdf twice, once with a red D that is checked and once with two yellow question marks that is NOT checked. This is what we want.\nCommit exactly as is. Use a message like “remove the stray pdf” and Push. Now, take a look at the PR on Github. It should be gone from the list of files in the PR.\nThere’s still that annoying two-yellow-question-mark version in the Git panel. Don’t click the check box (that will just redo everything we undid). Instead, highlight the file by clicking the file name, click the Gear Icon Dropdown, and then select “Revert”. Now it’s gone, and the pdf should disappear from your filesystem.\n\nSecond, let’s “undo” the deletion in the .Rmd. This is easy, and a useful pattern to remember.\nIn the Terminal, type\ngit checkout main -- lab01.Rmd\nWhat this does is grabs the version on main that isn’t messed up and puts it here, overwriting your changes. This isn’t the only way to fix your problem (you could have done the same thing we did with the pdf), but it’s pretty easy.\nStage commit and push. Now look at the PR on Github. Even though you made two changes (one deleting everything, and one restoring everything) to the lab01.Rmd, it should be “gone” from the PR now. That’s because the version on this branch looks just like the version on main, so there are no changes to be made into the main branch. This is just what we want.\n\nNow we’ve also fixed the third error already. None of those bogus changes to lab01 are in our PR for this week anymore."
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDefine an ensemble method\nDerive the connection between the bootstrap and variance reduction\nImplement the bagging and random forest algorithms in pseudocode\nArticulate advantages and disadvantages of these methods beyond improving accuracy (e.g., uncertainty quantification, computational trade-offs, etc.)"
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#learning-objectives",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#learning-objectives",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDefine an ensemble method\nDerive the connection between the bootstrap and variance reduction\nImplement the bagging and random forest algorithms in pseudocode\nArticulate advantages and disadvantages of these methods beyond improving accuracy (e.g., uncertainty quantification, computational trade-offs, etc.)"
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#motivation",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#motivation",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Motivation",
    "text": "Motivation\n\nIn the last lecture, we began our foray into computation/simulation-based approaches to learning.\nWe introduced the bootstrap as a mechanism for quantifying uncertainty with minimal assumptions*** about the data-generating distribution.\n\n\n\n\n\n\nWarning***The Fundamental Assumption of The Bootstrap\n\n\n\n\n\nThe only major assumption we made when introducing the bootstrap (beyond our standard i.i.d. data assumption) is that the empirical distribution of our training sample is a reasonable approximation of the true (unknown) data-generating distribution.\n\n\nWhat are conditions that might make this assumption invalid?\n\n\nSmall sample size\nHigh-dimensional covariate space\nRare examples\nHeavy-tailed distributions\nViolations of the i.i.d. assumption (e.g. biased sampling, time series data, etc.)\n\n\n\n\n\nIn this lecture, we will use the bootstrap (and other computation/simulation-based techniques) as a black-box mechanism for variance reduction.\nIn particular, we will use the bootstrap (and other sampling techniques) to construct ensembles of predictive models that, when combined, have lower variance (and thus lower risk) than any individual model in the ensemble.\nThese techniques are extremely useful for reducing the variance of non-closed-form predictive models, such as decision trees, where our standard variance reduction techniques (e.g. ridge regularization, lasso, etc.) do not apply."
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#ensemble-methods",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#ensemble-methods",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Ensemble Methods",
    "text": "Ensemble Methods\n\nAn ensemble is a collection of predictive models whose individual predictions are combined in some way (e.g., by averaging or majority vote) to produce a final prediction.\nFor example, let’s assume that we have \\(m\\) different predictive models, \\(\\hat{f}^{(1)}(X), \\hat{f}^{(2)}(X), \\ldots, \\hat{f}^{(m)}(X)\\), each of which yields a prediction \\(\\hat Y^{(i)}\\) of the response associated with \\(X\\).\n\\[\\begin{align*}\n\\hat Y^{(1)} &= \\hat{f}^{(1)}(X) \\\\\n\\hat Y^{(2)} &= \\hat{f}^{(2)}(X) \\\\\n&\\vdots \\\\\n\\hat Y^{(m)} &= \\hat{f}^{(m)}(X)\n\\end{align*}\\]\nImportantly, each of the individual models in the ensemble need to be different from one another. (We’ll see why in a second.) There are many mechanisms to achieve this diversity:\n\n\n\n\n\n\nTipHow Can we Create Diverse Models?\n\n\n\n\n\nThere are many possible answers. Here’s just a few that come to mind:\n\nTrain the same predictive model (e.g. OLS) on different training samples. These training samples can be subsets of the original data, for example.\nTrain the same predictive model (e.g. OLS) on different subsets of the covariates.\nTrain different types of predictive models (e.g. decision trees, OLS, ridge regression, k-nearest neighbors, etc.) on the same training data.\nVary the hyperparameters of the same predictive model (e.g. ridge regression with different values of \\(\\lambda\\)) on the same training data.\n\nImportantly, all of the supervised models/training procedures we’ve studied so far are deterministic given the same training data and hyperparameters. Therefore, we cannot simply re-train the same predictive model on the same data and expect to get different models.\n\n\n\nThe ensemble prediction \\(\\hat f_\\mathrm{ens}(X)\\) is given by averaging the predictions of the individual models:\n\\[\n\\hat{f}_{\\text{ens}}(X) = \\frac{1}{m} \\sum_{j=1}^m \\hat Y^{(j)} = \\frac{1}{m} \\sum_{j=1}^m \\hat{f}^{(j)}(X)\n\\]\nIn general, we’ll see that ensembles have lower risk than individual models in the ensemble. Whether this reduction in risk comes from lower bias or lower variance depends on the component models in the ensemble and how they’re constructed.\nToday, we’ll learn about two procedures for constructing ensembles (bagging and random forests) that yield a reduction in variance.\n\n\nIntuition: Why Would Ensembles Be Better Than Single Models?\n\nIntuitively, a good ensemble should be composed of accurate models that make diverse errors.\nImagine you are doing bar trivia with a team of friends. Each of you has some knowledge about the trivia topics, but none of you is an expert in all of them.\nYou might make errors on the music questions, while friend A makes errors on the history questions, and friend B makes errors on the sports questions.\nHowever, if you pool your knowledge together, you can cover each other’s weaknesses and answer more questions correctly as a team than any of you could individually.\nIn this analogy, each friend is like a model in the ensemble.\n\n\n\nHow Ensembles Can Reduce Variance\n\nThis intuition (models making diverse errors) can be made precise mathematically.\nHowever, these “errors” made by each of the models could either arise from component models with high bias or high variance.\nIn this lecture, we’ll see why ensembles could reduce variance when the component models are high-variance estimators.\n\n\nSetup: Models trained on Different Training Samples\n\nLet’s imagine that we have \\(m\\) different training samples, \\(\\mathcal{D}_1, \\mathcal{D}_2, \\ldots, \\mathcal{D}_m\\), each of which is drawn independently from the same data-generating distribution.\nWe train full-depth decision trees on each of these training samples to get \\(m\\) different models, \\(\\hat{f}_{\\mathcal{D}_1}(X), \\hat{f}_{\\mathcal{D}_2}(X), \\ldots, \\hat{f}_{\\mathcal{D}_m}(X)\\).\nEach of these models is a high-variance estimator, meaning that small changes in the training data can lead to large changes in the fitted model.\nLet’s compute the bias and the variance of the ensemble predictor:\n\\[\n\\hat{f}_{\\text{ens}}(X) = \\frac{1}{m} \\sum_{j=1}^m \\hat{f}_{\\mathcal{D}_j}(X)\n\\]\nThe bias of the ensemble predictor is the same as the bias of the individual model:\n\n\n\n\n\n\nTipBias Derivation\n\n\n\n\n\nThe expected ensemble is exactly the same as the expected individual model:\n\\[\\begin{align*}\n  \\mathbb{E}[\\hat{f}_{\\text{ens}}(X) \\mid X]\n  &= \\mathbb{E}\\left[\\frac{1}{m} \\sum_{j=1}^m \\hat{f}_{\\mathcal{D}_j}(X) \\mid X\\right] \\\\\n  &= \\frac{1}{m} \\sum_{j=1}^m \\mathbb{E}[\\hat{f}_{\\mathcal{D}_j}(X) \\mid X] \\\\\n  &= \\mathbb{E}[\\hat{f}_{\\mathcal{D}_j}(X) \\mid X].\n\\end{align*}\\]\nwhere the final equality follows from the fact that all of the \\(\\hat{f}_{\\mathcal{D}_j}(X)\\) are identically distributed and thus have the same expectation. Therefore, the bias of the ensemble is also the same as the bias of the individual model:\n\\[\\begin{align*}\n\\text{Bias}[\\hat{f}_{\\text{ens}}(X)] &= \\mathbb{E} \\left[ \\mathbb{E}[\\hat{f}_{\\text{ens}}(X) \\mid X] - \\mathbb{E}[Y \\mid X] \\right] \\\\\n&= \\mathbb{E} \\left[ \\mathbb{E}[\\hat{f}_{\\mathcal{D}_j}(X) \\mid X] - \\mathbb{E}[Y \\mid X] \\right] \\\\\n&= \\text{Bias}[\\hat{f}_{\\mathcal{D}_j}(X)]\n\\end{align*}\\]\n\n\n\nHowever, the variance of the ensemble predictor is reduced by a factor of \\(m\\) if the training samples are independent:\n\n\n\n\n\n\nTipVariance Derivation\n\n\n\n\n\nConsider the variance of the ensemble predictor for a fixed \\(X=x\\):\n\\[\n\\mathrm{Var}[\\hat{f}_{\\text{ens}}(X) \\mid X=x]\n= \\mathrm{Var}[\\hat{f}_{\\text{ens}}(x)]\n= \\mathrm{Var}\\left[\\frac{1}{m} \\sum_{j=1}^m \\hat{f}_{\\mathcal{D}_j}(x)\\right]\n\\]\nNote that when we fix \\(X=x\\), the only randomness remaining in \\(\\hat{f}_{\\text{ens}}(x)\\) comes from the randomness in the (independent) training samples \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_m\\). Using the following statistical identities:\n\n\\(\\mathrm{Var}[aZ] = a^2 \\mathrm{Var}[Z]\\) for any random variable \\(Z\\) and constant \\(a\\); and\nif \\(Z_1, \\ldots, Z_m\\) are independent random variables, then \\(\\mathrm{Var}\\left[\\sum_{j=1}^m Z_j\\right] = \\sum_{j=1}^m \\mathrm{Var}[Z_j]\\);\n\nwe have that\n\\[\n\\mathrm{Var}[\\hat{f}_{\\text{ens}}(x)] = \\frac{1}{m^2} \\sum_{j=1}^m \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(x)] = \\frac{1}{m} \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(x)],\n\\]\nwhere the final equality follows from the fact that all of the \\(\\hat{f}_{\\mathcal{D}_j}(x)\\) are identically distributed and thus have the same variance. (If you’re struggling to remember these identities, try to prove them on your own! They’re straightforward to derive from the definition of variance (\\(\\mathrm{Var}[Z] = \\mathbb{E}[(Z - \\mathbb E[Z])^2] = \\mathbb{E}[Z^2] - (\\mathbb{E}[Z])^2\\)) and linearity of expectation.)\nThus, the variance component of the risk, which now averages this variance over the distribution of \\(X\\), is also reduced by a factor of \\(m\\):\n\\[\\begin{align*}\n\\mathrm{Var}[\\hat{f}_{\\text{ens}}(X)]\n&= \\mathbb{E} \\left[ \\mathrm{Var} [\\hat{f}_{\\text{ens}}(X) \\mid X] \\right] \\\\\n&= \\mathbb{E} \\left[ \\frac{1}{m} \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(X) \\mid X] \\right] \\\\\n&= \\frac{1}{m} \\mathbb{E} \\left[ \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(X) \\mid X] \\right] \\\\\n&= \\frac{1}{m} \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(X)]\n\\end{align*}\\]\n\n\n\nThus, even though each individual model in the ensemble is a high-variance estimator, with a large enough ensemble (of models trained on independent training samples), we can reduce the variance of the ensemble predictor to be arbitrarily small!\n\\[ \\mathcal{R}(\\hat{f}_{\\text{ens}}) = \\text{Bias}[\\hat{f}_{\\mathcal{D}_j}(X)]^2 \\quad + \\underbrace{\\frac{1}{m} \\mathrm{Var}[\\hat{f}_{\\mathcal{D}_j}(X)]}_\\text{goes to 0 with large enough $m$!} + \\quad \\text{irreducible noise} \\]\n\nWe’ve seen the math, but why does the ensemble not overfit the training data, even though each individual model overfits?\n\n\nAnswer\n\nThe key is that each individual model is overfitting in different ways, because they are overfit to different training samples. Model \\(i\\) is both learning the true signal, as well as whatever noise happened to be in training sample \\(\\mathcal{D}_i\\). When we average the predictions of all \\(m\\) models, the true signal is reinforced (since all models are trying to learn it), while the noise is averaged out (since the noise in each training sample is independent)."
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#bagging-bootstrap-aggregating",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#bagging-bootstrap-aggregating",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Bagging: Bootstrap Aggregating",
    "text": "Bagging: Bootstrap Aggregating\nIn practice, we only have access to a single training sample \\(\\mathcal{D}\\). However, using our knowledge from the last lecture, we can use the bootstrap to simulate having multiple training samples \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_m\\) from our (true) training sample \\(\\mathcal{D}\\)!\n\n\n\n\n\n\nNoteThe Bagging Trees Algorithm\n\n\n\nGiven:\n\nA predictive modelling algorithm (e.g. decision trees)\nThe ensemble size \\(m\\) (i.e. number of trees to train)\nA training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\)\n\nFor \\(i \\in 1, \\ldots, m\\):\n\nDraw a bootstrap sample \\(\\mathcal{D}_i\\) (i.e. sample of size \\(n\\), with replacement) from \\(\\mathcal{D}\\)\nTrain \\(\\hat{f}_{\\mathcal{D}_i}(X)\\) (e.g. the decision tree) on \\(\\mathcal{D}_i\\)\n\nMake predictions using the ensemble predictor: \\(\\hat{f}_{\\text{ens}}(X) = \\frac{1}{m} \\sum_{j=1}^m \\hat{f}_{\\mathcal{D}_j}(X)\\)\n\n\nThis algorithm can be applied to any (high-variance) predictive modelling algorithm, but it is most commonly used with decision trees.\n\nWhy does this algorithm work?\n\nUnder the fundamental assumption of the bootstrap, the empirical distribution of our training sample \\(\\mathcal{D}\\) is a reasonable approximation of the true data-generating distribution.\nTherefore, bootstrap samples drawn from \\(\\mathcal{D}\\) can be viewed as approximate (independent) training samples drawn from the true data-generating distribution.\n\n\n\nThe Bias and Variance of Bagged Trees\n\nWhile \\(\\mathcal{D}_1, \\ldots, \\mathcal{D}_m\\) are approximately samples from the true data-generating distribution, they are not truly the same as independent samples from that distribution.\nHow does this affect the bias and variance of the bagging ensemble?\nLet’s compare the bias/variance of bagged trees to a single decision tree trained on the original training sample \\(\\mathcal{D}\\). (We will denote this single tree as \\(\\hat{f}_{\\mathcal{D}}(X)\\).)\n\nBias: \\(\\text{Bias}[\\hat{f}_{\\text{ens}}(X)] &gt; \\text{Bias}[\\hat{f}_{\\mathcal{D}}(X)]\\), but (hopefully) not by much.\n\n\nWhy?\n\nSince the bootstrap samples are drawn from the empirical distribution of \\(\\mathcal{D}\\), they may not capture the true data-generating distribution perfectly. Therefore, the models trained on these bootstrap samples may have slightly higher bias than a model trained on the true data-generating distribution.\n\nVariance: \\(\\mathrm{Var}[\\hat{f}_{\\text{ens}}(X)] &gt; \\frac{1}{m} \\mathrm{Var}[\\hat{f}_\\mathcal{D}(X)]\\), but (hopefully) also not by much.\n\n\nWhy?\n\nSince the bootstrap samples are drawn from the same training sample \\(\\mathcal{D}\\), they are not truly independent. Therefore, the variance reduction achieved by bagging is not as large as it would be if the models were trained on independent training samples.\n\nHowever, even with these caveats, often \\(\\mathrm{Var}[\\hat{f}_{\\text{ens}}(X)] \\ll \\mathrm{Var}[\\hat{f}_\\mathcal{D}(X)]\\), so the slight increase in bias is often worth the large decrease in variance!\n\n\nExample\nLet’s see what happens when we apply bagging to decision trees on the mobility dataset from earlier in the course, as we increase the number of bagged trees in the ensemble:\n\n\n\nBagging Error as a Function of Number of Trees\n\n\nWith a single bagged tree in our ensemble, the error is similar to that of a single decision tree (a little worse, due to the increase in bias from using bootstrap samples). However, as we increase the number of trees in the ensemble, the error decreases significantly, eventually leveling off at a value much lower than that of a single decision tree."
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#advantages-of-bagging",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#advantages-of-bagging",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Advantages of Bagging",
    "text": "Advantages of Bagging\nBagged ensembles (especially bagged decision trees) would almost* be my go-to off-the-shelf predictive model if I had to pick just one, for several reasons:\n\nBagged Decision Trees are Hyperparameter-Free\n\nRecall that the inputs to the bagging algorithm are:\n\nA predictive modelling algorithm (e.g. decision trees)\nThe ensemble size \\(m\\) (i.e. number of trees to train)\nA training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\)\n\nThe only hyperparameters to tune are (1) the predictive modelling algorithm’s hyperparameters (e.g. max depth of decision trees) and (2) the ensemble size \\(m\\).\n\n\n\n\n\n\nTipWhy Don’t We Have to Tune These Choices?\n\n\n\n\n\n\nBecause bagging reduces variance so effectively, we don’t have to worry about tuning the predictive modelling algorithm’s hyperparameters to balance bias and variance. Therefore, we can just use full-depth decision trees (i.e. no hyperparameter tuning needed)!\nFurthermore, we will get more variance reduction by increasing the ensemble size \\(m\\), so we can just set \\(m\\) to be however many trees we can afford computationally!\nIn practice, we can just set \\(m\\) to be a few hundred or a few thousand trees, and keep increasing it until the CV error stabilizes.\n\n\n\n\n\n\n\nBagged Decision Trees are Non-Parametric (and Universal Approximators)\n\nMax-depth decision trees will create a split for every unique data point in the (bootstrap) training sample.\nThus, the complexity of bagged (max-depth) decision trees grows with the size of the training data.\nAs we discussed in a previous lecture, an axis-aligned piecewise constant function (i.e. a decision tree) can approximate any continuous function arbitrarily well, given enough splits (i.e. enough data).\nTherefore, bagged decision trees are universal approximators, meaning that they can approximate any continuous function arbitrarily well, given enough data.\n\n\n\nUncertainty Quantification “For Free”\n\nRecall that the bootstrap can be used to approximate the variance of any quantity (with respect to the data-generating distribution).\nSince each model in the ensemble is trained on a bootstrapped sample of our training data, we can thus estimate \\(\\mathrm{Var}[ \\hat{f}_{\\mathcal D}(x) ] = \\frac{1}{m-1} \\sum_{i=1}^m \\left( \\hat{f}_{\\mathcal{D}_i}(x) - \\hat{f}_{\\text{ens}}(x) \\right)^2\\) for any fixed set of test covariates \\(x\\).\n\n\n\n\n\n\n\nTipWhat is this Variance Measuring?\n\n\n\n\n\n\nFor a fixed \\(x\\), the only randomness in the quantity \\(\\hat{f}_{\\mathcal D}(x)\\) comes from the randomness in the training sample \\(\\mathcal D\\).\nTherefore, the variance \\(\\mathrm{Var}[ \\hat{f}_{\\mathcal D}(x) ]\\) measures how much the prediction at \\(x\\) would change if we had a different training sample.\nThis variance is a measure of our uncertainty about the prediction at \\(x\\) due to limited training data.\nIf a prediction at \\(x\\) has high variance, it means that our prediction is very sensitive to the training data, and thus we should be less confident in that prediction.\n\n\n\n\n\n\nBagged Decision Trees Give Us a Risk Estimate “For Free”\n\nBecause bootstrapping samples with replacement from the training data, on average, about 63% of the training data points appear in each bootstrap sample.\nTherefore, for each individual \\(\\hat f_{\\mathcal{D}_i}(X)\\) in the bagged ensemble, about 37% of the training data points are “left out” of the bootstrap sample used to train that model. We refer to these left-out data points as the out-of-bag (OOB) samples for model \\(i\\).\nEach data point \\((X_j, Y_j) \\in \\mathcal D\\) in the training data will be an OOB sample for about 37% of the models in the ensemble.\nTherefore, we can use the OOB samples to estimate the prediction error of the bagged ensemble without needing to do cross-validation or hold out a separate validation set!\n\n\n\n\n\n\nNoteOut-of-Bag Error Estimation\n\n\n\nFor each training data point \\((X_j, Y_j) \\in \\mathcal D\\):\n\nIdentify the subset of bootstrap samples \\(S_j \\subseteq \\{ \\mathcal{D}_i \\}_{i=1}^m\\) for which \\((X_j, Y_j)\\) is an OOB sample.\nCompute the average prediction for \\(X_j\\) using the OOB models (i.e. the models trained on the bootstrap samples in \\(S_j\\)):\n\\[\n\\hat Y_j^{(\\mathrm{OOB})} = \\frac{1}{|S_j|} \\sum_{\\mathcal{D}_i \\in S_j} \\hat{f}_{\\mathcal{D}_i}(X_j)\n\\]\nEstimate the risk as the average loss over all OOB predictions on the training set:\n\\[\n\\widehat{\\mathcal{R}}_{\\mathrm{OOB}} = \\frac{1}{n} \\sum_{j=1}^n L(Y_j, \\hat Y_j^{(\\mathrm{OOB})})\n\\]\n\n\n\n\nWe can think of the OOB error as an approximation of the cross-validation estimate of risk. We are using the left-out samples from each bootstrap sample as a “validation set” for that model, just like we would in cross-validation.\nHowever, unlike cross-validation, we don’t need to retrain the models multiple times on different training/validation splits!"
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#random-forests",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#random-forests",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Random Forests",
    "text": "Random Forests\n*I say that bagged decision trees would “almost” be my go-to off-the-shelf predictive model. My actual go-to model is a slight modification of bagged decision trees called random forests, which are one of the most popular and effective predictive modelling algorithms used in practice. I’d say that random forests are almost the best or second-best model on \\(80\\%\\) of supervised learning problems.\n\nReducing Correlation Between Trees to Reduce Variance\n\nIf we had truly independent training samples for each tree in the bagged ensemble, the variance reduction over a single tree would be exactly \\(1/m\\).\nHowever, because the bootstrap samples are correlated (they are all drawn from the same training sample \\(\\mathcal{D}\\)), we don’t get this full variance reduction.\n\n\n\n\n\n\nTipCorrelation Reduces Variance Reduction\n\n\n\nTo understand why correlation between the trees limits variance reduction, imagine that all of the bootstrap samples were nearly identical. Would we expect any variance reduction from bagging in this case?\n\n\nAnswer\n\nNo! If all of the bootstrap samples were nearly identical, then all of the trees in the ensemble would be nearly identical as well. Therefore, the ensemble predictor would be nearly identical to any individual tree in the ensemble, and we would not see any variance reduction.\n\n\n\nTherefore, if we can reduce the correlation between the trees in the ensemble, we can achieve even greater variance reduction than bagged trees. If this correlation reduction doesn’t increase the bias too much, then we can achieve even lower risk than bagged trees!\n\n\n\nReducing Correlation by Subsampling Covariates\n\nA simple way to reduce correlation between trees is to train each tree on a random subset of the covariates (without replacement).\nThis way, even if two bootstrap samples are similar, the trees trained on those samples will be different because they will be splitting on different covariates.\nSince decision trees are greedy algorithms that split on the most informative covariates first, forcing each tree to consider only a random subset of covariates will lead to more diverse trees in the ensemble, without increasing the bias too much.\n\n\n\n\n\n\n\nNoteThe Random Forests Algorithm\n\n\n\nGiven:\n\nA training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\) with \\(p\\) covariates\nThe ensemble size \\(m\\) (i.e. number of trees to train)\n\nFor \\(i \\in 1, \\ldots, m\\):\n\nDraw a bootstrap sample \\(\\mathcal{D}_i\\) (i.e. sample of size \\(n\\), with replacement) from \\(\\mathcal{D}\\)\nSelect a random subset of \\(k = \\sqrt{p}\\) covariates from the \\(p\\) total covariates\nTrain \\(\\hat{f}_{\\mathcal{D}_i}(X)\\) (e.g. the decision tree) on \\(\\mathcal{D}_i\\), using only the selected \\(k\\) covariates for splitting.\n\nMake predictions using the ensemble predictor: \\(\\hat{f}_{\\text{ens}}(X) = \\frac{1}{m} \\sum_{j=1}^m \\hat{f}_{\\mathcal{D}_j}(X)\\)\n\n\n\nThe only difference between random forests and bagged trees is the random subset of covariates used for splitting in each tree.\n\nWhy \\(k = \\sqrt{p}\\)?: This value of \\(k\\) has been theoretically shown to yield the best trade-off between bias and variance in practice. With this hyperparameter set, random forests (like bagged decision trees) have no hyperparameters to tune!"
  },
  {
    "objectID": "schedule/lectures/lecture_16_bagging_and_random_forests.html#summary",
    "href": "schedule/lectures/lecture_16_bagging_and_random_forests.html#summary",
    "title": "Lecture 16: Bagging and Random Forests",
    "section": "Summary",
    "text": "Summary\n\nEnsemble methods combine multiple predictive models to produce a final prediction.\nBagging (bootstrap aggregating) uses bootstrap samples to create an ensemble of high-variance models, leading to significant variance reduction and lower risk.\nRandom forests further reduce correlation between trees in the ensemble by training each tree on a random subset of covariates, leading to even greater variance reduction and lower risk.\nBoth bagged decision trees and random forests are hyperparameter-free, non-parametric, and provide out-of-bag error estimates “for free”, making them powerful and convenient off-the-shelf predictive models."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html",
    "href": "schedule/lectures/lecture_14_clustering.html",
    "title": "Lecture 14: Clustering",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nFormulate a clustering problem and explain its goals/applications.\nImplement and apply k-means clustering to a dataset.\nMeasure clustering quality and choose an appropriate number of clusters for a given dataset.\nDetermine when clustering (via k-means) is more appropriate than dimensionality reduction (via PCA).\nIdentify when an alternative clustering algorithm or variant of k-means is more appropriate for a given dataset."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#learning-objectives",
    "href": "schedule/lectures/lecture_14_clustering.html#learning-objectives",
    "title": "Lecture 14: Clustering",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nFormulate a clustering problem and explain its goals/applications.\nImplement and apply k-means clustering to a dataset.\nMeasure clustering quality and choose an appropriate number of clusters for a given dataset.\nDetermine when clustering (via k-means) is more appropriate than dimensionality reduction (via PCA).\nIdentify when an alternative clustering algorithm or variant of k-means is more appropriate for a given dataset."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#motivation",
    "href": "schedule/lectures/lecture_14_clustering.html#motivation",
    "title": "Lecture 14: Clustering",
    "section": "Motivation",
    "text": "Motivation\n\nIn the last lecture, we explored dimensionality reduction (through PCA) as our first unsupervised learning method.\nPCA, and its basis expansion/kernel variants, reduce the complexity of data by representing a \\(\\mathbb R^p\\) set of covariates by a \\(\\mathbb R^k\\) vector with \\(k \\ll p\\).\nIn this lecture, we’ll explore an alternative simplification: representing a \\(\\mathbb R^p\\) set of covariates by a \\(\\{1, 2, \\ldots, k\\}\\) integer.\nThis integer represents a category, that each observation belongs to, which we refer to as a cluster label.\nWe refer to this unsupervised learning method as clustering.\n\nTo restate:\n\nClustering maps a set of covariates \\(X \\in \\mathbb R^p\\) to a discrete label \\(C \\in \\{1, 2, \\ldots, k\\}\\).\nPCA (and its variants) maps a set of covariates \\(X \\in \\mathbb R^p\\) to a continuous vector \\(Z \\in \\mathbb R^k, k \\ll p\\).\n\n\n\n\n\n\n\nNoteIsn’t Clustering Just Classification?\n\n\n\nWhile clustering and classification both assign labels to observations, there is a key difference.\n\nIn classification, we have labeled training data \\(\\mathcal D = \\{ (X_1, Y_1), \\ldots, (X_N, Y_N) \\}\\). We aim to learn a function to predict the responses \\(Y_1, \\ldots, Y_N\\) from the covariates \\(X_1, \\ldots, X_N\\).\nIn clustering, we do not have labeled training data; i.e. our training data are just the covariates \\(\\mathcal D = \\{ X_1, \\ldots, X_N \\}\\). The cluster labels do not correspond to any response; they are simply a way to describe the covariates by a much simpler representation (an integer label rather than a \\(p\\)-dimensional vector).\n\n\n\n\n\n\n\n\n\nMapping\nSupervised Learning Problem\nUnsupervised Learning Problem\n\n\n\n\n\\(\\hat f_{\\mathcal D}(X): \\mathbb R^p \\to \\mathbb R^k\\)\nRegression\nDimensionality Reduction\n\n\n\\(\\hat f_{\\mathcal D}(X): \\mathbb R^p \\to \\{1, 2, \\ldots, k\\}\\)\nClassification\nClustering\n\n\n\n\n\n\nExample of Clustering\nConsider the following synthetic dataset of \\(n=140\\) set of \\(p=2\\)-dimensional covariates: \\(\\mathcal D \\in \\{ X_1, \\ldots X_{140} \\}, X_i \\in \\mathbb R^2\\).\n\n\nCode\nlibrary(Stat406)\nlibrary(mvtnorm)\nlibrary(ggplot2)\nset.seed(406406406)\nX1 &lt;- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))\nX2 &lt;- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))\nX3 &lt;- rmvnorm(40, c(4, 4))\ndata &lt;- rbind(X1, X2, X3)\ntibble(\n  x1 = data[, 1],\n  x2 = data[, 2],\n) |&gt; ggplot(aes(x = x1, y = x2)) +\n  geom_point(size = 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nYou’ll notice that the data are not uniformly distributed throughout the 2D space.\nIf you squint, it looks like the data belong to three groups, or clusters.\nIf we apply a clustering algorithm to learn a mapping \\(\\hat f_{\\mathcal D}(X): \\mathbb R^2 \\to \\{1, 2, 3\\}\\), we might get the following result:\n\n\n# data: 140 x 2 matrix with our covariates\nassignments &lt;- kmeans(data, centers = 3)$cluster\ntibble(\n  x1 = data[, 1],\n  x2 = data[, 2],\n  cluster = as.factor(assignments)\n) |&gt; ggplot(aes(x = x1, y = x2, colour = cluster)) +\n  geom_point(size = 2) +\n  scale_colour_manual(values = c(\"blue\", \"orange\", \"green\"))\n\n\n\n\n\n\n\n\n\nThese cluster labels that we assign to the points may or may not be semantically meaningful. However, it provides us with a way to analyze and summarize the data more easily, and gives us a toehold for discovering patterns and further analysis."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#the-simplest-clustering-algorithm-k-means",
    "href": "schedule/lectures/lecture_14_clustering.html#the-simplest-clustering-algorithm-k-means",
    "title": "Lecture 14: Clustering",
    "section": "The Simplest Clustering Algorithm: K-Means",
    "text": "The Simplest Clustering Algorithm: K-Means\n\nWhile there are many clustering algorithms, the simplest and most widely used is k-means clustering.\nK-means clustering aims to partition \\(n\\) observations into \\(k\\) clusters such that the within-cluster variation is minimized.\nIntuitively, we want to group the data so that the points in each cluster are as similar to each other as possible.\nAs with k-nearest neighbours and kernel methods, similarity will be measured through Euclidean distance.\n\n\nMathematical Formalization\nAs with (almost) all other learning procedures we’ve discussed, we will formalize this clustering procedure as an optimization problem.\n\nAssume we have fixed \\(k\\), i.e. the number of clusters we hope to partition our data into. (We’ll discuss how to choose \\(k\\) momentarily.)\nWhat we are trying to learn: \\(\\hat f_{\\mathcal D}(X) : \\mathbb R^p \\to \\{ 1, \\ldots, k \\}\\), or some function that maps covariates onto clusters.\nFor any given \\(\\hat f_{\\mathcal D}(X) : \\mathbb R^p \\to \\{ 1, \\ldots, k \\}\\), let \\(C_1, \\ldots, C_k \\subset \\mathcal D\\) represent the clusters induced by this mapping; i.e.\n\\[C_j = \\{ X_i \\in \\mathcal D : \\hat f_{\\mathcal D}(X_i) = j \\}\\]\nIn other words, \\(C_j\\) is the set of all training covariates assigned to cluster \\(j\\).\nThe within-distance variation of this clustering is defined as:\n\\[ W_j := \\frac{1}{|C_j|^2} \\sum_{i, {i'} \\in C_j} \\| X_i - X_{i'} \\|_2^2 \\]\nwhere \\(|C_j|\\) is the number of points in cluster \\(j\\). It is the average pairwise-distance between points in cluster \\(j\\).\nWe want to minimize the total within-cluster variation, but weight each cluster by its size so larger clusters contribute more.\nMathematically, this gives us:\n\\[ \\mathrm{argmin}_{\\hat f_{\\mathcal D}(X)} \\frac{1}{2}  \\sum_{j=1}^k \\vert C_j \\vert W_j = \\frac{1}{2} \\sum_{j=1}^k \\frac{1}{\\vert C_j \\vert} \\sum_{i, i' \\in C_j} \\Vert X_i - X_{i'} \\Vert^2_2. \\]\nThe fraction \\(1/2\\) is for mathematical convenience (it does not affect the maximization), as will become clear shortly.\n\n\n\nCluster Centroids: A Crucial Simplification\n\nThis optimization problem is unwieldy, especially because we have to consider all pairwise distances between points in each cluster. (This is, worst case scenario, \\(O(n^2)\\) pairwise distances to consider!)\nFortunately, we can make use of the following identity to simplify from a pair-wise summation to a per-data-point summation:\n\\[ \\frac{1}{2} \\sum_{i, i' \\in C_j} \\| X_i - X_{i'} \\|_2^2 = |C_j| \\sum_{i \\in C_j} \\| X_i - \\mu_j \\|_2^2 \\]\nwhere \\(\\mu_j\\) is the centroid of cluster \\(j\\), or the empirical average of the covariates within that cluster:\n\\[ \\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} X_i. \\]\n\n\n\n\n\n\nTipDeriving this Identity\n\n\n\nIf you’re confused about where this identity comes from, consider our favourite trick that we used when deriving the bias-variance tradeoff: adding and subtracting zero:\n\\[ \\sum_{i, i' \\in C_j} \\| X_i - X_{i'} \\|_2^2 = \\sum_{i, i' \\in C_j} \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2. \\]\nJust like we did with the bias-variance tradeoff, expand the square and you’ll find that the cross terms vanish!\n\n\nDetails\n\nWhile we could go through this derivation step-by-step, it’s perhaps simpler if we replace the summations with expectations over the empirical distribution of points in cluster \\(C_j\\).\n\\[\n\\sum_{i, i' \\in C_j} \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2\n= \\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[ \\| (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\|_2^2 \\right]\n\\]\nWe also note that \\(\\mu_j\\) is the expectation of \\(X\\) under the empirical distribution \\(\\hat P_{C_j}\\):\n\\[\n\\begin{aligned}\n&\\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[\\Vert (X_i {\\color{blue}- \\mu_j}) + ({\\color{blue}\\mu_j -} X_{i'}) \\Vert_2^2 \\right]\n\\\\\n=&\n\\vert C_j \\vert^2 \\mathbb E_{X, X' \\sim \\hat P_{C_j}} \\left[\\Vert  (X_i - \\mathbb E[X]) + (\\mathbb E[X] - X_{i'}) \\Vert_2^2 \\right]\n\\end{aligned}\n\\]\nand now this derivation should look exactly like what we did in the bias-variance tradeoff! Follow those steps to see the cross terms disappear!\n\n\n\nIntuitively, these centroids represent the “average” point in each cluster. They are represented on the plots below by black crosses.\n\n\nCode\ncentroids &lt;- as.data.frame(kmeans(data, centers = 3)$centers)\nggplot() +\n  geom_point(data = tibble(\n    x1 = data[, 1],\n    x2 = data[, 2],\n    cluster = as.factor(assignments)\n  ), aes(x = x1, y = x2, colour = cluster), size = 2) +\n  geom_point(data = centroids, aes(x = V1, y = V2), colour = \"black\", size = 5, shape = 3) +\n  scale_colour_manual(values = c(\"blue\", \"orange\", \"green\"))\n\n\n\n\n\n\n\n\n\n\n\n\nSolving the Optimization Problem\nEven after simplifying the optimization problem using centroids, it is still impossible to solve it exactly. Instead, we rely on an iterative algorithm to approximate a solution.\n\nThe Idea\n\nWe produce a series of clustering assignments\n\\[ f_{\\mathcal D}^{(0)}(X), f_{\\mathcal D}^{(1)}(X), f_{\\mathcal D}^{(2)}(X), \\ldots f_{\\mathcal D}^{(t)}(X), \\ldots \\]\nso that \\(\\sum_{j=1}^k |C_j| W_j\\) decreases as \\(t\\) increases.\n\\(f_{\\mathcal D}^{(0)}(X)\\) will essentially be random:\n\nRandomly initialize \\(\\mu_1, \\ldots, \\mu_k\\) somewhere in \\(\\mathbb R^p\\).\nAssign each point to the cluster corresponding to the nearest \\(\\mu_j\\) value:\n\n\\[ f_{\\mathcal D}^{(0)}(X_i) = \\mathrm{argmin}_{j \\in \\{1, \\ldots, k\\}} \\| X_i - \\mu_j \\|_2^2. \\]\n\nUpdate \\(\\mu_1, \\ldots, \\mu_k\\) to be the centroids of the clusters induced by \\(f_{\\mathcal D}^{(0)}(X)\\):\n\n\\[ \\mu_j = \\frac{1}{|C_j|} \\sum_{i \\in C_j} X_i. \\]\nTo get \\(f_{\\mathcal D}^{(1)}(X)\\) from \\(f_{\\mathcal D}^{(0)}(X)\\), we repeat steps 2 and 3 above.\nMore generally, we repeat steps 2 and 3 until the cluster centroids are stable (i.e. when the distance that the cluster centroids change is \\(&lt; \\epsilon\\) for some pre-defined constant \\(\\epsilon\\). Most implementations will define this constant for you.)\n\n\n\n\nVisualization of k-means iterations (from https://ai.plainenglish.io/)\n\n\n\nWhile this algorithm does not guarantee that we find the optimal clustering assignment, it does guarantee that the within-cluster variation \\(\\sum_{j=1}^k |C_j| W_j\\) decreases at each iteration, and thus will eventually converge to some (possibly local) minimum.\n\n\n\n\n\n\n\nWarningK-Means May be Sensitive to Initialization\n\n\n\nBecause k-means starts with a random initialization of the cluster centroids, it may converge to different clustering assignments on different runs. Most implementations of k-means (including R’s built-in kmeans function) will try many random initializations and return the clustering assignment with the lowest within-cluster variation.\nHowever, note that you may get different clustering assignments on different runs of k-means, especially if the clusters are not well-separated."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#choosing-the-number-of-clusters-k",
    "href": "schedule/lectures/lecture_14_clustering.html#choosing-the-number-of-clusters-k",
    "title": "Lecture 14: Clustering",
    "section": "Choosing the Number of Clusters \\(k\\)",
    "text": "Choosing the Number of Clusters \\(k\\)\n\nSo far, we’ve assumed that the number of clusters \\(k\\) is fixed. Now we’re going to discuss how to perform model selection to choose the best value of \\(k\\).\nSince this algorithm is an unsupervised method, there’s not really a notion of risk (or cross-validation for that matter), so we’re going to have to rely on some heuristic methods to choose \\(k\\).\n\n\nWhat Do We Want From Our Clustering?\nIn general, there are three key desiderata for our clusters. We’ve already talked about one of them:\n\nPoints within clusters should be similar to each other. This quantity is exactly what k-means optimizes for:\n\n\\[ W := \\frac{1}{2} \\sum_{j=1}^k \\vert C_j \\vert W_j = \\frac{1}{2}\\sum_{j=1}^k \\sum_{i \\in C_j} \\left\\| X_i - \\mu_j \\right\\|_2^2 \\]\n\nPoints in different clusters should be dissimilar to each other. This quantity is not directly optimized by k-means, but is still important. It is measured by the between-cluster variation quantity, which measures the weighted average distance between pairs of cluster centroids:\n\n\\[ B := \\frac{1}{2} \\sum_{j=1}^k \\sum_{j'=1}^k \\frac{|C_j| |C_{j'}|}{n} \\| \\mu_j - \\mu_{j'} \\|_2^2. \\]\nAs with the within-cluster variation, this summation over all pairs can be simplified to a summation over individual clusters:\n\\[ B = \\sum_{j=1}^k |C_j| \\| \\mu_j - \\mu \\|_2^2 \\]\nwhere \\(\\mu = \\frac{1}{n} \\sum_{i=1}^n X_i\\) is the overall mean of the covariates.\n\nWe don’t want too many clusters. If we let \\(k=n\\), then each point is its own cluster, which is not a useful clustering!\n\n\n\n\n\n\n\nTipThe kmeans function in R automatically computes all of these variables\n\n\n\n\n\n\nkmeans(data, centers = 3)\n\nK-means clustering with 3 clusters of sizes 49, 39, 42\n\nCluster means:\n        [,1]      [,2]\n1 -0.9528507  2.141750\n2  1.8193830 -1.531834\n3  4.0791847  3.836696\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2\n [75] 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n\nWithin cluster sum of squares by cluster:\n[1]  98.81053 111.78974  81.12076\n (between_SS / total_SS =  80.2 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\n\nYou can guess what each of these variable names mean ;)\n\n\n\nWe want to minimize within-cluster variation and maximize between-cluster variation. However, these quantities will often be at odds with one another.\n\n\n\n\n\n\nNoteQuiz\n\n\n\nAs the total number of clusters \\(k\\) increases…\n\nWill the within-cluster variation \\(W\\) increase or decrease?\nWill the between-cluster variation \\(B\\) increase or decrease?\n\n\n\nAnswer\n\n\nAs we add more clusters, each cluster will have fewer points. Thus, the points in each cluster will be closer together, and the within-cluster variation decreases.\nAs we add more clusters, the between-cluster variation increases! (In my opinion, the intuition for this idea is best understood in reverse. \\(B\\) essentially measures the empirical variance of our \\(X_i\\) if we were to replace each \\(X_i\\) with its cluster centroid \\(\\mu_{f_{\\mathcal D}(X_i)}\\). As we decrease the number of clusters, we are replacing many \\(X_i\\) with the same centroid, which reduces the empirical variance of the data.)\n\nIf you want to understand see this relationship more rigorously, try to describe it via the law ot total variance.\n\n\n\nA simple heuristic to balance these three criteria is the following ratio:\n\\[ \\mathrm{CH} := \\frac{B/(k-1)}{W/(n-k)}. \\]\n\nNote that both \\(B\\) increases and \\(W\\) decreases as \\(k\\) increases.\nThe normalization of both factors by \\(k-1\\) and \\(n-k\\) ensures that we don’t just keep increasing \\(k\\) to maximize this quantity.\nThis quantity is known as the Calinski-Harabasz (CH) index.\n\n\n\n\n\n\n\nNoteK-means Clustering With Model Selection\n\n\n\nFor all \\(k \\in \\{2, 3, \\ldots, k_{\\max}\\}\\) (where \\(k_{\\max} &lt; n\\) is some maximum number of clusters we want to consider):\n\nRun k-means clustering with \\(k\\) clusters to get cluster assignments \\(C_1, \\ldots, C_k\\).\nCompute the CH index for this clustering assignment:\n\nChoose the number of clusters \\(\\hat k\\) that maximizes the CH index."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#k-means-vs-pca",
    "href": "schedule/lectures/lecture_14_clustering.html#k-means-vs-pca",
    "title": "Lecture 14: Clustering",
    "section": "K-Means vs PCA",
    "text": "K-Means vs PCA\n\nWe now have learned about two unsupervised learning methods: PCA and k-means clustering.\nBoth are useful when you want to simplify a complex dataset for further analysis but you don’t have labeled training data.\nHowever, when should you use one method over the other?\nThe answer depends on your analysis, goals, and the data. If you want to compress the data into a distinct number of categories, clustering is likely more appropriate.\nHowever, if you want to represent data on a spectrum (or to visualize the data), PCA is more important.\n\nExample clustering problem: You are studying birds, and you have collected measurements of their beak length, beak depth, wing length, and weight. You suspect there might be several distinct species of birds in your dataset. Since species are distinct categories, it makes sense to use clustering to group the birds into categories that may correspond to species or sub-species.\nExample dimensionality reduction problem: You are studying mental health data. You have collected survey responses from individuals on various aspects of their mental health, including stress levels, anxiety, depression, and overall well-being. You want to reduce these multiple dimensions into a smaller set of underlying factors that capture the main variations in mental health. Since many mental health factors exist on a spectrum, dimensionality reduction is more appropriate here."
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#other-clustering-variants",
    "href": "schedule/lectures/lecture_14_clustering.html#other-clustering-variants",
    "title": "Lecture 14: Clustering",
    "section": "Other Clustering Variants",
    "text": "Other Clustering Variants\nThere are many alternative flavours of clustering that can be more appropriate for different applications/data types:\n\nHierarchical clustering organizes clusters in a tree-like structure, creating hierarchical relationships between the different clusters. This algorithm is especially popular for biological data.\nGaussian mixture models are a generalization of k-means where the cluster distances are generalized and cluster membership becomes “fuzzy” or probabilistic. You may learn about this method in a more advanced machine learning class.\nSpectral clustering is similar to k-means clustering, except it is designed for graphical data (i.e. if your data is a social network, a “musical influence” chart, etc.)\n\nYou don’t need to know any of these, but be aware that they’re out there. Also, if k-means doesn’t immediately work for your data, with some googling you’ll likely find a variant that does!"
  },
  {
    "objectID": "schedule/lectures/lecture_14_clustering.html#summary",
    "href": "schedule/lectures/lecture_14_clustering.html#summary",
    "title": "Lecture 14: Clustering",
    "section": "Summary",
    "text": "Summary\n\nClustering is an unsupervised learning method that maps covariates \\(X \\in \\mathbb R^p\\) to discrete cluster labels \\(C \\in \\{1, 2, \\ldots, k\\}\\).\nK-means clustering is a simple and widely used clustering algorithm that aims to minimize within-cluster variation, as measured by Euclidean distance.\nthe centroids to be the mean of the points in each cluster.\nThe number of clusters \\(k\\) can be chosen using heuristic methods like the Calinski-Harabasz index.\nThere are many other clustering algorithms and variants that may be more appropriate for different data types (e.g. graph data) and applications (e.g. biological data)."
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html",
    "href": "schedule/lectures/lecture_12_trees.html",
    "title": "Lecture 12: Trees",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConnect decision trees to kNN and non-parametric methods\nPictorially represent a trained decision tree and its decision boundaries for classification and regression\nImplement (in rough pseudo-code) the greedy algorithm for training decision trees\nArticulate how the inductive bias of decision trees changes with depth (and data)\nIdentify characteristics of decision trees that lead to high bias or high variance"
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#learning-objectives",
    "href": "schedule/lectures/lecture_12_trees.html#learning-objectives",
    "title": "Lecture 12: Trees",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConnect decision trees to kNN and non-parametric methods\nPictorially represent a trained decision tree and its decision boundaries for classification and regression\nImplement (in rough pseudo-code) the greedy algorithm for training decision trees\nArticulate how the inductive bias of decision trees changes with depth (and data)\nIdentify characteristics of decision trees that lead to high bias or high variance"
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#motivation",
    "href": "schedule/lectures/lecture_12_trees.html#motivation",
    "title": "Lecture 12: Trees",
    "section": "Motivation",
    "text": "Motivation\n\nWith basis ridge regression, we saw a duality between parametric and non-parametric methods.\n\n\\(d &lt; \\infty\\) basis functions: parametric ridge regression model.\n\\(d \\to \\infty\\) basis functions: non-parametric kernel ridge regression.\n\nDoes a similar duality exist for kNN?\n\nkNN is non-parametric.\nCan we find a parametric model that approximates kNN?\n\n\n\nWhat Does a kNN Function Look Like?\nkNN regression produces a piecewise constant function.\n\nConsider the subset of all points \\(S \\subset \\mathcal X\\) that each have \\(X_{i1}, \\ldots, X_{ij}\\) as their \\(k\\) nearest neighbors.\nFor all \\(x \\in S\\), the kNN regression function is constant: \\[\n\\hat{f}_{\\mathrm{kNN}}(x) = \\frac{1}{k} \\sum_{X_i \\in N_k(x)} Y_i = \\text{constant for all } x \\in S.\n\\]\nMore generally, given all subsets \\(S_1, \\ldots, S_m\\) of \\(\\mathcal X\\) where each point in \\(S_j\\) has the same \\(k\\) nearest neighbors, the kNN regression function is piecewise constant over these subsets.\nThe subsets \\(S_1, \\ldots, S_m\\) are referred to as Voronoi cells, and they are convex polytopes in \\(\\mathbb{R}^p\\).\n\n\n\nCode\nknn_predict &lt;- function(train, test, y, k) {\n  sapply(1:nrow(test), function(i) {\n    dists &lt;- rowSums((t(t(train) - test[i, ]))^2)\n    neibs &lt;- sort.int(dists, index.return = TRUE)$ix[1:k]\n    mean(y[neibs])\n  })\n}\n\n# Train/test data\nlibrary(tidyverse)\nx_train &lt;- c(0.2, 0.3, 0.5, 0.6, 0.7)\ny_train &lt;- c(0.2, -0.2, 0.5, 0.7, 0.2)\nx_test &lt;- seq(0.0, 1.0, length.out = 101)\npreds &lt;- knn_predict(\n  matrix(x_train, nrow = length(x_train), ncol=1),\n  matrix(x_test, nrow = length(x_test), ncol=1),\n  y_train,\n  k = 2\n)\ntrain_data &lt;- tibble(x = x_train, y = y_train)\ntest_data &lt;- data.frame(x = x_test, pred = preds)\n\n# Plot\nlibrary(ggplot2)\nggplot(test_data, aes(x = x, y = pred)) +\n  geom_line(color = \"blue\") +\n  geom_point(data = train_data, aes(x = x, y = y), size = 1.5) +\n  scale_fill_viridis_c() +\n  geom_vline(xintercept = c(0.35, 0.45, 0.6), linetype = \"dotted\", color = \"blue\", linewidth = 0.5) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nA Parametric Approximation of Voronoi Cells\nLet’s assume we want to build a piecewise constant predictive model. kNN gives us one way to do this, but is there a parametric model that achieves something similar?\n\nSince kNN is non-parametric, we define the piecewise constant regions (i.e. the Voronoi cells \\(S_1, \\ldots, S_m\\)) implicitly through the training data.\nWhat if we instead defined the piecewise constant regions by explicitly defining the boundaries of the regions?\nTo simplify, let’s assume that the piecewise constant regions are axis-aligned rectangles in \\(\\mathbb{R}^p\\).\nWhile there are many mechanisms for representing such piecewise constant functions, one of the most common is through decision trees."
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#decision-trees-for-regression-and-classification",
    "href": "schedule/lectures/lecture_12_trees.html#decision-trees-for-regression-and-classification",
    "title": "Lecture 12: Trees",
    "section": "Decision Trees for Regression and Classification",
    "text": "Decision Trees for Regression and Classification\n\nA decision tree is a learned binary tree structure that recursively partitions the predictor space into axis-aligned rectangles.\n\nload(\"../data/mobility.rda\")\nlibrary(tree)\nlibrary(maptree)\nmob &lt;- mobility[complete.cases(mobility), ] |&gt;\n  dplyr::select(Income, Commute, Mobility) |&gt;\n  dplyr::mutate(Income = Income / 100000)\nsmalltree &lt;- tree(Mobility ~ ., data = mob, )\ndraw.tree(smalltree, digits = 2)\n\n\n\n\n\n\n\n\nEach of the leaf nodes of the tree corresponds to a single rectangle in the predictor space.\nEach internal node of the tree corresponds to a decision rule that recursively splits the predictor space along one of the covariate dimensions.\n\n\nA Simple Example\n\nConsider the decision tree above for a regression problem with two predictors, \\(X_1 = \\mathrm{Commute}\\) and \\(X_2 = \\mathrm{Income}\\) to predict \\(Y = \\mathrm{Mobility}\\).\nThe splits result in the following piecewise constant regions in the prediction space:\n\nplot(mob$Commute, mob$Income,\n  pch = 19, cex = .4, bty = \"n\", las = 1,\n  ylab = \"Income\", xlab = \"Commute\"\n)\npartition.tree(smalltree, add = TRUE, ordvars = c(\"Commute\", \"Income\"))\n\n\n\n\n\n\n\n\nThe prediction in each rectangle is the average response of all training points that fall within that rectangle.\nLet’s compare the piecewise constant regions defined by this decision tree to the Voronoi cells defined by kNN with \\(k=100\\):\n\n\n\nCode\n# Create meshgrid\nincome_grid &lt;- seq(0.2, 0.5, length.out = 100)\ncommute_grid &lt;- seq(0.2, 0.8, length.out = 100)\ngrid &lt;- expand.grid(Income = income_grid, Commute = commute_grid)\n\n# Get kNN predictions on the grid\ngrid$knn_pred &lt;- knn_predict(\n  mob |&gt; select(-Mobility) |&gt; as.matrix(),\n  grid |&gt; as.matrix(),\n  mob$Mobility,\n  k = 50\n)\n\n# Plot\nggplot(grid, aes(x = Commute, y = Income)) +\n  geom_tile(aes(fill = knn_pred)) +\n  geom_point(data = mob, aes(x = Commute, y = Income), size = 0.5, alpha = 0.5) +\n  scale_fill_viridis_c() +\n  labs(fill = \"Predicted\\nMobility\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThe axis-aligned rectangles are “simpler shapes” than the Voronoi cells, which can be more complex polytopes, and so kNN can approximate more complex functions than a decision tree of similar complexity.\nAdditionally, the number of piecewise constant regions created by kNN is combinatorial in \\(n\\) and \\(p\\), while the number of piecewise constant regions created by a decision tree is linear in the number of splits.\nHowever, the nonparametric nature of kNN requires storing all training data, while a decision tree only requires storing the tree structure and the values at the leaf nodes.\nMoreover, limiting the piecewise constant regions to axis-aligned rectangles also reduces the effects of the curse of dimensionality (though decision trees by themselves are usually either high-bias or high-variance models, as we will see later).\n\n\n\nTree Depth, Bias, and Variance\n\nThe depth of a decision tree is the length of the longest path from the root node to any leaf node.\nTypically, we control the complexity of a decision tree by limiting its depth.\nIf \\(\\mathrm{depth} = \\infty\\), then the leaf nodes of the decision tree will correspond to single training examples.\n\n\n\n\n\n\n\nTipQuiz: Effect of Depth\n\n\n\nA deeper decision tree will generally have (lower/higher) bias and (lower/higher) variance, while a shallower decision tree will generally have (lower/higher) bias and (lower/higher) variance.\n\n\nAnswer\n\nA deeper decision tree will generally have lower bias and higher variance, while a shallower decision tree will generally have higher bias and lower variance.\n\nA very deep tree will have one piecewise constant region per training example. Such a predictor will be highly dependent on the particular training sample, resulting in high variance.\nA very shallow tree will only have a few piecewise constant regions. On any training sample, the resulting model will be too simple to capture the underlying structure of the data, resulting in high bias."
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#learning-a-decision-tree-from-data",
    "href": "schedule/lectures/lecture_12_trees.html#learning-a-decision-tree-from-data",
    "title": "Lecture 12: Trees",
    "section": "Learning a Decision Tree from Data",
    "text": "Learning a Decision Tree from Data\n\nGiven a fixed depth \\(d\\), how do we learn a decision tree from data? We formulate the learning problem as an optimization problem over training data!\nWhat makes a good tree? If a piecewise constant function is going to approximate our data well, then we want all responses within a given piece to be as similar as possible. In other words, we want to minimize the impurity of each leaf node.\nMinimizing the impurity of all depth-\\(d\\) trees is a NP-hard problem, so instead we use a greedy strategy.\nWe will iteratively construct a depth-\\(d\\) tree, taking the most “impure” leaf node at each step and splitting it to reduce its impurity as much as possible.\n\n\nMetrics of Impurity\n\nRegression problems: we want to minimize the variance of responses within each piecewise region: \\(\\widehat{\\mathrm{Var}} = \\frac{1}{n-1} \\sum_{i=1}^n (Y_i - \\bar{Y})^2\\), where \\(\\bar{Y}\\) is the average response of all training points in the leaf.\nClassification problems: rather than minimizing variance, it is common to minimize Gini impurity of responses, \\(\\hat G = \\hat p (1 - \\hat p)\\), where \\(\\hat p\\) is the proportion of training points in the leaf with \\(Y=1\\).\n\n\n\nIntroducing a New Split\n\nWe will choose to split the most impure leaf node at each step.\nWe will split this most impure leaf node \\(R\\) into two child nodes \\(R^&gt;\\) and \\(R^&lt;\\).\nTo do this, we will consider all possible splits along all \\(p\\) predictor dimensions.\nMathematically:\n\nFor a given splitting variable \\(j\\) and split value \\(s\\), define \\[\n\\begin{align}\nR^&gt; &= \\{x \\in R : x^{(j)} &gt; s\\} \\\\\nR^&lt; &= \\{x \\in R : x^{(j)} &lt; s\\}\n\\end{align}\n\\]\nWe will choose \\(j\\) and \\(s\\) to minimize:\n\n\\[ \\mathrm{argmin}_{j,s} \\begin{cases}\n  \\vert R^&gt; \\vert \\widehat{\\mathrm{Var}}(R^&gt;) + \\vert R^&lt; \\vert  \\widehat{\\mathrm{Var}}(R^&lt;) & \\text{for regression} \\\\\n  \\vert R^&gt; \\vert \\widehat{G}(R^&gt;) + \\vert R^&lt; \\vert \\widehat{G}(R^&lt;) & \\text{for classification}\n\\end{cases} \\]\n\n\n\n\n\n\n\nTipWhy This Optimization Problem?\n\n\n\n\n\nWhen choosing a split for a given leaf node, we want to choose the split that minimizes the weighted impurity of the resulting child nodes."
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#advantages-and-disadvantages-of-decision-trees",
    "href": "schedule/lectures/lecture_12_trees.html#advantages-and-disadvantages-of-decision-trees",
    "title": "Lecture 12: Trees",
    "section": "Advantages and Disadvantages of Decision Trees",
    "text": "Advantages and Disadvantages of Decision Trees\n🎉 Trees are very easy to explain (much easier than even linear regression).\n🎉 Some people believe that decision trees mirror human decision.\n🎉 Trees can easily be displayed graphically no matter the dimension of the data.\n🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.\n💩 Trees aren’t very good at prediction.\n💩 Trees are highly variable. Small changes in training data \\(\\Longrightarrow\\) big changes in the tree.\nTo fix these last two, we can try to grow many trees and average their performance. We will cover this strategy in the next module."
  },
  {
    "objectID": "schedule/lectures/lecture_12_trees.html#summary",
    "href": "schedule/lectures/lecture_12_trees.html#summary",
    "title": "Lecture 12: Trees",
    "section": "Summary",
    "text": "Summary\n\nDecision trees are the parametric counterpart to kNN regression/classification.\n\nLike kNN, decision trees produce piecewise constant predictions.\nUnlike kNN, these piecewise constants are not defined through training data and are instead constrained to be axis-aligned rectangles.\n\nThe depth of decision trees controls the bias-variance tradeoff, where maximum depth corresponds to piecewise regions that contain only a single training point.\nTrees are constructed in a greedy fashion by iteratively splitting the most impure leaf node to reduce its impurity as much as possible.\nWhile decision trees are easy to interpret and explain, they tend to have high variance and/or high bias, leading to poor predictive performance."
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html",
    "href": "schedule/lectures/lecture_10_kernels.html",
    "title": "Lecture 10: Kernel Machines",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConvert ridge regression into kernel ridge regression using the representor theorem\nCompute the degrees of freedom for kernel ridge regressors (and \\(p &gt; n\\) basis regressors)\nIdentify valid kernel functions, and match them to their corresponding basis expansions (polynomial, Gaussian)"
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html#learning-objectives",
    "href": "schedule/lectures/lecture_10_kernels.html#learning-objectives",
    "title": "Lecture 10: Kernel Machines",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConvert ridge regression into kernel ridge regression using the representor theorem\nCompute the degrees of freedom for kernel ridge regressors (and \\(p &gt; n\\) basis regressors)\nIdentify valid kernel functions, and match them to their corresponding basis expansions (polynomial, Gaussian)"
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html#overview-of-this-module",
    "href": "schedule/lectures/lecture_10_kernels.html#overview-of-this-module",
    "title": "Lecture 10: Kernel Machines",
    "section": "Overview of This Module",
    "text": "Overview of This Module\n\nLast time: we hinted that degrees of freedom was bounded by \\(n\\) (number of training data).\nIn this module, we will discuss a category of predictive models where the degrees of freedom is capable of approaching (but never equaling or exceeding) \\(n\\).\nThese models are called non-parametric models, because the effective number of parameters (i.e., degrees of freedom) is not fixed, but rather grows with the amount of training data.\n\n\nNon-Parametric vs Parametric Models\n\nAs we will see, these models often base predictions off of similarity to other points, rather than the amount they express a particular feature\nHowever, these two mechanisms for modelling are deeply intertwined.\nToday, we will construct our non-parametric model by starting with a parametric model, and then taking the limit as the number of features \\(p \\to \\infty\\).\n\n\n\nIntuition: Taste versus Smell\n\nYour mouth has taste receptors for 5 basic tastes: sweet, sour, salty, bitter, and umami.\n\n(For those of you who are unaware, most of what you “taste” is actually smell!)\n\nYour nose has millions of smell receptors, each of which responds to a different combination of molecules\nThink about how you describe “taste” versus “smell”\n\nTaste: “It’s sweet and a little sour”\nSmell: “It smells like a mix of pine, citrus, and fresh-cut grass”\n\nTaste is described parametrically. It can be described by a small number of parameters (the 5 tastes).\nSmell is described non-parametrically. It is described by how similar it is to other smells you have experienced.\n\n\n\nParametric vs Non-Parametric is a Modelling Choice\n\nWe could describe smell parametrically, by defining the thousands of molecules that might be present in a smell. But this would be unwieldy, not how our brains work, and the non-parametric framework is more advantageous.\nSimilarly, we could describe taste non-parametrically, by describing how similar it is to other tastes we have experienced. But this would be less efficient, and the parametric framework is more advantageous."
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html#our-first-non-parametric-model-kernel-ridge-regression",
    "href": "schedule/lectures/lecture_10_kernels.html#our-first-non-parametric-model-kernel-ridge-regression",
    "title": "Lecture 10: Kernel Machines",
    "section": "Our First Non-Parametric Model: Kernel Ridge Regression",
    "text": "Our First Non-Parametric Model: Kernel Ridge Regression\n\nLet’s begin with ridge regression on 1D data, with \\(d\\)-Fourier basis functions:\n\\[\\begin{align*}\n\\hat \\beta_\\mathrm{ridge} = \\left( \\boldsymbol \\Phi^\\top \\boldsymbol \\Phi + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol \\Phi^\\top \\boldsymbol Y\n\\end{align*}\\]\nwhere \\(\\boldsymbol \\Phi \\in \\mathbb R^{n \\times d}\\) is the design matrix, with the \\(i^\\mathrm{th}\\) row given by:\n\\[\\begin{align*}\n  \\boldsymbol \\phi(X_i) = \\frac{1}{\\sqrt d} \\left[ \\cos(2 \\pi X_i), \\sin(2 \\pi X_i), \\ldots, \\cos(2 \\pi \\frac{d}{2} X_i), \\sin(2 \\pi \\frac{d}{2} X_i) \\right]\n\\end{align*}\\]\nNote that this is the same as normal ridge regression, except that the entries of the design matrix are given by basis functions \\(\\phi(x_i) \\in \\mathbb R^d\\), rather than the original features \\(x_i \\in \\mathbb R\\).\nNote the \\(1 / \\sqrt{d}\\) normalization constant. It isn’t necessary (and basically just changes the scale of the basis features), but it will be useful in a bit!\nI claim that we can also re-write the ridge regression solution in the following way, regardless of whether \\(d &lt; n\\) or \\(d &gt; n\\):\n\\[\\begin{align*}\n\\hat \\beta_\\mathrm{ridge} = \\boldsymbol \\Phi^\\top \\left( \\boldsymbol \\Phi \\boldsymbol \\Phi^\\top + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol Y\n\\end{align*}\\]\n\n\n\n\n\n\n\nTipDerivation\n\n\n\n\n\nWe can verify this by starting with the right-hand side, and using the SVD of \\(\\boldsymbol \\Phi = \\boldsymbol U \\boldsymbol D \\boldsymbol V.\\)\n\nFor now, let’s assume that \\(d &gt; n\\), so that \\(\\boldsymbol \\Phi\\) has full row rank. Then,\n\\(\\boldsymbol U \\in \\mathbb R^{n \\times n}\\) is orthogonal,\n\\(\\boldsymbol D \\in \\mathbb R^{n \\times n}\\) is diagonal with positive entries, and\n\\(\\boldsymbol V \\in \\mathbb R^{d \\times n}\\) has orthonormal columns.\n\nWe have:\n\\[\\begin{align*}\n\\left( \\boldsymbol \\Phi^\\top \\boldsymbol \\Phi + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol \\Phi^\\top\n&= \\left( \\boldsymbol V \\boldsymbol D^2 \\boldsymbol V^\\top + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\\\\n&= \\left( \\boldsymbol V \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right) \\boldsymbol V^\\top \\right)^{-1} \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\\\\n&= \\boldsymbol V \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol V^\\top \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\\\\n&= \\boldsymbol V \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol D \\boldsymbol U^\\top \\\\\n&= \\boldsymbol V \\boldsymbol D \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol U^\\top \\\\\n&= \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\boldsymbol U \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol U^\\top \\\\\n&= \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\left( \\boldsymbol U \\left( \\boldsymbol D^2 + \\lambda \\boldsymbol I \\right) \\boldsymbol U^\\top \\right)^{-1} \\\\\n&= \\boldsymbol V \\boldsymbol D \\boldsymbol U^\\top \\left( \\boldsymbol U \\boldsymbol D^2 \\boldsymbol U^\\top + \\lambda \\boldsymbol I \\right)^{-1} \\\\\n&= \\boldsymbol \\Phi^\\top \\left( \\boldsymbol \\Phi \\boldsymbol \\Phi^\\top + \\lambda \\boldsymbol I \\right)^{-1}\n\\end{align*}\\]\n\n\n\n\nWhen we make predictions at a new point \\(x\\), we have:\n\\[\\begin{align*}\n\\hat f(X) &= \\boldsymbol \\phi(X)^\\top \\hat \\beta_\\mathrm{ridge} \\\\\n&= \\boldsymbol \\phi(X)^\\top \\boldsymbol \\Phi^\\top \\left( \\boldsymbol \\Phi \\boldsymbol \\Phi^\\top + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol Y \\\\\n&= \\underbrace{\\left( \\boldsymbol \\phi(x)^\\top \\boldsymbol \\Phi^\\top \\right)}_{\\text{new } 1 \\times n \\text{ vector}} \\underbrace{\\left( \\boldsymbol \\Phi \\boldsymbol \\Phi^\\top + \\lambda \\boldsymbol I \\right)^{-1} }_{n \\times n \\text{ matrix}} \\boldsymbol Y.\n\\end{align*}\\]\nIf I define the function\n\\[\\begin{align*}\nk(x, x') = \\boldsymbol \\phi(x)^\\top \\boldsymbol \\phi(x')\n= \\frac{1}{d} \\sum_{j=1}^{d/2} \\left[ \\cos(2 \\pi j x) \\cos(2 \\pi j x') + \\sin(2 \\pi j x) \\sin(2 \\pi j x') \\right]\n\\end{align*}\\]\nthen I can write the prediction as:\n\\[\\begin{align*}\n\\hat f(X) &= \\underbrace{ \\begin{bmatrix}\nk(x, x_1) & k(x, x_2) & \\cdots & k(x, x_n)\n\\end{bmatrix} }_{:= \\boldsymbol k(X)^\\top} \\left( \\underbrace{ \\begin{bmatrix}\nk(x_1, x_1) & k(x_1, x_2) & \\cdots & k(x_1, x_n) \\\\\nk(x_2, x_1) & k(x_2, x_2) & \\cdots & k(x_2, x_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nk(x_n, x_1) & k(x_n, x_2) & \\cdots & k(x_n, x_n)\n\\end{bmatrix} }_{:= \\boldsymbol K} + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol Y\n\\end{align*}\\]\n\n\\(\\boldsymbol K = \\boldsymbol \\Phi \\boldsymbol \\Phi^\\top\\) is the \\(n \\times n\\) matrix with entries \\(K_{ij} = k(X_i, X_j)\\) (where \\(X_i\\), \\(X_j\\) are training points),\n\\(\\boldsymbol k(X)\\) is the \\(n \\times 1\\) vector with entries \\(k(X, X_i)\\) (where \\(X\\) is the test point, and the \\(X_i\\) are training points).\n\n\n\nKernel Ridge Regression\n\nWe call \\(k(X, X')\\) the kernel function.\nLet’s just assume for a second that it represents a notion of similarity between points \\(X\\) and \\(X'\\) (we’ll make this more precise later).\nThen, if we define:\n\\[\\begin{align*}\n  \\left( \\boldsymbol K + \\lambda \\boldsymbol I \\right)^{-1} \\boldsymbol Y =: \\boldsymbol \\alpha \\in \\mathbb R^n,\n\\end{align*}\\]\nthen we can write the prediction at a new point \\(X\\) as:\n\\[\\begin{align*}\n\\hat f(X) = \\sum_{i=1}^n \\alpha_i k(X, X_i).\n\\end{align*}\\]\nThe weights \\(k(X, X_i)\\) reflect how much we should base our prediction off of the similarity between \\(X\\) and each training point \\(X_i\\).\nThe coefficients \\(\\alpha_i\\) reflect how much each training point \\(X_i\\) should influence predictions in general.\nAgain, this model is exactly equivalent to good old ridge regression (with a basis expansion). We can always go back to the standard ridge formulation we learned in Lecture 6. However, we now have a nice non-parametric variant.\n\n\n\nDegrees of Freedom of Kernel Ridge Regression\n\nRecall that the degrees of freedom of ridge regression is given by:\n\\[\\begin{align*}\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{\\min(n, d)} \\frac{d_j^2}{d_j^2 + \\lambda}\n\\end{align*}\\]\nwhere \\(d_j\\) are the singular values of \\(\\boldsymbol \\Phi\\).\nNote that there are at most \\(\\min(n, d)\\) non-zero singular values.\nEven if \\(d \\to \\infty\\), the degrees of freedom will still be:\n\\[\\begin{align*}\n\\mathrm{df}(\\lambda) = \\sum_{j=1}^{n} \\frac{d_j^2}{d_j^2 + \\lambda} &lt; n\n\\end{align*}\\]\nwhere we have the strict inequality because \\(\\lambda &gt; 0\\).\nHowever, note that (if \\(d \\approx \\infty\\)) the degrees of freedom will increase as we add more training data.\nCompare this scenario to regression with \\(p &lt; n\\) covariates (or \\(d &lt; n\\) basis functions), where the degrees of freedom stays fixed with \\(p\\) (or \\(d\\)).\nThus, the complexity of our predictive model naturally scales with the amount of training data we have!"
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html#d-to-infty",
    "href": "schedule/lectures/lecture_10_kernels.html#d-to-infty",
    "title": "Lecture 10: Kernel Machines",
    "section": "\\(d \\to \\infty\\)",
    "text": "\\(d \\to \\infty\\)\n\nThe above formula is valid for any \\(d\\), even if \\(d \\gg n\\).\nWhat happens if we let \\(d \\to \\infty\\)?\nImagine that we made our basis expansions random, rather than fixed:\n\\[\\begin{align*}\n  \\phi_{2j - 1} (X_i) = \\frac{1}{\\sqrt d} \\cos(W_j X_i),\n  \\quad\n  \\phi_{2j} (X_i) = \\frac{1}{\\sqrt d} \\sin(W_j X_i)\n  \\qquad\n  W_j \\sim \\text{i.i.d. } \\mathcal{N}(0, 1/\\gamma^2)\n\\end{align*}\\]\ni.e. we replace the fixed frequencies \\(2 \\pi, 4 \\pi, \\ldots, 2 \\pi \\frac{d}{2}\\) with random frequencies \\(W_i\\) drawn from \\(\\mathcal{N}(0, 1/\\gamma^2)\\).\nThen our kernel function becomes:\n\\[\\begin{align*}\nk(x, x') &= \\frac{1}{d} \\sum_{j=1}^{d/2} \\left[ \\cos(W_j x) \\cos(W_j x') + \\sin(W_j x) \\sin(W_j x') \\right] \\\\\n\\end{align*}\\]\nFor a fixed \\(x\\) and \\(x'\\), the summation terms \\(\\left[ \\cos(W_j x) \\cos(W_j x') + \\sin(W_j x) \\sin(W_j x') \\right]\\) are i.i.d. random variables.\n\n\nWhy?\n\nAfter fixing \\(x\\) and \\(x'\\), the only random quantity is \\(W_j\\). Since the \\(W_j\\) are i.i.d., the summation terms are also i.i.d.\n\nBy the law of large numbers, as \\(d \\to \\infty\\), we have:\n\\[\\begin{align*}\nk(X, X') &\\to \\mathbb E_{W \\sim \\mathcal N(0, 1)} \\left[ \\cos(W x) \\cos(W x') + \\sin(W x) \\sin(W x') \\mid X, X' \\right] \\\\\n&= e^{-\\frac{1}{2 \\gamma^2} (X - X')^2}\n\\end{align*}\\]\n(Don’t worry about the details of how to compute this expectation; just trust me that it is true!)\n\n\nInfinitely Powerful Models in Closed Form\nLet’s take a step back and think about what we’ve just done:\n\nWe have a basis regressor, where we are including an infinite number of basis functions.\nThe degrees of freedom is still bounded by \\(n\\) (the number of training points), but can approach \\(n\\) as \\(\\lambda \\to 0\\).\nSince we have infinitely many features, \\(d &gt; n\\), so the degrees of freedom (i.e. the complexity of our predictive model) will always increase as we add more training data. In other words, our model is nonparametric.\nHowever, we are able to compute predictions in closed form, using the kernel function \\(k(X, X')\\):\n\\[\\begin{gather*}\n\\hat f_\\infty(X) = \\sum_{i=1}^n \\alpha_i k(X, X_i)\n= \\sum_{i=1}^n e^{-\\frac{1}{2} (X - X_i)^2} \\alpha_i,\n\\\\\n\\begin{bmatrix} \\alpha_1 \\\\ \\vdots \\\\ \\alpha_n \\end{bmatrix}\n= \\left( \\begin{bmatrix}\n  e^{-\\frac{1}{2} (X_1 - X_1)^2} & \\cdots & e^{-\\frac{1}{2} (X_1 - X_n)^2} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  e^{-\\frac{1}{2} (X_n - X_1)^2} & \\cdots & e^{-\\frac{1}{2} (X_n - X_n)^2}\n\\end{bmatrix} + \\lambda \\boldsymbol I \\right)^{-1}\n\\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix}\n\\end{gather*}\\]\nThis last fact should blow your mind 🤯🤯🤯 and it may be hard to wrap your head around. That’s okay! This idea is one of the most powerful but confusing concepts in all of machine learning.\n\n\n\nKernel Functions as Similarity Measures\n\nRecall that I said that non-parametric models often base predictions off of similarity to other points.\nWe can see that clearly with this infinite-basis kernel:\n\\[\\begin{align*}\n\\hat f_\\infty(X) = \\sum_{i=1}^n e^{-\\frac{1}{2} (X - X_i)^2} \\alpha_i\n\\end{align*}\\]\nIf \\(X\\) is close to a training point \\(X_i\\), then \\(k(X, X_i) = e^{-\\frac{1}{2} (X - X_i)^2} \\approx 1\\).\nIf \\(X\\) is far from a training point \\(X_i\\), then \\(k(X, X_i) = e^{-\\frac{1}{2} (X - X_i)^2} \\approx 0\\).\nThe hyperparameter \\(\\gamma\\), known as the bandwidth, controls how quickly similarity decays with distance.\nTherefore, the prediction \\(\\hat f_\\infty(X)\\) is mostly based off of the \\(\\alpha_i\\) coefficients of training points \\(X_i\\) that are similar to \\(X\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_10_kernels.html#summary",
    "href": "schedule/lectures/lecture_10_kernels.html#summary",
    "title": "Lecture 10: Kernel Machines",
    "section": "Summary",
    "text": "Summary\n\nMost of what we have studied so far are parametric models, where the complexity of the model is fixed and the predictive model is based on an explicit set of parameters (e.g., coefficients for each feature).\nNon-parametric models are models where the complexity of the model grows with the amount of training data, and predictions are often based on similarity to other points.\nFor ridge regression with \\(d &gt; n\\) basis expansions, we can re-formulate the model in a non-parametric way, writing predictions as a weighted sum of kernel functions that measure similarity to training points.\nWe can often even let \\(d \\to \\infty\\), resulting in a model with infinitely many basis functions, with a closed-form solution! (Other basis expansions similarly have closed-form solutions, but the kernel function will be different.)\nThe degrees of freedom of these models is still bounded by \\(n\\) (the number of training points), but can approach \\(n\\) as \\(\\lambda \\to 0\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html",
    "href": "schedule/lectures/lecture_08_basis_expansions.html",
    "title": "Lecture 8: Basis Expansions",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConstruct polynomial, spline, and Fourier basis expansions for regression\nSelect appropriate basis functions based on problem characteristics\nArticulate how basis expansions affect bias and variance\nDifferentiate between linearity and nonlinearity in terms of parameterization versus functional form"
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#learning-objectives",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#learning-objectives",
    "title": "Lecture 8: Basis Expansions",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nConstruct polynomial, spline, and Fourier basis expansions for regression\nSelect appropriate basis functions based on problem characteristics\nArticulate how basis expansions affect bias and variance\nDifferentiate between linearity and nonlinearity in terms of parameterization versus functional form"
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#overview",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#overview",
    "title": "Lecture 8: Basis Expansions",
    "section": "Overview",
    "text": "Overview\nWe have spent the last few lectures discussing methods to reduce variance in linear models.\n\nManual variable selection\nRidge regression (shrink all parameters)\nLasso regression (set some parameters to zero)\n\n\nIn this lecture, we will focus on the other side of the bias-variance tradeoff, and introduce a method to reduce bias in linear models.\nCrucially, our bias reduction method will allow us to retain linearity with respect to the learned parameters while allowing for nonlinearity with respect to the input features.\nThe method, basis expansions, is a generalization of the idea of interaction terms that you saw in STAT 306."
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#motivation",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#motivation",
    "title": "Lecture 8: Basis Expansions",
    "section": "Motivation",
    "text": "Motivation\n\nWe have typically assumed a statistical model where \\(\\mathbb E[Y \\mid X]\\) is a linear function of the input features \\(X\\), i.e. \\(\\mathbb E[Y \\mid X] = X^\\top \\beta\\) for some \\(\\beta \\in \\mathbb R^p\\).\nWe will now relax this assumption of linearity, and instead assume that\n\\[\\mathbb E[Y \\mid X] = f(X)\\]\nfor some unknown function \\(f: \\mathbb R^p \\to \\mathbb R\\).\nUsing a predictive model of the form \\(\\hat f_\\mathcal{D}(X) = X^\\top \\hat \\beta\\), like what we get from OLS, will not work well if \\(f\\) is highly nonlinear; it will be a high-bias predictor.\n\n\n\n\n\n\n\nImportantBut Isn’t OLS Unbiased?\n\n\n\n\nRecall that OLS is unbiased only under the assumptions made by the linear statistical model.\nUnder the more general model \\(\\mathbb E[Y \\mid X] = f(X)\\), OLS is biased unless \\(f\\) is actually linear.\nTo see why this is the case, let’s do a Taylor expansion of \\(f\\) around \\(0\\).\n\n\\[f(X) = f(0) + \\nabla f(0)^\\top X + \\frac{1}{2} X^\\top H_f(0) X + \\ldots\\]\nEven if OLS were to perfectly estimate \\(\\hat \\beta = \\nabla f(0)\\), the higher-order terms would still be missing, and so \\(\\mathbb E[\\hat f_\\mathcal{D}(X) \\mid X] \\neq f(X)\\).\n\n\n\n\n\n\n\n\nTipQuiz: Will Ridge or Lasso Be Beneficial in this Situation?\n\n\n\n\n\nProbably not.\n\nRidge and Lasso reduce variance and introduce bias, and we will likely be in a high-bias situation already.\nMore specifically, Ridge and Lasso still produce predictors of the form \\(\\hat f_\\mathcal{D}(X) = X^\\top \\hat \\beta\\), which are still missing the higher-order terms that create the bias problem in the first place."
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#warmup-polynomial-basis-expansions-for-p1",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#warmup-polynomial-basis-expansions-for-p1",
    "title": "Lecture 8: Basis Expansions",
    "section": "Warmup: Polynomial Basis Expansions for \\(p=1\\)",
    "text": "Warmup: Polynomial Basis Expansions for \\(p=1\\)\n\nWhy don’t we just use a predictive model that includes the higher-order terms?\nFor example, if \\(p=1\\), we could learn a model of the form:\n\\[\\hat f_\\mathcal{D}(x) = \\hat \\beta_0 + \\hat \\beta_1 x + \\hat \\beta_2 x^2 + \\ldots + \\hat \\beta_d x^d.\\]\nAs \\(d \\to \\infty\\), there exists some set of coefficients \\(\\hat \\beta_0, \\ldots, \\hat \\beta_d\\) such that \\(f(x) = \\sum_{j=0}^d \\beta_j x^j\\).\nThis model is known as a polynomial regression model, or a polynomial basis expansion.\n\n\n\n\n\n\n\nNoteLinearity vs. Nonlinearity\n\n\n\n\nThis model is non-linear in \\(X\\) (because of the \\(x^2, \\ldots, x^d\\) terms)\nHowever, it is still linear in the parameters \\(\\hat \\beta_0, \\ldots, \\hat \\beta_d\\).\nTherefore, we can pretend as if we have a dataset with \\(d\\) features \\((X, X^2, \\ldots, X^d)\\) and use OLS/Ridge/LASSO to learn the parameters \\(\\hat \\beta_0, \\ldots, \\hat \\beta_d\\).\n\n\n\n\nBias Reduction\nUsing higher-order polynomial terms can significantly reduce bias.\n\nLet’s assume that, under our statistical model, \\(\\mathbb E[Y \\mid X] = f(x)\\) for some \\(d^\\mathrm{th}\\) degree polynomial \\(f\\).\nLet’s also assume that we use a \\(d^\\mathrm{th}\\) degree polynomial basis expansion to learn a predictor \\(\\hat f_\\mathcal{D}(x) = \\sum_{j=0}^d \\hat \\beta_j x^j\\) using OLS.\nThen \\(\\hat f_\\mathcal{D}(x)\\) is an unbiased estimator of \\(f(x)\\)!\n\n\nWhy\n\n\nAgain, we can pretend as if we’re working with a dataset with \\(d\\) features \\((X, X^2, \\ldots, X^d)\\).\nUnder our statistical model, \\(\\mathbb E[Y \\mid X, X^2, \\ldots, X^d] = \\sum_{i=1}^d \\beta_i X^i\\)\nBy what we derived two lectures ago, OLS with the features \\((X, X^2, \\ldots, X^d)\\) will be unbiased for estimating \\(\\beta_0, \\ldots, \\beta_d\\).\n\n\n\n\n\nExample\n\nBelow we’ll plot the OLS fit using polynomial basis expansions of different orders on the arcuate dataset from the Stat406 package.\nThe standard linear model (no basis expansion) is a poor fit for the data. This error is likely due to high bias, because there’s enough data to estimate two parameters with little variance, but the linear model is too simple to capture the relationship between position and fa.\nAs we increase the order of the polynomial basis expansion, the fit improves significantly, and the bias is reduced.\n\n\n\nCode\nset.seed(406406)\nlibrary(tidyverse)\ndata(arcuate, package = \"Stat406\")\narcuate &lt;- arcuate |&gt; slice_sample(n = 220)\narcuate |&gt;\n  ggplot(aes(position, fa)) +\n  geom_point(color = \"black\") +\n  geom_smooth(aes(color = \"a\"), formula = y ~ x, method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"b\"), formula = y ~ poly(x, 4), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"c\"), formula = y ~ poly(x, 7), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"d\"), formula = y ~ poly(x, 25), method = \"lm\", se = FALSE) +\n  scale_color_manual(\n    name = \"Taylor order\",\n    values = c(\"a\" = \"grey\", \"b\" = \"blue\", \"c\" = \"red\", \"d\" = \"green\"),\n    labels = c(\"1 term\", \"4 terms\", \"7 terms\", \"25 terms\")\n  )"
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#polynomial-basis-expansions-for-p1",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#polynomial-basis-expansions-for-p1",
    "title": "Lecture 8: Basis Expansions",
    "section": "Polynomial Basis Expansions for \\(p>1\\)",
    "text": "Polynomial Basis Expansions for \\(p&gt;1\\)\n\nFor \\(p&gt;1\\), we have to include a few more terms.\nFor \\(x \\in \\mathbb R^p\\), the Taylor expansion of \\(f(x)\\) around \\(0\\) is:\n\\[f(X) = f(0) + \\nabla f(0)^\\top X + \\frac{1}{2} X^\\top H_f(0) X + \\ldots,\\]\nwhere \\(\\nabla f(0) \\in \\mathbb R^p\\) is the gradient of \\(f\\) at \\(0\\), and \\(H_f(0) \\in \\mathbb R^{p \\times p}\\) is the Hessian of \\(f\\) at \\(0\\).\nThe gradient has \\(p\\) entries, and the Hessian has \\(p(p+1)/2\\) unique entries (since it is a symmetric matrix).\nThus, the second-order polynomial basis expansion will be of the form:\n\\[\\hat f_\\mathcal{D}(x) = \\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_j + \\sum_{j=1}^p \\sum_{k=j}^p \\hat \\beta_{jk} x_j x_k.\\]\nThe \\(\\beta_{jk}\\) parameters for \\(j \\neq k\\) are called interaction terms, which you studied in STAT 306.\nThis model contains \\(1 + p + p(p+1)/2 = 1 + p(p+3)/2\\) parameters, which is \\(O(p^2)\\).\nIf \\(n\\) is not much larger than \\(p^2\\), then including these interaction terms could push us back into a high-variance regime.\nWe will come back to this parameter growth issue in the next module.\n\n\n\n\n\n\n\nTipBasis Expansions and Regularization\n\n\n\n\nIf \\(p\\) is large, it may be a good idea to use basis expansions in conjunction with Ridge or Lasso.\nAdding \\(O(p^2)\\) interaction terms will reduce bias (at the cost of increased variance), and Ridge/Lasso can help reduce the variance that is introduced.\nThis is just one example where we need to use both bias reduction and variance reduction techniques together to get a good predictor!"
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#other-basis-expansions",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#other-basis-expansions",
    "title": "Lecture 8: Basis Expansions",
    "section": "Other Basis Expansions",
    "text": "Other Basis Expansions\n\nBesides polynomials, there are two other common basis expansions: Fourier basis expansions and splines.\nAs with polynomials, both create nonlinear functions of the input features while remaining linear in the parameters.\n\n\nFourier Basis Expansions\n\nRecall that (most) \\(\\mathbb R \\to \\mathbb R\\) functions can be expressed by their Fourier series:\n\\[f(x) = a_0 + \\sum_{j=1}^\\infty a_j \\cos(j 2 \\pi x) + b_j \\sin(j 2 \\pi x)\\]\nWe can thus consider a predictive model of the form:\n\\[\\hat f_\\mathcal{D}(x) = \\hat a_0 + \\sum_{j=1}^d \\hat a_j \\cos(j 2 \\pi x) + \\hat b_j \\sin(j 2 \\pi x)\\]\nfor some \\(d \\in \\mathbb N\\).\nHigher values of \\(d\\) will be capable of fitting more complex functions (i.e. reduce bias), but as \\(d\\) gets close to \\(n\\), we will likely enter a high-variance regime.\n\n\n\nSplines\n\nA spline is a piecewise polynomial function that is smooth at the places where the pieces meet.\nFor example, a linear spline is a function of the form:\n\\[\nf(x) =\n\\begin{cases}\n  \\beta_0 + \\beta_1 x & x &lt; k_1 \\\\\n  \\beta_0 + \\beta_1 x + \\beta_2 (x - k_1) & k_1 \\leq x &lt; k_2 \\\\\n  \\beta_0 + \\beta_1 x + \\beta_2 (x - k_1) + \\beta_3 (x - k_2) & k_2 \\leq x &lt; k_3 \\\\\n  \\ldots\n\\end{cases}\n\\]\nwhere \\(k_1, k_2, \\ldots\\) are called knots.\nThe function is continuous at the knots, but not differentiable.\nAgain, increasing the number of knots will reduce bias but increase variance.\n\n\n\nComparison of Basis Expansions\n\nBelow is a comparison of the first 5 “features” created by each of the three basis expansions we discussed: polynomial, linear splines, and Fourier for a \\(p=1\\) input.\nAll three basis expansions can represent (nearly) all functions as \\(d\\to\\infty\\), but some basis expansions may be more appropriate for certain problems for a fixed value of \\(d\\).\n\n\n\nCode\nlibrary(cowplot)\nlibrary(ggplot2)\n\nrelu_shifted &lt;- function(x, shift) {pmax(0, x - shift)}\n\n# Create a sequence of x values\nx_vals &lt;- seq(-3, 3, length.out = 1000)\n\n# Create a data frame with all the shifted functions\ndata &lt;- data.frame(\n  x = rep(x_vals, 5),\n  polynomial = c(x_vals, x_vals^2, x_vals^3, x_vals^4, x_vals^5),\n  linear.splines = c(relu_shifted(x_vals, 2), relu_shifted(x_vals, 1), relu_shifted(x_vals, 0), relu_shifted(x_vals, -1), relu_shifted(x_vals, -2)),\n  fourier = c(cos(pi / 2 * x_vals), sin(pi / 2 * x_vals), cos(pi / 4 * x_vals), sin(pi / 4 * x_vals), cos(pi * x_vals)),\n  function_label = rep(c(\"f1\", \"f2\", \"f3\", \"f4\", \"f5\"), each = length(x_vals))\n)\n\n# Plot using ggplot2\ng1 &lt;- ggplot(data, aes(x = x, y = polynomial, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\ng2 &lt;- ggplot(data, aes(x = x, y = linear.splines, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\ng3 &lt;- ggplot(data, aes(x = x, y = fourier, color = function_label)) +\n      geom_line(size = 1, show.legend=FALSE) +\n      theme(axis.text.y=element_blank())\n\nplot_grid(g1, g2, g3, ncol = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChoosing a Basis Expansion\n\n\n\n\nThere is no universally best basis expansion.\nYou can try all three basis expansions and use cross-validation to select the best one.\nYou can include combinations of basis expansions (e.g. polynomial + splines), and use LASSO to select the best ones.\nYou can also use domain knowledge to select a basis expansion that is appropriate for your problem."
  },
  {
    "objectID": "schedule/lectures/lecture_08_basis_expansions.html#summary",
    "href": "schedule/lectures/lecture_08_basis_expansions.html#summary",
    "title": "Lecture 8: Basis Expansions",
    "section": "Summary",
    "text": "Summary\n\nBasis expansions are a method to reduce bias in linear models by allowing for nonlinearity with respect to the input features while retaining linearity with respect to the parameters.\nCommon basis expansions include polynomial basis expansions, Fourier basis expansions, and splines.\nBasis expansions can be used in conjunction with Ridge or Lasso to control variance when the number of parameters grows large.\nThere is no universally best basis expansion; you can try multiple basis expansions.\nThe number of parameters for basis expansions can grow quickly with the number of input features; we will discuss this issue in more detail in the next module."
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html",
    "href": "schedule/lectures/lecture_06_ridge_regression.html",
    "title": "Lecture 6: Ridge Regression",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement ridge regression using both constrained and penalized formulations\nDerive the closed-form solution for ridge regression\nArticulate how the regularization parameter affects bias, variance, and the learned parameters"
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#learning-objectives",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#learning-objectives",
    "title": "Lecture 6: Ridge Regression",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement ridge regression using both constrained and penalized formulations\nDerive the closed-form solution for ridge regression\nArticulate how the regularization parameter affects bias, variance, and the learned parameters"
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#overview",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#overview",
    "title": "Lecture 6: Ridge Regression",
    "section": "Overview",
    "text": "Overview\n\nIn the last lecture, we saw that the risk of a learned model decomposes into three components: bias, variance, and irreducible error.\nWe also saw that there is often a tradeoff between bias and variance: more flexible models tend to have lower bias but higher variance, while less flexible models tend to have higher bias but lower variance.\nOver the next few lectures, we will explore a set of techniques to help us navigate this tradeoff.\nWe have already seen one such technique: adding (or removing) covariates to our model to reduce bias (or reduce variance).\nWe will start with ridge regularization, a technique that will help us reduce the variance of our learned models by introducing some bias."
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#motivation-risk-analysis-of-ordinary-least-squares",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#motivation-risk-analysis-of-ordinary-least-squares",
    "title": "Lecture 6: Ridge Regression",
    "section": "Motivation: Risk Analysis of Ordinary Least Squares",
    "text": "Motivation: Risk Analysis of Ordinary Least Squares\n\nConsider the OLS estimator for linear regression:\n\\[ \\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y. \\]\nwhere \\(\\boldsymbol X \\in \\mathbb R^{n \\times p}\\) is the design matrix (with rows \\(x_i^\\top\\)) and \\(\\boldsymbol Y \\in \\mathbb R^n\\) is the vector of training responses.\nNow let’s assume that our data i.i.d. generated according to the linear model \\[ Y = X^\\top \\beta + \\epsilon, \\qquad \\epsilon \\sim \\mathcal N(0, \\sigma^2), \\]\nfor some true parameter vector \\(\\beta \\in \\mathbb R^p\\).\nBy this model (and again assuming that our data are i.i.d.), we can write the following model for our training data in matrix form:\n\n\\[ \\boldsymbol Y = \\boldsymbol X \\beta + \\boldsymbol \\epsilon, \\qquad \\boldsymbol \\epsilon \\sim \\mathcal N(\\boldsymbol 0, \\sigma^2 I). \\]\n\nLet’s now analyze the bias and variance of the OLS estimator \\(\\hat{\\beta}_\\mathrm{OLS}\\) under this model!\n\n\nBias of OLS\nI claim that the OLS estimator is unbiased, i.e.,\n\\[ \\mathbb E[\\hat{\\beta}_\\mathrm{OLS}] = \\beta. \\]\n\n\n\n\n\n\nTipDerivation\n\n\n\n\n\n\\[\\begin{align*}\n\\mathbb E[\\hat{\\beta}_\\mathrm{OLS}]\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol Y \\right] \\\\\n&= \\mathbb E\\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\mid \\boldsymbol X \\right]}_{= \\boldsymbol X \\beta} \\right] \\\\\n&= \\mathbb E\\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\right] \\\\\n&= \\beta\n\\end{align*}\\]\n\n\n\nSince we have an unbiased estimator, our average model (\\(\\mathbb E[\\hat f_\\mathcal{D}(X) \\mid X ]\\) from last lecture) will produce the prediction \\(X^\\top \\beta\\), so the bias component of the risk will be zero!\n\n\n\n\n\n\nCautionCheck Your Assumptions!\n\n\n\nWe have shown that the bias associated with OLS is zero under the assumption that the linear model is correct.\nIn practice, it’s very unlikely that the linear model is exactly correct (i.e. the true value of \\(Y\\) may depend on covariates we don’t have access to, interaction terms, etc.). In that case (i.e. when the linear model is too simple of an approximation for the ground-truth data-generating process), the bias of OLS is non-zero.\n\n\n\n\n(Co-)Variance of OLS\nThe covariance of this estimator is a little more complicated to derive, but we can use similar techniques to show that:\n\\[ \\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\mathbb E \\left[ \\left( \\hat \\beta_\\mathrm{OLS}  - \\beta \\right) \\left(\\hat \\beta_\\mathrm{OLS} - \\beta \\right)^\\top \\right]= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]. \\]\n\n\n\n\n\n\nTipDerivation\n\n\n\n\n\n\\[\\begin{align*}\n\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS})\n&= \\mathbb E \\left[ \\hat \\beta_\\mathrm{OLS} \\hat \\beta_\\mathrm{OLS} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\underbrace{\\mathbb E \\left[ \\boldsymbol Y \\boldsymbol Y^\\top \\mid \\boldsymbol X \\right]}_{\\boldsymbol X \\beta \\beta^\\top \\boldsymbol X^\\top + \\sigma^2 \\boldsymbol I} \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\mathbb E \\left[ \\underbrace{(\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X}_{= \\boldsymbol I} \\beta \\beta^\\top \\underbrace{\\boldsymbol X^\\top \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1}}_{= \\boldsymbol I} \\right] \\\\\n  &\\:\\:\\:+ \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^T \\boldsymbol X (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right]\n  - \\beta \\beta^\\top \\\\\n&= \\beta \\beta^\\top - \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right] - \\beta \\beta^\\top \\\\\n&= \\sigma^2 \\mathbb E \\left[ (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\right].\n\\end{align*}\\]\n\n\n\n\n\nWhen is Variance Large? (An SVD Analysis)\n\nTo understand when the variance of OLS becomes problematic, we can use the singular value decomposition (SVD) of \\(\\boldsymbol X\\).\nRecall that any rectangular \\(n \\times p\\) matrix \\(\\boldsymbol X\\) has an SVD \\(\\boldsymbol X = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top\\)\n\n\n\n\n\n\n\nTipSVD Review\n\n\n\n\n\n\n\\(\\boldsymbol U\\) is \\(n \\times p\\) with orthonormal columns: \\(\\boldsymbol U^\\top \\boldsymbol U = \\boldsymbol I\\)\n\\(\\boldsymbol V\\) is \\(p \\times p\\) and orthonormal (rows and columns): \\(\\boldsymbol V^\\top \\boldsymbol V = \\boldsymbol V \\boldsymbol V^\\top = \\boldsymbol I\\)\n\\(\\boldsymbol D\\) is \\(p \\times p\\), diagonal, and nonnegative (contains the singular values)\nUsing the SVD, we can rewrite the covariance of OLS as:\n\n\\[\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\sigma^2 (\\boldsymbol X^\\top \\boldsymbol X)^{-1} = \\sigma^2 (\\boldsymbol V \\boldsymbol D^2 \\boldsymbol V^\\top)^{-1} = \\sigma^2 \\boldsymbol V (\\boldsymbol D^2)^{-1} \\boldsymbol V^\\top\\]\n\n\n\n\nThis means that \\(\\text{Cov}(\\hat{\\beta}_\\mathrm{OLS}) = \\sigma^2 \\boldsymbol V \\text{diag}(d_1^{-2}, d_2^{-2}, \\ldots, d_p^{-2}) \\boldsymbol V^\\top\\), where \\(d_1, d_2, \\ldots, d_p\\) are the singular values.\nKey insight: When \\(\\boldsymbol X\\) has small singular values (i.e., when \\(d_j \\approx 0\\) for some \\(j\\)), then \\(d_j^{-2}\\) becomes very large, leading to high variance in \\(\\hat{\\beta}_\\mathrm{OLS}\\).\nThis situation arises when \\(\\boldsymbol X\\) is ill-conditioned or nearly rank-deficient, which happens when we have multicollinearity: a linear combination of predictor variables is nearly equal to another predictor variable.\nIn such cases, \\(\\hat{\\beta}_\\mathrm{OLS}\\) has large, unstable values with high variance."
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#intuition-fight-variance-by-shrinking-large-coefficients",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#intuition-fight-variance-by-shrinking-large-coefficients",
    "title": "Lecture 6: Ridge Regression",
    "section": "Intuition: Fight Variance by “Shrinking” Large Coefficients",
    "text": "Intuition: Fight Variance by “Shrinking” Large Coefficients\n\nMain idea: To combat the high variance problem, we can constrain the values of \\(\\beta\\) to be small.\n\n\nConstrained Optimization Problem\n\nInstead of solving the unconstrained OLS problem: \\[\\min_\\beta \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2\\]\nWe can solve a constrained optimization problem for some \\(s &gt; 0\\): \\[\\min_\\beta \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq s\\]\nHere, \\(\\|\\beta\\|_2^2 = \\sum_{j=1}^p \\beta_j^2\\) is the squared \\(\\ell_2\\)-norm of \\(\\beta\\).\nThis constraint prevents the coefficients from becoming arbitrarily large, which may happen when \\(\\boldsymbol X\\) is ill-conditioned (and thus \\(\\mathbb E[(\\boldsymbol X^\\top \\boldsymbol X)^{-1}]\\) has large entries).\n\n\n\n\nIllustration of constrained vs. unconstrained optimization solutions (\\(\\hat \\beta_\\mathrm{OLS}\\) vs \\(\\hat \\beta_\\mathrm{ridge}\\)) on a 2D toy problem. Contours represent level sets of mean squared error (MSE), i.e. \\(\\frac{1}{N} \\Vert \\boldsymbol Y - \\boldsymbol X \\hat \\beta \\Vert_2^2\\), for various values of \\(\\hat \\beta\\)\n\n\n\n\nGraphical Perspective\n\nIntuitively, why does this constraint reduce variance?\nRecall from the previous lecture that high variance indicates that our learned model is very sensitive to the training data.\nThe OLS solution \\(\\hat \\beta_\\mathrm{OLS}\\) depends on the training data \\(\\mathcal D = \\{(x_i, y_i)\\}_{i=1}^n\\).\nIf we had a different training set \\(\\mathcal D'\\), we can change the OLS optimization landscape a lot, which can lead to a solution \\(\\hat \\beta_\\mathrm{OLS}'\\) that is very far away from \\(\\hat \\beta_\\mathrm{OLS}\\).\nIf we are instead constrained to have our solution inside the ball of radius \\(\\sqrt{s}\\), then the optimal solutions for \\(\\mathcal D\\) and \\(\\mathcal D'\\) (\\(\\hat \\beta_s\\) and \\(\\hat \\beta'_s\\)) will be less far apart!\n\n\n\n\nIllustration of why ridge reduces variance. Two different training samples (\\(\\mathcal D\\) and \\(\\mathcal D'\\)) yield different mean squared error (MSE) optimization problems with different solutions that may be far apart. If we instead constrain the MSE to live inside a ball, the resulting solutions will be less far apart.\n\n\n\n\n\n\n\n\nWarningThe Trade-off\n\n\n\nBy constraining the coefficients, we can gain a significant reduction in variance, though we will introduce some bias (as we will soon see)."
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#ridge-regression",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#ridge-regression",
    "title": "Lecture 6: Ridge Regression",
    "section": "Ridge Regression",
    "text": "Ridge Regression\n\nRegularization\n\nAn equivalent way to write the constrained optimization problem \\[\\hat{\\beta}_s = \\mathrm{argmin}_{\\beta} \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 \\quad \\text{subject to} \\quad \\|\\beta\\|_2^2 \\leq s\\] is as a regularized (or penalized) optimization with regularization weight \\(\\lambda\\): \\[\\hat{\\beta}_\\lambda = \\mathrm{argmin}_{\\beta} \\frac{1}{n}\\|\\boldsymbol Y - \\boldsymbol X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_2^2\\]\nFor every \\(\\lambda\\) there is a unique \\(s\\) (and vice versa) that makes \\(\\hat{\\beta}_s = \\hat{\\beta}_\\lambda\\). We will work with the \\(\\lambda\\) formulation.\n\n\n\nClosed Form\n\nRidge regression has a closed-form solution (set the derivative with respect to \\(\\beta\\) to zero): \\[\\hat{\\beta}_\\lambda = (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol Y\\]\nCompare to OLS: \\(\\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y\\)\nThe \\(+ \\lambda \\boldsymbol I\\) term stabilizes the inversion when \\(\\boldsymbol X^\\top \\boldsymbol X\\) is ill-conditioned, preventing division by near-zero singular values.\n\n\n\nShrinkage\n\nUsing the SVD \\(\\boldsymbol X = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top\\), we can show that: \\[\\hat{\\beta}_\\lambda = \\boldsymbol V (\\boldsymbol D^2 + \\lambda \\boldsymbol I)^{-1} \\boldsymbol D \\boldsymbol U^\\top \\boldsymbol Y\\]\nNotice that OLS depends on \\(d_j/d_j^2 = 1/d_j\\) while ridge depends on \\(d_j/(d_j^2 + \\lambda)\\).\nShrinkage effect: Ridge regression makes the coefficients smaller relative to OLS, but when \\(\\boldsymbol X\\) has small singular values, ridge compensates with \\(\\lambda\\) in the denominator.\n\n\n\n\n\n\n\nNoteKey Observations\n\n\n\n\n\\(\\lambda = 0\\) makes \\(\\hat{\\beta}_\\lambda = \\hat{\\beta}_\\mathrm{OLS}\\)\n\n\nWhy?\n\nWhen \\(\\lambda = 0\\), \\((\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol y = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol y = \\hat \\beta_\\mathrm{OLS}\\).\n\n\\(\\lambda \\to \\infty\\) makes \\(\\hat{\\beta}_\\lambda \\to \\boldsymbol{0}\\)\n\n\nWhy?\n\n\nWe can write \\((\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} = \\frac{1}{\\lambda} (\\frac{1}{\\lambda} \\boldsymbol X^\\top \\boldsymbol X + \\boldsymbol I)^{-1}\\).\nAs \\(\\lambda \\to \\infty\\), \\(\\frac{1}{\\lambda} \\to 0\\), so \\(\\frac{1}{\\lambda} (\\frac{1}{\\lambda} \\boldsymbol X^\\top \\boldsymbol X + \\boldsymbol I)^{-1} \\to \\frac{1}{\\lambda} \\boldsymbol I^{-1} = \\boldsymbol 0\\).\n\n\nAny \\(0 &lt; \\lambda &lt; \\infty\\) penalizes larger values of \\(\\beta\\), effectively shrinking them"
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#effect-on-biasvariance",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#effect-on-biasvariance",
    "title": "Lecture 6: Ridge Regression",
    "section": "Effect on Bias/Variance",
    "text": "Effect on Bias/Variance\n\nEffect on Bias: Ridge regression introduces some bias because it shrinks the unbiased OLS coefficients towards zero.\n\n\n\n\n\n\n\nTipA Fun Mathematical Interpretation\n\n\n\n\n\nNote that we can rewrite the ridge estimator as:\n\\[\\begin{align*}\n\\hat{\\beta}_\\lambda\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} {\\color{blue} \\boldsymbol I} \\boldsymbol X^\\top \\boldsymbol Y \\\\\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} {\\color{blue} \\boldsymbol X^\\top \\boldsymbol X} \\underbrace{{\\color{blue} \\left( \\boldsymbol X^\\top \\boldsymbol X \\right)} \\boldsymbol X^\\top \\boldsymbol Y}_{\\hat \\beta_\\mathrm{OLS}} \\\\\n&= (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol X \\hat \\beta_\\mathrm{OLS}.\n\\end{align*}\\]\nSo \\(\\hat{\\beta}_\\lambda\\) is a transformed version of \\(\\hat \\beta_\\mathrm{OLS}\\). Since \\(\\hat \\beta_\\mathrm{OLS}\\) is unbiased, and \\((\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol X\\) is not the identity, \\(\\hat{\\beta}_\\lambda\\) must be biased!\n\n\n\n\nEffect on Variance: As we’ve discussed, ridge regression reduces variance by preventing coefficients from becoming too large.\n\n\nPicking the Regularization Parameter\n\nWith ridge regression, we’re accepting a slight increase in bias with the hope that we’ll get a large decrease in variance.\nAs \\(\\lambda\\) increases, bias increases and variance decreases.\nWe want to choose the best value of \\(\\lambda\\) that hits the sweet spot in the bias-variance tradeoff.\nWe can use cross-validation to select the value of \\(\\lambda\\) that minimizes risk."
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#example",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#example",
    "title": "Lecture 6: Ridge Regression",
    "section": "Example",
    "text": "Example\nLet’s see ridge regression in action on the prostate cancer dataset from Lab 1:\n\ndata(prostate, package = \"ElemStatLearn\")\n\nY &lt;- prostate$lpsa\nX &lt;- model.matrix(~ ., data = prostate |&gt; dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge &lt;- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\nplot(ridge, xvar = \"lambda\", lwd = 3)\n\n\n\n\n\n\n\n\n\nThis first plot shows the values of the \\(\\hat \\beta_\\mathrm{OLS}\\) coefficients as a function of \\(\\log(\\lambda)\\).\nWhen \\(\\log(\\lambda)\\) is very negative (i.e., \\(\\lambda\\) is close to zero), the coefficients are at their largest values.\nAs \\(\\log(\\lambda)\\) increases, the coefficients shrink towards zero.\n\n\nplot(ridge, main = \"Ridge\")\n\n\n\n\n\n\n\n\n\nThis second plot shows the cross-validation estimation of risk as a function of \\(\\log(\\lambda)\\).\nWe can see that the risk is minimized at some intermediate value of \\(\\lambda\\), indicating that it is worthwhile introducing some bias to reduce variance!\nHowever, the risk with \\(\\lambda = 0\\) (i.e., OLS) is not much worse than the optimal risk, indicating that we may not have been in the high-variance regime with our OLS estimator.\n(Maybe we’re actually in the high bias regime? We’ll learn some techniques to address that soon!)"
  },
  {
    "objectID": "schedule/lectures/lecture_06_ridge_regression.html#summary",
    "href": "schedule/lectures/lecture_06_ridge_regression.html#summary",
    "title": "Lecture 6: Ridge Regression",
    "section": "Summary",
    "text": "Summary\n\nWe have now introduced our first method to help us navigate the bias-variance tradeoff: ridge regression, which introduces bias to reduce variance.\nThe Problem: OLS has high variance when \\(\\boldsymbol X\\) is ill-conditioned (multicollinearity), which occurs when some singular values are near zero, making \\((\\boldsymbol X^\\top \\boldsymbol X)^{-1}\\) unstable.\nThe Solution: Ridge regression constrains coefficients via \\(\\|\\beta\\|_2^2 \\leq s\\) (or equivalently adds penalty \\(\\lambda \\|\\beta\\|_2^2\\)), which shrinks coefficients toward zero and stabilizes the solution with closed form \\(\\hat{\\beta}_\\lambda = (\\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top \\boldsymbol Y\\).\nThe Tradeoff: Ridge introduces bias but significantly reduces variance, and we can use cross-validation to find the optimal \\(\\lambda\\) that minimizes prediction risk in the bias-variance tradeoff."
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html",
    "href": "schedule/lectures/lecture_04_model_selection.html",
    "title": "Lecture 4: Model Selection",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDefine and differentiate between testing error, expected test error, and risk\nChoose amongst metrics and estimators for model selection on a variety of problems\nIdentify when a validation-set estimator of risk is biased or (nearly) unbiased\nPerform cross-validation to perform variable selection in linear regression"
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#learning-objective",
    "href": "schedule/lectures/lecture_04_model_selection.html#learning-objective",
    "title": "Lecture 4: Model Selection",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDefine and differentiate between testing error, expected test error, and risk\nChoose amongst metrics and estimators for model selection on a variety of problems\nIdentify when a validation-set estimator of risk is biased or (nearly) unbiased\nPerform cross-validation to perform variable selection in linear regression"
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#motivation",
    "href": "schedule/lectures/lecture_04_model_selection.html#motivation",
    "title": "Lecture 4: Model Selection",
    "section": "Motivation",
    "text": "Motivation\nWe are now going to fill in the remaining steps of the learning procedure from a statistical perspective. We’ve already covered defining the statistical model, estimation, and prediction.\n\n\n\n\n\n\n\n\n\nStep\nCS Perspective\nStatistical Perspective\nExample: Linear Regression\n\n\n\n\n1\nSplit data into train/test/val\n???\n???\n\n\n2\nHypothesis Class\nStatistical Model\n\\(\\mathbb{E}[Y \\mid X = x] = x^\\top \\beta\\)\n\n\n3\nTraining\nEstimation\n\\(\\hat{\\beta}_\\mathrm{MLE/OLS} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top \\boldsymbol{Y}\\)\n\n\n4\nValidation\n???\n???\n\n\n5\nIteration\n???\n???\n\n\n6\nTesting (Inference)\nPrediction\n\\(\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\)\n\n\n\n\nFrom a statistical perspective, steps 1, 4, and 5 blur together in a procedure we call model selection.\nAt a high level, the purpose of these three steps is to choose the best statistical model given our training data."
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#defining-the-best-model",
    "href": "schedule/lectures/lecture_04_model_selection.html#defining-the-best-model",
    "title": "Lecture 4: Model Selection",
    "section": "Defining “The Best Model”",
    "text": "Defining “The Best Model”\n\nPossible Metrics for “Best Model”\n\nTest error\n\nOn (almost) every learning task, we usually withhold some test data \\(\\left\\{ (X_j, Y_j) \\right\\}_{j=1}^t\\), assumed to be i.i.d. from the same distribution as our training data.\nGiven our training set \\(\\mathcal D = \\left\\{ (X_i, Y_i) \\right\\}_{i=1}^n\\), the test error is:\n\\[ \\widehat{\\mathrm{Err}}_\\mathcal{D} := \\frac{1}{t} \\sum_{j=1}^t L(Y_j, f_\\mathcal{D}(X_j)) \\]\nI.e. we train our model \\(\\hat f\\) on \\(\\mathcal D\\), and then compute its average loss on the test dataset.\n\nExpected test error\n\nAs \\(t \\to \\infty\\), \\(\\widehat{\\mathrm{Err}}_\\mathcal{D}\\) converges (by the law of large numbers) to: \\[ \\mathrm{Err}_\\mathcal{D} := \\mathbb E \\left[ L(Y, \\hat f_\\mathcal{D}(X)) \\mid \\mathcal D \\right] \\]\nWe call this the expected test error.\nNote that the training set \\(\\mathcal D\\) is fixed.\n\n\nWhy?\n\nWe are assuming that the number of test samples \\(t\\) goes to infinity, but we’re assuming the training set doesn’t change. So we’re averaging over the randomness in the test point \\((X, Y)\\) only, but not the randomness in the training set \\(\\mathcal D\\).\n\nIntuition: your boss hands you training set \\(\\mathcal D\\), and you train a \\(\\hat f_\\mathcal{D}\\) that’s going to be deployed for a very very long time. \\(\\mathrm{Err}_\\mathcal{D}\\) estimates the error you’ll get on the many many predictions that will be made by your model.\n\nRisk\n\nRisk, which is one level more abstract than expected test error, averages over all sources of randomness: \\[ \\mathcal R := \\mathbb E \\left[ L \\left( Y, \\hat f_{\\mathcal D}(X) \\right) \\right] \\]\nNote that all sources of randomness includes the test datapoint \\((X, Y)\\) as well as the training dataset \\(\\mathcal D\\).\nWe can relate \\(\\mathcal R\\) to the expected test error via:\n\\[ \\mathcal R = \\mathbb E \\left[ \\mathrm{Err}_\\mathcal{D} \\right] \\]\n\n\nWhy?\n\nTower rule!!!\n\\[\\begin{align*}\n    \\mathcal R &= \\mathbb E \\left[ L \\left( Y, \\hat f_{\\mathcal D}(X) \\right) \\right] \\\\\n    &= \\mathbb E \\left[ \\mathbb E \\left[ L \\left( Y, \\hat f_{\\mathcal D}(X) \\right)  \\mid \\mathcal D \\right] \\right] \\\\\n    &= \\mathbb E \\left[ \\mathrm{Err}_\\mathcal{D} \\right]\n  \\end{align*}\\]\n\nIntuition: if you were to get a new training set \\(\\mathcal D'\\) and retrain your model, how well would it perform on average?\n\n\n\n\nIn a Perfect World, Which Metric Should We Use?\n\nLet’s ignore the fact that we have access to limited amounts of data, and pretend that we happened to know all three of these metrics exactly. Which one should we use?\nTest error is out. There’s nothing special about the particular test set we happened to withhold. We’d much rather know the average error over all possible points, so expected test error is strictly better.\nRegarding expected test error vs. risk, there’s some debate. In general I would argue that risk is actually the metric we care most about.\n\n\n\n\n\n\n\nNoteExpected Test Error vs. Risk\n\n\n\n\n\nReasons to prefer expected test error\n\nIn practice, we only have one training set \\(\\mathcal D\\).\nExpected test error tells us how well our model will perform on the particular training set we are given.\n\nReasons to prefer risk\n\nIn practice, we might get new data to retrain our model on; we don’t want a metric that’s dependent on a particular training set.\nIf we’re trying to select a “best” model, we’d ideally like one that’s not too sensitive to the particular training set we happened to get.\nAs we will see next lecture, risk is easier to analyze theoretically.\n\n\n\n\n\nIn 75% of scenarios, it doesn’t matter if we target risk or expected test error. (In practice, most estimators of risk are also estimators of expected test error, and vice versa.) However, we’ll see some cases (e.g. on Homework 2) where the distinction matters."
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#estimating-risk",
    "href": "schedule/lectures/lecture_04_model_selection.html#estimating-risk",
    "title": "Lecture 4: Model Selection",
    "section": "Estimating Risk",
    "text": "Estimating Risk\n\nBad Idea: Training Error\n\nA common mistake is to use training error as an estimator of risk:\n\\[ \\widehat{\\mathrm{Err}}_\\mathrm{train} := \\frac{1}{n} \\sum_{i=1}^n L(Y_i, f_\\mathcal{D}(X_i)) \\]\nTraining error is almost always a severely biased estimator of risk, and should never be used for model selection.\n\n\nWhy?\n\n\nThe training error is computed on the same data that was used to train the model.\n\nTherefore, the model has “seen” this data before, and has likely fit it quite well.\nIn contrast, risk is computed on new, unseen data.\nAs a result, training error underestimates the true risk of the model.\n\nExample: consider a dataset with \\(p\\) points and a linear model with \\(p\\) parameters. Running OLS on this dataset will result in zero training error, but the risk of this model will be much higher than zero on new data!\n\n\n(Training error does indeed have its uses; we’ll see one example next lecture!)\n\n\n\nIdeal Solution: Many Samples of Training Data + One Test Point\n\nImagine we had access to a generator that produces random \\(\\left\\{ (X_i, Y_i) \\right\\}_{i=1}^{n+1}\\) samples, where the \\(n+1\\) pairs are i.i.d.\nWe could estimate risk by:\n\nGenerating \\(m\\) samples \\(\\left\\{ \\left\\{ (X_i^{(j)}, Y_i^{(j)} \\right\\}_{i=1}^{n+1} \\right\\}_{j=1}^{m}\\)\nEstimate risk as: \\[ \\hat{\\mathcal R} \\approx \\frac{1}{m} \\sum_{i=1}^m L_j,\n  \\quad L_j = L\\left( Y_{n+1}^{(j)}, f_{\\mathcal D^{(j)}}( X^{(j)}_{n+1}) \\right),\n  \\quad \\mathcal D^{(j)} = \\left\\{ (X_i^{(j)}, Y_i^{(j)}) \\right\\}_{i=1}^n.\n  \\]\n\n\nIn other words, for each of the \\(m\\) samples, we train a model on the first \\(n\\) data points, and compute its loss on the \\(n+1^\\mathrm{th}\\) datapoint we withhold from training.\n\nBy the law of large numbers, as \\(m \\to \\infty\\) we have:\n\\[\\hat{\\mathcal R} \\to \\mathbb E[ L( Y_{n+1}, \\hat f_\\mathcal{D}( X_{n+1}) )] = \\mathcal R\\]\n\n\n\nPractical Solution: Cross-Validation\n\nIn practice, we only have access to a single dataset \\(\\mathcal D = \\left\\{ (X_i, Y_i) \\right\\}_{i=1}^n\\) with \\(n\\) data points.\nNevertheless: we can approximate the ideal solution above using leave-one-out cross-validation (LOO-CV).\n\n\n\n\n\n\n\nTipCross Validation\n\n\n\n\nGiven: a single dataset \\(\\mathcal D = \\left\\{ (X_i, Y_i) \\right\\}_{i=1}^n\\)\nFor each \\(i = 1, \\ldots, n\\):\n\nTrain a model \\(\\hat f_{\\mathcal D_{-i}}\\) on the dataset with the \\(i^\\mathrm{th}\\) point removed: \\(\\mathcal D_{-i} := \\left\\{ (X_j, Y_j) \\right\\}_{j \\neq i}\\)\nCompute the loss on the withheld point: \\(L_i := L(Y_i, \\hat f_{\\mathcal D_{-i}}(X_i))\\)\nEstimate risk as: \\(\\hat{\\mathcal R}_\\mathrm{LOOCV} := \\frac{1}{n} \\sum_{i=1}^n L_i\\)\n\n\n\n\n\nWhy Does LOO-CV Work?\n\nEach \\(L_i\\) is an unbiased estimate of risk on a training set of size \\(n-1\\):\n\\[\\begin{align*}\n  \\mathbb E[L_i] &= \\mathbb E \\left[ L(Y_i, \\hat f_{\\mathcal D_{-i}}(X_i)) \\right] =: \\mathcal R_{n-1} \\\\\n\\end{align*}\\]\nBy linearity of expectation, we also have that \\(\\hat{\\mathcal R}_\\mathrm{LOOCV}\\) is an unbiased estimate of risk on a training set of size \\(n-1\\):\n\n\nDerivation:\n\n\\[\\begin{align*}\n    \\mathbb E[\\hat{\\mathcal R}_\\mathrm{LOOCV}] = \\mathbb E \\left[ \\frac{1}{n} \\sum_{i=1}^n L_i \\right]\n    = \\frac{1}{n} \\sum_{i=1}^n \\mathbb E[L_i] = \\mathcal R_{n-1}\n  \\end{align*}\\]\n\nUnlike our “ideal estimator” above, the \\(L_i\\) are not independent, and therefore we can’t apply the law of large numbers to conclude that \\(\\hat{\\mathcal R}_\\mathrm{LOOCV}\\) converges to \\(\\mathcal R\\).\nIn practice, however, \\(\\hat{\\mathcal R}_\\mathrm{LOOCV}\\) is often a good estimator of risk.\n\n\n\nA More Efficient Solution: K-Fold Cross Validation\n\nLOO-CV requires training \\(n\\) separate models, which can be very expensive.\nA more efficient alternative is K-fold cross validation (K-CV):\n\n\n\n\n\n\n\nTipK-Fold Cross Validation\n\n\n\n\nGiven: a single dataset \\(\\mathcal D = \\left\\{ (X_i, Y_i) \\right\\}_{i=1}^n\\)\nRandomly split \\(\\mathcal D\\) into \\(K\\) (roughly) equal-sized “folds”: \\(\\mathcal D_1, \\ldots, \\mathcal D_K\\)\nFor each \\(k = 1, \\ldots, K\\):\n\nTrain a model \\(\\hat f_{\\mathcal D_{-k}}\\) on the dataset with the \\(k^\\mathrm{th}\\) fold removed: \\(\\mathcal D_{-k} := \\bigcup_{j \\neq k} \\mathcal D_j\\)\nCompute the loss on the withheld fold: \\(L_k := \\frac{1}{|\\mathcal D_k|} \\sum_{(X_i, Y_i) \\in \\mathcal D_k} L(Y_i, \\hat f_{\\mathcal D_{-k}}(X_i))\\)\nEstimate risk as: \\(\\hat{\\mathcal R}_\\mathrm{K-CV} := \\frac{1}{K} \\sum_{k=1}^K L_k\\)\n\n\n\n\n\nDecreasing \\(K\\) decreases the number of models we need to train.\nHowever, decreasing \\(K\\) also results in a worse estimator of risk.\n\n\nWhy?\n\nIntuitively: as \\(K\\) decreases, each model is trained on less data, and we’re trying to estimate the risk of a model trained on all \\(n\\) data points.\nFormally: each \\(L_k\\) is an unbiased estimate of risk on a training set of size \\(n - n/K\\), but the difference between \\(\\mathcal R_n\\) and \\(\\mathcal R_{n - n/K}\\) can be significant for small \\(K\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#model-selection-finally",
    "href": "schedule/lectures/lecture_04_model_selection.html#model-selection-finally",
    "title": "Lecture 4: Model Selection",
    "section": "Model Selection (Finally!)",
    "text": "Model Selection (Finally!)\nNow that we’ve chosen risk as our metric for “the best model,” model selection amounts to choosing the statistical model with the best risk.\n\n\n\n\n\n\nTipModel Selection\n\n\n\n\nGiven: a single dataset \\(\\mathcal D\\)\nfor however long you have,\n\nPropose a statistical model (e.g. linear regression, decision tree, neural network, etc.)\nEstimate its parameters on \\(\\mathcal D\\) to get \\(\\hat f_\\mathcal{D}\\)\nEstimate its risk \\(\\hat{\\mathcal R}\\) using K-CV (or some other method)\n\nChoose the model with the lowest estimated risk \\(\\hat{\\mathcal R}\\)\n\n\n\n\nExample: Variable Selection\nConsider the following synthetic dataset, where we have:\n\\[ Y = 3 X_1 + \\frac{1}{3} X_2 + \\epsilon, \\quad \\epsilon \\sim \\mathcal N(0, 0.5^2) \\]\n\nlibrary(tidyverse)\n\nset.seed(42)\nn &lt;- 50\ndf &lt;- tibble( # like data.frame, but columns can be functions of preceding\n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 0.5) # function of x1 and x2 only\n)\n\nNote that \\(Y\\) is a function of \\(X_1\\) and \\(X_2\\) only, and \\(X_3\\) and \\(X_4\\) are irrelevant to predicting \\(Y\\).\nWe might consider the following statistical models (here written as linear regression models), in all cases assuming \\(\\epsilon \\sim \\mathcal N(0, \\sigma^2)\\) for some \\(\\sigma &gt; 0\\):\n\nModel 1: \\(Y = \\beta_0 + \\beta_1 X_1 + \\epsilon\\)\nModel 2: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\epsilon\\)\nModel 3: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\epsilon\\)\nModel 4: \\(Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\epsilon\\)\n\n\n\n\n\n\n\nImportantVariable Selection is a Specific Instance of Model Selection\n\n\n\n\n\nYou might recognize this setup as a variable selection problem. (Even more specifically, these model choices may remind you of forward stepwise selection from STAT 306).\nAdding covariates to our regression changes the set of distributions that we are considering to represent our data (i.e. it changes our statistical model).\nNote that model selection is a much broader concept than variable selection. For example, we might also consider:\n\\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + \\beta_4 X_4 + \\beta_5 X_1^2 + \\epsilon\\]\nThis model doesn’t add any new covariates, but it does change the set of distributions we are considering.\n\n\n\nLet’s use 5-fold cross validation to estimate the risk of each of these models:\n\nlibrary(cv) # NOTE: you can't use this package on Homework 1\n            # but for sure use it on your own projects!\n\nmodel1 &lt;- lm(y ~ x1, data = df)\nmodel2 &lt;- lm(y ~ x1 + x2, data = df)\nmodel3 &lt;- lm(y ~ x1 + x2 + x3, data = df)\nmodel4 &lt;- lm(y ~ x1 + x2 + x3 + x4, data = df)\n\nrisks &lt;- tibble(\n  model = c(\"Model 1\", \"Model 2\", \"Model 3\", \"Model 4\"),\n  risk = c(\n    cvInfo(cv(model1, k = 5, criterion = mse)),\n    cvInfo(cv(model2, k = 5, criterion = mse)),\n    cvInfo(cv(model3, k = 5, criterion = mse)),\n    cvInfo(cv(model4, k = 5, criterion = mse))\n  )\n)\nrisks %&gt;%\n  ggplot(aes(x = model, y = risk)) +\n  geom_col() +\n  labs(title = \"Estimated Risk by Model\", y = \"Estimated Risk (MSE)\", x = \"Model\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nBased on our model selection procedure (using 5-fold CV to estimate risk), we would choose Model 2, which—in this case—is indeed the correct model!\n\n\nConnection to the Algorithmic Perspective\n\nStep 1 train/test/val split: the train/val split is implicit in K-fold CV, and the test set is used only at the very end to estimate test error of the final model. (Do not touch the test set until the very end!)\nStep 4 validation: here is where we actually estimate risk using K-fold CV.\nStep 5 iteration: we iterate over different statistical models, estimating their risk using K-fold CV, and performing model selection along the way (i.e. refining our statistical model in ways to reduce risk)."
  },
  {
    "objectID": "schedule/lectures/lecture_04_model_selection.html#summary",
    "href": "schedule/lectures/lecture_04_model_selection.html#summary",
    "title": "Lecture 4: Model Selection",
    "section": "Summary",
    "text": "Summary\n\nModel selection is the process of choosing the “best statistical model” given our training data.\nWe can define “best” in terms of expected test error, or risk. In practice, risk is often the most appropriate metric.\nWe can estimate risk using cross-validation. (We will see other estimators later in the course.)\nNever use training error to select models!"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDistinguish between “learning,” “supervised learning,” “prediction,” and “inference.”\nTranslate between algorithmic and statistical perspectives on learning\n\n\nDefine a statistical model\nDefine an estimator\nDefine a prediction rule\n\n\nDefine the standard linear regression model\nDerive ordinary least squares from either the maximum likelihood estimation (MLE) or the empirical risk minimization (ERM) framework"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#learning-objectives",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#learning-objectives",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDistinguish between “learning,” “supervised learning,” “prediction,” and “inference.”\nTranslate between algorithmic and statistical perspectives on learning\n\n\nDefine a statistical model\nDefine an estimator\nDefine a prediction rule\n\n\nDefine the standard linear regression model\nDerive ordinary least squares from either the maximum likelihood estimation (MLE) or the empirical risk minimization (ERM) framework"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#what-is-supervised-learning",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#what-is-supervised-learning",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "What is (Supervised) Learning?",
    "text": "What is (Supervised) Learning?\nThere are many formulations of “learning” in statistics and machine learning. The main focus of this course is supervised learning.\n\nGoal of supervised learning: predict a response variable \\(Y\\) given a set of covariates \\(X\\)\n\nFor example, predicting house prices based on features like size, location, and number of bedrooms.\n\n\nLearning from data:\nWe are given a training dataset: \\[ \\mathcal{D} = \\{(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\} \\in \\mathbb R^p \\times \\mathcal Y\\]\nwhere:\n\n\\(X_i \\in \\mathbb R^p\\) are the \\(p\\)-dimensional covariates (e.g. size, location, number of bedrooms, etc. for each house)\n\\(Y_i \\in \\mathcal Y\\) is the response variable (e.g. house price)\n\\(\\mathcal Y\\) is the space of possible responses.\n\nFor our problem and other regression problems, \\(\\mathcal Y = \\mathbb R\\).\nFor classification problems (i.e. predicting whether the house is for sale or not) \\(\\mathcal Y\\) is a finite set of classes (e.g. “for sale” vs “not for sale”).\n\n\nFrom this training set, we want to learn a function \\(\\hat f : \\mathbb R^p \\to \\mathcal Y\\) that accurately predicts the response for new observations.\n\nGiven a set of new covariates \\(X_\\mathrm{new}\\) (e.g. a new house that we don’t know the price of)\nWe predict \\(\\hat Y_\\mathrm{new} = \\hat f(X_\\mathrm{new})\\) (e.g. our predicted price for the new house)\nOur goal is for \\(\\hat Y_\\mathrm{new} \\approx Y_\\mathrm{new}\\): the true (but unknown) price of the new house.\n\nAssumptions: what is random?\n\nUsing the notation from last lecture, you’ll note that the training data \\((X_i, Y_i)\\) and the test data \\((X_\\mathrm{new}, Y_\\mathrm{new})\\) are random variables.\nThis is a modelling choice. As statisticians, our primary tool for reasoning about the learning process is assuming something is random.\nIn particular, we’re going to make the following assumptions about our data:\n\nThere is some joint distribution \\(P(X, Y)\\) that governs the covariate/response pairs we see in the world\nOur training data are i.i.d. (independently and identically distributed) random samples from that distribution\nOur test data are also i.i.d. random samples from that distribution.\n\n\nBut isn’t our data given to us?\n\nIn the real world, your training data will actually be some set of numbers. For example, your boss may hand you a CSV with covariate/response pairs. How are these quantities random, if the data are fixed and handed to us?\nAgain, randomness is a modelling assumption that we use to simplify our lives. So how does this assumption help us?\nWhat we want to codify in our model: the data we make predictions on (our test data) are “sufficiently similar” to our training data\nHow do we codify “sufficiently similar” in a probabilistic way?\n\nOne way is to assume that they come from the same population distribution.\nTo “come from the same population distribution” means that, mathematically, we treat them as random samples from \\(P(Y, X)\\).\n\n\n\nPrediction versus Inference\nIn learning we are primarily concerned with prediction over inference.\n\nInference: The goal is making a probabilistic statement about the relationship between \\(X\\) and \\(Y\\).\n\n(For example, we might want to know how much the price of a house increases for each additional bedroom.)\n\nPrediction: The goal is producing accurate \\(\\hat Y_\\mathrm{new}\\).\n\nWe don’t really care about the underlying relationship, or even about making probabilistic statements about it.\nWe just want some procedure to give us good predictions."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#the-supervised-learning-procedure-two-perspectives",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#the-supervised-learning-procedure-two-perspectives",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "The Supervised Learning Procedure: Two Perspectives",
    "text": "The Supervised Learning Procedure: Two Perspectives\n\n\nCS/Algorithmic Perspective\nLearning is often presented as an algorithm for producing a prediction rule \\(\\hat f\\) from data \\(\\mathcal{D}\\). This algorithm consists of 6 steps, which we will examine in the context of linear regression:\n\nTrain/val/test split: Divide data into training and testing sets\n\nGiven our dataset \\(\\mathcal{D}\\), we split it into a training set \\(\\mathcal{D}_\\mathrm{train}\\), a validation set \\(\\mathcal{D}_\\mathrm{val}\\), and a test set \\(\\mathcal{D}_\\mathrm{test}\\).\n\nHypothesis class: Select a set of candidate \\(\\hat f\\) which might make a good prediction rule.\n\nFor linear regression, the hypothesis class is linear functions of the form \\(\\hat f(x) = x^\\top \\hat\\beta\\) for some \\(\\hat \\beta\\).\n\nTraining: Define a training algorithm (a procedure to choose a \\(\\hat \\beta\\) from the training data) and apply it to \\(\\mathcal{D}_\\mathrm{train}\\).\n\nMost training algorithms involve minimizing a loss function over training data.\nFor linear models, we often choose \\(\\hat f\\) to minimize the squared error loss over training data. \\[L(Y, \\hat Y) = (Y - \\hat Y)^2, \\qquad \\hat \\beta = \\mathrm{argmin}_{\\beta} \\frac{1}{n} \\sum_{i=1}^{n} L(Y_i, X_i^\\top \\beta). \\]\n\nValidation: Test performance on withheld validation data.\n\nWe compute the average loss on the validation set to evaluate how well our model generalizes to unseen data.\n\nIteration: Refine model based on evaluation results\n\nWe may choose to change our hypothesis class, the set of features, the loss function, or the training algorithm to try to reduce validation error.\n\nTesting (confusingly referred to by some as “Inference”): Once satisfied with the model, evaluate on the test set to estimate performance on new data.\n\n\n\nStatistical Perspective\nOver the next few lectures, we will derive the CS/algorithmic perspective from first statistical principles. For this lecture, we will focus on a statistical perspective on Steps 2, 3, and 6:\n\n\n\nStep\nCS Perspective\nStatistical Perspective\n\n\n\n\n2\nHypothesis Class\nStatistical Model\n\n\n3\nTraining\nEstimation\n\n\n6\nTesting (Inference)\nPrediction"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#statistical-models",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#statistical-models",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model defines the possible relationships between the covariates \\(X\\) and the response variable \\(Y\\).\n\nFormally, it is a set of probability distributions that could possibly generate the data we observe.\nFor the learning problem, where we care about predicting \\(Y\\) given \\(X\\), we will consider statistical models that are sets of conditional distributions \\(P(Y \\mid X)\\).\n\n\nThe Linear Regression Model\n\nFrom STAT 306, you may recall that we typically assume that the relationship between \\(X\\) and \\(Y\\) can be described as: \\[Y = X^\\top \\beta + \\varepsilon\\]\n\n\\(\\beta\\) is the vector of parameters that we are trying to estimate.\n\\(\\varepsilon\\) is the random error term, which is typically i.i.d. \\(\\varepsilon \\sim N(0, \\sigma^2)\\) for some \\(\\sigma^2 &gt; 0\\).\n(You may notice that we don’t have an intercept term \\(\\beta_0\\) in this model. We assume that \\(X\\) has an “all-ones” covariate, i.e. \\(X = [1, X_1, X_2, \\ldots, X_p]\\), so that the intercept term \\(\\beta_0\\) is included with the other \\(\\beta\\) terms, i.e. \\(X^\\top \\beta = \\beta_0 + X_1^\\top \\beta_1 + \\ldots + X_p^\\top \\beta_p\\).)\n\nFor any given \\(\\beta\\), if we are given \\(X = x\\), then we have that \\(Y \\sim \\mathcal{N}(x^\\top\\beta, \\sigma^2)\\). Thus, the corresponding statistical model/set of possible conditional distributions is:\n\n\\[\\left\\{ P \\: : \\: P(Y \\mid X = x) \\: = \\: \\mathcal{N}(x^\\top\\beta, \\sigma^2), \\:\\: \\beta \\in \\mathbb R^p \\right\\}.\\]\n\nWe refer to \\(\\beta\\) as the parameters of the model, as a given value of \\(\\beta\\) specifies a particular distribution within the set.\n\n\n\n\n\n\n\nTipExercise: What is Random?\n\n\n\nGoing back to the linear model \\(Y = X^\\top \\beta + \\varepsilon,\\) which of these values are random?\n\n\nAnswer\n\n\\[ \\underbrace{Y}_\\text{random} = \\underbrace{X^\\top}_\\text{random} \\underbrace{\\beta}_\\text{fixed} + \\underbrace{\\varepsilon}_\\text{random} \\]\n\n\\(\\varepsilon\\) is random; note that above we are assuming that \\(\\varepsilon \\sim N(0, \\sigma^2)\\)\n\n\\(X\\) is random; again we make the assumption that our data are random samples from a distribution (also it’s noted as a capital letter!)\n\\(\\beta\\) is fixed! It is the set of parameters that defines the particular distribution in our statistical model that actually relates \\(X\\) and \\(Y\\)\n\\(Y\\) is also random; again by assumption, but also because \\(Y\\) equals some function involving random variables\n\nNote that if we instead consider \\(Y \\mid X = x\\), then\n\\[ \\underbrace{Y \\mid X=x}_\\text{random} = \\underbrace{x^\\top}_\\text{fixed} \\underbrace{\\beta}_\\text{fixed} + \\underbrace{\\varepsilon}_\\text{random} \\]\n\n\n\n\n\n\nOther Models\nThis linear regression model is just one statistical model we could use to describe our data. Almost any family of conditional distributions can be used instead. For example:\n\nThe more general linear regression model: we could consider the slightly more general case of conditional distributions defined by an expected value condition: \\[\\left\\{ P \\: : \\: \\mathbb E[Y \\mid X = x] \\: = \\: x^\\top\\beta, \\:\\: \\beta \\in \\mathbb R^p \\right.\\}\\]\n\nNote that \\(\\mathbb E[\\mathcal{N}(x^\\top\\beta, \\sigma^2)] = x^\\top \\beta\\), and so the linear regression model is a subset of this more general model.\nHowever, this model allows for the possibility of non-normal residuals.\n\nThe polynomial regression model: we could instead consider the slightly more general case where the expectation is a polynomial function of \\(X\\), rather than just a linear function: \\[\\left\\{ P \\: : \\: \\mathbb E[Y \\mid X = x] \\: = \\: p(x), \\:\\: p \\text{ is a polynomial of degree } d \\right\\}\\]\n\nNote again that our general linear model is a special case; however this model allows for more complex relationships between \\(X\\) and \\(Y\\).\nThis model also has parameters (the coefficients of the polynomial), though they are not explicit in this formulation.\n\nAll possible distributions: what if we used the most general set of distributions? \\[\\left\\{ P \\: : \\: P(Y \\mid X = x) \\text{ is any distribution} \\right\\}\\]\n\nWhile this is a valid set, it turns out that it is too general for the purposes of learning.\nSpecifically, the no free lunch theorem states that we will not be able to identify a “good” model from this set even if we were given infinite training data!\n(We need to have some “structure” or simplifying assumptions in our model to be able to learn from data.)"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#prediction",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#prediction",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Prediction",
    "text": "Prediction\nGoing a bit out of order, let’s discuss how we make predictions on new data once we’ve selected a specific distribution from our statistical model to describe our data.\n\nWorking with the linear statistical model, each distribution is parameterized by some coefficients \\(\\beta \\in \\mathbb R^p\\).\nOnce we have chosen some \\(\\hat{\\beta}\\) (i.e. the parameters of the distribution that “best” describes our data) we can make predictions on new data points \\(X_\\mathrm{new}\\).\nFor linear models, we often use the prediction rule: \\[\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top\\hat{\\beta}\\]\nBut where does that come from?\n\nDecision theory for predictions: We want a principled reason for using this prediction rule.\n\nFirst, we need a way of judging the quality of any single prediction.\nWe define a loss function \\(L(Y, \\hat Y)\\) that measures how bad our prediction \\(\\hat Y\\) matches the true response \\(Y\\)\n\nA common loss function for regression is the squared error \\(L(Y, \\hat Y) = (Y - \\hat Y)^2\\).\nNote that \\(L(Y, \\hat Y)\\) only equals 0 when \\(Y = \\hat Y\\), and is \\(&gt; 0\\) when \\(Y \\ne \\hat Y\\).\n\nDecision theory: choose a prediction \\(\\hat{Y}_\\mathrm{new}\\) that minimizes the expected loss:\n\\[\\hat{Y}_\\mathrm{new} = \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\mathbb E[L(Y, \\hat{y}_\\mathrm{new}) \\mid X_\\mathrm{new}, \\hat \\beta].\\]\nSolving this optimization problem for the squared error loss gives us\n\\[\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\]\n\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\nAccording to the distribution parameterized by \\(\\hat \\beta\\), we have\n\\[P(Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta) = \\mathcal{N}(X_\\mathrm{new}^\\top\\hat{\\beta}, \\sigma^2).\\]\nPlugging in the squared error loss function into this formula, we get:\n\\[\\begin{align*}\n  \\hat{Y}_\\mathrm{new} &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\mathbb E[(Y_\\mathrm{new}- \\hat{y}_\\mathrm{new})^2 \\mid X_\\mathrm{new}, \\hat \\beta] \\\\\n  &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\left( \\underbrace{\\mathbb E[Y_\\mathrm{new}^2\\mid X_\\mathrm{new}, \\hat \\beta]}_{\n    \\underbrace{\\mathrm{Var}[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta]}_{\\sigma^2} +\n    \\underbrace{(\\mathbb E[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta])^2}_{(X_\\mathrm{new}^\\top \\hat{\\beta})^2}\n  } - 2\\hat{y}_\\mathrm{new} \\underbrace{\\mathbb E[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat\\beta]}_{X_\\mathrm{new}^\\top \\hat{\\beta}}\n  + \\hat{y}_\\mathrm{new}^2 \\right) \\\\\n  &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\left(\n    \\underbrace{\\sigma^2}_\\mathrm{const.} + \\underbrace{\n      (X_\\mathrm{new}^\\top \\hat{\\beta})^2- 2\\hat{y}_\\mathrm{new} (X_\\mathrm{new}^\\top \\hat{\\beta}) + \\hat{y}_\\mathrm{new}^2\n    }_{(\\hat{y}_\\mathrm{new} - X_\\mathrm{new}^\\top \\hat{\\beta})^2}\n  \\right)\n\\end{align*}\\]\nWe can drop the \\(\\sigma^2\\) constant, since it doesn’t affect the minimum, and we are left with:\n\\[\\hat{Y}_\\mathrm{new} = \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} (\\hat{y}_\\mathrm{new} - X_\\mathrm{new}^\\top \\hat{\\beta})^2,\\]\nwhich, after taking the derivative and setting to zero, gives us: \\(\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\).\n\n\n\n\nImportant! If we had chosen a different loss function, we could end up with a different prediction rule."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#estimation",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#estimation",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Estimation",
    "text": "Estimation\nWe’ve talked about making predictions from a distribution selected by our statistical model. But how did we choose that distribution? (Or alternatively, how did we choose \\(\\hat\\beta\\)?)\n\nThe estimation step involves us choosing a specific distribution from the model (or alternatively, the parameters \\(\\hat \\beta\\) defining the distribution) that best fits our training data.\nThere are many statistically valid estimation procedures. We’ll derive two that you’ve likely seen before: Maximum Likelihood Estimation (MLE) and Empirical Risk Minimization (ERM). (We’ll talk about others throughout this course.)\n\nWe will work through these two procedures using the linear regression model as our statistical model.\n\n1. Empirical Risk Minimization (ERM)\nGoal: Find the parameters that minimize the loss on our training data.\n\nFrom STAT 306, you may remember a procedure known as Ordinary Least Squares (OLS) to estimate the parameters of a linear regression model.\nThis procedure is a special case of a more general procedure Empirical Risk Minimization (ERM) for reasons that we will see in a few lectures.\n\nThe OLS/ERM Procedure:\nThe goal of ERM is to choose the distribution (i.e. set of parameters) that minimizes the loss over predictions on our training data.\n\nGiven a set of parameters \\(\\hat \\beta\\), let \\(\\hat Y_i\\) be the predictions that we make on the training data using the prediction rule derived from the loss \\(L(Y, \\hat Y)\\).\nThe empirical risk is defined as the total loss over our training data:\n\\[\\hat{R}(\\hat \\beta) =\\frac{1}{n}\\sum_{i=1}^n L(Y_i, \\hat Y_i)\\]\nwhere we (typically) use the same loss that governs our prediction rule. Using the squared loss, \\(\\hat{R}(\\hat\\beta) = \\frac{1}{n}\\sum_{i=1}^n (Y_i - X_i^\\top \\hat\\beta)^2\\)\nThe ERM estimator is the set of parameters \\(\\hat{\\beta}\\) (i.e. the distribution from our model) that minimizes this empirical risk:\n\\[\\hat{\\beta}_\\mathrm{OLS} = \\arg\\min_{\\beta} \\hat{R}(\\beta) = \\arg\\min_{\\beta} \\frac{1}{n}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2\\]\nTaking the derivative and setting to zero:\n\\[\\frac{\\partial}{\\partial \\beta}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2 = -2\\sum_{i=1}^n X_i(Y_i - X_i^\\top\\beta) = 0\\]\nBy arranging all of our training data into a matrix \\(\\boldsymbol X \\in \\mathbb{R}^{n \\times p}\\) and a vector \\(\\boldsymbol Y \\in \\mathbb{R}^n\\), with\n\\[\\boldsymbol X = \\begin{bmatrix} -X_1^\\top- \\\\ -X_2^\\top- \\\\ \\vdots \\\\ -X_n^\\top- \\end{bmatrix} \\in \\mathbb{R}^{n \\times p}, \\qquad \\boldsymbol Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix},\\]\nwe get:\n\\[\\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y.\\]\n\n\n\n2. Maximum Likelihood Estimation (MLE)\nGoal: Find the parameters that maximize the likelihood of observing our data.\n\nThe likelihood function maps a distribution from our model (parameterized by \\(\\beta\\)) to the probability density of our observed training data.\n\nLikelihood function for linear regression.\n\nFor the linear regression model with normal errors, the likelihood function is: \\[\\mathcal{L} (\\beta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_i - X_i^\\top\\beta)^2}{2\\sigma^2}\\right)\\]\nThe term inside the product is the density of the normal distribution \\(\\mathcal{N}(X_i^\\top\\beta, \\sigma^2)\\) evaluated at \\(Y_i\\).\nWe take the product over all \\(n\\) training examples \\((X_i, Y_i)\\) because we assume that the data points are i.i.d.\nWe choose \\(\\hat\\beta\\) to maximize the likelihood function, i.e. the \\(\\hat\\beta\\) that make observed data most probable.\n\nLog-likelihoods are easier to work with.\n\nTaking the log of the likelihood function, log-likelihood: \\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2\\]\nThe logarithm is a monotonic function, so maximizing the likelihood is equivalent to maximizing the log-likelihood.\nThe maximizer of this log likelihood function (and thus the maximum of the likelihood function) is thus equal to: \\[\\mathrm{argmax}_{\\beta} -\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2 = \\mathrm{argmin}_{\\beta} \\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2,\\]\n\n\nDetails:\n\nThe \\(-\\frac{n}{2}\\log(2\\pi\\sigma^2)\\) constant and the \\(\\frac{1}{2\\sigma^2}\\) scaling term also don’t affect the maximum, so we can ignore them.\n\nThis is exactly the same optimization problem we derived for ERM! Thus, \\[ \\hat{\\beta}_\\mathrm{MLE} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y = \\hat{\\beta}_\\mathrm{OLS} \\]\n\nKey Insight: For linear regression with normal errors, MLE and OLS/ERM give identical results!\n\nIf we had used a different loss function (e.g. \\(L(Y_i, \\hat{Y_i}) = |Y_i - \\hat{Y_i}|\\)), the ERM estimator would not be the same as the OLS/MLE estimator.\nThere are many possible loss functions with different properties, and we will explore them in future lectures."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#summary",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#summary",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced the statistical framework for learning:\n\nStatistical models define the probabilistic structure of our data\nEstimation finds the best parameters using MLE or ERM\nPrediction uses fitted models to make predictions on new data\n\n\nThis framework will extend to more complex models and methods throughout the course.\nLinear regression serves as our foundational example, where MLE and ERM produce identical estimators under normal error assumptions.\nIn the next lecture, we will apply this statistical framework to classification problems."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#section",
    "href": "schedule/slides/00-version-control.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "00 Git and Github",
    "text": "00 Git and Github\nStat 406\nGeoff Pleiss\nLast modified – 01 September 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control",
    "href": "schedule/slides/00-version-control.html#why-version-control",
    "title": "UBC Stat406 2025 W1",
    "section": "Why version control?",
    "text": "Why version control?\n\nMuch of this lecture is based on material from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control-1",
    "href": "schedule/slides/00-version-control.html#why-version-control-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWhen you get really good\n\n\nVersion control can act as a living lab notebook"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#overview",
    "href": "schedule/slides/00-version-control.html#overview",
    "title": "UBC Stat406 2025 W1",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "href": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "title": "UBC Stat406 2025 W1",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#typical-workflow",
    "href": "schedule/slides/00-version-control.html#typical-workflow",
    "title": "UBC Stat406 2025 W1",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\n```{bash}\ngit clone https://github.com/stat550-2021/lecture-slides.git\n```\n\nCreate a branch\n\n```{bash}\ngit branch &lt;branchname&gt;\n```\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\n```{bash}\ngit add &lt;name/of/tracked/file&gt;\n```\n\nCommit your changes\n\n```{bash}\ngit commit -m \"Some explanatory message\"\n```\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to GitHub\n\n```{bash}\ngit push\ngit push -u origin &lt;branchname&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "href": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "title": "UBC Stat406 2025 W1",
    "section": "Commit messages should be meaningful",
    "text": "Commit messages should be meaningful\n\n\n\n\n\nInstead, try “Update linear model in Question 1.2”"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nDefinitely\n\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\n\n\n\nPossibly\n\njupyter notebooks, images (that won’t change), …\n\n\n\n\nQuestionable\n\nprocessed data, static pdfs, …\n\n\n\n\nDefinitely not\n\nfull data, continually updated pdfs, other things compiled from source code, logs…"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nTLDR\nAny file that YOU edit should be tracked\nAny file that’s computer generated should PROBABLY NOT be tracked\nHowever, in this course you will track rendered PDFs of your homeworks/labs. This makes it easier for the graders."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\nA file called .gitignore tells git files or types to never track\n```{bash}\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n```\nShortcut to track everything (use carefully):\n```{bash}\ngit add .\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr",
    "href": "schedule/slides/00-version-control.html#whats-a-pr",
    "title": "UBC Stat406 2025 W1",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "href": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "href": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "title": "UBC Stat406 2025 W1",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThere are guardrails in place. But those won’t stop a bulldozer.\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 9 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#protection",
    "href": "schedule/slides/00-version-control.html#protection",
    "title": "UBC Stat406 2025 W1",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#guardrails",
    "href": "schedule/slides/00-version-control.html#guardrails",
    "title": "UBC Stat406 2025 W1",
    "section": "Guardrails",
    "text": "Guardrails\n\nIn this course, we protect main so that you can’t push there\n\n\n\n\n\n\nWarning\n\n\nIf you try to push to main, it will give an error like\n```{bash}\nremote: error: GH006: Protected branch update failed for refs/heads/main.\n```\nThe fix is: make a new branch, then push that."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "href": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "title": "UBC Stat406 2025 W1",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\nCovers:\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\nCommand line versions (of the same)\n```{bash}\ngit add &lt;name/of/file&gt;\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b &lt;name/of/branch&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "href": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "title": "UBC Stat406 2025 W1",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\n```{bash}\ngit config user.name --global \"Geoff Pleiss\"\ngit config user.email --global \"geoff.pleiss@stat.ubc.ca\"\ngit config core.editor --global nano\n# or emacs or ... (Geoff loves vim and you should too!)\n```\nStaging\n```{bash}\ngit add name/of/file # stage 1 file\ngit add . # stage all\n```\nCommitting\n```{bash}\n# stage/commit simultaneously\ngit commit -am \"message\"\n\n# open editor to write long commit message\ngit commit\n```\nPushing\n```{bash}\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n```\n\n\nBranching\n```{bash}\n# switch to branchname, error if uncommitted changes\ngit checkout branchname\n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\n```\nCheck the status\n```{bash}\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#conflicts",
    "href": "schedule/slides/00-version-control.html#conflicts",
    "title": "UBC Stat406 2025 W1",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\n\nThey look like this:\n\nHere are lines that are either unchanged from\nthe common ancestor, or cleanly resolved\nbecause only one side changed.\n\nBut below we have some troubles\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs:sample.txt\n\nAnd here is another line that is cleanly\nresolved or unmodified.\n\n\nYou get to decide, do you want to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nBut always delete the &lt;&lt;&lt;&lt;&lt;, ======, and &gt;&gt;&gt;&gt;&gt; lines.\nOnce you’re satisfied, committing resolves the conflict."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-other-pointers",
    "href": "schedule/slides/00-version-control.html#some-other-pointers",
    "title": "UBC Stat406 2025 W1",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "href": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "title": "UBC Stat406 2025 W1",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\n\nIn RStudio, select the file and click   then select  Revert…\n\n```{bash}\n# grab the previously committed version\ngit checkout -- README.md\n```\n2. Staged but not committed\n\nIn RStudio, uncheck the box by the file, then use the method above.\n\n```{bash}\n# unstage\ngit reset HEAD README.md\ngit checkout -- README.md\n```\n\n\n3. Committed\n\nNot easy to do in RStudio…\n\n```{bash}\n# check the log to see where you made the chg,\ngit log\n# go one step before that (eg to 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n```\n\n```{bash}\n# alternatively\n# if it happens to also be on another branch\ngit checkout otherbranch -- README.md\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#recovering-from-things",
    "href": "schedule/slides/00-version-control.html#recovering-from-things",
    "title": "UBC Stat406 2025 W1",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main, Tried to Push but got refused\n\n```{bash}\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# undo everything that hasn't been pushed to main\ngit fetch && git reset --hard origin/main\ngit checkout newbranch\n```\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\n```{bash}\ngit checkout main\ngit branch -d badbranch\n```\n\nAnything more complicated, either post to Slack or LMGTFY"
  },
  {
    "objectID": "schedule/slides/00-intro.html#section",
    "href": "schedule/slides/00-intro.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "Methods for Statistical Learning",
    "text": "Methods for Statistical Learning\nStat 406\nGeoff Pleiss\nLast modified – 09 September 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-intro.html#who-am-i",
    "href": "schedule/slides/00-intro.html#who-am-i",
    "title": "UBC Stat406 2025 W1",
    "section": "Who am I?",
    "text": "Who am I?\n\n\nGeoff Pleiss\n\ngeoff.pleiss@stat.ubc.ca\nhttp://geoffpleiss.com/\nAssistant Professor, Department of Statistics\nResearch interests: machine learning\n\nUncertainty quantification\nSequential decision making\nDeep learning\nML for science"
  },
  {
    "objectID": "schedule/slides/00-intro.html#tas",
    "href": "schedule/slides/00-intro.html#tas",
    "title": "UBC Stat406 2025 W1",
    "section": "TAs",
    "text": "TAs\n\n\n\nAtabak Eghbal\n\n\nJunsong Tang\n\n\nParsa Delivary"
  },
  {
    "objectID": "schedule/slides/00-intro.html#who-are-you",
    "href": "schedule/slides/00-intro.html#who-are-you",
    "title": "UBC Stat406 2025 W1",
    "section": "Who are You?",
    "text": "Who are You?\n\nStats major?\nTook STAT 306?\nTook CPSC 340?\nFeel “knowledgable” about ML?\nNeed this course to graduate?"
  },
  {
    "objectID": "schedule/slides/00-intro.html#this-course",
    "href": "schedule/slides/00-intro.html#this-course",
    "title": "UBC Stat406 2025 W1",
    "section": "This Course",
    "text": "This Course\nGoal:\n\nDevelop deep statistical intuitions about prediction and learning\nDraw connections between modelling/learning paradigms\n\nAssumptions:\n\nYou have familiarity with linear models (STAT 306) or ML basics (CPSC 340)\nYou are willing to put in the work!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#differences-from-prior-courses",
    "href": "schedule/slides/00-intro.html#differences-from-prior-courses",
    "title": "UBC Stat406 2025 W1",
    "section": "Differences from Prior Courses",
    "text": "Differences from Prior Courses\n\n\n\nIf You’ve Taken STAT306\n\nRisks analyses\nHigh dimensional learning methods\nNon-linear learning methods\nNon-parametric learning methods\nUnsupervised learning methods\n“Modern” methods (deep learning/ensembles)\n\n\nIf You’ve Taken CPSC340\n\nSurface level: mostly same content\nUnder the hood: more depth/stats\n\nStatistical modelling/model selection\nBias-variance tradeoff\nCurse of dimensionality\nBlack-box computational methods\nGenerative versus discrminative modelling"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nA history lesson\n\n\nEarly AI, Summers, and Winters\n\n1950s: “intelligent machines”, Turing test\n1960s-80s: Early perceptrons, rule-based systems, hype cycles\n1970s-80s: AI winter(s)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-1",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nMeanwhile, in Statistics Land…\nStatisticians are developing frameworks for reasoning, predicting, and making decision from data.\n\n1800s (Legendre and Gauss): predicting form data (least squares)\n1910s (Fisher): estimating unknown parameters (MLE)\n1940s (Shannon): quantifying information in data (information theory)\n1950s (Wald): making decisions from data (decision theory)\n1970s-80s: blackbox computer-based algorithms (bootstrap, MCMC)\n\n\n🧐 Aren’t these the same goals that the AI community has? 🧐"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-2",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-2",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\nStatistians Guide AI\n\n1980s (Vapnik, Chervonenkis): statistical learning theory\n\nStudy learning general knowledge from data\nMathematical formalism for when learning is possible\n\n1990s-2000s: new algorithms based on stats\n\n\nSupport vector machines\nBagging/random forests\nTopic modelling\n\nApplications in the new internet economy (search, ads, product recommendations)\nAI \\(\\rightarrow\\) machine learning"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-3",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-3",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\nStatisticians Play Catch-Up\n\n2010s: deep learning revolution\n\nStats: ideas as “unprincipled”\nCS: see lots of empirical success\n\n2020s: generative AI\n\nBreakthroughs come from scale (more data, more compute)\nBig gap between theory and practice\nCore ideas are stat. learning principles"
  },
  {
    "objectID": "schedule/slides/00-intro.html#course-learning-outcomes",
    "href": "schedule/slides/00-intro.html#course-learning-outcomes",
    "title": "UBC Stat406 2025 W1",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\n\nFormulate and analyze machine learning algorithms from a statistical perspective\nCategorize learning methods through multiple criteria\nDraw connections between any two learning algorithms and paradigms\nReason about which methods are most appropriate for a given situation\nApply these methods correctly, troubleshoot common pitfalls, and identify probable steps for improving\nImplement and utilize models in R using best coding practices"
  },
  {
    "objectID": "schedule/slides/00-intro.html#modules",
    "href": "schedule/slides/00-intro.html#modules",
    "title": "UBC Stat406 2025 W1",
    "section": "6 Modules",
    "text": "6 Modules\nEach module has a technical content theme (and a statistical principles theme)\n\nBasic regression/classification (statistical modelling/selection)\nLinear methods, regularization, featurization (bias-variance tradeoff)\nNonparametric methods (curse of dimensionality)\nUnsupervised learning (generative vs. discriminative modelling)\nEnsembles (black-box methods)\nDeep learning"
  },
  {
    "objectID": "schedule/slides/00-intro.html#course-components",
    "href": "schedule/slides/00-intro.html#course-components",
    "title": "UBC Stat406 2025 W1",
    "section": "Course Components",
    "text": "Course Components\n\nIn-Class (discussion, mathematical derivations)\nLabs (coding, tools, how-tos)\nHomework Assignments (synthesis, debugging, analysis)\nFinal Exam (all of the above)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#grading-structure",
    "href": "schedule/slides/00-intro.html#grading-structure",
    "title": "UBC Stat406 2025 W1",
    "section": "Grading Structure",
    "text": "Grading Structure\n\n\nKnowledge Based (40 points)\n\n\n\nComponent\nPoints\n\n\n\n\nMidterm\n10 points\n\n\nFinal\n30 points\n\n\nTotal\n40 points\n\n\n\n\nEffort Based (60 points)\n\n\n\nComponent\nPoints\n\n\n\n\nIn-Class\n15 points\n\n\nLabs\n20 points\n\n\nHomework\n40 points\n\n\nTotal\n60 points"
  },
  {
    "objectID": "schedule/slides/00-intro.html#section-1",
    "href": "schedule/slides/00-intro.html#section-1",
    "title": "UBC Stat406 2025 W1",
    "section": "🧐 15 + 20 + 40 > 60??? 🧐",
    "text": "🧐 15 + 20 + 40 &gt; 60??? 🧐\n\n\n\n\n\nComponent\nPoints\n\n\n\n\nIn-Class\n15 points\n\n\nLabs\n20 points\n\n\nHomework\n40 points\n\n\nTotal\n60 points\n\n\n\n\n\n\n\n\\[\\text{effort} = \\min\\left\\{60, \\text{class} + \\text{lab} + \\text{hw}\\right\\}\\]\nGet 60 of the 75 total points any way your want\n\nSkip class + perfect on labs/homework = 60 points\nMiss 1 homework + get in-class points = 60 points\n(More on this later)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#class-structure",
    "href": "schedule/slides/00-intro.html#class-structure",
    "title": "UBC Stat406 2025 W1",
    "section": "Class Structure",
    "text": "Class Structure\n\nLecture: I’ll go over content\nDerivations: You’ll do math in groups\nDiscussions: We’ll analyze content together\nClickers: You’ll apply your knowledge (in pairs)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n8am is hard.\nAttendance is optional."
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-skip-class",
    "href": "schedule/slides/00-intro.html#reasons-to-skip-class",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Skip Class",
    "text": "Reasons to Skip Class\n\nIf you’re sleep deprived\nGetting more sleep may be beneficial to your learning in the long run\nIf you’re not going to engage/participate\nLectures are recorded, watch on your own time\nIf you’re going to have your laptop open\nDo your homework/social media browsing/k-pop video watching in the comfort of your own home; then revisit the course material on your own time"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n1. Time to Think/Digest/Struggle with the Material\n\nDeveloping deep intuitions takes time/effort.\nClass gives you 3 hours/week to chew on the material.\nI’ll pace the material accordingly"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-1",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n2. In-Class is Where We Wrestle with Math\n\nThere is lots of math in the course\nWe will spend time in class working through derivations together\nWorking through derivations builds intuitions and deep knowledge\nFew opportunities on labs/homeworks for this kind of work"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-2",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-2",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n3. Questions and Discussions\n\nThere’s plenty of online materials that teach you this content (books, videos, etc)\nThere’s few opportunities to discuss content with peers/experts\nIf you’re graduating this year, this course may be one of your last opportunities for this type of learning!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-3",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-3",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n4. Effort-Based Points\n\nIf you skip class, you’ll need to get a perfect grade on labs/homeworks to get a maximum effort-based grade\nIf you attend class, you can skip a homework, mess up on all of them, and still get a perfect grade!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-4",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-4",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n5. Recommendation Letters\n\n\nWhat I Write for Students who Attend/Participate/Ask Questions \n\nWhat I Write for Students who Just “Show Up”\nI don’t get to know you, so all I can talk about is your grade."
  },
  {
    "objectID": "schedule/slides/00-intro.html#tldr",
    "href": "schedule/slides/00-intro.html#tldr",
    "title": "UBC Stat406 2025 W1",
    "section": "TLDR",
    "text": "TLDR\n\nWaking up early, avoiding distractions, and participating is hard\nIf you don’t have the energy or mental bandwidth on any given day, give yourself the gift of staying home.\nIf you come in, be prepared to engage and put in effort.\nYou’ll get a lot out of attending/engaging. I’ll make sure of it!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#earning-points",
    "href": "schedule/slides/00-intro.html#earning-points",
    "title": "UBC Stat406 2025 W1",
    "section": "Earning Points",
    "text": "Earning Points\n\n\nParticipation (5 points)\n\nWe will (try to) use Agora for virtual “hand-raising”\n1 point for “raising” your hand\n1 point if you are called on (assigned randomly by Agora)\nGrade: \\(\\min\\left\\{ 5, 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{classes\\_w/\\_agora}} \\right\\}\\)\n\n\nClickers (10 points)\n\nWe will use iClicker for questions that are similar to the midterm/final\n0 points for skipping, 2 points for trying, 4 points for correct\n\nAverage of 3 = 10 points (the max)\nAverage of 2 = 5 points\nAverage of 1 = 0 points\n\nGrade: \\(\\max\\left\\{ 0, \\min\\left\\{ 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{questions}} - 5, 10 \\right\\} \\right\\}\\)\nBe sure to sync your device in Canvas."
  },
  {
    "objectID": "schedule/slides/00-intro.html#mechanics-and-grading",
    "href": "schedule/slides/00-intro.html#mechanics-and-grading",
    "title": "UBC Stat406 2025 W1",
    "section": "Mechanics and Grading",
    "text": "Mechanics and Grading\nThe goal is to “Do the work”\n\n\nLabs\n\nLabs should give you practice, allow for questions with the TAs.\nThey are due at 2300 on Friday, lightly graded.\nYou may do them at home, but you must submit individually (in lab, you may share submission)\nLabs are lightly graded\n\n\nAssignments\n\nNot easy, especially the first 2, especially if you are unfamiliar with R / Rmarkdown / ggplot\nYou may revise to raise your score to 7/10, see Syllabus. Only if you lose 3+ for content (penalties can’t be redeemed).\nDon’t leave these for the last minute"
  },
  {
    "objectID": "schedule/slides/00-intro.html#tools-r-and-github",
    "href": "schedule/slides/00-intro.html#tools-r-and-github",
    "title": "UBC Stat406 2025 W1",
    "section": "Tools: R and Github",
    "text": "Tools: R and Github\n\n\n\nLanguage/Libraries: R + Tidyverse\nSubmission: via Github\n\n\n\n\n\n\n\n\nImportant\n\n\nWe assume you’re familiar with these tools (R, Tidyverse, Git, Github)\nIf you’re not, it’s your responsibility to get up-to-speed with them.\nSee Canvas for tutorials / the website for resources.\n\n\n\n\n\nWorkflow\n\nYou each have your own repo\nYou make a branch\nDO NOT rename files\nMake enough commits (3 for labs, 5 for HW).\nPush your changes (at anytime) and make a PR against main when done.\nTAs review your work.\nIf you want to revise HWs, make changes in response to feedback and push to the same branch. Then “re-request review”."
  },
  {
    "objectID": "schedule/slides/00-intro.html#generative-ai-policy",
    "href": "schedule/slides/00-intro.html#generative-ai-policy",
    "title": "UBC Stat406 2025 W1",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\n\nAI tools are quite capable with this material (I use them frequently in my own work!)\nStrong recommendation: Use AI as little as possible to maximize learning\nIf you use AI, use it to supplement your thinking (code completion, rewriting) not replace your thinking (feeding entire assignments to chatbots)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#why-minimize-ai-use",
    "href": "schedule/slides/00-intro.html#why-minimize-ai-use",
    "title": "UBC Stat406 2025 W1",
    "section": "Why Minimize AI Use?",
    "text": "Why Minimize AI Use?\n\nDeep intuitions require struggle\nThe only way to develop intuitions about challenging material is to wrestle with content\nStand out from the crowd\nAnyone can use Claude/Copilot for simple ML. Demonstrate thinking beyond these tools will make you more hireable/trusted\nCourse-specific subtleties\nEven with good prompting, chatbots likely won’t score above 7-8 on assignments\nNo AI on exams\nDon’t become too dependent on tools you can’t use during midterm/final"
  },
  {
    "objectID": "schedule/slides/00-intro.html#if-you-use-generative-ai",
    "href": "schedule/slides/00-intro.html#if-you-use-generative-ai",
    "title": "UBC Stat406 2025 W1",
    "section": "If you Use Generative AI…",
    "text": "If you Use Generative AI…\n\nSelf-reporting required:\n\nDescribe your AI usage on all assignments and labs.\nInclude all prompts you use.\nPlease be honest (it’ll help me improve the course/your learning experience)\nNo grade penalty"
  },
  {
    "objectID": "schedule/slides/00-intro.html#late-policy",
    "href": "schedule/slides/00-intro.html#late-policy",
    "title": "UBC Stat406 2025 W1",
    "section": "Late policy",
    "text": "Late policy\nIf you have not submitted your lab/assignment by the time grading starts, you will get a 0.\n\n\n\n\n\n\n\n\n\nWhen you submit\nLikelihood that your submission gets a 0\n\n\n\n\nBefore 11pm on due date (i.e. on time)\n0%\n\n\n11:01pm on due date\n0.01%\n\n\n9am after due date\n50%\n\n\n2 weeks after due date\n99.99999999%\n\n\n\n\n\n\nException: when you have grounds for academic consession. (See the UBC policy.)\n\n\n\n\n\n\nTip\n\n\nRemember: you can still get a “perfect” effort grade even if you get a 0 on one assignment."
  },
  {
    "objectID": "schedule/slides/00-intro.html#textbooks",
    "href": "schedule/slides/00-intro.html#textbooks",
    "title": "UBC Stat406 2025 W1",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\nJames, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: http://statlearning.com/\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning\n\n\nHastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\n\nEach class has a “required” reading from Introduction to Statistical Learning\nNo quizzes/knowledge demonstration/accountability for not reading, but please do them!\nReading before class will improve your preparation/ability to engage"
  },
  {
    "objectID": "schedule/slides/00-intro.html#exams",
    "href": "schedule/slides/00-intro.html#exams",
    "title": "UBC Stat406 2025 W1",
    "section": "Exams",
    "text": "Exams\n\n\nMidterm Exam\n\nIn-class, October 23\nIt is hard\nComputer-based\nT/F, multiple choice, etc.\n\n\nFinal Exam\n\nScheduled by the university.\nIt is hard\nThe median last year was 50% \\(\\Rightarrow\\) A-\nFormat: TBD\n\n\n\n\nPhilosophy\n\nIf you put in the effort, you’re guaranteed a C+.\nTo get an A+, you need to deeply understand the material.\nNo penalty for skipping the midterm/final."
  },
  {
    "objectID": "schedule/slides/00-intro.html#time-expectations-per-week",
    "href": "schedule/slides/00-intro.html#time-expectations-per-week",
    "title": "UBC Stat406 2025 W1",
    "section": "Time expectations per week:",
    "text": "Time expectations per week:\n\nComing to class – 3 hours\nReading the book – 1 hour\nLabs – 1 hour\nHomework – 4 hours\nStudy / thinking / playing – 1 hour"
  },
  {
    "objectID": "schedule/slides/00-intro.html#computer",
    "href": "schedule/slides/00-intro.html#computer",
    "title": "UBC Stat406 2025 W1",
    "section": "Computer",
    "text": "Computer\n\n\n\n\n\nWe will use R and we assume some background knowledge.\nSuggest you use RStudio IDE\nSee https://ubc-stat.github.io/stat-406/ for what you need to install for the whole term.\nLinks to useful supplementary resources are available on the website."
  },
  {
    "objectID": "schedule/slides/00-intro.html#other-resources",
    "href": "schedule/slides/00-intro.html#other-resources",
    "title": "UBC Stat406 2025 W1",
    "section": "Other Resources",
    "text": "Other Resources\n\nCourse website\nAll the material (slides, extra worksheets) https://ubc-stat.github.io/stat-406\nCanvas (minimal)\nQuiz 0, grades, course time/location info, links to videos from class\nSlack\nDiscussion board, questions\nGithub\nHomework / Lab submission"
  },
  {
    "objectID": "schedule/slides/00-intro.html#final-words-of-wisdom",
    "href": "schedule/slides/00-intro.html#final-words-of-wisdom",
    "title": "UBC Stat406 2025 W1",
    "section": "Final Words of Wisdom",
    "text": "Final Words of Wisdom\n\n\n8am is hard. But you can do it!\n\nI strongly urge you to get up at the same time everyday.\n\n\n\n\n\nIf you need help, please ask!\n\nI’m here to encourage you, get you un-stuck, and ponder ML mysteries with you\n\n\n\n\n\nThink of this course as preparation for a race/performance/etc.\n\nYou have to work out/practice, and there’s no shortcuts.\nTraining is hard, but you’ll be pleased with the outcome if you put in the work.\nWe’re here to help you stay excited and motivated on your journey!\n\n\n\n\n\nJoin me on a fun exploration of cool material!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#todos",
    "href": "schedule/slides/00-intro.html#todos",
    "title": "UBC Stat406 2025 W1",
    "section": "TODOs",
    "text": "TODOs\n\n\nBy EOD Tomorrow:\n\nRead the syllabus\nInstall the R package, read docs, check your LaTeX installation\nBE SURE to follow the Computer Setup instructions on the website!\nTake Quiz 00 on Canvas\n\n\nBefore Next Tuesday:\n\nSign up for Slack (see Canvas)\nSign up for Agora (details on Canvas)\nLink your iClicker account with Canvas\n(If needed) watch Github and R tutorials\n\n\n\n\nAgora Signup"
  },
  {
    "objectID": "course-setup.html",
    "href": "course-setup.html",
    "title": "Guide for setting up the course infrastructure",
    "section": "",
    "text": "Version 2025\nThis guide (hopefully) gives enough instructions for recreating new iterations of Stat 406."
  },
  {
    "objectID": "course-setup.html#create-a-github.com-organization",
    "href": "course-setup.html#create-a-github.com-organization",
    "title": "Guide for setting up the course infrastructure",
    "section": "Create a GitHub.com organization",
    "text": "Create a GitHub.com organization\n\nThis is free for faculty with instructor credentials.\n\nNote make sure you upgrade the organization to a “Github Team.” If you have registered your instructor credentials with Github, you should be able to upgrade for free from the Github Global Campus page under “Upgrade your academic organizations.”\n\nAllows more comprehensive GitHub actions, PR templates and CODEOWNER behaviour than the UBC Enterprise version (last I checked)\nDownside is getting students added (though we include R scripts for this)\n\nOnce done, go to https://github.com/watching. Click the Red Down arrow “Unwatch all”. Then select this Org. The TAs should do the same.\n\nPermissions and structure\nSettings &gt; Member Privileges\nWe list only the important ones.\n\nBase Permissions: No Permission\nRepository creation: None\nRepo forking: None\nPages creation: None\nTeam creation rules: No\n\nBe sure to click save in each area after making changes.\nSettings &gt; Actions &gt; General\nAll repositories: Allow all actions and reusable workflows.\nWorkflow permissions: Read and write permissions.\n\n\nTeams\n\n2 teams, one for the TAs and one for the students\nYou must then manually add the teams to any repos they should access\n\nI generally give the TAs “Write” permission, and the students “Read” permission with some exceptions. See the Repos section below."
  },
  {
    "objectID": "course-setup.html#repos",
    "href": "course-setup.html#repos",
    "title": "Guide for setting up the course infrastructure",
    "section": "Repos",
    "text": "Repos\nThere are typically about 10 repositories. Homeworks and Labs each have 3 with very similar behaviours.\nBe careful copying directories. All of them have hidden files and folders, e.g. .git. Of particular importance are the .github directories which contain PR templates and GitHub Actions. Also relevant are the .Rprofile files which try to override Student Language settings and avoid unprintible markdown characters.\n\nHomeworks\n\nhomework-solutions\nThis is where most of the work happens. My practice is to create the homework solutions first. I edit these (before school starts) until I’m happy. I then duplicate the file and remove the answers. The result is hwxx-instructions.Rmd. The .gitignore file should ignore all of the solutions and commit only the instructions. Then, about 1 week after the deadline, I adjust the .gitignore and push the solution files.\n\nStudents have Read permission.\nTAs have Write permission.\nThe preamble.tex file is common to HWs and Labs. It creates a lavender box where the solution will go. This makes life easy for the TAs.\n\n\n\nhomework-template\nThis is a “template repo” used for creating student specific homework-studentgh repos (using the setup scripts).\nVery Important: copy the hwxx-instructions files over to a new directory. Do NOT copy the directory or you’ll end up with the solutions visible to the students.\nThen rename hwxx-instructions.Rmd to hwxx.Rmd. Now the students have a .pdf with instructions, and a template .Rmd to work on.\nOther important tasks:\n\nThe .gitignore is more elaborate in an attempt to avoid students pushing junk into these repos.\nThe .github directory contains 3 files:\n\nCODEOWNERS begins as an empty doc which will be populated with the assigned grader later;\npull_request_template.md is used for all HW submission PRs;\nworkflows contains a GH-action to comment on the PR with the date+time when the PR is opened.\n\nUnder Settings &gt; General, select “Template repository”. This makes it easier to duplicate to the student repos.\nSetup branch protection rules for the main branch. Create a new ruleset for default branches, and select the following:\n\nRequire a pull request before merging\nRequire review from Code Owners\nBlock force pushes\nI recommend adding the @TAs team to the bypass list.\n\n\n\n\n\nLabs\nThe three Labs repos operate exactly as the analogous homework repos.\n\nlabs-solutions\nDo any edits here before class begins.\n\n\nlabs-template\nSame as with the homeworks\n\n\n\nclicker-solutions\nThis contains the complete set of clicker questions.\nAnswers are hidden in comments on the presentation.\nI release them incrementally after each module (copying over from my clicker deck).\n\n\npractice-final\nContains a lengthy practice exam as well as solutions. I usually only post this during the last week of class.\n\n\nopen-pr-log\nThis contains a some GitHub actions to automatically keep track of open PRs for the TAs.\nIt’s still in testing phase, but should work properly. It will create two markdown docs, 1 for labs and 1 for homework. Each shows the assigned TA, the date the PR was opened, and a link to the PR. If everything is configured properly, it should run automatically at 3am every night.\n\nOnly the TAs should have access.\nUnder Settings &gt; Secrets and Variables &gt; Actions you must add a “Repository Secret”. This should be a GitHub Personal Access Token created in your account (Settings &gt; Developer settings &gt; Tokens (classic)). It needs Repo, Workflow, and Admin:Org permissions. I set it to expire at the end of the course. I use it only for this purpose (rather than my other tokens for typical logins).\n\n\n\n.github / .github-private\nThese contain README files that give some basic information about the available repos and the course.\nIt’s visible Publically, and appears on the Org homepage for all to see. The .github-private has the same function, but applies only to Org members.\n\n\nbakeoff-bakeoff\nThis is for the bonus for HW4. Both TAs and Students have access. I put the TA team as CODEOWNERS and protect the main branch (Settings &gt; Branches &gt; Branch Protection Rules). Here, we “Require approvals” and “Require Review from Code Owners”."
  },
  {
    "objectID": "computing/windows.html",
    "href": "computing/windows.html",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#installation-notes",
    "href": "computing/windows.html#installation-notes",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#terminal",
    "href": "computing/windows.html#terminal",
    "title": " Windows",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works."
  },
  {
    "objectID": "computing/windows.html#github",
    "href": "computing/windows.html#github",
    "title": " Windows",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/windows.html#git-bash-and-windows-terminal",
    "href": "computing/windows.html#git-bash-and-windows-terminal",
    "title": " Windows",
    "section": "Git, Bash, and Windows Terminal",
    "text": "Git, Bash, and Windows Terminal\nAlthough these three are separate programs, we are including them in the same section here since they are packaged together in the same installer on Windows. Briefly, we will be using the Bash shell to interact with our computers via a command line interface, Git to keep a version history of our files and upload to/download from to GitHub, and Windows Terminal to run the both Bash and Git.\nGo to https://git-scm.com/download/win and download the windows version of git. After the download has finished, run the installer and accept the default configuration for all pages except for the following:\n\nOn the Select Components page, add a Git Bash profile to Windows Terminal.\n\n\nTo install windows terminal visit this link and click Get to open it in Windows Store. Inside the Store, click Get again and then click Install. After installation, click Launch to start Windows Terminal. In the top of the window, you will see the tab bar with one open tab, a plus sign, and a down arrow. Click the down arrow and select Settings (or type the shortcut Ctrl + ,). In the Startup section, click the dropdown menu under Default profile and select Git Bash.\n\nYou can now launch the Windows terminal from the start menu or pin it to the taskbar like any other program (you can read the rest of the article linked above for additional tips if you wish). To make sure everything worked, close down Windows Terminal, and open it again. Git Bash should open by default, the text should be green and purple, and the tab should read MINGW64:/c/Users/$USERNAME (you should also see /c/Users/$USERNAME if you type pwd into the terminal). This screenshot shows what it should look like:\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we refer to “the terminal” in these installation instructions, we want you to use the Windows Terminal that you just installed with the Git Bash profile. Do not use Windows PowerShell, CMD, or anything else unless explicitly instructed to do so.\n\n\nTo open a new tab you can click the plus sign or use Ctrl + Shift + t (you can close a tab with Ctrl + Shift + w). To copy text from the terminal, you can highlight it with the mouse and then click Ctrl + Shift + c. To paste text you use Ctrl + Shift + v, try it by pasting the following into the terminal to check which version of Bash you just installed:\nbash --version\nThe output should look similar to this:\nGNU bash, version 4.4.23(1)-release (x86_64-pc-sys)\nCopyright (C) 2019 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nNote\n\n\n\nIf there is a newline (the enter character) in the clipboard when you are pasting into the terminal, you will be asked if you are sure you want to paste since this newline will act as if you pressed enter and run the command. As a guideline you can press Paste anyway unless you are sure you don’t want this to happen.\n\n\nLet’s also check which version of git was installed:\ngit --version\ngit version 2.32.0.windows.2\n\n\n\n\n\n\nNote\n\n\n\nSome of the Git commands we will use are only available since Git 2.23, so make sure your if your Git is at least this version.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/windows.html#latex",
    "href": "computing/windows.html#latex",
    "title": " Windows",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nNote that you might see two error messages regarding lua during the installation, you can safely ignore these, the installation will complete successfully after clicking “OK”.\nIf it still fails, proceed with the instructions\n\nIn RStudio, run the following commands to install the tinytex package and setup tinytex:\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nIn order for Git Bash to be able to find the location of TinyTex, you will need to sign out of Windows and back in again. After doing that, you can check that the installation worked by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2021/W32TeX)\nkpathsea version 6.3.3\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/windows.html#github-pat",
    "href": "computing/windows.html#github-pat",
    "title": " Windows",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/windows.html#post-installation-notes",
    "href": "computing/windows.html#post-installation-notes",
    "title": " Windows",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/windows.html#attributions",
    "href": "computing/windows.html#attributions",
    "title": " Windows",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/mac_x86.html",
    "href": "computing/mac_x86.html",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#installation-notes",
    "href": "computing/mac_x86.html#installation-notes",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#terminal",
    "href": "computing/mac_x86.html#terminal",
    "title": " MacOS x86",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_x86.html#github",
    "href": "computing/mac_x86.html#github",
    "title": " MacOS x86",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_x86.html#git",
    "href": "computing/mac_x86.html#git",
    "title": " MacOS x86",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_x86.html#latex",
    "href": "computing/mac_x86.html#latex",
    "title": " MacOS x86",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_x86.html#github-pat",
    "href": "computing/mac_x86.html#github-pat",
    "title": " MacOS x86",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_x86.html#post-installation-notes",
    "href": "computing/mac_x86.html#post-installation-notes",
    "title": " MacOS x86",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_x86.html#attributions",
    "href": "computing/mac_x86.html#attributions",
    "title": " MacOS x86",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/index.html",
    "href": "computing/index.html",
    "title": " Computing",
    "section": "",
    "text": "In order to participate in this class, we will require the use of R, and encourage the use of RStudio. Both are free, and you likely already have both.\nYou also need Git, Github and Slack.\nBelow are instructions for installation. These are edited and simplified from the DSCI 310 Setup Instructions. If you took DSCI 310 last year, you may be good to go, with the exception of the R package."
  },
  {
    "objectID": "computing/index.html#laptop-requirements",
    "href": "computing/index.html#laptop-requirements",
    "title": " Computing",
    "section": "Laptop requirements",
    "text": "Laptop requirements\n\nRuns one of the following operating systems: Ubuntu 20.04, macOS (version 11.4.x or higher), Windows 10 (version 2004, 20H2, 21H1 or higher).\n\nWhen installing Ubuntu, checking the box “Install third party…” will (among other things) install proprietary drivers, which can be helpful for wifi and graphics cards.\n\nCan connect to networks via a wireless connection for on campus work\nHas at least 30 GB disk space available\nHas at least 4 GB of RAM\nUses a 64-bit CPU\nIs at most 6 years old (4 years old or newer is recommended)\nUses English as the default language. Using other languages is possible, but we have found that it often causes problems in the homework. We’ve done our best to fix them, but we may ask you to change it if you are having trouble.\nStudent user has full administrative access to the computer."
  },
  {
    "objectID": "computing/index.html#software-installation-instructions",
    "href": "computing/index.html#software-installation-instructions",
    "title": " Computing",
    "section": "Software installation instructions",
    "text": "Software installation instructions\nPlease click the appropriate link below to view the installation instructions for your operating system:\n\nmacOS x86 or macOS arm\nUbuntu\nWindows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 406",
    "section": "",
    "text": "Jump to Schedule\n\n\nSyllabus"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Working with Git in RStudio involves several key steps:\n\nEnsure you are on the main branch. Pull in remote changes by clicking the down arrow .\nCreate a new branch by clicking the branch icon .\nWork on your documents and save frequently.\nStage your changes by checking the appropriate boxes.\nCommit your changes by clicking Commit.\nRepeat steps 3-5 as needed.\nPush to GitHub by clicking the up arrow .\nOpen a pull request (PR) on GitHub.\nUse the dropdown menu to return to main to avoid potential issues.\n\n\n\n\nIf you prefer using the command line, follow these steps:\n\nPull remote changes (optional but recommended): git pull\nCreate a new branch: git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes:\n\nFor specific documents: git add &lt;name-of-document1&gt;\nFor all changed documents: git add .\n\nCommit your changes with a meaningful message: git commit -m \"Descriptive commit message\"\nRepeat steps 3-5 as needed.\nPush to GitHub: git push (follow any suggested command variations)\nOpen a pull request on GitHub.\nSwitch back to main: git checkout main"
  },
  {
    "objectID": "faq.html#course-workflow-homework-and-labs",
    "href": "faq.html#course-workflow-homework-and-labs",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Working with Git in RStudio involves several key steps:\n\nEnsure you are on the main branch. Pull in remote changes by clicking the down arrow .\nCreate a new branch by clicking the branch icon .\nWork on your documents and save frequently.\nStage your changes by checking the appropriate boxes.\nCommit your changes by clicking Commit.\nRepeat steps 3-5 as needed.\nPush to GitHub by clicking the up arrow .\nOpen a pull request (PR) on GitHub.\nUse the dropdown menu to return to main to avoid potential issues.\n\n\n\n\nIf you prefer using the command line, follow these steps:\n\nPull remote changes (optional but recommended): git pull\nCreate a new branch: git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes:\n\nFor specific documents: git add &lt;name-of-document1&gt;\nFor all changed documents: git add .\n\nCommit your changes with a meaningful message: git commit -m \"Descriptive commit message\"\nRepeat steps 3-5 as needed.\nPush to GitHub: git push (follow any suggested command variations)\nOpen a pull request on GitHub.\nSwitch back to main: git checkout main"
  },
  {
    "objectID": "faq.html#homework-regrade-procedure",
    "href": "faq.html#homework-regrade-procedure",
    "title": " Frequently asked questions",
    "section": "Homework Regrade Procedure",
    "text": "Homework Regrade Procedure\n\n\n\n\n\n\nTipRegrade Eligibility\n\n\n\n\nDeductions must exceed 3 points and be related to content (not penalties).\nErrors can be corrected to potentially raise the grade to 7/10.\nRevisions and review requests must be submitted within one week of the initial review.\n\n\n\nRegrade process:\n\nLocate the local branch for the specific homework assignment.\n\nCheck the “Pull Requests” tab in your GitHub repository if you can’t recall the branch name.\n\nMake necessary corrections to the files.\nCommit and push changes, ensuring the PDF is re-rendered if needed.\nFind the original PR for the assignment on GitHub.\nAdd a concise, clear comment to the TA describing your changes.\nClick the recycling (🔁) button under “Reviewers” to request a review."
  },
  {
    "objectID": "faq.html#common-git-and-workflow-issues",
    "href": "faq.html#common-git-and-workflow-issues",
    "title": " Frequently asked questions",
    "section": "Common Git and Workflow Issues",
    "text": "Common Git and Workflow Issues\n\nmaster/main\n“master” has some pretty painful connotations. So as part of an effort to remove racist names from code, the default branch is now “main” on new versions of GitHub. But old versions (like the UBC version) still have “master”. Below, I’ll use “main”, but if you see “master” on what you’re doing, that’s the one to use.\n\n\nStart from main\nBranches should be created from the main branch, not the one you used for the last assignment.\ngit checkout main\nThis switches to main. Then pull and start the new assignment following the workflow above. (In Rstudio, use the dropdown menu.)\n\n\nYou forgot to work on a new branch\nUgh, you did some labs before realizing you forgot to create a new branch. Don’t stress. There are some things below to try. But if you’re confused ASK. We’ve had practice with this, and soon you will too!\n(1) If you started from main and haven’t made any commits (but you SAVED!!):\ngit branch -b &lt;new-branch-name&gt;\nThis keeps everything you have and puts you on a new branch. No problem. Commit and proceed as usual.\n(2) If you are on main and made some commits:\ngit branch &lt;new-branch-name&gt;\ngit log\nThe first line makes a new branch with all the stuff you’ve done. Then we look at the log. Locate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit reset ac2a8 --hard\nThis rolls main back to that commit. You don’t need the whole string, just the first few characters. Finally\ngit checkout &lt;new-branch-name&gt;\nand continue working.\n(3) If you started work on &lt;some-old-branch&gt; for work you already submitted: This one is harder, and I would suggest getting in touch with the TAs. Here’s the procedure.\ngit commit -am \"uhoh, I need to be on a different branch\"\ngit branch &lt;new-branch-name&gt;\nCommit your work with a dumb message, then create a new branch. It’s got all your stuff.\ngit log\nLocate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit rebase --onto main ac2a8 &lt;new-branch-name&gt;\ngit checkout &lt;new-branch-name&gt;\nThis makes the new branch look like main but without the differences from main that are on ac2a8 and WITH all the work you did after ac2a8. It’s pretty cool. And should work. Finally, we switch to our new branch."
  },
  {
    "objectID": "faq.html#improving-your-r-programming-skills",
    "href": "faq.html#improving-your-r-programming-skills",
    "title": " Frequently asked questions",
    "section": "Improving Your R Programming Skills",
    "text": "Improving Your R Programming Skills\n\nLearning Approach\nLearning to code is an active, immersive process. Simply reading books or watching videos is insufficient. To truly learn R:\n\nComplete tutorials multiple times\nExplore textbook code thoroughly\nQuestion the rationale behind function choices\nExperiment with different coding approaches\nBreak down and understand each line of code\n\n\n\nRecommended Learning Resources\n\nData Science: A First Introduction\nR for Data Science\nDSCI 310 Course Notes\nHappy Git with R\nModern Dive: Statistical Inference via Data Science\nStat545"
  },
  {
    "objectID": "faq.html#debugging-code",
    "href": "faq.html#debugging-code",
    "title": " Frequently asked questions",
    "section": "Debugging Code",
    "text": "Debugging Code\n\nGeneral Debugging Workflow\nWhen your code doesn’t run:\n\nIf the code runs but doesn’t produce expected results, see the code quality section.\nRead the Error Message\n\nError messages provide crucial debugging hints\nParsing them can be challenging but is a valuable skill\n\nExample:\n\nset.seed(12345)\ny &lt;- rnorm(10)\nx &lt;- matrix(rnorm(20), 2)\nlinmod &lt;- lm(y ~ x)\n## Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE): variable lengths differ (found for 'x')\n\nNotice the error about variable lengths and matrix dimensions.\nConsult Documentation\n\nUse function-specific help (e.g., ?matrix)\n\nSearch Online\n\nCopy error messages into search engines\nRemove specific, identifying information\n\nSeek Peer Help\n\nUse class Slack channels\nPrepare a minimal working example (MWE)\n\nInstructor/TA Consultation\n\nBe prepared to show your code\nProvide a reproducible example\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen seeking help, always be ready to share your code or MWE.\n\n\nNote: If the error cannot be reproduced, it is unlikely that anyone can help you effectively."
  },
  {
    "objectID": "faq.html#crafting-minimal-working-examples-mwes",
    "href": "faq.html#crafting-minimal-working-examples-mwes",
    "title": " Frequently asked questions",
    "section": "Crafting Minimal Working Examples (MWEs)",
    "text": "Crafting Minimal Working Examples (MWEs)\nAn MWE is a compact code snippet that:\n\nReproduces an error on any machine\nIsolates the specific problem\nMinimizes external dependencies\n\nBenefits of creating MWEs:\n\nOften helps you solve the problem independently\nReveals the root cause of issues\nMakes it easier for others to help you\n\n\nMWE Tips\n\nSet random seeds for reproducibility\nUse minimal, generic data\nInclude only essential code\n\nPreparing an MWE is a valuable debugging skill. By stripping your problem down to its bare essence, you often uncover the root issue. For instance, the previous debugging example was an MWE: it used a fixed seed, ensured data reproducibility, and focused on a specific error.\nFor further guidance, consult:\n\nR Overview Slides\nStack Exchange Discussion on MWEs"
  },
  {
    "objectID": "faq.html#writing-high-quality-code",
    "href": "faq.html#writing-high-quality-code",
    "title": " Frequently asked questions",
    "section": "Writing High-Quality Code",
    "text": "Writing High-Quality Code\nThis is covered in much greater detail in the lectures. Here are key principles for writing clean, efficient R code:\n\nUse Script Files\n\nSave and source scripts\nAvoid console-only work\nTreat R as a scripting language, not a calculator\n\nAvoid Code Repetition\n\nNever copy and paste code\nDefine constants at the script’s beginning\nCreate reusable functions\n\nFunction Design\n\nFunctions are easily testable\nVerify inputs and outputs\nCatch potential errors through comprehensive testing\n\nError Types\n\nSyntax errors (detectable by R)\n\nMissing parentheses\nIncorrect arguments\n\nLogical errors (require thorough testing)\n\nRequire manual verification of results\n\n\nAvoid Magic Numbers\n\nAlways define constants\nMake numerical values meaningful and clear\n\nUse Meaningful Names\nBad example:\ndata(\"ChickWeight\")\nout &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\nGood example:\ndata(\"ChickWeight\")\nchick_weight_model &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\nComment Strategically\n\nExplain code that isn’t immediately clear\nFocus on the “why”, not just the “what”\n\nExample of helpful commenting:\n# Calculate weighted average of chick weights, squared and adjusted\nchick_weight_summary &lt;- with(\n  ChickWeight,\n  by(weight, Chick, function(x) (x^2 + 23) / length(x))\n)\n\nRemember: Clear, readable code is as much about communication as it is about functionality."
  },
  {
    "objectID": "computing/mac_arm.html",
    "href": "computing/mac_arm.html",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#installation-notes",
    "href": "computing/mac_arm.html#installation-notes",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#terminal",
    "href": "computing/mac_arm.html#terminal",
    "title": " MacOS ARM",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_arm.html#github",
    "href": "computing/mac_arm.html#github",
    "title": " MacOS ARM",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_arm.html#git",
    "href": "computing/mac_arm.html#git",
    "title": " MacOS ARM",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_arm.html#latex",
    "href": "computing/mac_arm.html#latex",
    "title": " MacOS ARM",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_arm.html#github-pat",
    "href": "computing/mac_arm.html#github-pat",
    "title": " MacOS ARM",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_arm.html#post-installation-notes",
    "href": "computing/mac_arm.html#post-installation-notes",
    "title": " MacOS ARM",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_arm.html#attributions",
    "href": "computing/mac_arm.html#attributions",
    "title": " MacOS ARM",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/ubuntu.html",
    "href": "computing/ubuntu.html",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#installation-notes",
    "href": "computing/ubuntu.html#installation-notes",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#ubuntu-software-settings",
    "href": "computing/ubuntu.html#ubuntu-software-settings",
    "title": " Ubuntu",
    "section": "Ubuntu software settings",
    "text": "Ubuntu software settings\nTo ensure that you are installing the right version of the software in this guide, open “Software & Updates” and make sure that the boxes in the screenshot are checked (this is the default configuration)."
  },
  {
    "objectID": "computing/ubuntu.html#github",
    "href": "computing/ubuntu.html#github",
    "title": " Ubuntu",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/ubuntu.html#git",
    "href": "computing/ubuntu.html#git",
    "title": " Ubuntu",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, so if your Git is older than this version, we ask you to update it using the following commands:\nsudo apt update\nsudo apt install git\nYou can check your git version with the following command:\ngit --version\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Linux section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/ubuntu.html#latex",
    "href": "computing/ubuntu.html#latex",
    "title": " Ubuntu",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it still fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/ubuntu.html#github-pat",
    "href": "computing/ubuntu.html#github-pat",
    "title": " Ubuntu",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/ubuntu.html#post-installation-notes",
    "href": "computing/ubuntu.html#post-installation-notes",
    "title": " Ubuntu",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/ubuntu.html#attributions",
    "href": "computing/ubuntu.html#attributions",
    "title": " Ubuntu",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus (2025 W1)",
    "section": "",
    "text": "Dates:\nTerm 2025 W1, 04 Sep - 04 Dec 2025\nInstructor:\nGeoff Pleiss\nWebsite: https://geoffpleiss.com/\nEmail: geoff.pleiss@stat.ubc.ca\nSlack: @geoff\nOffice hours:\nSee Canvas for times and locations.\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGitHub: https://github.com/stat-406-2025/\nCanvas: https://canvas.ubc.ca/courses/147492/\nLectures/Labs:\nSee Canvas for times and locations.\nTextbooks:\n[ISLR] [ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus (2025 W1)",
    "section": "",
    "text": "Dates:\nTerm 2025 W1, 04 Sep - 04 Dec 2025\nInstructor:\nGeoff Pleiss\nWebsite: https://geoffpleiss.com/\nEmail: geoff.pleiss@stat.ubc.ca\nSlack: @geoff\nOffice hours:\nSee Canvas for times and locations.\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGitHub: https://github.com/stat-406-2025/\nCanvas: https://canvas.ubc.ca/courses/147492/\nLectures/Labs:\nSee Canvas for times and locations.\nTextbooks:\n[ISLR] [ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus (2025 W1)",
    "section": "Course Objectives",
    "text": "Course Objectives\nThis course introduces machine learning from a statistical perspective. Beginning with linear models and building beyond STAT 306 content, we will progress to an in-depth coverage of classic and modern learning methods with greater mathematical and statistical depth than CPSC 340.\n\nLearning Outcomes\nThe primary aim of this course is to develop deep statistical intuitions about different learning methods and the connections between these methods. If we are successful, by the end of the course you will be able to:\n\nFormulate and analyze machine learning algorithms from a statistical perspective\nCategorize learning methods through multiple criteria (supervised versus unsupervised, linear versus nonlinear, parametric versus nonparametric, high bias versus high variance)\nDraw connections between any two learning algorithms and paradigms covered in this course\nReason about which methods (regularization, nonlinearities, ensembles) are most appropriate for a given situation using mathematical tools like the bias/variance tradeoff and curse of dimensionality\nApply these methods correctly, troubleshoot common pitfalls, and identify probable steps for improving model performance\nImplement and utilize models in R using best coding practices\n\n\nWhat This Course Is Not\nThis course is not a practitioner’s introduction to machine learning. While you will gain practical skills in the course and you will be equipped to run and troubleshoot models, the emphasis is not on the latest and greatest models, software packages, using high performance compute infrastructure, or data wrangling. There is no project component where you get to build a model of your choice on your own dataset. If this is the content you are looking for, I suggest CPSC 330 or any number of tutorials that exist on the internet.\nThis course is also not a comprehensive introduction to all of the latest and greatest ML methods. Methods evolve constantly, and there are too many of them to introduce in one course. I suggest advanced topics courses in computer science or statistics if you want a flavour of new methods. The foundational methods introduced in this course are building blocks used by all machine learning algorithms. Mastering these concepts will dramatically improve your ability to understand new methods, and it will also help you cut through the BS components of modern ML/AI systems that don’t actually do anything too fancy.\nBuilding fluency with tools and processes is an important part of becoming a statistician or machine learning practitioner, as is being up to date with methods. It is equally important to be able to analyze and reason about the methods and tools that you’re using. The latter is the niche filled by this course."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus (2025 W1)",
    "section": "Textbooks",
    "text": "Textbooks\nThere are short required readings before each lecture. You will not be quizzed or graded on the readings, but going over the material before seeing it in lecture will accelerate your learning of the material.\n\nRequired\nAn Introduction to Statistical Learning, James, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: https://www.statlearning.com\n\n\nOptional (But Excellent)\nThe Elements of Statistical Learning, Hastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\nThis second book is a more advanced treatment of a superset of the topics we will cover. All readings from [ESL] are optional."
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus (2025 W1)",
    "section": "Lectures",
    "text": "Lectures\nThis will be taught primarily via lecture, with interactive activities, questions, and discussions interspersed throughout. Attendance is strongly encouraged.\nDeveloping intuitions for this material takes time and effort, and the lectures provide an excellent setting and opportunity for you to ponder, struggle, and engage with the material.\nWhen you attend lecture, I ask that you minimize the use of phones and computers other than for participation and note taking. I know that it is easy to be distracted by homework, social media, or k-pop videos.[^The lecture recordings capture the audience, so I get to see the fun things you’re looking at on your laptops!]\nIf you aren’t able to provide your full attention during class, especially during the participatory activities, I kindly ask that you stay home instead. Your grade won’t suffer (I don’t track attendance) and your health and academic performance will benefit from the extra sleep.\nHere are the benefits for being participatory during lecture:\n\nYou’ll put in the work to develop deep intuitions. We’ll work through derivations during lecture that you won’t see on labs or assignments. Participating in these derivations gives you a different mode for absorbing the material.\nYou get to ask questions. This material is my bread and butter, it’s my area of research, and I have lots of insights that I’m excited to share! I’ll also answer questions over slack and during office hours, but your best bet to get good answers from me is during class.\nYou’ll get participation points. See below for a way to boost your grade by participating during lecture.\nYou’ll get a better letter of recommendation. If you want to apply to graduate school and would consider me for a letter of recommendation, I should know something about you beyond your letter grade. Asking and answering questions during lecture and office hours gives me good material to write about."
  },
  {
    "objectID": "syllabus.html#course-assessment-opportunities",
    "href": "syllabus.html#course-assessment-opportunities",
    "title": "Syllabus (2025 W1)",
    "section": "Course Assessment Opportunities",
    "text": "Course Assessment Opportunities\n\nEffort-Based Component\nThe effort-based assessments are your main tools to develop the deep intuitions about ML that we’re trying to achieve in this course. They will ask you to implement and troubleshoot models, wrestle with complex mathematical ideas, and ponder strange phenomena. Think of these assessments like eating your vegetables, doing a workout, or practicing an instrument. The work is hard, but it builds a deeper understanding about statistics and ML methods that you can’t get through other means—helping you learn to formulate algorithms from a statistical perspective, connect different learning methods, and figure out which approaches work best in different situations.\n\n\n\nComponent\nPossible Points\n\n\n\n\nParticipation\n5\n\n\nClickers\n10\n\n\nLabs\n20\n\n\nHomeworks\n40\n\n\nTotal\nmin(60, Participation + Clickers + Labs + Homeworks)\n\n\n\n\nGenerative AI Policy\nChatbots and agentic tools are quite adept with the content that we’ll cover. I used them to help me develop materials for this course (mostly to flesh out/polish ideas rather than to generate content from scratch). I believe they have the potential to aid in your learning, but also the potential to be a tempting shortcut that inhibits your potential to grow from this course.\nTo get the most out of this course, I strongly recommend using generative AI as little as possible. If you feel the need to use it, use it to supplement and strengthen your thinking (e.g. code completion, rewriting bullet points into paragraphs, etc.) rather than to replace your thinking (e.g. feeding the entire assignment into a chatbot or agentic coding tool). Here’s why:\n\nThe goal of this course is developing deep intuitions about challenging material. Developing intuitions requires thinking, meditating, and struggling with content.\nAnyone can use Claude Code, Copilot, or Cursor to develop simple ML models and pipelines. If you want to stand out as a job or graduate student candidate, you need to demonstrate the ability to think beyond what these tools can do.\nThere are enough questions with course-specific subtleties where a chatbot will produce a subpar or wrong answer. Even with really good prompting, you likely won’t score above a 7 or 8 on assignments (which is the median effort grade anyways).\nYou won’t be able to use any AI tools during the midterm or final. There likely won’t be any coding questions, but you don’t want to be too dependent on these tools when it comes to proving your knowledge.\n\nIf you use any generative AI tool on homeworks or labs, I ask that you self report how you used the tools and how they contributed to your learning/ability to understand the assignment. (There will be a box to fill in at the top of every lab and assignment.) Include every prompt that you used. Please be as honest as possible, even if you did just get ChatGPT to answer every question for you. Your responses will help me structure the material better especially as these tools evolve.\n\n\n\nParticipation\nI believe that active participation during lecture is the best way to learn the material. To that end, I’ve crafted lectures to be entertaining (especially for 8am!), interactive, and engaging for all learning styles. You can help me (and yourself!) by asking questions, working through course exercises with your neighbours, and sharing insights.\nYou earn participation points by answering questions or contributing to discussions. I’ll use the Agora platform to monitor hand raises and select students. Every time you raise your hand in the Agora app, you earn 1 point. When you’re randomly called on from raised hands, you also gain a point. Your final participation score is calculated as min(5, 5 * num_points / num_classes_with_agora). In essence, if you raise your hand at least once every class, you’ll receive the maximum participation grade.\nNote on attendance. I will not be tracking attendance, so you do not need to alert me if you need to skip the occasional lecture. You can still get a perfect effort-based grade even if you do not participate (though I sincerely hope you choose to do so!)\n\n\n\nClickers\nThroughout lecture, I will ask short multiple choice and True/False questions, which you will answer using the iClicker app. For each question, correct answers are worth 4, incorrect answers are worth 2. You get 0 points for not answering.\nSuppose there are N total clicker questions, and you have x points. Your final score for the clicker component is max(0, min(5 * x / N - 5, 10)).\nNote that if your average is less than 1, you get 0 points in this component.\n\n\n\nLabs\nThese are intended to keep you on track. They are to be submitted via pull requests in your personal labs-&lt;username&gt; repo (see the computing tab for descriptions on how to do this).\nLabs typically have a few questions for you to answer or code to implement. These are designed to be done during lab periods, but you can do them on your own as well. These are worth 2 points each up to a maximum of 20 points. They are due at 2300 on the day of your assigned lab section.\nIf you attend lab, you may share a submission with another student (with acknowledgement on the PR). If you do not attend lab, you must work on your own (subject to the collaboration instructions for Assignments below).\n\nRules.\nYou must submit via PR by the deadline. Your PR must include at least 3 commits. After lab 2, failure to include at least 3 commits will result in a maximum score of 1.\n\n\n\n\n\n\nTip\n\n\n\nIf you attend your lab section, you may work in pairs, submitting a single document to one of your Repos. Be sure to put both names on the document, and mention the collaboration on your PR. You still have until 11pm to submit.\n\n\n\n\nMarking.\nThe overriding theme here is “if you put in the effort, you’ll get all the points.” Grading scheme:\n\n2 if basically all correct\n1 if complete but with some major errors, or mostly complete and mostly correct\n0 otherwise\n\nYou may submit as many labs as you wish up to 20 total points. There are no appeals on grades.\n\n\n\n\nHomeworks\nThere will be 4 homework assignments. These are submitted via pull request similar to the labs but to the homework-&lt;username&gt; repo. Each assignment is worth up to 10 points. They are due by 2300 on the deadline. You must make at least 5 commits. Failure to have at least 5 commits will result in a 25% deduction on HW1 and a 50% deduction thereafter. No exceptions.\nAssignments are typically lightly marked. The median last year was 8/10. But they are not easy. Nor are they short. They often involve a combination of coding, writing, description, and production of statistical graphics.\nAfter receiving a mark and feedback, if you score less than 7, you may make corrections to bring your total to 7. This means, if you fix everything that you did wrong, you get 7. Not 10. The revision must be submitted within 1 week of getting your mark. Only 1 revision per assignment. The TA decision is final. Note that the TAs will only regrade parts you missed, but if you somehow make it worse, they can deduct more points.\nThe revision allowance applies only if you got 3 or more points of “content” deductions. If you missed 3 points for content and 2 more for “penalties” (like insufficient commits, code that runs off the side of the page, etc), then you are ineligible.\n\nPolicy on collaboration on assignments\nDiscussing assignments with your classmates is allowed and encouraged, but it is important that every student get practice working on these problems. This means that all the work you turn in must be your own. The general policy on homework collaboration is:\n\nYou must first make a serious effort to solve the problem.\nIf you are stuck after doing so, you may ask for help from another student. You may discuss strategies to solve the problem, but you may not look at their code, nor may they spell out the solution to you step-by-step.\nOnce you have gotten help, you must write your own solution individually. You must disclose, in your GitHub pull request, the names of anyone from whom you got help.\nThis also applies in reverse: if someone approaches you for help, you must not provide it unless they have already attempted to solve the problem, and you may not share your code or spell out the solution step-by-step.\n\n\n\n\n\n\n\nWarning\n\n\n\nAdherence to the above policy means that identical answers, or nearly identical answers, cannot occur. Thus, such occurrences are violations of the Course’s Academic honesty policy.\n\n\nYou can always, of course, ask me for help on Slack. And public Slack questions are allowed and encouraged.\nYou may also use external sources (books, websites, papers, …) to\n\nLook up programming language documentation, find useful packages, find explanations for error messages, or remind yourself about the syntax for some feature. I do this all the time in the real world. Wikipedia is your friend.\nRead about general approaches to solving specific problems (e.g. a guide to dynamic programming or a tutorial on unit testing in your programming language), or\nClarify material from the course notes or assignments.\n\nIf you use code from online or other sources (including generative AI), you must include code comments identifying the source. It must be clear what code you wrote and what code is from other sources. This rule also applies to text, images, and any other material you submit.\nPlease talk to me if you have any questions about this policy. Any form of plagiarism or cheating will result in sanctions to be determined by me, including grade penalties (such as negative points for the assignment or reductions in letter grade) or course failure. I am obliged to report violations to the appropriate University authorities. See also the text below.\n\n\n\n\nYour score on HW, Labs, and Clickers\nThe total you can accumulate across these 3 components is 60 points. But you can get there however you want. The total available is 75 points. The rest is up to you. But with choice, comes responsibility.\nRules:\n\nNothing dropped.\nNo extensions.\nIf you miss a lab or a homework deadline, then you miss it.\nMake up for missed work somewhere else.\nIf you get sick, fine. You miss a few clickers and maybe a lab (though you can do it remotely).\nIf you have a job interview and can’t complete an assignment on time, then skip it.\n\nI’m not going to police this stuff. You don’t need to let me know if you miss an assignment. There is no reason that every single person enrolled in this course shouldn’t get &gt; 65 in this class.\nIllustrative scenarios:\n\nDoing 80% on 4 homeworks (32 points), getting 5 clicker points, completing 9 labs with perfect scores, and gaining 5 participation points gets you 60 points.\nDoing 90% on 4 homeworks (36 points), getting 7 clicker points, completing 6 labs with perfect scores, and gaining 5 participation points gets you 60 points.\nGetting full homeworks (40 points) and full labs (20 points), with 0 clicker and 0 participation points gets you 60 points.\n\nChoose your own adventure. Note that the biggest barrier to getting to 60 is skipping the assignments.\n\n\n\nLate policy\nLate lab/homework submissions will not be accepted.\nMore specifically, any submission that we receive after grading has commenced will receive a 0. We likely won’t start grading at 11:01pm on the due date, so don’t worry if you’re a few minutes late. On the other hand, don’t even bother submitting if you’ve missed the deadline by a few days. This policy may seem harsh, but remember that there are many paths to a full 65 on the effort-based grade. If you miss one assignment, focus on doing well on the other labs/assignments and you might still end up with an A in the course.\n\n\n\n\nSummative Assessment\n\nMidterm Exam\n10 points, in class, on Canvas.\n\nAll multiple choice, True/False, matching.\nThe clickers are the best preparation.\nQuestions may ask you to understand or find mistakes in code.\nNo writing code.\n\n\n\nFinal Exam\n30 points, hand written.\n\nThe midterm and final are very hard. It is intended to separate those who really understand the material from those who don’t. Last year, the median grade on the final was 50%.\nYou can still end up with a very good grade even if you don’t do well on the exams. If you put in the work (do all the effort points) and the median grade on the midterm and final (50%), you’ll get an 80. If you put in the work (do all the effort points) and skip the midterm and final, you get a 60. You do not have to pass the final to pass the course. You don’t even have to take the final.\nThe point of this scheme is for those who work hard to do well. But only those who really understand the material will get 90+."
  },
  {
    "objectID": "syllabus.html#health-issues-and-considerations",
    "href": "syllabus.html#health-issues-and-considerations",
    "title": "Syllabus (2025 W1)",
    "section": "Health Issues and Considerations",
    "text": "Health Issues and Considerations\n\n\n\n\n\n\nWarning\n\n\n\nIf you are sick, it’s important that you stay home – no matter what you think you may be sick with (e.g., cold, flu, covid, other).\n\n\n\nSleep is one of the most important factors for your health, and I realize that an 8am class makes good sleep challenging. Work on developing good habits that will enable you to attend class while staying well-rested. However, it may be better to sacrifice in-person attendance than your sleep, especially if you would be so sleep-deprived that you won’t be able to benefit from class. I hope you make responsible decisions.\nYour health precautions help reduce risk and keep everyone safer. In this class, the marking scheme provides flexibility so that you can prioritize your health and still succeed. All work can be completed outside of class with reasonable time allowances.\nIf you do miss class because of illness:\n\nMake connections early in the term with other students in the class. You can help each other by sharing notes. If you don’t yet know anyone in the class, post on the discussion forum to connect with other students.\nConsult the class resources here and on Canvas. We will post all slides, readings, and recordings for each class.\nUse Slack for help.\nCome to virtual office hours.\nSee the marking scheme for reassurance about your flexibility options. No part of your final grade will be directly impacted by missing class.\n\nIf you are sick on midterm day or final exam day, do not attend the exam. You must follow up with your home faculty’s advising office to apply for deferred standing. Students who are granted deferred standing write the final exam at a later date. If you’re a Science student, you must apply for deferred standing (an academic concession) through Science Advising no later than 48 hours after the missed final exam/assignment. Learn more and find the application online. For additional information about academic concessions, see the UBC policy here.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease talk with me if you have any concerns or ask me if you are worried about falling behind."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "Syllabus (2025 W1)",
    "section": "University policies",
    "text": "University policies\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here.\n\nAcademic honesty and standards\nUBC Vancouver Statement\nAcademic honesty is essential to the continued functioning of the University of British Columbia as an institution of higher learning and research. All UBC students are expected to behave as honest and responsible members of an academic community. Breach of those expectations or failure to follow the appropriate policies, principles, rules, and guidelines of the University with respect to academic honesty may result in disciplinary action.\nFor the full statement, please see the 2022/23 Vancouver Academic Calendar\nCourse specific\nWhile course materials are freely available online, selling or distributing homework solutions, lab answers, or exam questions to other students or commercial services is strictly prohibited and violates UBC’s academic integrity policies. Violations will be reported to the Dean of Science and may result in serious academic consequences, including course failure.\nI have caught students cheating on exams in previous years. In my experience, cheating typically stems from students not understanding the material, which usually results in a failing grade even before any penalties are imposed and the incident is reported to the Dean’s office. Please do your own work and utilize the TAs and me as resources. We are here to help if you are struggling.\n\n\n\n\n\n\nCaution\n\n\n\nIf I suspect cheating, your case will be forwarded to the Dean’s office. No questions asked.\n\n\n\n\nAcademic Concessions\nThese are handled according to UBC policy. Please see\n\nUBC student services\nUBC Vancouver Academic Calendar\nFaculty of Science Concessions\n\n\n\nMissed final exam\nStudents who miss the final exam must report to their Faculty advising office within 72 hours of the missed exam, and must supply supporting documentation. Only your Faculty Advising office can grant deferred standing in a course. You must also notify your instructor prior to (if possible) or immediately after the exam. Your instructor will let you know when you are expected to write your deferred exam. Deferred exams will ONLY be provided to students who have applied for and received deferred standing from their Faculty.\n\n\nTake care of yourself\nCourse work at this level can be intense, and I encourage you to take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. I struggle with these issues too, and I try hard to set aside time for things that make me happy (cooking, playing/listening to music, exercise, going for walks).\nAll of us benefit from support during times of struggle. If you are having any problems or concerns, do not hesitate to speak with me. There are also many resources available on campus that can provide help and support. Asking for support sooner rather than later is almost always a good idea.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support. UBC Science has resources on their website. UBC Counseling Services is here to help: call 604 822 3811 or visit their website. Consider also reaching out to a friend, faculty member, or family member you trust to help get you the support you need."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": " Schedule",
    "section": "",
    "text": "Required readings are listed below for each module. Readings from ISLR are required, while those from ESL (in parentheses) are optional and supplemental."
  },
  {
    "objectID": "schedule/index.html#the-learning-procedure---models-fitting-model-selection",
    "href": "schedule/index.html#the-learning-procedure---models-fitting-model-selection",
    "title": " Schedule",
    "section": "1 The Learning Procedure - Models, Fitting, Model Selection",
    "text": "1 The Learning Procedure - Models, Fitting, Model Selection\nTopics: Learning through statistician and algorithmic lenses, model selection; cross validation\nLearning Objectives:\n\nFormulate learning problems in terms of statistical models, estimators, and model selection\nIdentify criteria for good statistical models, estimators, and model selection metrics\n\nHandouts and Resources:\n\nProgramming in R .Rmd, .pdf\nUsing RMarkdown .Rmd, .pdf\nOverview of R and Tidyverse (Slides from last term)\nOverview of git and version control (Slides from last term)\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nSep 2\n(no class, Imagine UBC)\n\n\n\n\nSep 4\nClass Overview (slides)Probability Review (notes)\n\n\n\n\nSep 9\nIntroduction to Learning, Regression(notes)\nISLR 2.1(ESL 2.4, 2.6)\n\n\n\nSep 11\nLearning (cont.), Classification(notes)\nISLR 4.3(ESL 4.4)\n\n\n\n\n\n\nLab 00 (Sep 12)\n\n\nSep 16\nModel Selection, Cross Validation(notes)\nISLR 5.1(ESL 2.9, 7.10)"
  },
  {
    "objectID": "schedule/index.html#bias-variance-tradeoff-linear-methods",
    "href": "schedule/index.html#bias-variance-tradeoff-linear-methods",
    "title": " Schedule",
    "section": "2 Bias-Variance Tradeoff, Linear Methods",
    "text": "2 Bias-Variance Tradeoff, Linear Methods\nTopics: bias/variance tradeoff; regularized regression (ridge and lasso); non-linearities via basis functions; advanced model selection and analysis\nLearning Objectives:\n\nDecompose prediction error into bias and variance components\nImplement regularized versions of linear regression (ridge, lasso) and understand their impact on bias and variance\nImplement basis expansions for linear regression and understand their impact on bias and variance\nApply closed-form selection techniques to linear methods, and identify factors in the formula that affect bias and variance\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nSep 18\nBias-Variance Tradeoff(notes)\nISLR 2.2(ESL 7.1-7.3)\n\n\n\n\n\n\nLab 01 (Sep 19)\n\n\nSep 23\nRidge Regression(notes)\nISLR 6.2.1(ESL 3.4.0-3.4.1)\nHW 1 due\n\n\nSep 25\nLasso Regression(notes)\nISLR 6.2.2-6.2.3(ESL 3.4.2-3.4.3)\n\n\n\n\n\n\nLab 02 (Sep 26)\n\n\nSep 30\n(no class, Truth and Reconciliation)\n\n\n\n\nOct 2\nBasis Functions(notes)\nISLR 7.1, 7.4(ESL 5.1-5.3)\n\n\n\n\n\n\nLab 03 (Oct 3)\n\n\nOct 7\ncatch up"
  },
  {
    "objectID": "schedule/index.html#nonparametric-methods-curse-of-dimensionality",
    "href": "schedule/index.html#nonparametric-methods-curse-of-dimensionality",
    "title": " Schedule",
    "section": "3 Nonparametric Methods, Curse of Dimensionality",
    "text": "3 Nonparametric Methods, Curse of Dimensionality\nTopics: kNN; trees; kernel machines; curse of dimensionality\nLearning Objectives:\n\nAnalyze how dimensionality affects the performance of parametric vs nonparametric methods\nImplement nonparametric methods (kNN, kernel smoothing, kernel machines) and analyze their properties\nWrite the parametric version of nonparametric methods (e.g. kernel ridge regression) and vice versa\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nOct 9\nModel Selection for Linear Methods(notes)\n(ESL 7.6-7.7)\nHW 2 due\n\n\n\n\n\nLab 04 (Oct 10)\n\n\nOct 14\nKernel Machines(notes)\nISLR 3.5(ESL 2.3.2, 5.4.1)\n\n\n\nOct 16\nkNN, Curse of Dimensionality(notes)\nISLR 3.5, 8.1(ESL 2.3.2, 5.4.1, 9.2)\n\n\n\nOct 21\nReview"
  },
  {
    "objectID": "schedule/index.html#midterm-exam",
    "href": "schedule/index.html#midterm-exam",
    "title": " Schedule",
    "section": "Midterm Exam",
    "text": "Midterm Exam\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\nOct 23\nMIDTERM EXAM (In Class)\n\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou must bring your computer as the exam will be given through Canvas\nPlease arrange to borrow one from the library if you do not have your own. Let me know ASAP if this may pose a problem.\nYou may bring 2 sheets of front/back 8.5 × 11 inch paper with handwritten notes you want to use. No other materials will be allowed.\nThere will be no required coding, but I may show code or output and ask questions about it.\nIt will be entirely multiple choice / True-False / matching, etc. Delivered on Canvas."
  },
  {
    "objectID": "schedule/index.html#unsupervised-learning",
    "href": "schedule/index.html#unsupervised-learning",
    "title": " Schedule",
    "section": "4 Unsupervised Learning",
    "text": "4 Unsupervised Learning\nTopics: dimension reduction and clustering\nLearning Objectives:\n\nImplement dimensionality reduction techniques (PCA, kernel PCA) and analyze their impact on data representation\nApply clustering algorithms (k-means, Gaussian mixture models) and evaluate their performance using appropriate metrics\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nOct 28\nTrees (catch up from last module)(Notes)\nISLR 8.1, 4.2.0(ESL 9.2)\n\n\n\nOct 30\nDimensionality Reduction, Intro to Unsupervised Learning(Notes)\nISLR 12.2(ESL 14.5.1, 14.5.4)\n\n\n\n\n\n\nLab 05 (Oct 31)\n\n\nNov 04\nClustering (Notes)\nISLR 12.4.1(ESL 14.3)\n\n\n\nNov 06\ncatch up\n\nHW 3 due\n\n\n\n\n\nLab 07 (Nov 7)NOTE: SKIP LAB 06"
  },
  {
    "objectID": "schedule/index.html#ensembles-black-box-methods",
    "href": "schedule/index.html#ensembles-black-box-methods",
    "title": " Schedule",
    "section": "5 Ensembles, Black-Box Methods",
    "text": "5 Ensembles, Black-Box Methods\nTopics: ensembles; bootstrap; bagging; boosting; random forests\nLearning Objectives:\n\nImplement bootstrap and ensembling methods, reason through computational tradeoffs\nDifferentiate ensemble methods that reduce bias or variance\nUtilize “hidden advantages” of ensembles around feature importance, uncertainty quantification, etc.\nIdentify assumptions in black-box methods of uncertainty quantification, variance reduction, and bias reduction\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nNov 11\n(no class, Midterm Break)\n\n\n\n\nNov 13\nThe Bootstrap(Notes)\nISLR 5.2(ESL 7.11, 8.2)\n\n\n\nNov 18\nBagging, Random Forests(Notes)\nISLR 8.2.0-8.2.2(ESL 8.7, 15.1-15.3)\n\n\n\nNov 20\nBoosting(Notes)\nISLR 8.2.3(ESL 10.1-10.5, 10.9)\n\n\n\n\n\n\nLab 08 (Nov 21)"
  },
  {
    "objectID": "schedule/index.html#deep-learning",
    "href": "schedule/index.html#deep-learning",
    "title": " Schedule",
    "section": "6 Deep Learning",
    "text": "6 Deep Learning\nTopics: neural networks; deep learning architectures; generative AI\nLearning Objectives:\n\nConstruct a basic neural network architecture from simple mathematical building blocks\nArticulate the effects of depth and width on the representational capacity and generalization of neural networks\nConnect neural networks to other methods covered in the course (basis functions, kernel methods, boosting methods)\nDerive the backpropagation algorithm\nEvaluate modern neural network architectures for different problem types\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nNov 25\nIntroduction to Neural Networks\nISLR 10.1-10.2(ESL 11.1, 11.3)\n\n\n\nNov 27\nNeural Network OptimizationGeneralization\nISLR 10.7-10.8(ESL 11.4)\nHW 4 due\n\n\n\n\n\nLab 09 (Nov 28)\n\n\nDec 2\nNeural Net ArchitecturesGenerative AI\n\n\n\n\nDec 4\n(No Class / Zoom Review)"
  },
  {
    "objectID": "schedule/index.html#final-exam",
    "href": "schedule/index.html#final-exam",
    "title": " Schedule",
    "section": "Final Exam",
    "text": "Final Exam\n\n\n\n\n\n\nImportant\n\n\n\nDo not make any plans to leave Vancouver before the final exam date is announced.\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou may bring 2 sheets of front/back 8.5 × 11 inch paper with handwritten notes you want to use. No other materials will be allowed."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section",
    "href": "schedule/slides/00-r-review.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour",
    "text": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour\nStat 406\nGeoff Pleiss\nLast modified – 25 August 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "href": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "title": "UBC Stat406 2025 W1",
    "section": "Tour of Rstudio",
    "text": "Tour of Rstudio\nThings to note\n\nConsole\nTerminal\nScripts, .Rmd, Knit\nFiles, Projects\nGetting help\nEnvironment, Git"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "href": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "title": "UBC Stat406 2025 W1",
    "section": "R and the {tidyverse}",
    "text": "R and the {tidyverse}\n\n\n\n\nToday is going to be a whirlwind tour of R.\nIf you are new to R: read the first 4 chapters of Data Science: A First Introduction.\nIt’s available for free at https://datasciencebook.ca. It covers:\n\nData loading from .csv, Excel, database, and web sources\nData saving to .csv files\nData wrangling with tidyverse functions\nPlotting with ggplot"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures",
    "href": "schedule/slides/00-r-review.html#basic-data-structures",
    "title": "UBC Stat406 2025 W1",
    "section": "Basic data structures",
    "text": "Basic data structures\n\n\nVectors:\n\nx &lt;- c(1, 3, 4)\nx[1]\n\n[1] 1\n\nx[-1]\n\n[1] 3 4\n\nrev(x)\n\n[1] 4 3 1\n\nc(x, x)\n\n[1] 1 3 4 1 3 4\n\n\n\n\n\nMatrices:\n\nx &lt;- matrix(1:25, nrow = 5, ncol = 5)\nx[1,]\n\n[1]  1  6 11 16 21\n\nx[,-1]\n\n     [,1] [,2] [,3] [,4]\n[1,]    6   11   16   21\n[2,]    7   12   17   22\n[3,]    8   13   18   23\n[4,]    9   14   19   24\n[5,]   10   15   20   25\n\nx[c(1,3),  2:3]\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    8   13\n\n\n\nAll elements of a vector/matrix must be of the same type"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures-cont.",
    "href": "schedule/slides/00-r-review.html#basic-data-structures-cont.",
    "title": "UBC Stat406 2025 W1",
    "section": "Basic data structures (cont.)",
    "text": "Basic data structures (cont.)\n\n\nLists\n\n(l &lt;- list(\n  a = letters[1:2],\n  b = 1:4,\n  c = list(a = 1)))\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] 1 2 3 4\n\n$c\n$c$a\n[1] 1\n\nl$a\n\n[1] \"a\" \"b\"\n\nl$c$a\n\n[1] 1\n\nl[\"b\"] # compare to l[[\"b\"]] == l$b\n\n$b\n[1] 1 2 3 4\n\n\n\n\nData frames\n\n(dat &lt;- data.frame(\n  z = 1:5,\n  b = 6:10,\n  c = letters[1:5]))\n\n  z  b c\n1 1  6 a\n2 2  7 b\n3 3  8 c\n4 4  9 d\n5 5 10 e\n\nclass(dat)\n\n[1] \"data.frame\"\n\ndat$b\n\n[1]  6  7  8  9 10\n\ndat[1,]\n\n  z b c\n1 1 6 a\n\n\n\n\nLists can have multiple element types; data frames are lists of vectors"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#functions-in-r",
    "href": "schedule/slides/00-r-review.html#functions-in-r",
    "title": "UBC Stat406 2025 W1",
    "section": "Functions in R",
    "text": "Functions in R\nA function is a mapping from inputs to outputs, and is defined with the function keyword.\nThe function’s body is wrapped in curly braces, and its output is given by the return keyword (or the last evaluated statement)\n\nf &lt;- function(x, y){\n  x+y\n}\n\nf(3,5)\n\n[1] 8\n\n\n\nf &lt;- function(x, y){\n  return(x+y)\n}\n\nf(3,5)\n\n[1] 8"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#function-signatures",
    "href": "schedule/slides/00-r-review.html#function-signatures",
    "title": "UBC Stat406 2025 W1",
    "section": "Function Signatures",
    "text": "Function Signatures\n\n\nCode\nsig &lt;- sig::sig\n\n\n\nsig(lm)\n\nfn &lt;- function(formula, data, subset, weights, na.action, method = \"qr\", model\n  = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts =\n  NULL, offset, ...)\n\nsig(`+`)\n\nfn &lt;- function(e1, e2)\n\nsig(dplyr::filter)\n\nfn &lt;- function(.data, ..., .by = NULL, .preserve = FALSE)\n\nsig(stats::filter)\n\nfn &lt;- function(x, filter, method = c(\"convolution\", \"recursive\"), sides = 2,\n  circular = FALSE, init = NULL)\n\nsig(rnorm)\n\nfn &lt;- function(n, mean = 0, sd = 1)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "href": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "title": "UBC Stat406 2025 W1",
    "section": "These are all the same",
    "text": "These are all the same\n\nset.seed(12345)\nrnorm(3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(3, 0, 1)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(sd = 1, n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\nFunctions can have default values.\nYou may, but don’t have to, name the arguments\nIf you name them, you can pass them out of order (but you shouldn’t)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "title": "UBC Stat406 2025 W1",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nf &lt;- function(arg1, arg2, arg3 = 12, ...) {\n  stuff &lt;- arg1 * arg3\n  stuff2 &lt;- stuff + arg2\n  plot(arg1, stuff2, ...)\n  return(stuff2)\n}\nx &lt;- rnorm(100)\n\n\n\n\ny1 &lt;- f(x, 3, 15, col = 4, pch = 19)\n\n\n\n\n\n\n\nstr(y1)\n\n num [1:100] -3.8 12.09 -24.27 12.45 -1.14 ..."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nSide effects are things a function changes in global scope\nOutputs can be assigned to variables\nA good example is the hist function\nYou have probably only seen the side effect which is to plot the histogram\n\n\nmy_histogram &lt;- hist(rnorm(1000))\n\n\n\n\n\n\n\n\n\n\n\nstr(my_histogram)\n\nList of 6\n $ breaks  : num [1:14] -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 ...\n $ counts  : int [1:13] 4 21 41 89 142 200 193 170 74 38 ...\n $ density : num [1:13] 0.008 0.042 0.082 0.178 0.284 0.4 0.386 0.34 0.148 0.076 ...\n $ mids    : num [1:13] -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ...\n $ xname   : chr \"rnorm(1000)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\nclass(my_histogram)\n\n[1] \"histogram\""
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "href": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "title": "UBC Stat406 2025 W1",
    "section": "When writing functions, program defensively, ensure behaviour",
    "text": "When writing functions, program defensively, ensure behaviour\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  x + 1\n}\n\nincrementer(2)\n\n[1] 3\n\nincrementer(1:4)\n\n[1] 2 3 4 5\n\nincrementer(\"a\")\n\nError in x + 1: non-numeric argument to binary operator\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  stopifnot(is.numeric(x))\n  return(x + 1)\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): is.numeric(x) is not TRUE\n\n\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + 1\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): `x` must be numeric\n\nincrementer(2, -3) ## oops!\n\n[1] 3\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + inc_by\n}\nincrementer(2, -3)\n\n[1] -1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#unit-testing",
    "href": "schedule/slides/00-r-review.html#unit-testing",
    "title": "UBC Stat406 2025 W1",
    "section": "Unit Testing",
    "text": "Unit Testing\nWhen you write functions, test them!\nUse testthat: check a few usual values and corner cases\n\n\n\nlibrary(testthat)\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  if (!is.numeric(inc_by)) {\n    stop(\"`inc_by` must be numeric\")\n  }\n  x + inc_by\n}\nexpect_error(incrementer(\"a\"))\nexpect_equal(incrementer(1:3), 2:4)\nexpect_equal(incrementer(2, -3), -1)\nexpect_error(incrementer(1, \"b\"))\nexpect_identical(incrementer(1:3), 2:4)\n\nError: incrementer(1:3) not identical to 2:4.\nObjects equal but not identical\n\n\n\n\n\nis.integer(2:4)\n\n[1] TRUE\n\nis.integer(incrementer(1:3))\n\n[1] FALSE\n\nexpect_identical(incrementer(1:3, 1L), 2:4)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDon’t copy code; write a function. Validate your arguments. Write tests to check if inputs result in predicted outputs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#classes",
    "href": "schedule/slides/00-r-review.html#classes",
    "title": "UBC Stat406 2025 W1",
    "section": "Classes",
    "text": "Classes\n\n\nWe saw some of these earlier:\n\ntib &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = x1 + 2 * x2 + rnorm(100)\n)\nmdl &lt;- lm(y ~ ., data = tib )\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(mdl)\n\n[1] \"lm\"\n\n\nThe class allows for the use of “methods”\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n    -0.1742       1.0454       2.0470  \n\n\n\n\n\nR “knows what to do” when you print() an object of class \"lm\".\nprint() is called a “generic” function.\nYou can create “methods” that get dispatched.\nFor any generic, R looks for a method for the class.\nIf available, it calls that function."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "href": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "title": "UBC Stat406 2025 W1",
    "section": "Viewing the dispatch chain",
    "text": "Viewing the dispatch chain\n\nsloop::s3_dispatch(print(incrementer))\n\n=&gt; print.function\n * print.default\n\nsloop::s3_dispatch(print(tib))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(print(mdl))\n\n=&gt; print.lm\n * print.default"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#generic-methods",
    "href": "schedule/slides/00-r-review.html#generic-methods",
    "title": "UBC Stat406 2025 W1",
    "section": "Generic Methods",
    "text": "Generic Methods\nThere are lots of generic functions in R\nCommon ones are print(), summary(), and plot().\nAlso, lots of important statistical modelling concepts: residuals() coef()\n(In python, these work the opposite way: obj.residuals. The dot after the object accesses methods defined for that type of object. But the dispatch behaviour is less robust.)\n\nThe convention is that the specialized function is named method.class(), e.g., summary.lm().\nIf no specialized function is defined, R will try to use method.default().\n\nFor this reason, R programmers try to avoid . in names of functions or objects."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#wherefore-methods",
    "href": "schedule/slides/00-r-review.html#wherefore-methods",
    "title": "UBC Stat406 2025 W1",
    "section": "Wherefore methods?",
    "text": "Wherefore methods?\n\nThe advantage is that you don’t have to learn a totally new syntax to grab residuals or plot things\nYou just use residuals(mdl) whether mdl has class lm or any other class you expect to have residuals\nThe one draw-back is the help pages for the generic methods tend to be pretty vague\nCompare ?summary with ?summary.lm."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#different-environments",
    "href": "schedule/slides/00-r-review.html#different-environments",
    "title": "UBC Stat406 2025 W1",
    "section": "Different environments",
    "text": "Different environments\n(known as scope in other languages)\n\nThese are often tricky, but are very common.\nMost programming languages have this concept in one way or another.\nIn R code run in the Console produces objects in the “Global environment”\nYou can see what you create in the “Environment” tab.\nBut there’s lots of other stuff.\nMany packages are automatically loaded at startup, so you have access to the functions and data inside\n\nFor example mean(), lm(), plot(), iris (technically iris is lazy-loaded, meaning it’s not in memory until you call it, but it is available)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-1",
    "href": "schedule/slides/00-r-review.html#section-1",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "Other packages require you to load them with library(pkg) before their functions are available.\nBut, you can call those functions by prefixing the package name ggplot2::ggplot().\nYou can also access functions that the package developer didn’t “export” for use with ::: like dplyr:::as_across_fn_call()\n\n\nThat is all about accessing “objects in package environments”"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "title": "UBC Stat406 2025 W1",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nAs one might expect, functions create an environment inside the function.\n\nz &lt;- 1\nfun &lt;- function(x) {\n  z &lt;- x\n  print(z)\n  invisible(z)\n}\nfun(14)\n\n[1] 14\n\n\n\nNon-trivial cases are data-masking environments.\n\ntib &lt;- tibble(x1 = rnorm(100),  x2 = rnorm(100),  y = x1 + 2 * x2)\nmdl &lt;- lm(y ~ x2, data = tib)\nx2\n\nError: object 'x2' not found\n\n\n\nlm() looks “inside” the tib to find y and x2\nThe data variables are added to the lm() environment"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nWhen Knit, .Rmd files run in their OWN environment.\nThey are run from top to bottom, with code chunks depending on previous\nThis makes them reproducible.\n\nObjects in your local environment are not available in the .Rmd\nObjects in the .Rmd are not available locally.\n\n\n\n\n\n\nTip\n\n\nThe most frequent error I see is:\n\nrunning chunks individually, 1-by-1, and it works\nKnitting, and it fails\n\nThe reason is almost always that the chunks refer to objects in the Environment that don’t exist in the .Rmd"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-2",
    "href": "schedule/slides/00-r-review.html#section-2",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "This error also happens because:\n\nlibrary() calls were made globally but not in the .Rmd\n\nso the packages aren’t loaded\n\npaths to data or other objects are not relative to the .Rmd in your file system\n\nthey must be\n\nCarefully keeping Labs / Assignments in their current location will help to avoid some of these.\n\n\n\n\n\n\n\nTip\n\n\nKnit frequently throughout your homework / lab so that you encounter environment errors early and often!"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#how-to-fix-code",
    "href": "schedule/slides/00-r-review.html#how-to-fix-code",
    "title": "UBC Stat406 2025 W1",
    "section": "How to fix code",
    "text": "How to fix code\n\nIf you’re using a function in a package, start with ?function to see the help\n\nMake sure you’re calling the function correctly.\nTry running the examples.\npaste the error into Google (if you share the error on Slack, I often do this first)\nGo to the package website if it exists, and browse around\n\nIf your .Rmd won’t Knit\n\nDid you make the mistake on the last slide?\nDid it Knit before? Then the bug is in whatever you added.\nDid you never Knit it? Why not?\nCall rstudioapi::restartSession(), then run the Chunks 1-by-1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#adding-browser",
    "href": "schedule/slides/00-r-review.html#adding-browser",
    "title": "UBC Stat406 2025 W1",
    "section": "Adding browser()",
    "text": "Adding browser()\n(known as a breakpoint in any other language)\n\nOnly useful with your own functions.\nOpen the script with the function, and add browser() to the code somewhere\nThen call your function.\nThe execution will Stop where you added browser() and you’ll have access to the local environment to play around"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#reproducible-examples",
    "href": "schedule/slides/00-r-review.html#reproducible-examples",
    "title": "UBC Stat406 2025 W1",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\n\n\n\n\n\nQuestion I frequently get:\n\n\n“I ran this code, but it didn’t work.”\n\n\n\n\nIf you want to ask me why the code doesn’t work, you need to show me what’s wrong.\n\n\n\n\n\n\n\nDon’t just paste a screenshot!\n\n\nUnless you get lucky, I won’t be able to figure it out from that. And we’ll both get frustrated.\n\n\n\nWhat you need is a Reproducible Example or reprex.\n\nThis is a small chunk of code that\n\nruns in it’s own environment\nand produces the error."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#the-reprex-package",
    "href": "schedule/slides/00-r-review.html#the-reprex-package",
    "title": "UBC Stat406 2025 W1",
    "section": "The {reprex} package",
    "text": "The {reprex} package\n\nOpen a new .R script.\nPaste your buggy code in the file (no need to save)\nEdit your code to make sure it’s “enough to produce the error” and nothing more. (By rerunning the code a few times.)\nCopy your code (so that it’s on the clipboard)\nCall reprex::reprex(venue = \"r\") from the console. This will run your code in a new environment and show the result in the Viewer tab. Does it create the error you expect?\nIf it creates other errors, that may be the problem. You may fix the bug on your own!\nIf it doesn’t have errors, then your global environment is Farblunget.\nThe Output is now on your clipboard. Go to Slack and paste it in a message. Then press Cmd+Shift+Enter (on Mac) or Ctrl+Shift+Enter (Windows/Linux). Under Type, select R.\nSend the message, perhaps with more description and an SOS emoji.\n\n\n\n\n\n\n\nNote\n\n\nBecause Reprex runs in it’s own environment, it doesn’t have access to any of the libraries you loaded or the stuff in your global environment. You’ll have to load these things in the script."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-pitfalls",
    "href": "schedule/slides/00-r-review.html#r-pitfalls",
    "title": "UBC Stat406 2025 W1",
    "section": "R Pitfalls",
    "text": "R Pitfalls\n\nR is very permissive, and this leads to frequent silent errors\n\nnonstandard evaluation of arguments, data masking\nallows dots in names (even though they mean something syntactically!)\nallows accessing attributes that don’t exist\npromotion of ints to floats, floats to strings 😱\n\nLots of unusual design decisions\n\nmany assignment operators (-&gt;, &lt;-, -&gt;&gt;, &lt;&lt;-, =)\nmany accessors (a$b is a[[\"b\"]] but not a[\"b\"])\nlacking basic data types (e.g., hash maps)\ninformal classes (class(x) &lt;- \"a weird new class!\")\ntonnes of functions/data/objects in the global namespace\n3 == \"3\" (evaluates to TRUE?!!?!)\n\nRscript executable treats code differently than the R REPL"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "href": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "title": "UBC Stat406 2025 W1",
    "section": "{tidyverse} is huge",
    "text": "{tidyverse} is huge\nCore tidyverse is ~30 different packages, but we’re going to just talk about a few.\nLoad all of them by calling library(tidyverse)\nPackages fall roughly into a few categories:\n\nConvenience functions: {magrittr} and many many others.\nData processing: {dplyr} and many others.\nGraphing: {ggplot2} and some others like {scales}.\nUtilities\n\n\n\nWe’re going to talk quickly about some of it, but ignore much of 2.\nThere’s a lot that’s great about these packages, especially ease of data processing.\nBut it doesn’t always jive with base R (it’s almost a separate proglang at this point)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-in-doubt",
    "href": "schedule/slides/00-r-review.html#when-in-doubt",
    "title": "UBC Stat406 2025 W1",
    "section": "When in doubt…",
    "text": "When in doubt…\n\n\n\n\nRead the first 4 chapters (especially 3 and 4!)\nhttps://datasciencebook.ca"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "href": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "title": "UBC Stat406 2025 W1",
    "section": "Piping with {magrittr}",
    "text": "Piping with {magrittr}\nThis was introduced by {magrittr} as %&gt;%,\nbut is now in base R (&gt;=4.1.0) as |&gt;.\nNote: there are other pipes in {magrittr} (e.g. %$% and %T%) but I’ve never used them.\nThe point of the pipe is to logically sequence nested operations\nThe pipe passes the left hand side as the first argument of the right hand side"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#example",
    "href": "schedule/slides/00-r-review.html#example",
    "title": "UBC Stat406 2025 W1",
    "section": "Example",
    "text": "Example\n\n\n\nselect(filter(mtcars, cyl == 6), mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse1 &lt;- print(\n  sum(\n    residuals(\n      lm(y~., data = mutate(\n        tib,\n        x3 = x1^2,\n        x4 = log(x2 + abs(min(x2)) + 1)\n      )\n      )\n    )^2\n  )\n)\n\n[1] 1.371438e-29\n\n\n\n\nmtcars |&gt; filter(cyl == 6) |&gt; select(mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse2 &lt;- tib |&gt;\n  mutate(\n    x3 = x1^2,\n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() |&gt;\n  print()\n\n[1] 1.371438e-29"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-3",
    "href": "schedule/slides/00-r-review.html#section-3",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "It may seem like we should push this all the way\n\ntib |&gt;\n  mutate(\n    x3 = x1^2,\n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() -&gt;\n  mse3\n\nThis technically works…but at a minimum it makes it hard to extend pipe sequences.\n\n\n\n\n\n\n\nNote\n\n\nOpinion zone: It’s also just weird. Don’t encourage the R devs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "href": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "title": "UBC Stat406 2025 W1",
    "section": "Data processing in {dplyr}",
    "text": "Data processing in {dplyr}\nThis package has all sorts of things. And it interacts with {tibble} generally.\nThe basic idea is “tibble in, tibble out”.\nSatisfies data masking which means you can refer to columns by name or use helpers like ends_with(\"_rate\")\nMajorly useful operations:\n\nselect() (chooses columns to keep)\nmutate() (showed this already)\ngroup_by()\npivot_longer() and pivot_wider()\nleft_join() and full_join()\nsummarise()\n\n\n\n\n\n\n\nNote\n\n\nfilter() and select() are functions in Base R.\nSometimes you get 🐞 because it called the wrong version.\nTo be sure, prefix it like dplyr::select()."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "href": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "title": "UBC Stat406 2025 W1",
    "section": "A useful data frame",
    "text": "A useful data frame\n\n7-day rolling avg COVID case/death counts for CA and WA from Aug 1-21, 2022 from Johns Hopkins\n\nlibrary(tidyverse)\ncovid &lt;- read_csv(\"data/covid.csv\") |&gt;\n  select(geo_value, time_value, signal, value)\n\ncovid\n\n# A tibble: 84 × 4\n   geo_value time_value signal                        value\n   &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;\n 1 ca        2022-08-01 confirmed_7dav_incidence_prop  45.4\n 2 wa        2022-08-01 confirmed_7dav_incidence_prop  27.7\n 3 ca        2022-08-02 confirmed_7dav_incidence_prop  44.9\n 4 wa        2022-08-02 confirmed_7dav_incidence_prop  27.7\n 5 ca        2022-08-03 confirmed_7dav_incidence_prop  44.5\n 6 wa        2022-08-03 confirmed_7dav_incidence_prop  26.6\n 7 ca        2022-08-04 confirmed_7dav_incidence_prop  42.3\n 8 wa        2022-08-04 confirmed_7dav_incidence_prop  26.6\n 9 ca        2022-08-05 confirmed_7dav_incidence_prop  40.7\n10 wa        2022-08-05 confirmed_7dav_incidence_prop  34.6\n# ℹ 74 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples",
    "href": "schedule/slides/00-r-review.html#examples",
    "title": "UBC Stat406 2025 W1",
    "section": "Examples",
    "text": "Examples\nRename the signal to something short.\n\ncovid &lt;- covid |&gt;\n  mutate(signal = case_when(\n    str_starts(signal, \"confirmed\") ~ \"case_rate\",\n    TRUE ~ \"death_rate\"\n  ))\n\nSort by time_value then geo_value\n\ncovid &lt;- covid |&gt; arrange(time_value, geo_value)\n\nCalculate grouped medians\n\ncovid |&gt;\n  group_by(geo_value, signal) |&gt;\n  summarise(med = median(value), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  geo_value signal        med\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 ca        case_rate  33.2  \n2 ca        death_rate  0.112\n3 wa        case_rate  23.2  \n4 wa        death_rate  0.178"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples-1",
    "href": "schedule/slides/00-r-review.html#examples-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Examples",
    "text": "Examples\nSplit the data into two tibbles by signal\n\ncases &lt;- covid |&gt;\n  filter(signal == \"case_rate\") |&gt;\n  rename(case_rate = value) |&gt; select(-signal)\ndeaths &lt;- covid |&gt;\n  filter(signal == \"death_rate\") |&gt;\n  rename(death_rate = value) |&gt; select(-signal)\n\nJoin them together\n\njoined &lt;- full_join(cases, deaths, by = c(\"geo_value\", \"time_value\"))\n\nDo the same thing by pivoting\n\ncovid |&gt; pivot_wider(names_from = signal, values_from = value)\n\n# A tibble: 42 × 4\n   geo_value time_value case_rate death_rate\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ca        2022-08-01      45.4      0.105\n 2 wa        2022-08-01      27.7      0.169\n 3 ca        2022-08-02      44.9      0.106\n 4 wa        2022-08-02      27.7      0.169\n 5 ca        2022-08-03      44.5      0.107\n 6 wa        2022-08-03      26.6      0.173\n 7 ca        2022-08-04      42.3      0.112\n 8 wa        2022-08-04      26.6      0.173\n 9 ca        2022-08-05      40.7      0.116\n10 wa        2022-08-05      34.6      0.225\n# ℹ 32 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "href": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "title": "UBC Stat406 2025 W1",
    "section": "Plotting with {ggplot2}",
    "text": "Plotting with {ggplot2}\n\nEverything you can do with ggplot(), you can do with plot(). But the defaults are much prettier.\nIt’s also much easier to adjust by aesthetics / panels by factors.\nIt also uses “data masking”: data goes into ggplot(data = mydata), then the columns are available to the rest.\nIt (sort of) pipes, but by adding layers with +\nIt strongly prefers “long” data frames over “wide” data frames.\n\n\nI’ll give a very fast overview of some confusing bits."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html",
    "href": "schedule/lectures/lecture_01_probability.html",
    "title": "Lecture 1: Probability Review",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine a random variable and understand when to use them to model quantities\nDerive probability rules through the product and sum rules\nApply linearity of expectation and the law of total expectation to simplify calculations"
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#learning-objectives",
    "href": "schedule/lectures/lecture_01_probability.html#learning-objectives",
    "title": "Lecture 1: Probability Review",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine a random variable and understand when to use them to model quantities\nDerive probability rules through the product and sum rules\nApply linearity of expectation and the law of total expectation to simplify calculations"
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#random-variables",
    "href": "schedule/lectures/lecture_01_probability.html#random-variables",
    "title": "Lecture 1: Probability Review",
    "section": "Random Variables",
    "text": "Random Variables\n\nMotivation\n\nExample: Let’s say you want to grab a coffee at Loafe and you want to know how long you’ll have to wait in line.\nDenote this time by the variable \\(A\\)\n\\(A\\) depends on a multitude of factors:\n\nHow hot it is outside\nWhat day of the week it is\nHow late Josh and his friends were up playing video games, thus leading them to take up spots in line\n\nWhile we could try to model all of these factors, it would be infeasible to do so.\nInstead, we can treat \\(A\\) as a random variable: a variable whose value is randomly sampled from some distribution.\n\n\n\nNotation\n\nWe (almost) always denote random variables with uppercase letters (e.g., \\(A\\))\nWe (almost) always denote their realizations (i.e., specific values they can take) with lowercase letters (e.g., \\(a\\)).\n\n\n\nJoint Random Variables\n\nThroughout this class, most of the probability we will encounter will be concerned with relationships between two or more random variables.\nExample: maybe we want to understand how the temperature outside, denoted by \\(B\\), affects the Loafe line length.\nAgain, \\(B\\) depends on many factors:\n\nWhat time of year it is\nWhether or not it’s sunny outside\nHow many flights Sarah took last year, thus leading to an increase in greenhouse gases\n\nWe can treat \\(B\\) as a random variable as well.\n\\(A\\) and \\(B\\) are related to one another, potentially in a causal manner. If we treat them as joint random variables, we can derive many useful probabilistic representations about their relationship."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#distributions-and-the-two-rules-of-probability",
    "href": "schedule/lectures/lecture_01_probability.html#distributions-and-the-two-rules-of-probability",
    "title": "Lecture 1: Probability Review",
    "section": "Distributions and The Two Rules of Probability",
    "text": "Distributions and The Two Rules of Probability\n\nGiven two random variables (A) and (B), we can describe this relationship through a joint probability distribution\n\\[\n\\begin{cases}\n    P(A=a, B=b) & \\text{for discrete random variables} \\\\\n    f_{A,B}(a, b) & \\text{for continuous random variables}\n\\end{cases}\n\\]\nWithout loss of generality, we will use the discrete random variable notation throughout the rest of this lecture (and throughout most of the course).\nWe can also describe (A) and (B) through:\n\nConditional distributions, i.e., (P(A=a | B=b)) or (P(B=b | A=a))\nMarginal distributions, i.e., (P(A=a)) or (P(B=b))\n\nWhile there are many fundamental rules of probability to manipulate these distributions, most of them can be derived from two basic rules: the product rule and the sum rule."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#the-product-rule",
    "href": "schedule/lectures/lecture_01_probability.html#the-product-rule",
    "title": "Lecture 1: Probability Review",
    "section": "The Product Rule",
    "text": "The Product Rule\nThe product rule allows us to decompose a joint distribution into the product of a conditional and marginal probability:\n\\[\\begin{align*}\nP(A=a, B=b) &= P(A=a|B=b)P(B=b) \\\\\n&= P(B=b|A=a)P(A=a)\n\\end{align*}\\]\n\nThis rule can be applied recursively in the case of more than two random variables.\nThis rule gives rise to lots of useful facts from probability theory.\n\n\nIndependence\n\nWe say that \\(A\\) and \\(B\\) are independent if \\(P(A=a, B=b) = P(A=a) P(B=b)\\); that is, their joint probability (density) is the product of their marginal probability (densities).\nBy the product rule, for independent random variables we have that\n\\[ P(A=a) P(B=b) = P(A=a, B=b) = P(A=a | B=b) P(B=b), \\]\nand, through some algebra, that \\(P(A=a) = P(A=a | B=b)\\). (Similarly, \\(P(B=b) = P(B=b | A=a)\\).)\nIn other words, when \\(A\\) and \\(B\\) are independent, the occurrence of \\(B\\) does not affect the probability of \\(A\\), and vice versa.\n\n\n\nBayes’ Rule\nWe can derive Bayes’ formula\n\\[ P(B=b | A=a) = \\frac{P(A=a | B=b) P(B=b)}{P(A=a)} \\]\nusing the product rule by starting from the identity \\(P(B=b, A=a) = P(A=a, B=b)\\) and simplifying."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#the-sum-rule",
    "href": "schedule/lectures/lecture_01_probability.html#the-sum-rule",
    "title": "Lecture 1: Probability Review",
    "section": "The Sum Rule",
    "text": "The Sum Rule\nThe sum rule allows us to obtain a marginal probability for \\(A\\) (or \\(B\\)) from a joint probability over \\(A\\) and \\(B\\):\n\\[ P(A=a) = \\int_{b} P(A=a, B=b) \\: \\mathrm{d}b. \\]\n\nHere, we are again assuming \\(A\\) and \\(B\\) are continuous and \\(P\\) represents a density. The integral becomes a summation in the case of discrete random variables.\nAgain, this rule can be extended to three or more variables recursively.\nThis rule is instrumental in establishing properties about expectations:\n\n\nLinearity of Expectation\n\nWe define the expected value of \\(A\\) as:\n\\[ \\mathbb{E}[A] := \\int_{a} a \\: P(A=a) \\: \\mathrm{d}a. \\]\nSimilarly, the expected value of some function of \\(A\\) and \\(B\\) is defined as:\n\\[ \\mathbb{E}[g(A, B)] := \\int_a \\int_b g(a, b) P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a. \\]\nWe can use the sum rule in conjunction with Fubini’s theorem to derive one of the most important formulas in all of probability:\n\\[ \\mathbb E[A + B] = \\mathbb E[A] + \\mathbb E[B] \\]\n\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\\begin{align*}\n  \\mathbb{E}[A + B]\n  &= \\int_a \\int_b (a + b) P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a \\int_b a P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\ &\\quad + \\int_a \\int_b b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a a \\int_b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\ &\\quad + \\int_b b \\int_a P(A=a, B=b) \\: \\mathrm{d}a \\: \\mathrm{d}b\n  \\\\\n  &= \\int_a a P(A=a) \\: \\mathrm{d}a + \\int_b b P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\mathbb{E}[A] + \\mathbb{E}[B]\n  \\end{align*}\\]\n\n\n\n\nThis formula, known as linearity of expectation, holds even when \\(A\\) and \\(B\\) are not independent! We will use this fact constantly throughout the course.\nAs a fun challenge problem, try using this formula to derive the famous inclusion-exclusion principle:\n\\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B). \\]\nHint: note that \\(P(A) = \\mathbb{E}[\\mathbf{1}_A]\\), where \\(\\mathbf{1}_A\\) is the indicator random variable for event \\(A\\). Similarly, \\(P(A \\cap B) = \\mathbb{E}[\\mathbf{1}_A \\mathbf{1}_B]\\) and \\(P(A \\cup B) = 1 - P(\\overline{A} \\cap \\overline{B})\\).\n\n\n\nConditional Expectations and The Tower Rule\n\nThe conditional expectation of \\(A\\) given \\(B=b\\) is defined as:\n\\[ \\mathbb{E}[A \\mid B = b] := \\int_{a} a \\: P(A=a \\mid B=b) \\: \\mathrm{d}a. \\]\nIt is the average value that \\(A\\) takes when we have the additional information that the random variable \\(B\\) takes on the value \\(b\\).\nNote that this conditional expectation is a function of \\(b\\); i.e. we can write\n\\[ \\mathbb{E}[A \\mid B = b] = g(b) \\qquad \\text{for some function} g.\\]\nThe expression \\(\\mathbb{E} [A \\mid B]\\) should then be read as:\n\\[ \\mathbb{E}[A \\mid B] = g(B). \\]\nI.e. that we are applying the conditional expectation function to the random variable \\(B\\) rather than to a specific realization \\(b\\).\nTo relate \\(\\mathbb E[A \\mid B]\\) to \\(\\mathbb E[A]\\), we can use the sum and product rules to get:\n\\[ \\mathbb E[A] = \\mathbb E[ \\mathbb E[ A \\mid B]] \\]\n\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\\[\\begin{align*}\n  \\mathbb{E}[A] &= \\int_a a \\int_b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a a \\int_b P(A=a \\mid B=b) P(B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_b \\left( \\int_a a P(A=a \\mid B=b) \\: \\mathrm{d}a \\right) P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\int_b \\mathbb{E}[A \\mid B=b] P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\mathbb{E} \\left[ \\mathbb{E}[ A \\mid B ] \\right]\n  \\end{align*}\\]\n\n\n\n\nThis rule is known as the Tower Rule. It allows us to express marginal expectations as recursive applications of conditional expectations.\nThis notation is often confusing and scary the first few times you encounter it. Try translating it back into probabilities via the sum and product rules, and you’ll fluently understand it in no time!\n\n\n\n\n\n\n\nImportantImportant: What is Random?\n\n\n\n\nA (standard) expectation \\(\\mathbb E[A]\\) is not a random variable (despite the fact that there’s a random variable inside the expectation).\n\n\nWhy?\n\nIf we go back to the definition: $ E[A] = a P(A=a) da $, note that the two terms in the integral \\(a\\), \\(P(A=a)\\) are functions of \\(a\\), which is a realized quantity and therefore not random. So we’re integrating non-random quantities together, giving us a non-random output.\n\nThe conditional expectation \\(\\mathbb E[A \\mid B=b]\\) is also not a random variable.\n\n\nWhy?\n\nAgain going back to the definition: $ E[A B=b] = a P(A=a B=b) da$, the terms inside the integral are functions of \\(a\\) and \\(b\\) (realized quantities, not random variables).\n\nHowever, the conditional expectation \\(\\mathbb E[A \\mid B]\\) is a random variable.\n\n\nWhy?\n\nRecall that we can view \\(\\mathbb E[A \\mid B=b]\\) as some function of \\(b\\); i.e. \\(\\mathbb E[A \\mid B=b] = g(b)\\). When we plug a realization (i.e. not a random variable) into \\(g\\), the output is a fixed (not random) quantity.\nHowever, when we plug in a random variable into \\(g\\), the output is a random quantity! So \\(\\mathbb E[A \\mid B] = g(B)\\) is a random variable due to the randomness in \\(B\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#conclusion",
    "href": "schedule/lectures/lecture_01_probability.html#conclusion",
    "title": "Lecture 1: Probability Review",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe will use (jointly-distributed) random variables to model data, models that depend on data, and predictions that depend on models that depend on data.\nYou will need to manipulate marginal, joint, and conditional probabilities and expectations of these random variables throughout the course.\nThis review has covered most of the probability rules that we’ll use, but just remember that you can always derive any of them through the product and sum rules!"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine the log-odds statistical model for binary classification, and justify its advantages\nDerive logistic regression through MLE and ERM\nConstruct predictions for classifiers based on different notions of risk"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#learning-objectives",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#learning-objectives",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine the log-odds statistical model for binary classification, and justify its advantages\nDerive logistic regression through MLE and ERM\nConstruct predictions for classifiers based on different notions of risk"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#supervised-learning-for-classification-statistical-perspective",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#supervised-learning-for-classification-statistical-perspective",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Supervised Learning for Classification: Statistical Perspective",
    "text": "Supervised Learning for Classification: Statistical Perspective\nIn the previous lecture, we developed a statistical perspective of the supervised learning procedure, and we worked through the steps of the learning procedure in a regression context.\n\n\n\n\n\n\n\n\n\nStep\nCS Perspective\nStatistical Perspective\nExample: Linear Regression\n\n\n\n\n2\nHypothesis Class\nStatistical Model\n\\(\\mathbb{E}[Y \\mid X = x] = x^\\top \\beta\\)\n\n\n3\nTraining\nEstimation\n\\(\\hat{\\beta}_\\mathrm{MLE/OLS} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top \\boldsymbol{Y}\\)\n\n\n6\nTesting (Inference)\nPrediction\n\\(\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\)\n\n\n\nIn this lecture, we will work through the same steps, but this time for classification problems.\nClassification versus regression:\n\nIn regression, the response variable \\(Y\\) is continuous (i.e. \\(\\mathcal Y = \\mathbb{R}\\)).\nIn binary classification, the response variable \\(Y\\) is boolean (i.e. \\(\\mathcal Y = \\{0, 1\\}\\)), where\n\n\\(Y = 1\\) represents the “positive” class\n\\(Y = 0\\) represents the “negative” class\n(You might see \\(Y = -1\\) used to denote the negative class. It doesn’t make a huge difference, but we’ll stick with \\(Y=0\\) for mathematical simplicity.)\n\nThe goal remains the same: learn a function \\(\\hat{f}: \\mathbb{R}^p \\to \\{0, 1\\}\\) that accurately predicts the class label for new observations.\n\nLet’s now derive a statistical model estimation procedure, and prediction rule for binary classification!\nSneak peek: logistic regression\n\nWe will ultimately derive logistic regression,"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#statistical-model-linear-log-odds",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#statistical-model-linear-log-odds",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Statistical Model: Linear Log-Odds",
    "text": "Statistical Model: Linear Log-Odds\n\nRecall that a statistical model is a set of probability distributions.\nIn classification, we are interested in families of \\(P(Y=1 \\mid X = x)\\) distributions\n\nThese distributions implicitly define the distribution of \\(P(Y=0 \\mid X = x)\\) by the law of total probability, i.e. \\(P(Y=0 \\mid X = x) = 1 - P(Y=1 \\mid X = x)\\).\n\nNotation: For simplicity, let’s define:\n\n\\(\\pi_1(x) := P(Y = 1 \\mid X = x)\\) (probability of positive class)\n\\(\\pi_0(x) := P(Y = 0 \\mid X = x)\\) (probability of negative class)\nNote that \\(\\pi_0(x) = 1 - \\pi_1(x)\\).\n\n\nSneak peek: the logistic regression model\n\nYou may remember from STAT 406 or CPSC 340 a set of distributions that look like: \\[P(Y = 1 \\mid X = x) = \\pi_1(x) = \\frac{\\exp(x^\\top \\beta)}{1 + \\exp(x^\\top \\beta)}\\]\n\n\\(\\beta \\in \\mathbb{R}^p\\) is a vector of parameters.\n\nThis statistical model is known as logistic regression, and it is a good starting point for binary classification.\nWe will now derive this model from first principles.\n\nWhat type of distribution should we model?\n\nIt’s hard to directly model \\(\\pi_1(x)\\) or \\(\\pi_0(x)\\) distributions.\nConsider \\(\\pi_1(x) = x^\\top \\beta\\). What’s the problem with this?\n\nIf \\(x^\\top \\beta &gt; 1\\), then \\(\\pi_1(x) = P(Y=1|X=x) &gt; 1\\), which is not a valid probability.\nIf \\(x^\\top \\beta &lt; 0\\), then \\(\\pi_1(x) = P(Y=1|X=x) &lt; 0\\), which is also not a valid probability.\n\n\nModelling the log-odds\n\nInstead of defining distributions through \\(P(Y = 1 \\mid X = x)\\) directly, we will instead define distributions through the log-odds ratio\nThe log-odds ratio is then: \\[r(x) := \\log\\left(\\frac{\\pi_1(x)}{\\pi_0(x)}\\right) = \\log\\left(\\frac{\\pi_1(x)}{1 - \\pi_1(x)}\\right)\\]\nI claim that this ratio can take any real value, i.e. \\(r(x) \\in \\mathbb{R}\\).\n\n\n\n\\(\\pi_1(x)\\)\nOdds Ratio\n\\(r(x)\\) (Log Odds Ratio)\n\n\n\n\n\\(\\approx 1\\)\n\\(\\pi_1(x) / \\pi_0(x) \\to \\infty\\)\n\\(r(x) \\to \\infty\\)\n\n\n\\(\\approx 0\\)\n\\(\\pi_1(x) / \\pi_0(x) \\to 0\\)\n\\(r(x) \\to -\\infty\\)\n\n\n\n\nThe linear log-odds model\nWe’re now ready to define a statistical model for binary classification.\n\nWe will consider\n\\[\\log\\left(\\frac{\\pi_1(x)}{\\pi_0(x)}\\right) = x^\\top\\beta\\]\nThus our statistical model is the following set of distributions:\n\n\\[\\left\\{ P(Y=1 \\mid X) \\: : \\: P(Y=1 \\mid X=x) = \\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)}, \\quad \\beta \\in \\mathbb{R}^p \\right\\}\\]\n\n\n\n\n\n\nNoteDerivation\n\n\n\n\n\n\nStart with \\(\\log\\left(\\frac{\\pi_1(x)}{\\pi_0(x)}\\right) = x^\\top\\beta\\)\nBy substituting \\(\\pi_0(x) = 1 - \\pi_1(x)\\) and then solving for \\(\\pi_1(x)\\), we get:\n\\[\\pi_1(x) = \\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)}\\]\nOur statistical model is then the set of distributions \\(P(Y=1 \\mid X=x)\\) that can take the form of \\(\\pi_1\\).\n\n\n\n\nThe logistic function:\n\nThe function in the above equation is known as the logistic function or sigmoid function:\n\\[\\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)} = \\frac{1}{1 + \\exp(-x^\\top\\beta)} := \\sigma(x^\\top \\beta)\\]\nIt has many useful properties\n\nRange: \\(\\sigma(z) \\in (0, 1)\\) for all \\(z \\in \\mathbb{R}\\)\nSymmetric: \\(\\sigma(-z) = 1 - \\sigma(z)\\)\nConvenient derivative: \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\)\n\nIt can be seen as a “smooth approximation” to the 0-1 step function:\n\n\n\n\nLogistic Function"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#prediction-and-different-notions-of-risk",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#prediction-and-different-notions-of-risk",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Prediction and Different Notions of Risk",
    "text": "Prediction and Different Notions of Risk\nLet’s discuss how we can make predictions using the logistic regression model.\nMaking predictions:\nGiven\n\na \\(\\hat \\beta\\) estimate\na new observation \\(X_{\\text{new}}\\)\na loss function \\(L(Y, \\hat{Y})\\) that quantifies how “bad” a prediction is\n\nrecall that we make predictions by minimizing the expected loss:\n\\[\\hat{Y}_{\\text{new}} = \\mathrm{argmin}_{\\hat{y}} \\mathbb{E}[L(Y, \\hat{y}) \\mid X_{\\text{new}}, \\hat \\beta]\\]\n0/1 Loss:\n\nThe most natural loss function for classification is the 0/1 loss:\n\n\\[L_{0/1}(Y, \\hat{Y}) = \\mathbb{I}(Y \\neq \\hat{Y}) = \\begin{cases} 0 & \\text{if } Y = \\hat{Y} \\\\ 1 & \\text{if } Y \\neq \\hat{Y} \\end{cases}\\]\n\nUnder our logistic model, we have:\n\n\\[\\begin{align*}\n\\hat{Y}_{\\text{new}} &= \\mathrm{argmin}_{\\hat{y}} \\mathbb{E}[ \\mathbb{I}(Y \\neq \\hat{y}) \\mid X_{\\text{new}}, \\hat \\beta] \\\\\n&= \\mathrm{argmin}_{\\hat{y}} P(Y \\neq \\hat{y} \\mid X_{\\text{new}}, \\hat \\beta) \\\\\n&= \\mathrm{argmin}_{\\hat{y}} \\left[\n   \\underbrace{P(Y = 1 \\mid X_{\\text{new}}, \\hat \\beta)}_{\\sigma(X_\\mathrm{new}^\\top \\hat \\beta)} \\mathbb{I}(\\hat{y} = 0) +\n   \\underbrace{P(Y = 0 \\mid X_{\\text{new}}, \\hat \\beta)}_{1 - \\sigma(X_\\mathrm{new}^\\top \\hat \\beta)} \\mathbb{I}(\\hat{y} = 1) \\right] \\\\\n&= \\begin{cases}\n1 & \\text{if } \\sigma(X_\\mathrm{new}^\\top \\hat \\beta) &gt; 0.5 \\\\\n0 & \\text{if } \\sigma(X_\\mathrm{new}^\\top \\hat \\beta) \\leq 0.5\n\\end{cases}\n\\end{align*}\\].\nDecision boundary:\n\nNote that\n\n\\(\\sigma(X_\\mathrm{new}^\\top \\hat \\beta) &gt; 0.5\\) when \\(X_\\mathrm{new}^\\top \\hat \\beta &gt; 0\\)\n\\(\\sigma(X_\\mathrm{new}^\\top \\hat \\beta) \\leq 0.5\\) when \\(X_\\mathrm{new}^\\top \\hat \\beta \\leq 0\\).\n\nThe decision boundary, defined by:\n\\[x^\\top\\beta = 0\\]\nis the hyperplane that separates the positively-classified \\(x\\) from the negatively classified \\(x\\).\nThis decision boundary for logistic regression is linear in \\(x\\).\n\n\n\n\nDecision Boundary\n\n\nOther losses:\nThere are other losses that we could use to generate different prediction rules:\n\nProbabilistic loss: \\(L_\\mathrm{prob}(Y, \\hat{Y}) = -\\log P(Y = \\hat Y)\\)\n\nThis loss produces “soft” predictions (e.g. \\(\\hat{Y}_{\\text{new}} = 0.273\\)) that give a probability estimate of the positive class.\n\nAsymmetric losses: \\(L_\\alpha(Y, \\hat{Y}) = \\alpha \\mathbb{I}(Y = 1, \\hat{Y} = 0) + (1-\\alpha) \\mathbb{I}(Y = 0, \\hat{Y} = 1)\\) for some \\(\\alpha \\in (0, 1)\\)\n\nThis loss allows us to penalize false positives and false negatives differently, which can be useful in imbalanced datasets.\n\n\nWe will explore these losses, as well as metrics derived from these losses, in a homework assignment."
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#estimation",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#estimation",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Estimation",
    "text": "Estimation\nAs with regression, we will derive estimators for the logistic regression parameters \\(\\beta\\) through the principle of maximum likelihood estimation (MLE).\n\nMaximum Likelihood Estimation (MLE)\nRecall the MLE estimator is given by\n\\[\\hat{\\beta}_\\mathrm{MLE} = \\mathrm{argmax}_{\\beta} \\mathcal L(\\beta) = \\mathrm{argmax}_{\\beta} \\ell(\\beta)\\]\nwhere \\(\\mathcal{L}(\\beta)\\) is the likelihood function and \\(\\ell(\\beta)\\) is the log-likelihood function.\nLog likelihood of the linear log-odds model:\n\nGiven training data \\(\\mathcal{D} = \\{(X_1, Y_1), \\ldots, (X_n, Y_n)\\}\\), recall that the likelihood/log-likelihood function is:\n\\[\\begin{gather}\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n P(Y_i \\mid X_i; \\beta) \\\\\n\\ell(\\beta) = \\sum_{i=1}^n \\log P(Y_i \\mid X_i; \\beta)\n\\end{gather}\\]\nPlugging in our model, we have that:\n\\[\nP(Y_i \\mid X_i; \\beta) = \\begin{cases}\n   \\sigma(X_i^\\top \\beta) & \\text{if } Y_i = 1 \\\\\n   1 - \\sigma(X_i^\\top \\beta) & \\text{if } Y_i = 0\n\\end{cases}\n\\]\nWe can write this equation more compactly as:\n\\[P(Y_i \\mid X_i, \\beta) = \\sigma(X_i^\\top \\beta)^{Y_i} (1 - \\sigma(X_i^\\top \\beta))^{1-Y_i}.\\]\nThus the log likelihood function is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right]\n\\]\n\nComputing the MLE estimator:\n\\[\n\\hat \\beta_\\mathrm{MLE} = \\mathrm{argmax}_{\\beta} \\sum_{i=1}^n \\left[ Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right]\n\\]\n\nUnlike linear regression, we cannot compute this maximum of this log-likelihood in closed form.\nWe can numerically solve for the optimization using a technique called gradient descent, which we will cover in a future lecture.\n\n\n\nEmpirical Risk Minimization (ERM)\n\nWhat about ERM?\nWith the 0/1 loss, the optimization problem becomes\n\n\\[\\hat \\beta_\\mathrm{ERM} = \\mathrm{argmin}_{\\beta} \\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(Y_i \\neq \\hat{Y}_i)\n= \\begin{cases} 1 & Y_i = 1, X_i^\\top \\beta &lt; 0 \\\\ 1 & Y_i = 0, X_i^\\top \\beta &gt; 0 \\\\ 0 & \\mathrm{o.w.} \\end{cases}\\]\n\nUnfortunately, this optimization is NP-hard (i.e. computationally intractable), and so we can’t even solve it numerically for large values of \\(n\\)!\nAlternative loss functions can yield numerically-solvable ERM solutions. For example, the “probabilistic loss” \\(L(Y, \\hat{Y}) = - (1 - Y_i) \\log \\hat Y - Y_i \\log (1 - \\hat Y_i)\\), lead to the same optimization problem as MLE.\n\n\n\n\n\n\n\nNoteDerivation of ERM with Probabilistic Loss\n\n\n\n\n\nUnder the probabilistic loss \\(L(Y, \\hat{Y}) = - Y_i \\log \\hat Y - (1 - Y_i) \\log (1 - \\hat Y_i)\\), the prediction rule for logistic regression becomes:\n\\[\\begin{align*}\n\\hat{Y}_{\\text{new}} &= \\mathrm{argmin}_{\\hat{y}} \\mathbb{E}[ - Y \\log \\hat y - (1 - Y) \\log (1 - \\hat y) \\mid X_{\\text{new}}, \\hat \\beta] \\\\\n&= \\mathrm{argmin}_{\\hat{y}} - \\mathbb{E}[Y \\mid X_{\\text{new}}, \\hat \\beta] \\log \\hat y - \\mathbb{E}[1 - Y \\mid X_{\\text{new}}, \\hat \\beta] \\log (1 - \\hat y) \\\\\n&= \\mathrm{argmin}_{\\hat{y}} - \\sigma(X_\\mathrm{new}^\\top \\hat \\beta) \\log \\hat y - (1 - \\sigma(X_\\mathrm{new}^\\top \\hat \\beta)) \\log (1 - \\hat y) \\\\\n\\end{align*}\\]\nwhere the second line follows from linearity of expectation, and the third line follows from our logistic model (and recognizing that \\(\\mathbb{E}[Y \\mid X_{\\text{new}}, \\hat \\beta] = P(Y=1 \\mid X_{\\text{new}}, \\hat \\beta)\\)).\nSolving for when the derivative is zero, we get:\n\\[ \\frac{\\sigma(X_\\mathrm{new}^\\top \\hat \\beta)}{\\hat Y_\\mathrm{new}} - \\frac{1 - \\sigma(X_\\mathrm{new}^\\top \\hat \\beta)}{1 - \\hat Y_\\mathrm{new}} = 0, \\]\nand thus \\(\\hat Y_\\mathrm{new} = \\sigma(X_\\mathrm{new}^\\top \\hat \\beta)\\).\nWith this prediction rule, the ERM optimization becomes:\n\\[\\begin{align*}\n\\hat \\beta_\\mathrm{ERM} &= \\mathrm{argmin}_{\\beta} \\frac{1}{n} \\sum_{i=1}^n L(Y_i, \\hat Y_i) \\\\\n&= \\mathrm{argmin}_{\\beta} \\frac{1}{n} \\sum_{i=1}^n - Y_i \\log(\\sigma(X_i^\\top \\beta)) - (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\\\\n&= \\mathrm{argmax}_{\\beta} \\frac{1}{n} \\sum_{i=1}^n Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)),\n\\end{align*}\\]\nwhich is the same optimization problem as MLE!"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#summary",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#summary",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Summary",
    "text": "Summary\nThis lecture extended the statistical framework from regression to classification:\n\nStatistical Model: The log-odds model provides a principled way to model binary responses while ensuring probabilities stay in \\([0, 1]\\).\nPrediction: The optimal prediction rule depends on the loss function.\nLinear Decision Boundary: Under the \\(0/1\\) loss, the \\(Y=0\\) and \\(Y=1\\) predictions are separated by a hyperplane defined by the decision boundary \\(X^\\top \\beta = 0\\).\nEstimation: The MLE solution cannot be computed analytically; it requires a numerical methods.\n\nIn the next lecture, we will explore the last remaining steps of the learning procedure: model selection and evaluation."
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDerive the bias-variance decomposition for squared loss\nConnect bias and variance to overfitting and underfitting\nPredict factors that affect bias and variance in linear regression\nIdentify high-bias and high-variance settings through metrics"
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#learning-objectives",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#learning-objectives",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDerive the bias-variance decomposition for squared loss\nConnect bias and variance to overfitting and underfitting\nPredict factors that affect bias and variance in linear regression\nIdentify high-bias and high-variance settings through metrics"
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#overview",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#overview",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "Overview",
    "text": "Overview\nIn the previous lecture, we:\n\ndecided that risk (\\(\\mathbb E[ L(Y, f_\\mathcal{D}(X)) ]\\)) is the metric to quantify ``goodness’’ of a learned model \\(f_\\mathcal{D}\\), and\nused estimated quantities of risk (e.g. via cross-validation) to select between different models.\n\nAs it stands right now, this model selection procedure is a bit like guess-and-check. We make a change to our model (i.e. add a covariate, change the model class, etc.), and then see if it improves our estimated risk.\n\nThis Module\nThe goal of this module is to make this process a bit more systematic by:\n\nquantifying factors that influence risk, and\ncategorizing model changes based on how they affect these factors.\n\nThis lecture will focus on the first point. We will introduce the bias-variance decomposition of risk, which is one of the cornerstones of statistical learning theory. In subsequent lectures, we will introduce modeling and estimation procedures for linear models that specifically target these factors."
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#setup",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#setup",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "Setup",
    "text": "Setup\nGiven a learned model \\(f_\\mathcal{D}: \\mathcal{X} \\to \\mathcal{Y}\\), recall our definition of risk from the previous lecture:\n\\[ \\mathcal R = \\mathbb E[ L(Y, f_\\mathcal{D}(X)) ]  = \\mathbb E\\left[ \\mathbb E[ L(Y, f_\\mathcal{D}(X)) \\mid \\mathcal D \\right]. \\]\nThis lecture will focus specifically on the square loss (though the ideas will apply to other losses):\n\\[ \\mathcal R = \\mathbb E\\left[ (Y - f_\\mathcal{D}(X))^2 \\right] \\]\nCrucially, risk is an expectation over all sources of randomness, including:\n\nthe training data \\(\\mathcal D = \\{(X_i, Y_i)\\}_{i=1}^n \\overset{\\text{i.i.d.}}{\\sim} P(Y, X)\\)\nthe test data \\((X, Y) \\sim P(Y, X)\\)\n(any randomness in the estimation procedure).\n\nOur goal is to decompose this expectation into semantically meaningful components that we can reason about. Before we do so, we need to introduce some terms/ideas:\n\nExpected Label\nFor any given set of covariates \\(X\\), there is a distribution \\(P(Y \\mid X)\\) over possible labels. We can consider the expected label:\n\\[ \\bar{Y}(X) = \\mathbb E[ Y \\mid X ]. \\]\n\n\\(\\bar{Y}(X)\\) is sometimes called the regression function.\nIf we had access to the regression function to make predictions, i.e. \\(\\hat Y = \\bar{Y}(X)\\), it would obtain minimum risk over all possible prediction rules \\(\\hat Y = f(X)\\) (which we will prove in a moment!)\nHowever, the regression function is just a theoretical construct, and we will never know it. (If we did, we wouldn’t need to learn from data!)\n\n\n\nExpected Model\nOur learned model \\(f_\\mathcal{D}\\) is a random variable because it depends on the (random) training data \\(\\mathcal D\\). We can consider the expected model:\n\\[ \\bar{f}(X) = \\mathbb E[ f_\\mathcal{D}(X) \\mid X ]. \\]\n\nHere, we are averaging over training datasets \\(\\mathcal D\\), but nothing else.\nIntuition: imagine we were to train many models on different training datasets \\(\\mathcal D_1, \\ldots, \\mathcal D_M\\). Each model will make a different prediction because they are trained on different data. The expected model \\(\\bar{f}(X)\\) is the average of these predictions."
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#warmup-decomposition-estimation-versus-prediction-error",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#warmup-decomposition-estimation-versus-prediction-error",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "Warmup Decomposition (Estimation versus Prediction Error)",
    "text": "Warmup Decomposition (Estimation versus Prediction Error)\n\nTo decompose risk into multiple meaningful terms, we are going to play a mathematician’s favorite trick: adding zero.\nMore specifically, we are going to add and subtract a very specific quantity, the expected label \\(\\bar{Y}(X)\\), within the square loss portion of risk:\n\\[ \\mathcal R = \\mathbb E\\left[ (Y - f_\\mathcal{D}(X))^2 \\right] = \\mathbb E\\left[ \\left( Y \\overbrace{- \\bar{Y}(X) + \\bar{Y}(X)}^{= 0} - f_\\mathcal{D}(X) \\right)^2 \\right] \\]\nSo far, this is dumb. But watch what happens when we group terms…\n\\[ \\mathcal R = \\mathbb E\\left[ \\left( \\underbrace{(Y - \\bar{Y}(X))}_{A} + \\underbrace{(\\bar{Y}(X) - f_\\mathcal{D}(X))}_{B} \\right)^2 \\right] \\]\n… and then expand the square into \\(A^2 + 2AB + B^2\\). After simplifying…\n\n\n(see steps here)\n\n\\[\\begin{align*}\n    \\mathcal R &= \\mathbb E\\left[ A^2 + B^2 + 2AB \\right] \\\\\n    &= \\mathbb E\\left[ A^2 \\right] + \\mathbb E\\left[ B^2 \\right] + 2\\mathbb E\\left[ AB \\right]\n    \\end{align*}\\]\nConsider \\(\\mathbb E\\left[ AB \\right] = \\mathbb E\\left[ (Y - \\bar{Y}(X))(\\bar{Y}(X) - f_\\mathcal{D}(X)) \\right]\\). Applying the tower property, we have that:\n\\[ \\mathbb E\\left[ AB \\right] = \\mathbb E\\left[ \\mathbb E\\left[ (Y - \\bar{Y}(X))(\\bar{Y}(X) - f_\\mathcal{D}(X)) \\mid X, \\mathcal D \\right] \\right]. \\]\nWithin the inner expectation, \\(Y\\) is the only random term after conditioning on \\(\\mathcal D\\) and \\(X\\). Since \\(Y\\) is independent of our training data, we have that \\(\\mathbb E[ Y \\mid X, \\mathcal D ] = \\mathbb E[ Y \\mid X ] = \\bar{Y}(X)\\). Therefore,\n\\[\\begin{align*}\n      \\mathbb E\\left[ AB \\right]\n      &= \\mathbb E\\left[ \\mathbb E\\left[ (Y - \\bar{Y}(X))(\\bar{Y}(X) - f_\\mathcal{D}(X)) \\mid X, \\mathcal D \\right] \\right] \\\\\n      &= \\mathbb E\\left[ \\left( \\mathbb E[ Y \\mid X, \\mathcal D] - \\bar{Y}(X) \\right) \\left( \\bar{Y}(X) - f_\\mathcal{D}(X) \\right) \\right] \\\\\n      &= \\mathbb E\\left[ (\\bar{Y}(X) - \\bar{Y}(X)) \\left( \\bar{Y}(X) - f_\\mathcal{D}(X) \\right) \\right] = 0.\n    \\end{align*}\\]\nThus, the cross term vanishes, and we are left with\n\\[ \\mathcal R = \\mathbb E\\left[ A^2 \\right] + \\mathbb E\\left[ B^2 \\right].\\]\nPlugging back in the definitions of \\(A\\) and \\(B\\)…\n\n… we get that\n\\[ \\mathcal R = \\underbrace{\\mathbb E\\left[ \\left( Y - \\bar{Y}(X) \\right)^2 \\right]}_\\text{irreducible error} + \\underbrace{\\mathbb E\\left[ \\left( \\bar{Y}(X) - f_\\mathcal{D}(X) \\right)^2 \\right]}_\\text{estimation error} \\]\n\n\nEstimation versus Irreducible Error\nI claim these two terms have intuitive interpretations:\n\nThe first term, the irreducible error, quantifies the inherent uncertainty in the response variable. Even if we were to perfectly know the conditional distribution \\(P(Y \\mid X)\\), we would not be able to perfectly predict \\(Y\\) from \\(X\\) due to some inherent randomness in \\(Y \\mid X\\).\nThe second term, the estimation error, quantifies how far off our learned model \\(f_\\mathcal{D}\\) is from the optimal prediction rule \\(\\bar{Y}(X)\\). It quantifies how well our learned model approximates the regression function; i.e. the optimal prediction rule.\n\nThe irreducible error is independent of our model/learning procedure. In general, we have no control over this term; it’s always there.\n\n\n\n\n\n\nWarningCaveat\n\n\n\n\n\nTechnically, we can reduce the “irreducible” error by expanding our set of covariates \\(X\\).\nConsider, for example, if we are trying to predict the temperature outside \\(Y\\) given the time of day \\(X\\). If we were to add the additional covariate of the day of the year \\(X'\\), then \\(E[Y \\mid X, X']\\) would be a better predictor of \\(Y\\) than \\(E[Y \\mid X]\\) alone.\n\n\n\nThe estimation error is the term we control with our model/estimation/prediction procedures. It will always be non-negative because we can only estimate \\(\\bar Y(X)\\) so well from limited data.\n\n\n\n\n\n\nNote\n\n\n\nI also claim that this decomposition proves my earlier statement that the regression function \\(\\bar{Y}(X)\\) is the optimal prediction rule.\n\n\nWhy?\n\n\nThe first term, the prediction error, does not depend on our model \\(f_\\mathcal{D}\\) at all. It is a property of the data distribution \\(P(X, Y)\\), and it is non-negative.\nThe second term, the estimation error, is non-negative because it is an expectation of a squared term. It will be minimized when it equals zero, which happens when \\(f_\\mathcal{D}(X) = \\bar{Y}(X)\\) for all \\(X\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#the-bias-variance-decomposition",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#the-bias-variance-decomposition",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "The Bias-Variance Decomposition",
    "text": "The Bias-Variance Decomposition\nNow for the real deal. Can we further decompose our estimation error term into meaningful components that we can reason about?\nLet’s play the adding-zero trick again, this time adding and subtracting the expected model \\(\\bar{f}(X)\\):\n\\[ \\mathbb E\\left[ \\left( \\bar{Y}(X) - f_\\mathcal{D}(X) \\right)^2 \\right]\n= \\mathbb E\\left[ \\left( \\underbrace{\\left( \\bar{Y}(X) - \\bar{f}(X) \\right)}_{C} - \\underbrace{\\left( f_\\mathcal{D}(X) - \\bar{f}(X) \\right)}_{D} \\right)^2 \\right]. \\]\nA similar game will play out after we (1) expand the square and (2) apply linearity of expectation: the cross term will vanish…\n\n\n(details here)\n\nIf we take the cross term…\n\\[ \\mathbb E\\left[ CD \\right] = \\mathbb E\\left[ \\left( \\bar{Y}(X) - \\bar{f}(X) \\right) \\left( \\bar{f}(X) - f_\\mathcal{D}(X) \\right) \\right] \\]\n… and again apply the tower property (this time only conditioning on \\(X\\))…\n\\[ \\mathbb E\\left[ CD \\right] = \\mathbb E\\left[ \\mathbb E\\left[ \\left( \\bar{Y}(X) - \\bar{f}(X) \\right) \\left( \\bar{f}(X) - f_\\mathcal{D}(X) \\right) \\mid X \\right] \\right] \\]\n… then the only term in the inner expectation that is random (after conditioning on \\(X\\)) is \\(f_\\mathcal{D}(X)\\). Since \\(\\bar{f}(X) = \\mathbb E[ f_\\mathcal{D}(X) \\mid X ]\\), we have that\n\\[\\begin{align*}\n    \\mathbb E\\left[ CD \\right]\n    &= \\mathbb E\\left[ \\mathbb E\\left[ \\left( \\bar{Y}(X) - \\bar{f}(X) \\right) \\left( \\bar{f}(X) - f_\\mathcal{D}(X) \\right) \\mid X \\right] \\right] \\\\\n    &= \\mathbb E\\left[ \\left( \\bar{Y}(X) - \\bar{f}(X) \\right) \\left( \\bar{f}(X) - \\mathbb E\\left[ f_\\mathcal{D}(X) \\mid X \\right] \\right)\\right] \\\\\n    &= \\mathbb E\\left[ (\\bar{Y}(X) - \\bar{f}(X)) (\\bar{f}(X) - \\bar{f}(X)) \\right] = 0.\n  \\end{align*}\\]\n\n… and we will be left with\n\\[ \\underbrace{\\mathbb E\\left[ \\left( \\bar{Y}(X) - f_\\mathcal{D}(X) \\right)^2 \\right]}_\\text{estimation error}\n= \\underbrace{\\mathbb E\\left[ \\left( \\bar{Y}(X) - \\bar{f}(X) \\right)^2 \\right]}_\\text{squared bias}\n+ \\underbrace{\\mathbb E\\left[ \\left( \\bar{f}(X) - f_\\mathcal{D}(X) \\right)^2 \\right]}_\\text{variance}. \\]\n\nBias Versus Variance\nAgain, these two components have intuitive (and very important) interpretations:\nVariance captures how dependent our learned model \\(f_\\mathcal{D}\\) is on any particular training dataset \\(\\mathcal D\\). High variance is bad, because it means that our predictions depend more on a particular training sample rather than generalizable patterns in the data.\nBias: captures how much “generalizable truth” can actually be learned by our statistical model/estimation procedure. High bias implies that, even if we had access to many training samples to “average out” the noise, we would still only be able to learn a poor approximation of the regression function \\(\\bar{Y}(X)\\).\n\n\n\nCartoon illustration of bias and variance. (Source: http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n\n\nBoth bias and variance contribute to the risk/test error of our learned model.\nAs you might imagine, these two terms are often in tension with one another.\nFor example, a very simple model (e.g. linear regression with few features) will have high bias (it can’t capture complex patterns in the data), but low variance (it won’t change much if we retrain it on a different dataset).\nConversely, a very complex model (e.g. linear regression with millions of features) will have low bias (it can capture complex patterns in the data), but high variance (it will change a lot if we retrain it on a different dataset).\nBoth settings lead to high risk, but for different reasons.\nOptimizing risk requires balancing these two sources of error (e.g. finding the optimal number of features for the linear regression).\n\n\n\n\nBias and variance are at odds with one another. Effect of # of variables on bias/variance. (Figure derived from http://scott.fortmann-roe.com/docs/BiasVariance.html)\n\n\n\nWe will discuss other factors that affect bias and variance in future lectures (e.g. regularization, basis functions, etc.).\n\n\n\nDetecting High Bias and High Variance\nIf we want to make a modeling/estimation/prediction change to reduce risk, it’s important to know the source of the problem: is it high bias or high variance? As we will discuss in future lectures, these two regimes require different remedies, and applying the wrong remedy can make things worse!\n\n\nRegime 1: High Variance\nAlternative name: the overfitting regime\nSymptom: training error is much lower than cross validation error\nRemedies:\n\nAdd more training data\nReduce model complexity (complex models are prone to high variance)\n(More to come!)\n\n\n\nRegime 2: High Bias\nAlternative name: the underfitting regime\nSymptoms: training error and cross validation error are both high\nRemedies:\n\nAdd features\nUse a more complex statistical model\n(More to come!)"
  },
  {
    "objectID": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#summary",
    "href": "schedule/lectures/lecture_05_bias_variance_tradeoff.html#summary",
    "title": "Lecture 5: The Bias-Variance Tradeoff",
    "section": "Summary",
    "text": "Summary\n\nWe can understand sources of risk via the bias-variance decomposition\nEstimation error (the portion of risk we can control) decomposes into bias and variance\nBias reflects model assumptions and limitations\nVariance reflects sensitivity to training data\nDifferent remedies are appropriate for high bias versus high variance scenarios\nUnderstanding which regime you’re in should guide model selection strategies"
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html",
    "href": "schedule/lectures/lecture_07_lasso.html",
    "title": "Lecture 7: LASSO Regression",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement LASSO regression using both constrained and penalized formulations\nIdentify norm penalties that enforce sparsity versus shrinkage in parameter space\nCompare and contrast ridge regression vs LASSO in terms of variable selection and shrinkage\nReason about when LASSO, ridge, or variants are the most appropriate for a given problem\nConstruct a model selection procedure to choose between LASSO versus ridge"
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#learning-objectives",
    "href": "schedule/lectures/lecture_07_lasso.html#learning-objectives",
    "title": "Lecture 7: LASSO Regression",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement LASSO regression using both constrained and penalized formulations\nIdentify norm penalties that enforce sparsity versus shrinkage in parameter space\nCompare and contrast ridge regression vs LASSO in terms of variable selection and shrinkage\nReason about when LASSO, ridge, or variants are the most appropriate for a given problem\nConstruct a model selection procedure to choose between LASSO versus ridge"
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#recap-ridge-regression-shrinkage-versus-variable-selection",
    "href": "schedule/lectures/lecture_07_lasso.html#recap-ridge-regression-shrinkage-versus-variable-selection",
    "title": "Lecture 7: LASSO Regression",
    "section": "Recap: Ridge Regression, Shrinkage versus Variable Selection",
    "text": "Recap: Ridge Regression, Shrinkage versus Variable Selection\nSo far we have discussed two modelling choices to reduce variance in linear regression:\n\nVariable selection: use a subset of the features (e.g., best subset selection, forward stepwise selection, LAR)\nRidge regularization: apply a \\(\\lambda \\Vert \\beta \\Vert_2^2\\) penalty to the optimization objective to shrink coefficients towards zero\n(There’s always the option of adding more training data.)\n\n\nIt’s important to note that ridge and variable selection are doing VERY different things!\nRidge regression shrinks coefficients towards zero, but does not set any coefficients exactly to zero.\nVariable selection methods (best subset, forward stepwise, LAR) set some coefficients exactly to zero, but do not shrink the non-zero coefficients."
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#goal-automatic-variable-selection",
    "href": "schedule/lectures/lecture_07_lasso.html#goal-automatic-variable-selection",
    "title": "Lecture 7: LASSO Regression",
    "section": "Goal: Automatic Variable Selection?",
    "text": "Goal: Automatic Variable Selection?\n\nRidge added a constraint to our OLS optimization problem to limit the size of the coefficients.\nMaybe we could add a similar constraint to limit the number of non-zero coefficients?\n(Note that having \\(&lt; p\\) non-zero coefficients, or having \\(&gt; 0\\) coefficients set to zero, is equivalent to removing some covariates from the model.)\nSuch a constraint would yield the following optimization problem:\n\\[ \\mathrm{argmin}_{\\hat \\beta} \\frac 1 n \\sum_{i=1}^n (y_i - x_i^T \\hat\\beta)^2 \\quad \\text{subject to } \\Vert \\hat\\beta \\Vert_0 &lt; s\\]\nwhere \\(\\Vert \\hat\\beta \\Vert_0\\) is the number of non-zero entries in \\(\\hat\\beta\\).\nUnfortunately, this optimization problem takes \\(O(2^p)\\) time to solve.\n\n\nWhy?\n\n\nMathematically, it is a NP-hard optimization problem.\nIntuitively, solving this problem is equivalent to exhaustive subset search that you probably learned about in STAT 306. There are \\(2^p\\) possible subsets of \\(p\\) features, and we would need to evaluate the training error for each subset to find the best one.\n\n\nFortunately, we can solve a simpler optimization problem that encourages sparsity in the coefficients. (Mathematically, we solve a convex relaxation).\n\n\nSolution: Diamond Constraint Regions\n\nRecall that ridge constrained our solutions so that the \\(\\beta\\) vector lied within a circle (or sphere, or hypersphere) of radius \\(s\\).\nInstead, what if we set our constraint region to be a diamond?\n\n\n\n\nIllustration of the diamond constraint region for linear regression. It’s likely that, for a “tight-enough” diamond constraint, the minimal solution in the constraint region will live in one of the corners of the diamond, which corresponds to zero-ing out one or more coefficients.\n\n\n\nThe minimum sum-of-squares solution under this constraint will (not always, but often) be at a corner of the diamond, which is where one or more coefficients are exactly zero.\nCrucially, the sum-of-squares objective with this diamond constraint is a convex optimization problem that can be solved efficiently!"
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#lasso-regression",
    "href": "schedule/lectures/lecture_07_lasso.html#lasso-regression",
    "title": "Lecture 7: LASSO Regression",
    "section": "LASSO Regression",
    "text": "LASSO Regression\n\nLASSO regression aims to (approximately) do automated variable selection using this diamond constraint.\nIt is another regularized regression method, like ridge regression, which will reduce variance (at the expense of a bit of bias).\nUnlike ridge regression, the constraint/regularization yields a different effect on the learned coefficients: it encourages sparsity (i.e. some coefficients exactly zero) rather than just shrinkage (i.e. all coefficients small but non-zero).\n\n\nConstrained Formulation\n\nThis diamond constraint region is mathematically defined by the set of all \\(\\hat \\beta\\) where\n\\[ \\Vert \\hat\\beta \\Vert_1 := \\sum_{j=1}^p |\\hat\\beta_j| &lt; s\\]\nwhich yields the constrained optimization problem\n\\[ \\mathrm{argmin}_{\\hat \\beta} \\frac 1 n  \\sum_{i=1}^n (y_i - x_i^T \\hat\\beta)^2 \\quad \\text{subject to } \\Vert \\hat\\beta \\Vert_1 &lt; s\\]\nCompare this to the ridge regression constrained optimization problem. For ridge, the constraint was \\(\\Vert \\hat\\beta \\Vert_2^2 &lt; s\\) (i.e. a circle/sphere/hypersphere constraint rather than a diamond). Otherwise the problems are identical.\n\n\n\nRegularized (Penalized) Formulation\n\nAs with ridge regression, we can rewrite the constrained optimization problem as a penalized optimization problem:\n\\[ \\mathrm{argmin}_{\\hat \\beta} \\frac 1 n  \\sum_{i=1}^n (y_i - x_i^T \\hat\\beta)^2 + \\lambda \\Vert \\hat\\beta \\Vert_1 \\]\nwhere again each value of \\(s\\) in our constrained formulation corresponds to a value of \\(\\lambda\\) in our penalized formulation.\n\n\n\nComputing the LASSO Estimator\n\nUnlike ridge regression, there is no closed-form solution for the LASSO estimator \\(\\hat\\beta_\\lambda\\).\nInstead, we must use numerical optimization to solve the penalized optimization problem.\n\n\n\n\n\n\n\nTipHow do we Numerically Optimize the LASSO Objective?\n\n\n\n\n\n(Advanced content for those who are interested)\n\nThe LASSO problem is a convex optimization problem, so typically optimizers like gradient descent will (in theory) converge to the global optimum.\nHowever, gradient descent doesn’t do well with the non-differentiable corners of the \\(\\ell_1\\) norm.\nInstead, it’s common to use coordinate descent (which is what glmnet uses).\nProximal gradient descent algorithms are also popular for solving the LASSO problem."
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#aside-regularization-with-other-vector-norms",
    "href": "schedule/lectures/lecture_07_lasso.html#aside-regularization-with-other-vector-norms",
    "title": "Lecture 7: LASSO Regression",
    "section": "Aside: Regularization with Other Vector Norms",
    "text": "Aside: Regularization with Other Vector Norms\n\nThe ridge regularization penalty (\\(\\Vert \\beta \\Vert_2^2\\)), the LASSO regularization penalty (\\(\\Vert \\beta \\Vert_1\\)), and the “sparsity” penalty (\\(\\Vert \\beta \\Vert_0\\)) are all examples of vector norm penalties.\nMore specifically, each is an example of a \\(\\ell_p\\)-norm, where the general form is given by:\n\\[\\Vert \\beta \\Vert_p = \\left( \\sum_{j=1}^p |\\beta_j|^p \\right)^{1/p}\\]\nBelow is a plot of values of the constraint \\(\\Vert \\beta \\Vert_p = 1\\) for various values of \\(p\\):\nAny of these norms could be used as a regularization penalty in linear regression (though some may yield hard-to-solve optimization problems).\n\n\n\n\nIllustration of unit norm balls for various values of p.\n\n\n\nImportant Special Cases\n\n\\(p = 0\\) is the number of non-zero entries in \\(\\beta\\) (the “sparsity” penalty)\n\\(p = \\infty\\) is the maximum absolute value of any entry in \\(\\beta\\) (the “max” penalty)\n\\(p \\geq 1\\) gives a convex function of \\(\\beta\\), which makes optimization nice\n\\(p &lt; 2\\) gives a norm with “corners” (i.e., non-differentiable points on the axes), which encourage sparsity"
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#example-lasso-versus-ridge-regression",
    "href": "schedule/lectures/lecture_07_lasso.html#example-lasso-versus-ridge-regression",
    "title": "Lecture 7: LASSO Regression",
    "section": "Example: LASSO versus Ridge Regression",
    "text": "Example: LASSO versus Ridge Regression\n\nHere we’re going to run both ridge and LASSO regression on the prostate cancer dataset from Lab 01.\nI will do a CV sweep to find the best regularization parameter \\(\\lambda\\) for each method.\nAgain we’ll use glmnet to fit both models.\n\nThe argument alpha = 1 in cv.glmnet specifies LASSO, while alpha = 0 specifies ridge regression.\n\n\n\nlibrary(glmnet)\ndata(prostate, package = \"ElemStatLearn\")\nX &lt;- prostate |&gt; dplyr::select(-train, -lpsa) |&gt;  as.matrix()\nY &lt;- prostate$lpsa\nlasso &lt;- cv.glmnet(x = X, y = Y) # alpha = 1 by default\nridge &lt;- cv.glmnet(x = X, y = Y, alpha = 0)\n\n\nNow let’s plot the CV curves and coefficient paths for both ridge and LASSO as functions of their regularization parameter \\(\\lambda\\).\nNote that we cannot compare the \\(\\lambda\\) values directly between ridge and LASSO, since they are on different scales.\n(The \\(\\lambda\\) values also don’t have any intrinsic meaning. Larger values imply more regularization, but a \\(\\lambda = 4.7\\) means nothing in isolation.)\n\n\npar(mfrow = c(2, 2))\n# Get y-axis range for consistent scaling\nridge_mse &lt;- range(c(ridge$cvm - ridge$cvsd, ridge$cvm + ridge$cvsd))\nlasso_mse &lt;- range(c(lasso$cvm - lasso$cvsd, lasso$cvm + lasso$cvsd))\nylim_range &lt;- range(ridge_mse, lasso_mse)\nplot(ridge, main = \"Ridge CV\", ylim = ylim_range)\nplot(lasso, main = \"LASSO CV\", ylim = ylim_range)\n\n# Get coefficient range for consistent scaling\nridge_coef_range &lt;- range(ridge$glmnet.fit$beta)\nlasso_coef_range &lt;- range(lasso$glmnet.fit$beta)\ncoef_ylim &lt;- range(ridge_coef_range, lasso_coef_range)\nplot(ridge$glmnet.fit, main = \"Ridge Coeffs.\", xvar = \"lambda\", ylim = coef_ylim)\nabline(h = 0, lty = 2)\nplot(lasso$glmnet.fit, main = \"LASSO Coeffs.\", xvar = \"lambda\", ylim = coef_ylim)\nabline(h = 0, lty = 2)\n\n\n\n\n\n\n\n\n\nThe numbers on the top of the plots indicate the number of non-zero coefficients at each value of \\(\\lambda\\).\nNotice that ridge regression always has all 8 coefficients non-zero, while LASSO sets some coefficients exactly to zero for larger values of \\(\\lambda\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#when-should-i-consider-using-ridge-over-lasso",
    "href": "schedule/lectures/lecture_07_lasso.html#when-should-i-consider-using-ridge-over-lasso",
    "title": "Lecture 7: LASSO Regression",
    "section": "When Should I Consider using Ridge over LASSO?",
    "text": "When Should I Consider using Ridge over LASSO?\n\nIf all of the covariates are truly relevant, and dropping too many of them would hurt accuracy\n\n\nWhy?\n\n\nLASSO tends to select only a few variables, and set the rest to zero. If all variables are relevant, this can lead to underfitting.\nRidge regression, on the other hand, shrinks all coefficients towards zero, but keeps them all in the model.\n\n\nIf I create a one-hot encoding of a categorical variable with many levels (e.g., ZIP code)\n\n\nWhy?\n\n\nLASSO might drop some of the levels but not others, which can be hard to interpret\nRidge regression will tend to keep all levels, but shrink them towards zero\nA variant of LASSO called the group LASSO can be used to select or drop all levels of a categorical variable together.\n\n\nIf computational resources are limited\n\n\nWhy?\n\n\nLASSO requires numerical optimization to solve, while ridge regression has a closed-form solution.\nWe have to run multiple optimization problems for each value of \\(\\lambda\\) we want to test in cross-validation.\nRidge regression only requires a single SVD, which can be reused for all values of \\(\\lambda\\).\nMoreover, as we will see in a later lecture, we can compute the LOO-CV risk estimate for ridge almost for free, while LASSO requires refitting the model \\(n\\) times.\n\n\n\n\n\n\n\n\n\nCautionWhen Should I Avoid Using Both Ridge and LASSO?\n\n\n\nIf I’m in a high-bias/low-variance regime to begin with, I shouldn’t use either method.\n\n\nWhy?\n\n\nRegularization (both ridge and LASSO) increases bias but reduces variance.\nIf the un-regularized model is already low variance, regularization will likely hurt performance."
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#important-practical-considerations-when-using-lasso-or-ridge-regression",
    "href": "schedule/lectures/lecture_07_lasso.html#important-practical-considerations-when-using-lasso-or-ridge-regression",
    "title": "Lecture 7: LASSO Regression",
    "section": "Important Practical Considerations when Using LASSO or Ridge Regression",
    "text": "Important Practical Considerations when Using LASSO or Ridge Regression\n\nThe intercept \\(\\hat \\beta_0\\) is never penalized!\n\n\nWhy?\n\n\nThe intercept parameter is an estimate of the mean of \\(Y\\), and we generally do not want to shrink this estimate.\nIn practice, we usually standardize the response to have mean zero and variance one before fitting a regularized regression model.\n\n\nIt’s important to center the features (i.e. subtract off each covariate’s empirical mean) before fitting.\n\n\nWhy?\n\n\nIf the features are not centered, the intercept will implicitly be penalized, which can lead to suboptimal performance.\nCentering the features ensures that the intercept is the mean of \\(Y\\) when all other covariates are zero.\nStandardizing the features (i.e. scaling them to have unit variance) is also common, but not strictly necessary for LASSO or ridge regression.\n\n\nCategorical parameters require special care with LASSO.\n\n\nWhy?\n\n\nSee above\n\n\nRidge or LASSO penalties can be used in other (non-linear regression) models."
  },
  {
    "objectID": "schedule/lectures/lecture_07_lasso.html#summary",
    "href": "schedule/lectures/lecture_07_lasso.html#summary",
    "title": "Lecture 7: LASSO Regression",
    "section": "Summary",
    "text": "Summary\n\nRidge regression applies an \\(\\ell_2\\) penalty to shrink coefficients towards zero, but does not set any coefficients exactly to zero.\nLASSO regression applies an \\(\\ell_1\\) penalty to encourage sparsity in the coefficients, setting some coefficients exactly to zero.\nLASSO requires numerical optimization to solve, and care is needed when using categorical variables.\nBoth ridge and LASSO increase bias but reduce variance.\nThe choice between ridge and LASSO depends on the specific problem and goals of the analysis."
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html",
    "href": "schedule/lectures/lecture_09_information_criteria.html",
    "title": "Lecture 9: Information Criteria",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nCompute degrees of freedom for OLS/ridge/basis regression models\nApply GCV formulas to select regularization parameters\nIdentify nested models and when GCV/information criteria can and cannot be used for model selection\nConnect degrees of freedom to the bias-variance tradeoff"
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#learning-objectives",
    "href": "schedule/lectures/lecture_09_information_criteria.html#learning-objectives",
    "title": "Lecture 9: Information Criteria",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nCompute degrees of freedom for OLS/ridge/basis regression models\nApply GCV formulas to select regularization parameters\nIdentify nested models and when GCV/information criteria can and cannot be used for model selection\nConnect degrees of freedom to the bias-variance tradeoff"
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#overview-and-motivation",
    "href": "schedule/lectures/lecture_09_information_criteria.html#overview-and-motivation",
    "title": "Lecture 9: Information Criteria",
    "section": "Overview and Motivation",
    "text": "Overview and Motivation\nToday is our last lecture on the bias-variance tradeoff for linear models (though this trade-off will come up again and again in this course). We’ve discussed a few methods for reducing risk, whose applicability depends on whether we’re in the high variance or high bias regime.\nTechniques for reducing variance:\n\nAdding more training data\nVariable selection (manually removing predictors, LASSO regularization)\nShrinkage (ridge regularization)\n\nTechniques for reducing bias:\n\nVariable selection (manually adding predictors)\nBasis expansions (polynomial, splines, etc.)\n\nToday, we’ll revisit the concept of model selection, the concluding topic of the first module, but this time with an eye towards the bias-variance tradeoff.\n\nWe’ll learn a fun trick for computing the LOO-CV estimate of risk for OLS, ridge, and basis regression models without having to refit the model \\(n\\) times.\nWe’ll also learn a new risk estimate, generalized cross-validation (GCV), which is rarely used in practice these days but which gives us a lot of insight into the bias-variance tradeoff."
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#review-loo-cv-formula-for-ols",
    "href": "schedule/lectures/lecture_09_information_criteria.html#review-loo-cv-formula-for-ols",
    "title": "Lecture 9: Information Criteria",
    "section": "Review: LOO-CV Formula For OLS",
    "text": "Review: LOO-CV Formula For OLS\n\nRecall that LOO-CV is an almost-unbiased estimate of the risk of a predictive model \\(\\mathcal R = \\mathbb E[(Y - \\hat{f}(X))^2]\\) (where here we are assuming the use of the squared error loss).\nConsider our OLS estimator:\n\\[\n\\hat{\\beta}_{\\text{OLS}} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top \\boldsymbol{Y}\n\\]\nwhere, again, \\(\\boldsymbol X\\) and \\(\\boldsymbol Y\\) are the concatenations of our training data \\(\\{ (X_i, Y_i) \\}_{i=1}^n\\):\n\\[\n\\boldsymbol{X} = \\begin{bmatrix}--- & X_1^\\top & --- \\\\\n--- & X_2^\\top & --- \\\\\n& \\vdots & \\\\\n--- & X_n^\\top & --- \\end{bmatrix} \\in \\mathbb R^{n \\times p}\n\\quad \\text{and} \\quad\n\\boldsymbol{Y} = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} \\in \\mathbb R^{n}.\n\\]\nOur OLS predictions on the training data are:\n\\[\n\\hat{\\boldsymbol{Y}} = \\boldsymbol{X} \\hat{\\beta}_{\\text{OLS}} = \\overbrace{\\boldsymbol{X} (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top}^{:= \\boldsymbol H} \\boldsymbol{Y} = \\boldsymbol{H} \\boldsymbol{Y}.\n\\]\nThe matrix \\(\\boldsymbol H = \\boldsymbol{X} (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top\\) is often referred to as the hat matrix, and it will be important in a second.\nThe LOO-CV estimate of risk involves, for each \\(i = 1, \\ldots, n\\), computing the OLS estimator for the dataset \\(\\boldsymbol X_{-i}, \\boldsymbol Y_{-i}\\) (which is \\(\\boldsymbol X, \\boldsymbol Y\\) with the \\(i^\\mathrm{th}\\) row removed, and make predictions on \\(X_i\\):\n\\[\n\\hat{Y}^{-i}(X_i) = X_i^\\top \\hat{\\beta}_{\\text{OLS}}^{-i} = X_i^\\top (\\boldsymbol{X}_{-i}^\\top \\boldsymbol{X}_{-i})^{-1} \\boldsymbol{X}_{-i}^\\top \\boldsymbol{Y}_{-i}.\n\\]"
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#the-magic-loo-cv-formula-for-ols-and-some-other-linear-models",
    "href": "schedule/lectures/lecture_09_information_criteria.html#the-magic-loo-cv-formula-for-ols-and-some-other-linear-models",
    "title": "Lecture 9: Information Criteria",
    "section": "The Magic LOO-CV Formula for OLS (and Some Other Linear Models)",
    "text": "The Magic LOO-CV Formula for OLS (and Some Other Linear Models)\n\nThe LOO-CV estimate above requires \\(n\\) matrix inversions \\((\\boldsymbol{X}_{-i}^\\top \\boldsymbol{X}_{-i})^{-1}\\), which is computationally expensive.\nMoreover, the matrix inversion \\((\\boldsymbol{X}_{-i}^\\top \\boldsymbol{X}_{-i})^{-1}\\) is not too different from the matrix inversion \\((\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1}\\) that we already computed to get \\(\\boldsymbol H\\). Shouldn’t we be able to reuse that computation somehow?\nIt turns out that we can! With some ugly linear algebra, we can show that the LOO-CV predictions for OLS can be computed as:\n\\[\n\\hat{Y}^{-i}(X_i) = \\frac{\\hat{Y}_i - H_{ii} Y_i}{1 - H_{ii}}^2,\n\\]\nwhere \\(\\hat{Y}_i\\) is the \\(i^\\mathrm{th}\\) entry of \\(\\hat{\\boldsymbol{Y}} = \\boldsymbol{H} \\boldsymbol{Y}\\), and \\(H_{ii}\\) is the \\(i^\\mathrm{th}\\) diagonal entry of the hat matrix \\(\\boldsymbol{H}\\). ` :::{.callout-tip collapse=“true” title=“Deriving the Magic Formula”} The derivation of this formula is tedious (and not for the faint of heart), but all it requires is some linear algebra. The key idea is to express\n\n\\[(\\boldsymbol{X}_{-i}^\\top \\boldsymbol{X}_{-i})^{-1} = (\\boldsymbol{X}^\\top \\boldsymbol{X} - X_i X_i^\\top)^{-1}\\]\nand then use the Sherman-Morrison formula for inverting rank-one updates to a matrix. (Come see me in office hours if want to suffer through the details!) :::\n\nApplying this formula to estimate the risk associated with the square loss, the LOO-CV estimate of OLS risk is given by:\n\n\\[\n\\begin{aligned}\n\\widehat{\\mathcal{R}}_{\\text{LOO}}\n&= \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}^{-i}(X_i))^2\n\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{(1 - H_{ii}) Y_i}{1 - H_{ii}} - \\frac{\\hat{Y}_i - H_{ii} Y_i}{1 - H_{ii}} \\right)^2\n\\\\\n&= \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{Y_i - \\hat{Y}_i}{1 - H_{ii}} \\right)^2.\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNoteLOO-CV Formula for Ridge and Basis Regression\n\n\n\nThis formula holds for any so-called linear smoother, where the predictions on the training data can be expressed as:\n\\[\\hat{\\boldsymbol{Y}} = \\boldsymbol{H} \\boldsymbol{Y} \\quad \\text{for some } \\boldsymbol H \\in \\mathbb R^{n \\times n}.\\]\nHere’s some examples of linear smoothers (and their corresponding \\(\\boldsymbol H\\) matrices):\n\n\n\n\n\n\n\nMethod\n\\(\\boldsymbol{H}\\) Matrix\n\n\n\n\nOLS\n\\(\\boldsymbol H = \\boldsymbol{X} (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top\\)\n\n\nRidge Regression\n\\(\\boldsymbol H = \\boldsymbol{X} (\\boldsymbol{X}^\\top \\boldsymbol{X} + \\lambda \\boldsymbol{I})^{-1} \\boldsymbol{X}^\\top\\)\n\n\nBasis Regression\n\\(\\boldsymbol H = \\widetilde{\\boldsymbol{X}} (\\widetilde{\\boldsymbol{X}}^\\top \\widetilde{\\boldsymbol{X}})^{-1} \\widetilde{\\boldsymbol{X}}^\\top\\)\n\n\nLasso Regression\nN/A (cannot be expressed as a linear smoother)\n\n\n\nwhere the \\(\\widetilde{\\boldsymbol{X}}\\) matrix is the design matrix after applying basis expansions to the features; i.e.:\n\\[ \\widetilde{\\boldsymbol{X}} = \\begin{bmatrix}--- & \\phi(X_1)^\\top & --- \\\\\n--- & \\phi(X_2)^\\top & --- \\\\\n& \\vdots & \\\\\n--- & \\phi(X_n)^\\top & --- \\end{bmatrix} \\in \\mathbb R^{n \\times d}, \\]\nwhere \\(\\phi: \\mathbb R^p \\to \\mathbb R^d\\) is the function that produces the basis expansion of a feature vector."
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#generalized-cross-validation-gcv",
    "href": "schedule/lectures/lecture_09_information_criteria.html#generalized-cross-validation-gcv",
    "title": "Lecture 9: Information Criteria",
    "section": "Generalized Cross-Validation (GCV)",
    "text": "Generalized Cross-Validation (GCV)\n\nTo better understand the bias-variance tradeoff, it helps to simplify the LOO-CV formula above.\nConsider what happens if we replace each \\(H_{ii}\\) with the average value of the diagonal entries of \\(\\boldsymbol H\\):\n\\[\n\\frac{1}{n} \\sum_{i=1}^n H_{ii} = \\frac{1}{n} \\text{trace}(\\boldsymbol{H}) = \\frac{\\text{df}}{n},\n\\]\nwhere \\(\\text{df} := \\text{trace}(\\boldsymbol{H})\\) is called the degrees of freedom of the model.\nThe resulting estimate is called the generalized cross-validation (GCV) estimate of risk:\n\\[\n\\widehat{\\mathcal{R}}_{\\text{GCV}}\n= \\frac{\\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2}{(1 - \\text{df}/n)^2}\n= \\frac{\\mathrm{MSE}_\\mathrm{train}}{(1 - \\text{df}/n)^2},\n\\]\nwhere \\(\\mathrm{MSE}_\\mathrm{train} = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2\\) is the mean squared error on the training set.\n\n\n\n\n\n\n\nTipDegrees of Freedom for Linear Models\n\n\n\nThe degrees of freedom \\(\\text{df} = \\text{trace}(\\boldsymbol{H})\\) has an intuitive interpretation as the “effective number of parameters” in the model.\n\n\n\n\n\n\n\nMethod\nDegrees of Freedom\n\n\n\n\nOLS\n\\(\\text{df} = \\mathrm{tr}(\\boldsymbol X ( \\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top) = p\\)\n\n\nRidge Regression\n\\(\\text{df} = \\mathrm{tr}(\\boldsymbol X ( \\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top) = \\sum_{i=1}^p \\frac{d_i^2}{d_i^2 + \\lambda} &lt; p\\)\n\n\nBasis Regression\n\\(\\text{df} = \\mathrm{tr}(\\widetilde{\\boldsymbol X} ( \\widetilde{\\boldsymbol X}^\\top \\widetilde{\\boldsymbol X})^{-1} \\widetilde{\\boldsymbol X}^\\top) = d\\)\n\n\n\nwhere \\(d_1, \\ldots, d_p\\) are the singular values of \\(\\boldsymbol X\\).\n\n\nDerivation\n\nEach of these can be derived using the singular value decomposition (SVD) of \\(\\boldsymbol X = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top\\) (or \\(\\widetilde{\\boldsymbol X} = \\widetilde{\\boldsymbol U} \\widetilde{\\boldsymbol D} \\widetilde{\\boldsymbol V}^\\top\\) for basis regression). Let’s look at ridge regression:\n\\[\\begin{align*}\n\\text{df} &= \\mathrm{tr}(\\boldsymbol X ( \\boldsymbol X^\\top \\boldsymbol X + \\lambda \\boldsymbol I)^{-1} \\boldsymbol X^\\top) \\\\\n&= \\mathrm{tr}(\\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top (\\boldsymbol V \\boldsymbol D^\\top \\boldsymbol D \\boldsymbol V^\\top + \\lambda \\boldsymbol I)^{-1} \\boldsymbol V \\boldsymbol D^\\top \\boldsymbol U^\\top) \\\\\n&= \\mathrm{tr}(\\boldsymbol U \\boldsymbol D (\\boldsymbol D^\\top \\boldsymbol D + \\lambda \\boldsymbol I)^{-1} \\boldsymbol D^\\top \\boldsymbol U^\\top) \\\\\n&= \\mathrm{tr}(\\boldsymbol D (\\boldsymbol D^\\top \\boldsymbol D + \\lambda \\boldsymbol I)^{-1} \\boldsymbol D^\\top) \\\\\n&= \\sum_{i=1}^p \\frac{d_i^2}{d_i^2 + \\lambda}\n\\end{align*}\\]\n\n(Note that the “degrees of freedom” concept exists for methods that don’t admit the closed-form \\(\\boldsymbol H\\) matrix, like LASSO, but their derivation is PhD-level work.)\n\n\nFrom the GCV estimate of risk, we can see how the bias-variance tradeoff plays out. There are two ways for \\(\\widehat{\\mathcal{R}}_{\\text{GCV}}\\) to blow up:\n\nThe training error \\(\\mathrm{MSE}_\\mathrm{train}\\) is large (causing the numerator to blow up), which happens when the predictive model is unable to fit the training data well (high bias).\nThe degrees of freedom \\(\\text{df}\\) is close to \\(n\\) (causing the denominator to \\(\\to 0\\)), which happens when the predictive model has nearly as many “degrees of freedom” as training data (high variance).\n\n\n\n\n\n\n\nNoteInformation Criteria\n\n\n\nThe GCV estimate of risk is an example of an information criterion, which is a family of risk estimates that do not require a validation set or cross-validation. There are many others that you may encounter in the literature, including:\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\nMallows’ \\(C_p\\) (or Stein’s Unbiased Risk Estimate/SURE)\n\nEach of these has its own derivation and assumptions, but they all are similar in spirit to GCV. They all involve a tradeoff between training error and model complexity (degrees of freedom); i.e. a bias-variance tradeoff."
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#what-happens-to-gcv-when-p-geq-n",
    "href": "schedule/lectures/lecture_09_information_criteria.html#what-happens-to-gcv-when-p-geq-n",
    "title": "Lecture 9: Information Criteria",
    "section": "What Happens to GCV When \\(p \\geq n\\)?",
    "text": "What Happens to GCV When \\(p \\geq n\\)?\n\nWhen \\(p = n\\), note that our OLS model has \\(\\text{df} = p = n\\), so the GCV estimate of risk is infinite!\nWhen \\(p &gt; n\\), OLS is no longer defined (because the matrix \\(\\boldsymbol{X}^\\top \\boldsymbol{X}\\) is not invertible), so we can’t even compute the GCV estimate of risk.\nRidge regression can still be applied when \\(p \\geq n\\). However, and importantly, its \\(\\text{df}\\) does not exceed \\(n\\).\n\n\nWhy not?\n\n\nFor ridge, we have that \\(\\text{df} = \\sum_{i=1}^p \\frac{d_i^2}{d_i^2 + \\lambda}\\), where \\(d_1, \\ldots, d_p\\) are the (potentially zero) singular values of \\(\\boldsymbol X\\).\nWhen \\(p &gt; n\\), there are only \\(n\\) non-zero singular values, so there are only \\(n\\) non-zero terms in the sum.\nEach of the non-zero terms is strictly less than 1, so \\(\\text{df} &lt; n\\).\n\n\nWe’ll explore this last point a lot more in the next module!"
  },
  {
    "objectID": "schedule/lectures/lecture_09_information_criteria.html#summary",
    "href": "schedule/lectures/lecture_09_information_criteria.html#summary",
    "title": "Lecture 9: Information Criteria",
    "section": "Summary",
    "text": "Summary\n\nThere’s a magical formula for computing LOO-CV risk estimates for OLS, ridge, and basis regression models without having to refit the model \\(n\\) times. Use it!\nThe GCV estimate of risk is a simplified version of cross validation that (1) can be applied even when the “magic formula” doesn’t hold, and (2) does not require a validation set.\nThe GCV estimate of risk makes the bias-variance tradeoff explicit: risk is high when training error is high (high bias) or when degrees of freedom is close to \\(n\\) (high variance).\nIn general, CV/LOO-CV is preferred over GCV/information criteria for model selection, but GCV is a useful tool for understanding the bias-variance tradeoff.\nWe will explore what happens when the number of degrees of freedom exceeds the number of training samples in the next lecture!"
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement k-nearest neighbours/kernel smoothing for regression and classification\nAnalyze the bias-variance tradeoff as a function of k/bandwidth\nEstimate the degrees of freedom for kNN/kernel smoothed models\nDerive how prediction error scales with dimensionality for basis expansions, kNN, and kernel smoothing\nCompare the dimensional scaling of parametric vs nonparametric methods"
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#learning-objectives",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#learning-objectives",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nImplement k-nearest neighbours/kernel smoothing for regression and classification\nAnalyze the bias-variance tradeoff as a function of k/bandwidth\nEstimate the degrees of freedom for kNN/kernel smoothed models\nDerive how prediction error scales with dimensionality for basis expansions, kNN, and kernel smoothing\nCompare the dimensional scaling of parametric vs nonparametric methods"
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#motivation",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#motivation",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "Motivation",
    "text": "Motivation\n\nIn the last lecture, we considered ridge regression with \\(d &gt; n\\) basis expansions.\nWhen we took ridge regression with Fourier bases to the limit, we ended up with the following predictor:\n\\[\n\\hat{f}(X) = \\sum_{i=1}^n \\alpha_i K(X, X_i)\n\\]\n\n\\(K(X, X_i) = \\exp\\left( -\\tfrac{1}{2 \\gamma^2} \\Vert X - X_i \\Vert_2^2 \\right)\\) is a kernel function that measures similarity between \\(X\\) and \\(X_i\\).\n\\(\\gamma\\), the bandwidth, is a hyperparameter that can be tuned with cross validation.\nThe coefficients \\(\\alpha_i\\) can be computed in closed form, and depend on the ridge penalty \\(\\lambda\\).\n\nThis nonparametric model has two notable properties:\n\nIt defines its predictions through similarity between training points (rather than a fixed set of parameters \\(\\beta\\))\nIts degrees of freedom scales with \\(n\\) (the number of training points) rather than being fixed at \\(p\\) (the number of covariates).\n\n\n\nThe Best Possible Model?\nThere are many advantages of this nonparametric model:\n\nIt is infinitely powerful. Recall that an infinite Fourier series can represent (almost) any function, so this predictive model can represent almost any \\(\\mathbb E[Y|X]\\). (In contrast, linear regression can only represent linear \\(\\mathbb E[Y|X]\\), polynomial regression with \\(d^\\mathrm{th}\\) degree polynomials can only represent \\(\\mathbb E[Y|X]\\) that are polynomials of degree \\(d\\), etc.)\nIt is simple. Amazingly, we can write this complex model in a single line of math, and we can even derive the \\(\\alpha_i\\) coefficients in closed form.\nWe can control its variance. Even though the model is infinitely powerful, with a large enough ridge penalty \\(\\lambda\\) we can construct a model that balances bias and variance well.\n\nSo why would we ever consider using anything else? Why would a linear model ever be preferable to this?\n\n\nThe Drawback\n\nWhile this model is extremely powerful (and often very useful), it suffers from a phenomenon known as the curse of dimensionality.\nBriefly, this model (potentially) becomes exponentially less effective as we increase the number of covariates \\(p\\).\nTo understand this phenomenon, we will consider a related nonparametric model known as k-nearest neighbours (kNN) which will be easier to analyze."
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#our-second-nonparametric-model-k-nearest-neighbours",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#our-second-nonparametric-model-k-nearest-neighbours",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "Our Second Nonparametric Model: k-Nearest Neighbours",
    "text": "Our Second Nonparametric Model: k-Nearest Neighbours\n\nIf we analyze our kernel function:\n\\[\nK(X, X_i) = \\exp\\left( -\\tfrac{1}{2 \\gamma^2} \\Vert X - X_i \\Vert_2^2 \\right),\n\\]\nwe see that it is largest when \\(X\\) is close to \\(X_i\\), and decays to zero as \\(X\\) moves away from \\(X_i\\).\nIt is likely that this kernel function will be \\(\\approx 1\\) for only a few training points \\(X_i\\) that are close to \\(X\\), and \\(\\approx 0\\) for most other training points.\nTherefore, what if we make the following approximation to the kernel function:\n\\[\nK(X, X_i) \\approx \\begin{cases}\n1 & \\text{if } X_i \\text{ is one of the $k$ nearest neighbours of } X \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nwhere the nearest neighbours of \\(X\\) are the \\(k\\) training points \\(X_i\\) that are closest to \\(X\\) in Euclidean distance.\n\nHere, \\(k\\) (the number of nearest neighbours) is a hyperparameter that we will touch upon in a second.\nThis plot depicts the k-nearest neighbours (kNN) function with \\(k=3\\). The red points are the 3 nearest training \\(X_i\\)s of \\(X = 85\\) (the red vertical line).\n\n\nCode\nlibrary(tidyverse)\nlibrary(cowplot)\ndata(arcuate, package = \"Stat406\")\nset.seed(406406)\narcuate_unif &lt;- arcuate |&gt; slice_sample(n = 40) |&gt; arrange(position)\ntest_position &lt;- 85\nnn &lt;-  3\nseq_range &lt;- function(x, n = 101) seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out = n)\nneibs &lt;- sort.int(abs(arcuate_unif$position - test_position), index.return = TRUE)$ix[1:nn]\narcuate_unif$neighbours = seq_len(40) %in% neibs\nggplot(arcuate_unif, aes(position, fa, colour = neighbours)) +\n  geom_point() +\n  scale_colour_manual(values = c(\"blue\", \"red\")) +\n  geom_vline(xintercept = test_position, colour = \"red\") +\n  annotate(\"rect\", fill = \"red\", alpha = .25, ymin = -Inf, ymax = Inf,\n          xmin = min(arcuate_unif$position[neibs]),\n          xmax = max(arcuate_unif$position[neibs])\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\nWe could now approximate the \\(\\alpha_i\\) coefficients with an equal weighting of the responses of the \\(k\\) nearest neighbours:\n\\[\n\\alpha_i \\approx \\begin{cases}\n\\frac{1}{k} Y_i & \\text{if } X_i \\text{ is one of the $k$ nearest neighbours of } X \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nThe result is the following k-nearest neighbours (kNN) predictor:\n\\[\n\\hat{f}_\\mathcal{D}(X) = \\frac{1}{k} \\sum_{i \\in N_k(X)} Y_i,\n\\qquad \\frac{1}{k} \\sum_{i \\in N_k(X)} Y_i \\approx \\sum_{i=1}^n \\alpha_i K(X, X_i)\n\\]\nwhere \\(N_k(X)\\) is the set of indices of the \\(k\\) nearest neighbours of \\(X\\).\n\n\n\n\n\n\n\nTipkNN is a Common Method\n\n\n\nWhile we have derived kNN as an approximation to kernel ridge regression, it is a very well-used method in practice. (It is arguably the most common nonparametric method, and used in practice more than kernel ridge regression.)\n\n\n\nBias and Variance as a Function of k\n\n\nCode\nplot_knn &lt;- function(k) {\n  ggplot(arcuate_unif, aes(position, fa)) +\n    geom_point(colour = \"blue\") +\n    geom_line(\n      data = tibble(\n        position = seq_range(arcuate_unif$position),\n        fa = FNN::knn.reg(\n          arcuate_unif$position, matrix(position, ncol = 1),\n          y = arcuate_unif$fa,\n          k = k\n        )$pred\n      ),\n      colour = \"orange\", linewidth = 2\n    ) + ggtitle(paste(\"k =\", k))\n}\n\ng1 &lt;- plot_knn(1)\ng2 &lt;- plot_knn(5)\ng3 &lt;- plot_knn(length(arcuate_unif$position))\nplot_grid(g1, g2, g3, ncol = 3)\n\n\n\n\n\n\n\n\n\n\nThe above plots show the kNN regression function for different values of \\(k\\).\n\\(k=1\\) and \\(k=40\\) are poor fits and have high risk, while \\(k=5\\) is likely a good predictor.\nWhen \\(k\\) is too large or too small, we are not balancing bias and variance well. But which extreme are we in?\n\n\n\n\n\n\n\nNote(Intuitive) Bias-Variance Tradeoff for kNN\n\n\n\n\n\n\nAs \\(k \\to 1\\), our prediction at a point \\(X\\) depends only on a single training point \\((X_i, Y_i)\\), so it is very dependent on our specific training sample.\nAs \\(k \\to \\infty\\), all of our training points are used to make predictions at any point \\(X\\). \\(\\hat{f}_\\mathcal{D}(X) \\to \\frac{1}{n} \\sum_{i=1}^n Y_i\\) becomes the sample mean of the \\(Y_i\\), which is not going to be a good estimate of \\(\\mathbb E[Y|X]\\) regardless of our specific training sample.\nThus, small \\(k\\) leads to high variance, while large \\(k\\) leads to high bias.\n\n\n\n\n\n\nDegrees of Freedom of kNN\n\nAnother way we can analyze the bias-variance tradeoff of kNN is through its degrees of freedom.\nFirst, we note that the predictions on the training data \\(\\hat{\\boldsymbol Y} = [ \\hat Y_1, \\ldots, \\hat Y_n ]^\\top\\) can be written in matrix form as:\n\\[\n\\hat{\\boldsymbol Y} = \\underbrace{\\boldsymbol H}_{n \\times n} \\underbrace{\\boldsymbol Y}_{n \\times 1},\n\\qquad\nH_{ij} = \\begin{cases}\n\\frac{1}{k} & \\text{if } X_j \\text{ is one of the $k$ nearest neighbours of } X_i \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\]\nFor predictive models that can be written in this form, recall that the degrees of freedom is given by:\n\\[\n\\mathrm{df}_\\mathrm{kNN} = \\mathrm{trace}(\\boldsymbol H) = \\sum_{i=1}^n H_{ii}\n\\]\nFinally, noting that \\(X_i\\) is always a nearest neighbour of itself, we have \\(H_{ii} = \\frac{1}{k}\\) for all \\(i\\), and thus:\n\\[\n\\mathrm{df}_\\mathrm{kNN} = \\sum_{i=1}^n H_{ii} = \\sum_{i=1}^n \\frac{1}{k} = \\frac{n}{k}\n\\]\nIncreasing \\(k\\) leads to fewer degrees of freedom (i.e. a less flexible model, lower variance, etc.), but note that \\(\\mathrm{df} = O(n)\\) for any fixed \\(k\\).\nThus, kNN is non-parametric because its flexibility increases with the amount of training data."
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#the-curse-of-dimensionality",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#the-curse-of-dimensionality",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "The Curse of Dimensionality",
    "text": "The Curse of Dimensionality\nNow, onto the bad stuff.\n\nWith kNN as our easy-to-analyze nonparametric model, we can now analyze how its performance scales with the number of covariates \\(p\\).\nFor kNN (or nonparametric models in general) to perform well, \\(X\\) needs to have “similar enough” training points \\(X_i\\) nearby.\nHowever, as the number of covariates \\(p\\) increases, Euclidean distance breaks down as a meaningful measure of similarity, making it unlikely that \\(X\\)’s nearest neighbours are actually similar.\n\n\nEuclidean Distance Breaks Down in High Dimensions\nTo understand why distance breaks down in high dimensions, consider the following thought experiment:\n\nSuppose we have \\(x_1, x_2, \\ldots, x_n\\) training points distributed uniformly within a \\(p\\)-dimensional ball of radius 1.\nFor a test point \\(x\\) at the center of the ball, consider its \\(k = n/10\\) nearest neighbours.\n\n\n\n\n\n\n\n\n\n\n\nThe red points are the \\(n/10\\) nearest neighbours of \\(x\\) (the black point at the center), which are all contained within the inner dotted circle.\nThe inner circle is a lot smaller than the outer circle, but this \\(2D\\) plot gives the wrong intuition for higher dimensions!\n\n\n\\(p=2\\)\n\nWhat is the radius of the inner circle relative to the outer circle?\nIn expectation, the \\(n/10\\) nearest neighbours of \\(x\\) will take up \\(10\\%\\) of the area of the outer circle.\nSince the area of a circle scales with the square of its radius, the inner circle has a radius of \\(\\sqrt{0.1} \\approx 0.316\\).\n\n\n\n\\(p=3\\)\n\nNow consider \\(p=3\\) dimensions (so that our radius 1 ball is a unit sphere).\nThe sub-space containing its \\(n/10\\) nearest neighbours is now a sphere with \\(10\\%\\) of the volume of the outer sphere.\nSince the volume of a sphere scales with the cube of its radius, the inner sphere has a radius of \\(0.1^{1/3} \\approx 0.464\\).\n\n\n\nMost Points Live Near the Boundary as \\(p\\) Increases\nMore generally, for \\(p\\) dimensions, the radius \\(r\\) of the inner ball containing the \\(n/10\\) nearest neighbours is given by:\n\\[\n  r = 0.1^{1/p}\n  \\]\nwhere \\(p\\) is the number of dimensions (covariates). This number grows very quickly as \\(p\\) increases:\n\nWhen \\(p=10\\), \\(r = (0.1)^{1/10} \\approx 0.794\\)(!)\nWhen \\(p=100\\), \\(r = (0.1)^{1/100} \\approx 0.977\\)(!!)\nWhen \\(p=1000\\), \\(r = (0.1)^{1/1000} \\approx 0.999\\)(!!!)\n\nIn other words, in a \\(1000\\)-dimensional space, even the \\(10\\%\\) of nearest neighbours are going to live on the boundary of the unit ball!\nWhy is this problematic?\n\nAs dimensionality increases, all points become maximally far apart from one another.\nWith such large distances, we can’t meaningfully distinguish between “similar” and “different” inputs.\nDistance becomes (exponentially) meaningless in high dimensions."
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#how-to-overcome-this-curse-of-dimensionality",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#how-to-overcome-this-curse-of-dimensionality",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "How to Overcome this Curse of Dimensionality",
    "text": "How to Overcome this Curse of Dimensionality\n\nNonparametric methods, which typically make predictions from distance-based similarity, become exponentially less effective as the number of covariates \\(p\\) increases.\nTo meaningfully distinguish between “similar” and “different” inputs, we need an exponentially large amount of training data \\(n\\) as \\(p\\) increases.\nThe theoretical risk of kNN (and other nonparametric methods) after we tune \\(k\\) to optimally balance bias and variance scales as:\n\\[\nR_n^{(\\mathrm{kNN})} = \\underbrace{\\frac{C_1^{(\\mathrm{kNN})}}{n^{4/(4+p)}}}_{\\mathrm{bias}^2} +\n\\underbrace{\\frac{C_2^{(\\mathrm{kNN})}}{n^{4/(4+p)}}}_{\\mathrm{var}} +\n\\sigma^2\n\\]\nwhere \\(C_1^{(\\mathrm{kNN})}, C_2^{(\\mathrm{kNN})}\\) are constants that depend on the distribution of \\((X, Y)\\).\n\nTo halve the bias or variance, we need to increase \\(n\\) by a factor of \\(2^{(4+p)/4}\\).\nFor \\(p=4\\), this is a factor of \\(2\\).\nFor \\(p=36\\), this is a factor of \\(1024\\).\n\nIn contrast, the risk of linear regression scales as:\n\\[\nR_n^{(\\mathrm{OLS})} = \\underbrace{C_1^{(\\mathrm{OLS})}}_{\\mathrm{bias}^2} +\n\\underbrace{\\frac{C_2^{(\\mathrm{OLS})}}{n/p}}_{\\mathrm{var}} +\n\\sigma^2\n\\]\n\nTo halve the variance, we only need to increase \\(n\\) by a factor of \\(2\\) (regardless of \\(p\\)).\n(Of course, the bias \\(C_1^{(\\mathrm{OLS})}\\) may be large if \\(\\mathbb E[Y|X]\\) is not linear.)\n\n\n\nIn Practice\n\nSurprisingly, kNN and other nonparametric methods can work well in practice, even for moderate \\(p\\).\nFor example, on the classic classification dataset MNIST (\\(p=784\\)), kNN achieves \\(97.3\\%\\) accuracy from just \\(60,000\\) training points.\nThe reason for this empirical success cannot be easily formalized, but the intuition is that real-world data often has “low-dimensional structure” (e.g. images of handwritten digits lie on a low-dimensional manifold within \\(\\mathbb R^{784}\\)) where Euclidean distance is still meaningful.\nSo you should not avoid nonparametric methods altogether on high-dimensional data, but be aware of their limitations."
  },
  {
    "objectID": "schedule/lectures/lecture_11_curse_of_dimensionality.html#summary",
    "href": "schedule/lectures/lecture_11_curse_of_dimensionality.html#summary",
    "title": "Lecture 11: k-Nearest Neighbours, Curse of Dimensionality",
    "section": "Summary",
    "text": "Summary\n\nkNN is a simple nonparametric method that makes predictions based on the \\(k\\) nearest training points.\nHowever, nonparametric methods like kNN suffer from the curse of dimensionality, where distance becomes meaningless in high dimensions, and exponentially more training data is required to reduce risk.\nHigh dimensional spaces are weird and defy our common \\(\\mathbb R^2\\) and \\(\\mathbb R^3\\) intuitions."
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html",
    "href": "schedule/lectures/lecture_13_pca.html",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nFormulate methods for dimensionality reduction through singular value decompositions.\nImplement and derive PCA and kPCA, and explain their geometric intuition.\nIdentify scenarios where PCA versus kPCA is more applicable.\nDescribe the uses of PCA and kPCA for data analysis.\nEnumerate the differences between supervised and unsupervised learning."
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html#learning-objectives",
    "href": "schedule/lectures/lecture_13_pca.html#learning-objectives",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nFormulate methods for dimensionality reduction through singular value decompositions.\nImplement and derive PCA and kPCA, and explain their geometric intuition.\nIdentify scenarios where PCA versus kPCA is more applicable.\nDescribe the uses of PCA and kPCA for data analysis.\nEnumerate the differences between supervised and unsupervised learning."
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html#motivation",
    "href": "schedule/lectures/lecture_13_pca.html#motivation",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "Motivation",
    "text": "Motivation\n\nDespite the curse of dimensionality for distance-based predictive models, we saw that not all high dimensional data was doomed to fail with kNN or kernel ridge regression.\nI claimed that these methods are immune from the curse of dimensionality when the data are “intrinsically low-dimensional”.\nToday, we will define what “intrinsically low-dimensional” means, and how to find such low-dimensional structure in high-dimensional data.\nThe methods that will discover this low dimensional structure are the first unsupervised learning methods we will study in this course.\n\n\nLow-Dimensional Structure Through Variable Reduction\n\nWe’ve seen two examples of exploiting low-dimensional structure in high-dimensional data: variable selection/Lasso regression.\n\nBoth methods work because certain covariates are not sufficiently relevant for predicting the response, and so they can be dropped.\nWhat results is a reduction to \\(&lt;p\\) covariates, reducing the dimensionality of the input space.\n\nBut what if all \\(p\\) covariates are relevant? It is still possible to have low-dimensional structure by applying a rotation to our covariates!\n\n\n\nLow-Dimensional Structure Through Rotations + Variable Reduction\n\nConsider the seats in this lecture hall, which live in a three dimensional space.\nTo describe the position of each seat, we need an \\(x\\), \\(y\\), and \\(z\\) coordinate.\nHowever, I would argue that the seats really only live in a two-dimensional space, since the column and height of the seats are highly correlated.\nIf we were to rotate our coordinate system, we could find a two-dimensional plane that all of the seats lie close to.\nUsing the language of STAT306, the \\(y\\) and \\(z\\) variables are colinear, and thus can be reduced into a single variable.\n\n\nWhy are Rotations Valid?\n\nImagine that we have a linear model \\(\\hat f_{\\mathcal D}(X) = X^\\top \\hat \\beta\\).\nA rotation of \\(X \\in \\mathbb R^{p}\\) can be represented as \\(X' = \\boldsymbol Q^\\top X\\) for some orthogonal matrix \\(\\boldsymbol Q \\in \\mathbb R^{p \\times p}\\).\nThen, we can rewrite our linear model as\n\\[ \\hat f_{\\mathcal D}(X) = X^\\top \\boldsymbol Q \\boldsymbol Q^\\top \\hat \\beta = (X')^\\top \\underbrace{(\\boldsymbol Q^\\top \\hat \\beta)}_{\\hat \\beta'}. \\]\nIt is totally valid to perform variable selection/LASSO on the rotated variables \\(X'\\) instead of the original variables \\(X\\)."
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html#pca-the-optimal-rotationdimensionality-reduction",
    "href": "schedule/lectures/lecture_13_pca.html#pca-the-optimal-rotationdimensionality-reduction",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "PCA: The Optimal Rotation/Dimensionality Reduction",
    "text": "PCA: The Optimal Rotation/Dimensionality Reduction\n\nWhile we could perform LASSO or variable selection on all possible rotations of \\(X\\), this would be computationally infeasible.\nInstead, we will aim to learn a rotation that produces a set of \\(k &lt; p\\) features that contain the maximum variance.\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\nFeatures without much variance are approximately constants, and so they will not be useful for predicting the response. Conversely, features with high variance are more likely to contain some “signal” that will be useful to predict the response.\n\n\n\nWe can formulate this idea as an optimization problem:\n\\[\n\\begin{aligned}\n\\max_{\\boldsymbol Q \\in \\mathbb R^{p \\times k}} & \\,\\, \\sum_{i=1}^k \\mathrm{Var}([\\boldsymbol Q]_i^\\top X) \\\\\n\\text{subject to} & \\,\\, \\boldsymbol Q^\\top \\boldsymbol Q = \\boldsymbol I_k\n\\end{aligned}\n\\]\nWe will assume without loss of generality that\n\n\\(n &gt; p\\) and\nThe original covariates are centered; i.e. \\(\\sum_{i=1}^n X_{ij} = 0\\) for all \\(j = 1, \\ldots, p\\).\n\n\n\nWhy can we make these assumptions?\n\n\nTurns out, it won’t matter whether \\(n &gt; p\\) or \\(p &lt; n\\); we will always choose \\(k &lt; \\mathrm{min}(n, p)\\), so there will always be enough singular values and right singular vectors.\nIf our features aren’t centered, then we can always center them first by subtracting the empirical mean of each feature.\n\n\nAmazingly, with these assumptions in place, the optimization problem can be solved exactly via the SVD of the design matrix!\n\\[ \\underbrace{\\boldsymbol X}_{n \\times p} = \\underbrace{ \\boldsymbol U }_{n \\times p} \\underbrace{\\boldsymbol D}_{p \\times p} \\underbrace{\\boldsymbol V^\\top}_{p \\times p}. \\]\nThe matrix \\(\\boldsymbol Q \\in \\mathbb R^{p \\times k}\\) that solves the above problem is given by the first \\(k\\) right singular vectors of \\(X\\) (i.e. the first \\(k\\) columns of \\(\\boldsymbol V\\)).\n\n\n\n\n\n\nTipIntuition\n\n\n\n\n\n\nThe right singular vectors are the eigenvectors of \\(\\boldsymbol X^\\top \\boldsymbol X\\), which is proportional to the empirical covariance matrix of \\(X\\).\nThus, the right singular vectors point in the directions of maximum variance of the data.\n\n\n\n\nIllustration of singular vectors as eigenvectors of empirical covariance matrix\n\n\n\n\n\nWe can now represent our data in this new rotated space via\n\\[ \\boldsymbol Z = \\boldsymbol X [\\boldsymbol V]_{:,1:k} = \\boldsymbol X \\boldsymbol Q, \\]\nWe refer to the new features in \\(\\boldsymbol Z\\) as the principal components (PCs) of the data.\nCrucially, these PCs are ordered, since the earliest PCs correspond to the largest singular values and thus capture the most variance in the data.\n\n\nExample: Pop Music Data\nHere is a dataset that describes various music songs:\n\nlibrary(tidyverse)\nload(\"../data/popmusic_train.rda\")\npopmusic_train\n\n# A tibble: 1,269 × 15\n   artist      danceability energy   key loudness  mode speechiness acousticness\n   &lt;fct&gt;              &lt;dbl&gt;  &lt;dbl&gt; &lt;int&gt;    &lt;dbl&gt; &lt;int&gt;       &lt;dbl&gt;        &lt;dbl&gt;\n 1 Taylor Swi…        0.781  0.357     0   -16.4      1      0.912       0.717  \n 2 Taylor Swi…        0.627  0.266     9   -15.4      1      0.929       0.796  \n 3 Taylor Swi…        0.516  0.917    11    -3.19     0      0.0827      0.0139 \n 4 Taylor Swi…        0.629  0.757     1    -8.37     0      0.0512      0.00384\n 5 Taylor Swi…        0.686  0.705     9   -10.8      1      0.249       0.832  \n 6 Taylor Swi…        0.522  0.691     2    -4.82     1      0.0307      0.00609\n 7 Taylor Swi…        0.31   0.374     6    -8.46     1      0.0275      0.761  \n 8 Taylor Swi…        0.705  0.621     2    -8.09     1      0.0334      0.101  \n 9 Taylor Swi…        0.553  0.604     1    -5.30     0      0.0258      0.202  \n10 Taylor Swi…        0.419  0.908     9    -5.16     1      0.0651      0.00048\n# ℹ 1,259 more rows\n# ℹ 7 more variables: instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;,\n#   tempo &lt;dbl&gt;, time_signature &lt;int&gt;, duration_ms &lt;int&gt;, explicit &lt;lgl&gt;\n\n\n\nIt has \\(p=15\\) numerical and categorical covariates that describe various audio features of songs, such as “dancibility,” “energy,” and “key.”\nWe only use the numerical features for PCA.\nWe can get the PCA rotation matrix the prcomp() function in R:\n\nX &lt;- popmusic_train |&gt; select(danceability:energy, loudness, speechiness:valence)\npca &lt;- prcomp(X, scale = TRUE) ## DON'T USE princomp()\npca\n\nStandard deviations (1, .., p=8):\n[1] 1.6233469 1.3466829 1.0690017 0.9510417 0.7638669 0.6737958 0.5495869\n[8] 0.4054698\n\nRotation (n x k) = (8 x 8):\n                         PC1          PC2        PC3         PC4          PC5\ndanceability     -0.02811038  0.577020027  0.0304479 -0.25196472  0.739526795\nenergy           -0.56077454 -0.001137424  0.1486735 -0.06430307 -0.226318113\nloudness         -0.53893087  0.085912854 -0.2282323  0.14003225 -0.030032041\nspeechiness       0.30125038  0.431188730  0.3679826  0.07395515 -0.242371888\nacousticness      0.51172138  0.082108326 -0.1697964 -0.01726948 -0.273283653\ninstrumentalness  0.01374425 -0.370058813  0.3453232 -0.81091990  0.003302477\nliveness         -0.02282669 -0.194947054  0.7616642  0.45763444  0.213396810\nvalence          -0.20242211  0.540418541  0.2475007 -0.19995475 -0.471169943\n                         PC6        PC7         PC8\ndanceability      0.11900195 -0.1563096 -0.12786852\nenergy           -0.05219588 -0.2909147 -0.72160673\nloudness         -0.06730567 -0.5296193  0.58697944\nspeechiness      -0.68760754 -0.2202211  0.04897228\nacousticness      0.47023448 -0.6246205 -0.12773085\ninstrumentalness -0.05239401 -0.1935036  0.21407632\nliveness          0.30477087 -0.1495286  0.10550213\nvalence           0.43477471  0.3346472  0.20667773\n\n\nThis matrix is the entire \\(p \\times p\\) \\(\\boldsymbol V\\) matrix. If we only use the first \\(k &lt; p\\) columns, we will reduce the dimensionality to \\(k\\) dimensions at the loss of some (hopefully low signal) information.\n\nggplot(\n  tibble(var_explained = pca$sdev^2 / sum(pca$sdev^2), M = 1:ncol(X)),\n  aes(M, var_explained)\n) +\n  geom_point(color = \"orange\") +\n  scale_x_continuous(breaks = 1:10) +\n  geom_segment(aes(xend = M, yend = 0), color = \"blue\")\n\n\n\n\n\n\n\n\nIf we reduce to \\(k=2\\) dimensions, then we can plot a reduced version of our covariates in two dimensions!\n\nproj_pca &lt;- predict(pca)[,1:2] |&gt;\n  as_tibble() |&gt;\n  mutate(artist = popmusic_train$artist)\nggplot(proj_pca, aes(PC1, PC2, color = artist)) +\n  geom_point() +\n  theme(legend.position = \"none\") +\n  scale_color_viridis_d()\n\n\n\n\n\n\n\n\nNote that these reduced features, which are linear combinations of the original features, don’t do a great job of separating the different artists. However, they still contain a good deal of the signal from the original features!\nWe can visualize this signal by plotting the loadings of the first two principal components, which are the weights that each of the principal components place on the original features:\n\n# Loadings plot\npca$rotation[, 1:2] |&gt;\n  as_tibble() |&gt;\n  mutate(feature = names(X)) |&gt;\n  pivot_longer(-feature) |&gt;\n  ggplot(aes(value, feature, fill = feature)) +\n  facet_wrap(~name) +\n  geom_col() +\n  theme(legend.position = \"none\", axis.title = element_blank()) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  geom_vline(xintercept = 0)"
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html#pca-with-basis-expansions",
    "href": "schedule/lectures/lecture_13_pca.html#pca-with-basis-expansions",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "PCA with Basis Expansions",
    "text": "PCA with Basis Expansions\n\nThe PCA-reduced dimensions are linear combinations of the original features.\nWhat if we want to find a non-linear set of reduced dimensions?\n\nKey idea: first apply a basis expansion \\(\\phi: \\mathbb R^p \\to \\mathbb R^q\\) to the original features, then perform PCA on the expanded features!\n\nExample: Spiral Data\n\nConsider the following spiral dataset:\n\n\n\n\n\n\n\n\n\n\nThe colouring is to show that our spiral has a natural one-dimensional structure that we wish to preserve.\nI.e. we want a one-dimensional representation \\(z\\) of the data so that interior spiral points (blue) have low \\(z\\) values and exterior spiral points (yellow) have high \\(z\\) values.\n\nIf we apply PCA directly to this data, we only get a linear combination of our features, which fails to capture the spiral structure:\n\n\n\n\n\n\n\n\n\nHowever, if we first apply a 4th order polynomial basis expansion to our features…\n\\[\n  \\underbrace{\\boldsymbol X}_{n \\times 2} \\: \\longrightarrow \\: \\underbrace{\\boldsymbol \\Phi}_{n \\times 14} = \\begin{bmatrix}\n  X_{11} & X_{12} & X_{11}^2 & X_{11} X_{12} & X_{12}^2 & \\ldots & X_{12}^4 \\\\\n  X_{21} & X_{22} & X_{21}^2 & X_{21} X_{22} & X_{22}^2 & \\ldots & X_{22}^4 \\\\\n  \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  X_{n1} & X_{n2} & X_{n1}^2 & X_{n1} X_{n2} & X_{n2}^2 & \\ldots & X_{n2}^4\n  \\end{bmatrix}\n\\]\n… and then perform PCA on the expanded features (\\(\\widetilde{\\boldsymbol \\Phi}\\) is the centered version of \\(\\boldsymbol \\Phi\\))…\n\\[ \\widetilde{\\boldsymbol \\Phi} = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top,\n\\qquad\n\\underbrace{\\boldsymbol Z}_{n \\times 1} = \\widetilde{\\boldsymbol \\Phi} [\\boldsymbol V]_1, \\]\n… then our one-dimensional representation captures the spiral structure!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWhy Does This Work?\n\n\n\n\n\n\nRecall that basis regression models are non-linear with respect to the original features, but linear with respect to the basis-expanded features.\nThis principle also applies to PCA: if we apply a basis expansion first, then PCA will find linear combinations of the basis-expanded features, which are non-linear combinations of the original features.\nThe resulting reduced dimensions can thus capture non-linear structure in the data.\n\n\n\n\n\n\nPCA with Infinite-Dimensional Basis Expansions\n\nWhat happens if we want to use infinitely many basis functions, as we did with kernel ridge regression?\nWe can’t explicitly compute the basis expansion \\(\\phi: \\mathbb R^p \\to \\mathbb R^{\\infty}\\). However, we’ll be able to use a nonparametric representation of our data to perform PCA in this infinite-dimensional space!\nSpecifically, we can recover the \\({\\boldsymbol Z}_{n \\times k} = \\widetilde{\\boldsymbol \\Phi} [\\boldsymbol V]_{1:k}\\) matrix of reduced dimensions by collecting the first \\(k\\) eigenvectors of the \\(n \\times n\\) matrix\n\\[  \\underbrace{\\widetilde{\\boldsymbol \\Phi}}_{n \\times d} \\underbrace{\\widetilde{\\boldsymbol \\Phi}^\\top}_{d \\times n} \\]\n\n\n\n\n\n\nNoteWhy Does This Eigendecomposition Recover \\(Z\\)?\n\n\n\n\n\nIf \\(\\widetilde {\\boldsymbol \\Phi} = \\boldsymbol U \\boldsymbol D \\boldsymbol V^\\top\\), then \\[\n\\begin{aligned}\n\\underbrace{\\boldsymbol Z}_{n \\times k}\n&= \\widetilde{\\boldsymbol \\Phi} [\\boldsymbol V]_{1:k} \\\\\n&= \\boldsymbol U \\boldsymbol D \\underbrace{\\boldsymbol V^\\top [\\boldsymbol V]_{1:k}}_{= \\begin{bmatrix} \\boldsymbol I_k \\\\ \\boldsymbol 0 \\end{bmatrix}} \\\\\n&= \\boldsymbol U \\underbrace{\\boldsymbol D \\begin{bmatrix} \\boldsymbol I_k \\\\ \\boldsymbol 0 \\end{bmatrix}}_{= [\\boldsymbol D]_{1:k}} \\\\\n&= [\\boldsymbol U \\boldsymbol D]_{1:k}\n\\end{aligned}\n\\] Since \\(\\boldsymbol U\\) are the eigenvectors of \\(\\widetilde{\\boldsymbol \\Phi} \\widetilde{\\boldsymbol \\Phi}^\\top\\), \\(\\boldsymbol Z\\) is given by a (scaled) version of the first \\(k\\) eigenvectors of \\(\\widetilde{\\boldsymbol \\Phi} \\widetilde{\\boldsymbol \\Phi}^\\top\\).\n\n\n\nAs with kernel ridge regression, this \\(n \\times n\\) matrix can be thought of as applying a kernel function \\(k(x_i, x_{i'})\\) to each pair of data points.\n\\[\n\\begin{aligned}\n\\underbrace{\\widetilde{\\boldsymbol \\Phi}}_{n \\times d} \\underbrace{\\widetilde{\\boldsymbol \\Phi}^\\top}_{d \\times n}\n&= \\begin{bmatrix}\n\\widetilde\\phi(X_1)^\\top \\widetilde\\phi(X_1) & \\widetilde\\phi(X_1)^\\top \\widetilde\\phi(X_2) & \\ldots & \\widetilde\\phi(X_1)^\\top \\widetilde\\phi(X_n) \\\\\n\\widetilde \\phi(X_2)^\\top \\widetilde \\phi(X_1) & \\widetilde \\phi(X_2)^\\top \\widetilde \\phi(X_2) & \\ldots & \\widetilde \\phi(X_2)^\\top \\widetilde \\phi(X_n) \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\widetilde \\phi(X_n)^\\top \\widetilde \\phi(X_1) & \\widetilde \\phi(X_n)^\\top \\widetilde \\phi(X_2) & \\ldots & \\widetilde \\phi(X_n)^\\top \\widetilde \\phi(X_n)\n\\end{bmatrix}\n:= \\underbrace{\\mathbf{K}}_{n \\times n}\n\\end{aligned}\n\\]\nwhere, as with kernel ridge regression, we have defined \\(\\widetilde \\phi(X_i)^\\top \\widetilde \\phi(X_j) = k(X_i, X_j)\\) to be a kernel similarity function.\nAs with kernel ridge regression, using\n\\[ k(X_i, X_j) = \\exp\\left( -\\frac{\\|X_i - X_j\\|^2_2}{2\\gamma} \\right) \\]\ncorresponds to a basis expansion with infinitely many Fourier features!\n\n\n\n\n\n\n\nTipKernel PCA\n\n\n\n\nForm a kernel matrix \\(\\boldsymbol{K} \\in \\mathbb R^{n \\times n}\\) using a kernel function \\(k(x_i, x_j)\\) (such as the squared exponential kernel) that corresponds to a potentially-infinite basis expansion.\nCenter both the rows and columns of \\(\\boldsymbol K\\). Mathematically,\n\\[ \\widetilde{\\boldsymbol K} = \\boldsymbol P \\boldsymbol K \\boldsymbol P,\\qquad \\boldsymbol P = \\boldsymbol I_n - \\frac{1}{n} \\boldsymbol 1 \\boldsymbol 1^\\top. \\]\nGet the dimensionality-reduced representation \\(\\boldsymbol Z \\in \\mathbb R^{n \\times k}\\) by collecting the first \\(k\\) eigenvectors of \\(\\widetilde{\\boldsymbol K}\\).\n\n\n\n\n\nExample: Kernel PCA on the Spiral Data\n\n\n\n\n\n\n\n\n\nReducing infinitely-many basis functions captures the spiral structure perfectly!\n\n\n\n\n\n\nWarningImportant Note on Centering!\n\n\n\n\nFor (standard linear) PCA, you must center the features before applying PCA.\nFor kernel PCA, you must center the kernel matrix both row-wise and column-wise!\nFailing to do so will lead to incorrect results."
  },
  {
    "objectID": "schedule/lectures/lecture_13_pca.html#summary",
    "href": "schedule/lectures/lecture_13_pca.html#summary",
    "title": "Lecture 13: Dimensionality Reduction, Introduction to Unsupervised Learning",
    "section": "Summary",
    "text": "Summary\n\nPCA finds a rotation of the original features such that the first \\(k\\) rotated features capture the maximum variance in the data.\nPCA can be performed via the SVD of the centered design matrix.\nBy applying a basis expansion before PCA, we can capture non-linear structure in the data.\nKernel PCA allows us to implicitly perform PCA in an infinite-dimensional basis expansion by using a row- and column-wise centered kernel matrix.\nPCA and kernel PCA demonstrate that high-dimensional data can often be well-approximated by a low-dimensional representation, demonstrating why data that are actually high dimensional may not suffer from the curse of dimensionality.\nPCA and kernel PCA are our first unsupervised learning methods, since they do not use any response variable to learn the low-dimensional representation."
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html",
    "href": "schedule/lectures/lecture_15_bootstrap.html",
    "title": "Lecture 15: The Bootstrap",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nExplain the bootstrap procedure and its purpose in estimating confidence intervals\nIdentify when the bootstrap is more/less advantageous than analytic methods of uncertainty quantification\nArticulate the assumptions underpinning the bootstrap and its limits as an approximation\nImplement the bootstrap for any statistical algorithm in pseudocode\nDifferentiate between different bootstrap intervals and identify the “correct” one"
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#learning-objectives",
    "href": "schedule/lectures/lecture_15_bootstrap.html#learning-objectives",
    "title": "Lecture 15: The Bootstrap",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nExplain the bootstrap procedure and its purpose in estimating confidence intervals\nIdentify when the bootstrap is more/less advantageous than analytic methods of uncertainty quantification\nArticulate the assumptions underpinning the bootstrap and its limits as an approximation\nImplement the bootstrap for any statistical algorithm in pseudocode\nDifferentiate between different bootstrap intervals and identify the “correct” one"
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#motivation",
    "href": "schedule/lectures/lecture_15_bootstrap.html#motivation",
    "title": "Lecture 15: The Bootstrap",
    "section": "Motivation",
    "text": "Motivation\nWe spent much of the first half of the course on linear models, in part due to their ubiquity but also due to our ability to analyze them statistically.\n\nIn STAT 306, you derived analytic confidence intervals for linear models as a way to quantify uncertainty in our parameter estimates/the true responses due to limited data.\nIn this course, we probably demonstrated that methods like ridge regularization reduce variance.\nMoreover, we rigorously argued that methods like basis expansions reduce bias.\n\nHowever, these methods for uncertaity quantification, variance reduction, and bias reduction all relied on linear structure. Other predictive/statistical models do not have such convenient analytic properties.\nIn this module, we will replace mathematical analysis with computational simulation to achieve similar goals.\n\nIn this lecture, we will introduce the bootstrap, a computational method for estimating predictive uncertainty in arbitrary predictive models (with almost no assumptions about the statistical or predictive model)!\nIn the following lectures, we will introduce ensemble methods as generic tools to either provably reduce variance (bagging, random forests) or reduce bias (boosting)."
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#uncertainty-quantification",
    "href": "schedule/lectures/lecture_15_bootstrap.html#uncertainty-quantification",
    "title": "Lecture 15: The Bootstrap",
    "section": "Uncertainty Quantification",
    "text": "Uncertainty Quantification\nIn STAT 306, you produced confidence intervals for the regression function \\(\\mathbb E[Y \\mid X=x]\\) of a (fixed) test covariate \\(x\\) under the assumptions of a linear statistical model:\n\nlibrary(Stat406)\nlibrary(dplyr)\ndata(prostate, package = \"ElemStatLearn\")\nprostate &lt;- prostate |&gt; filter(train == TRUE) |&gt; select(-train)\nlm_fit &lt;- lm(lpsa ~ . , data = prostate)\ntest_x &lt;- data.frame(\n  lcavol = 0, lweight = 3, age = 65,\n  lbph = 0, svi = 0, lcp = 0, gleason = 7, pgg45 = 0\n)\npredict(lm_fit, newdata = test_x, interval = \"confidence\", level = 0.95)\n\n        fit       lwr      upr\n1 0.8296435 0.2661997 1.393087\n\n\n\nThese confidence intervals tell us that, under our linearity assumptions, the regression function evaluated at \\(x\\) (i.e. \\(\\mathbb E[Y\\mid X=x]\\) will fall within the interval with 95% probability.\n\n\n\n\n\n\nNoteWhat does “95% Probability” Mean?\n\n\n\n\n\n\nWe assume that our training data are random samples with \\(Y = X^\\top \\beta + \\epsilon\\), where \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\), for some fixed \\(\\beta, \\sigma\\).\n95% probability refers to the randomness in the training data.\nIf we repeatedly sampled new training datasets, trained linear models on them, and computed 95% confidence intervals for predictions at \\(x\\), then 95% of those confidence intervals would contain \\(\\mathbb E[Y \\mid X=x]\\).\n\n\n\n\nWe derive these confidence intervals analytically using our statistical model, properties of our OLS estimator:\n\\[\n\\begin{align}\n  \\mathbb E[ \\hat \\beta_\\mathrm{OLS}] &= \\beta \\\\\n  \\mathrm{Cov}[ \\hat \\beta_\\mathrm{OLS} \\mid \\boldsymbol X] &= \\sigma^2 (\\boldsymbol X^\\top \\boldsymbol X)^{-1},\n\\end{align}\n\\]\nand by using the fact that \\(\\hat \\beta_\\mathrm{OLS} \\mid \\boldsymbol X\\) is Gaussian under our statistical model.\n\nHowever, consider what happens if we use a predictive model that’s based on a more complicated (or implicit) statistical model, like a decision tree or k-nearest neighbors.\n\nWe haven’t necessarily defined the conditional distribution of \\(Y \\mid X\\).\nEven if we did, the recursive decision tree algorithm is too complicated to compute expectations/variances of the estimator analytically.\n\n\nSimulation-Based Statistics\nThe key idea in this lecture (and module) is to use brute-force computation when closed-form math becomes too difficult.\n\nImagine that we want to construct a confidence interval around the regression function \\(\\mathbb E[Y\\mid X=x]\\) for some (fixed) test covariates \\(x\\) based on our prediction \\(\\hat Y = \\hat f_{\\mathcal D}(x)\\)\n\nAs always, \\(\\hat f_{\\mathcal D}\\) is a predictive model trained on dataset \\(\\mathcal D\\).\n\n\n\n\n\n\n\n\nTipWhy do we Care About Confidence Intervals for Predictions?\n\n\n\n\n\n\nIntuitively, these confidence interval should reflect how much \\(\\hat Y\\) would change if we had a different training dataset \\(\\mathcal D'\\).\nIf the confidence interval is narrow, then \\(95\\%\\) of all training samples would lead to similar predictions at \\(x\\), giving us confidence that this prediction is likely accurate.\nConversely, if the confidence interval is wide, then we know that the prediction is highly sensitive to the training data, and we should be less confident in its accuracy.\n\n\n\n\n\nIn order to compute a confidence interval for \\(Y\\) based on the central limit theorem, we need to estimate \\(\\mathrm{Var}[ \\hat Y \\mid X=x ] = \\mathrm{Var}[ \\hat f_{\\mathcal D}(x) ]\\)\nIf we had access to \\(100\\) different training samples, we could train \\(100\\) different models \\(\\hat f_{\\mathcal D^{(1)}}, \\ldots, \\hat f_{\\mathcal D^{(100)}}\\), compute their predictions at \\(x\\), and estimate this variance empirically.\nHowever, we only have access to one training dataset \\(\\mathcal D\\). We thus need a way to simulate different training datasets \\(\\mathcal D^{(1)}, \\ldots, \\mathcal D^{(100)}\\) using only the data in \\(\\mathcal D\\).\nThis idea of (1) simulating new datasets and (2) using those datasets to estimate uncertainty is the core idea behind simulation-based statistics. We use computation to approximate mathematical expectations/variances that we cannot compute analytically."
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#approximating-new-datasets-via-the-bootstrap",
    "href": "schedule/lectures/lecture_15_bootstrap.html#approximating-new-datasets-via-the-bootstrap",
    "title": "Lecture 15: The Bootstrap",
    "section": "Approximating New Datasets via the Bootstrap",
    "text": "Approximating New Datasets via the Bootstrap\n\nImagine that I have \\(X_1, \\ldots, X_n\\) drawn i.i.d. from some distribution \\(P(X)\\).\nThe empirical distribution \\(\\hat P(X)\\) is the discrete distribution that places mass \\(1/n\\) at each observed data point \\(X_i\\):\n\\[\n\\hat P(X=x) = \\frac{1}{n} \\sum_{i=1}^n \\delta_{X_i}(x)\n\\]\nwhere \\(\\delta_{X_i}(x)\\) is a point mass at \\(X_i\\).\nIf \\(n\\) is large enough, then \\(\\hat P(X) \\approx P(X)\\):\n\n\nCode\nset.seed(1)\nn &lt;- 100\nx &lt;- rnorm(n)\nhist(x, probability = TRUE, breaks = 10, main = \"Empirical Distribution vs True Distribution\")\ncurve(dnorm(x), col = \"red\", add = TRUE, lwd = 2)\nlegend(\"topright\", legend = c(\"True Distribution\", \"Empirical Distribution\"), col = c(\"red\", \"black\"), lwd = 2)\n\n\n\n\n\nEmpirical distribution approximating true distribution\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat is the Implication of \\(\\hat P(X) \\approx P(X)\\)?\n\n\n\n\nLet’s imagine I sample a new point \\(X^*\\) from \\(P(X)\\), and another point \\(\\hat X^{*}\\) from \\(\\hat P(X)\\).\nIf \\(\\hat P(X) \\approx P(X)\\), then the distribution of \\(X^*\\) and \\(\\hat X^{*}\\) should be similar.\nThus, sampling from the empirical distribution \\(\\hat P(X)\\) is (approximately) equivalent to sampling from the true distribution \\(P(X)\\)!\n\n\n\n\nIf we wish to sample a whole-new dataset \\(\\mathcal D' = \\{ X_1', \\ldots, X_n' \\}\\) that has (approximately) the same distribution as our original dataset \\(\\mathcal D = \\{ X_1, \\ldots, X_n \\}\\), we can thus sample each \\(X_i'\\) i.i.d. from the empirical distribution \\(\\hat P(X)\\).\n\n\nThe Bootstrap\nThis idea gives us a recipe to sample many datasets \\(\\mathcal D^{(1)}, \\ldots, \\mathcal D^{(B)}\\) that are (approximately) drawn from the same distribution as our original dataset \\(\\mathcal D\\), which is often referred to as the bootstrap:\n\n\n\n\n\n\nTipThe Bootstrap\n\n\n\n\nConstruct the empirical distribution \\(\\hat P(X, Y)\\) based on the observed data points \\((X_i, Y_i)\\) in \\(\\mathcal D\\).\nFor \\(b = 1, \\ldots, B\\):\n\nSample \\(n\\) data points \\((X_i^{(b)}, Y_i^{(b)})\\) with replacement from \\(\\hat P(X, Y)\\) to form dataset \\(\\mathcal D^{(b)}\\).\nCompute the statistic of interest (e.g. prediction \\(\\hat f_{\\mathcal D^{(b)}}(x)\\)) on dataset \\(\\mathcal D^{(b)}\\). (Important: this step will require training \\(b\\) models on the \\(b\\) simulated datasets if the statistic of interest is a model prediction.)\n\nCompute the empirical variance \\(\\mathrm{Var}[ \\hat f_{\\mathcal D^{(b)}}(x) ]\\), or construct a confidence interval from the \\(B\\) computed statistics (using methods to be explained shortly).\n\n\n\n\nNote that we sample with replacement from the empirical distribution because we want each data point in the new dataset to be an independent draw from \\(\\hat P(X, Y) \\approx P(X, Y)\\).\nOnce we have sampled these new datasets, we can train models \\(\\hat f_{\\mathcal D^{(1)}}, \\ldots, \\hat f_{\\mathcal D^{(B)}}\\) on each dataset and use the predictions at \\(x\\) to estimate the variance of \\(\\hat Y = \\hat f_{\\mathcal D}(x)\\).\nAmazingly, this simple procedure can be used to estimate the variance of the predictions from any model, or just about any statistic that we could ever care about! We don’t need to derive complicated analytic expressions, nor do we even need to make any (significant) assumptions about the statistical model underlying our data!\n\n\n\n\n\n\n\nWarningThe Catch\n\n\n\n\nThe Fundamental Premise (TM) of the Bootstrap is that a sufficiently large sample looks like the population, so that sampling from the sample looks like sampling from the population.\nIf \\(\\mathcal D\\) is too small or not representative of the true data distribution, then the bootstrap will not yield datasets that approximate the true data distribution well, resulting in poor confidence intervals.\n\n\n\n\n\nExample\n\n\n\n\n\n\nTipA (Non-Predictive) Example\n\n\n\nWhile we motivated the bootstrap as a way to obtain confidence intervals around the regression function \\(\\mathbb E[Y\\mid X=x]\\) in learning problems, it’s really a general way to form confidence intervals around any statistical quantity.\nHere we will walk through a non-predictive example, since you will look at a predictive example for homework!\n\n\nImagine that we have \\(X_1, \\ldots, X_n\\) drawn i.i.d. from \\(\\mathrm{Exponential}(1/5)\\) with pdf \\(f(x) = \\frac{1}{5}e^{-x/5}\\).\n\nImagine I want to estimate a confidence interval for the sample mean \\(\\bar X\\).\nThe CLT (if \\(n\\) is sufficiently large) gives \\[\n\\frac{\\sqrt{n}(\\bar X - \\mathbb E[X])}{{\\mathrm{se}}} \\approx \\mathcal{N}(0, 1),\n\\] where \\(\\mathrm{se}\\) is the standard error of the sample mean, giving us a 95% confidence interval \\[\n\\bar X \\pm 2 \\hat{\\mathrm{se}}/\\sqrt{n}.\n\\]\nHowever, what if we wanted to estimate a confidence interval for the sample median instead?\n\n\n\n\n\n\n\nNoteVisualizing the Population Distribution\n\n\n\nThe median is different from the mean for the exponential distribution:\n\n\nCode\nlibrary(tidyverse)\nggplot(data.frame(x = c(0, 12)), aes(x)) +\n  stat_function(fun = function(x) dexp(x, 1 / 5), color = \"orange\") +\n  geom_vline(xintercept = 5, color = \"blue\") + # mean\n  geom_vline(xintercept = qexp(.5, 1 / 5), color = \"red\") + # median\n  annotate(\"label\",\n    x = c(2.5, 5.5, 10), y = c(.15, .15, .05),\n    label = c(\"median\", \"mean\", \"pdf\"), parse = TRUE,\n    color = c(\"red\", \"blue\", \"orange\"), size = 6\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nWe can use the bootstrap along with the pivotal interval to construct a confidence interval for the sample median!\n\nAssume I have some dataset \\(\\mathcal D = \\{ X_1, \\ldots, X_n \\}\\) of size \\(n=500\\) drawn i.i.d. from \\(\\mathrm{Exponential}(1/5)\\).\n\nset.seed(406406406)\nn &lt;- 500\nx &lt;- rexp(n, 1 / 5)\n\nFirst I will draw \\(100\\) bootstrap samples \\(\\mathcal D^{(1)}, \\ldots, \\mathcal D^{(100)}\\) by sampling \\(n\\) points with replacement from my original dataset \\(\\mathcal D\\).\n\nset.seed(406406406)\nB &lt;- 100\nbootstrap_samples &lt;- map(1:B, ~ sample(x, replace = TRUE))\n\nNext, I will compute the sample median on each bootstrap sample:\n\nmedian_bootstrap &lt;- map_dbl(bootstrap_samples, ~ median(.x))\n\nThen I will compute a 95% confidence interval for the sample median using the bootstrap sample medians (using a CI formula that we are about to discuss):\n\nalpha &lt;- 0.05\nmed &lt;- median(x)  # Sample median\n# Use the pivotal confidence interval formula for CI\nCI &lt;- 2 * med - quantile(median_bootstrap, probs = c(1 - alpha / 2, alpha / 2))\n\n\nThe plot below shows the distribution of the bootstrap sample medians:\n\n\nCode\nggplot(data.frame(median_bootstrap), aes(median_bootstrap)) +\n  geom_density(color = \"orange\") +\n  geom_vline(xintercept = CI, color = \"orange\", linetype = 2) +\n  geom_vline(xintercept = med, col = \"blue\") +\n  geom_vline(xintercept = qexp(.5, 1 / 5), col = \"red\") +\n  annotate(\"label\",\n    x = c(3.15, 3.5, 3.75), y = c(.5, .5, 1),\n    color = c(\"orange\", \"red\", \"blue\"),\n    label = c(\"widehat(F)\", \"true~median\", \"widehat(median)\"),\n    parse = TRUE\n  ) +\n  xlab(\"x\") +\n  geom_rug(aes(2 * med - median_bootstrap))\n\n\n\n\n\n\n\n\n\n\nThe blue line is the sample median.\nThe orange dashed lines are the bounds of our 95% confidence interval.\nThe rug plot at the bottom shows the distribution of the bootstrap medians reflected around the sample median.\n\nThis confidence interval is quite good because it contains the true median (the red line) and is fairly tight around the sample median!"
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#from-bootstrap-samples-to-confidence-intervals",
    "href": "schedule/lectures/lecture_15_bootstrap.html#from-bootstrap-samples-to-confidence-intervals",
    "title": "Lecture 15: The Bootstrap",
    "section": "From Bootstrap Samples to Confidence Intervals",
    "text": "From Bootstrap Samples to Confidence Intervals\n\nRecall that we want to generate a confidence interval for our regression function \\(\\mathbb E[Y \\mid X=x]\\) based on our prediction \\(\\hat Y = \\hat f_{\\mathcal D}(x)\\) and bootstrapped predictions \\(\\hat Y^{(1)} = \\hat f_{\\mathcal D^{(1)}}(x), \\ldots, \\hat Y^{(B)} = \\hat f_{\\mathcal D^{(B)}}(x)\\).\nThere are two primary techniques used to construct confidence intervals from bootstrap samples.\n\n\nThe Percentile Interval\n\nThe percentile interval is given by the empirical quantiles of the bootstrap sample statistics:\n\\[\n[\\hat Y^{(b)}_{\\alpha/2},\\ \\hat Y^{(b)}_{1-\\alpha/2}]\n\\]\nwhere \\(\\hat Y^{(b)}_q\\) is the \\(q\\) quantile of \\(\\hat Y^{(b)}\\).\nWe teach this one in DSCI100 because it’s easy and “looks right”, but it’s not theoretically justified in general. It does have the right coverage (asymptotically) in some specific cases.\n\n\n\n\n\n\nWarningWhat does the Percentile Interval Measure?\n\n\n\n\nRecall that the bootstrap gives us many samples of \\(\\hat Y\\) from (approximate/Bootstrap) training samples.\nThe percentile interval constructs an (approximate) \\(95\\%\\) confidence interval of \\(\\hat Y\\); i.e., \\(95\\%\\) of models trained on different training samples would (approximately) produce predictions within this interval.\nHowever, this is not the same as a \\(95\\%\\) confidence interval for the true response \\(Y \\mid X=x\\)!\nThus, the percentile interval may not have the correct coverage for the true response!\n\n\n\n\n\nWhen is the Percentile Interval Valid?\n\nIf there exists a monotonic function \\(m: \\mathbb R \\to \\mathbb R\\) so that \\(m(\\hat Y)\\) is Gaussian distributed, i.e. \\(m(\\hat Y) \\sim N(m(\\hat Y), c^2)\\), then the percentile interval has asymptotically correct coverage.\nFor most complicated statistics (e.g. the predictions of a decision tree), it’s hard to know whether such a function \\(m\\) exists.\n\nIf we want (asymptotically) correct confidence intervals around the true regression function, we should use the pivotal interval.\n\n\n\nThe Pivotal Interval\n\nThe pivotal interval is given by\n\\[\n[2\\hat Y - \\hat Y^{(b)}_{1-\\alpha/2},\\ 2\\hat Y - \\hat Y^{(b)}_{\\alpha/2}]\n\\]\nwhere again \\(\\hat Y\\) is the statistic computed on the original dataset, and \\(\\hat Y^{(b)}_q\\) is the \\(q\\) quantile of the bootstrap sample statistics.\nThis interval is justified under fairly general conditions, and thus is the preferred method for constructing bootstrap confidence intervals of the true regression function \\(Y \\mid X=x\\)\nFor details on why this interval works, see All of Statistics, Ch. 8.3, or visit me in office hours!\n\n\n\n\n\n\n\nWarningWhat about Other Bootstrap Intervals?\n\n\n\n\nThere are many other bootstrap confidence intervals (e.g. the bias-corrected and accelerated (BCa) interval), but they are more complicated to compute and don’t have significant advantages over the pivotal interval in most cases.\nAt the very least, you should definitely NOT use the “normal” confidence interval \\([\\hat{Y} - z_{1-\\alpha/2}\\hat{\\mathrm{se}}, \\:\\: \\hat{Y} + z_{1-\\alpha/2}\\hat{\\mathrm{se}}]\\), which is only likely to work if \\(\\hat Y\\) is approximately Gaussian."
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#sources-of-error-and-improving-the-bootstrap",
    "href": "schedule/lectures/lecture_15_bootstrap.html#sources-of-error-and-improving-the-bootstrap",
    "title": "Lecture 15: The Bootstrap",
    "section": "Sources of Error, and Improving the Bootstrap",
    "text": "Sources of Error, and Improving the Bootstrap\nThe bootstrap is, at the end of the day, just an approximation. There are two sources of error that could make our variance estimates or confidence intervals incorrect:\n\n\n\n\n\n\nTipSources of Error in the Bootstrap\n\n\n\n\n\n\nStatistical error from using only \\(n\\) data points to estimate the true data distribution. We don’t have the whole population so we make an error by using a sample. (Note: this part is what always happens with data, and what the science of statistics analyzes.)\nSimulation error from using only \\(B\\) samples to estimate \\(\\mathrm{Var}[\\hat Y]\\).\n\n\n\n\n\nCorrecting for Statistical Error\n\nWe can used more advanced methods to alleviate the statistical error from using only \\(n\\) data points.\nFor example, the parametric bootstrap assumes some assumptions about the data-generating distribution (e.g. \\(\\hat Y\\) is Gaussian) to improve the quality of the bootstrap samples.\nHowever, if you make incorrect assumptions about the data distribution, you may make things worse!\n\n\n\nCorrecting for Simulation Error\n\nThis one’s easier, at least in theory!\nYou can always use more Bootstrap samples \\(B\\) to reduce simulation error.\nIn practice, start with \\(B=100\\) or \\(B=1000\\) samples and compute your variance/confidence intervals. Then try adding \\(100\\) more samples. If your confidence intervals change significantly, keep adding more samples until they stabilize.\n\n\n\n\n\n\n\nTipMore is Better\n\n\n\n\n“Just add more samples” will be a common refrain throughout this module.\nIn simulation-based statistics, or when using any computational black-box methods, accuracy will almost always improve if you throw more compute at the problem (i.e. more samples, more models, etc.)\nIn practice, you will have a limited amount of computing power, but we’ve essentially removed one design decision: use as much computing power as you have available to get the best results."
  },
  {
    "objectID": "schedule/lectures/lecture_15_bootstrap.html#summary",
    "href": "schedule/lectures/lecture_15_bootstrap.html#summary",
    "title": "Lecture 15: The Bootstrap",
    "section": "Summary",
    "text": "Summary\n\nSimulation-based statistics uses computation to approximate mathematical expectations/variances that we cannot compute analytically.\nThe bootstrap is a general-purpose method for estimating variances/confidence intervals based on simulating new datasets from the empirical distribution of the observed data.\nThe bootstrap can be used to estimate uncertainty for almost any statistic, including predictions from arbitrary predictive models.\nThe bootstrap will only work if our empirical data distribution is large enough to approximate the true population.\nThe pivotal interval is the preferred method for constructing bootstrap confidence intervals for true responses.\n\nIn the homework, you’ll apply the bootstrap to compute uncertainty estimates of machine learning models. In the next lecture, we’ll show how this bootstrap principle can be applied to make predictions more accurate!"
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html",
    "href": "schedule/lectures/lecture_17_boosting.html",
    "title": "Lecture 17: Boosting",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDifferentiate strong learners from weak learners\nConnect weak learners and strong learners through the lens of boosting\nDerive the AdaBoost algorithm in pseudocode\nIdentify the settings in which boosting is likely beneficial\nArticulate advantages and disadvantages of boosted (small) trees versus bagged (large) trees beyond accuracy"
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#learning-objectives",
    "href": "schedule/lectures/lecture_17_boosting.html#learning-objectives",
    "title": "Lecture 17: Boosting",
    "section": "",
    "text": "By the end of this lecture, you should be able to:\n\nDifferentiate strong learners from weak learners\nConnect weak learners and strong learners through the lens of boosting\nDerive the AdaBoost algorithm in pseudocode\nIdentify the settings in which boosting is likely beneficial\nArticulate advantages and disadvantages of boosted (small) trees versus bagged (large) trees beyond accuracy"
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#motivation-high-bias-ensembles",
    "href": "schedule/lectures/lecture_17_boosting.html#motivation-high-bias-ensembles",
    "title": "Lecture 17: Boosting",
    "section": "Motivation: High-Bias Ensembles",
    "text": "Motivation: High-Bias Ensembles\n\nIn the last lecture, we saw that high-variance/low-bias models (such as full-depth decision trees) can be ensembled using bagging/random forests to produce low-variance/low-bias predictions.\nIn this lecture, we are going to explore an orthogonal ensembling strategy called boosting, which aims to combine many high-bias/low-variance models into a single low-bias/low-variance prediction."
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#weak-learners-versus-strong-learners",
    "href": "schedule/lectures/lecture_17_boosting.html#weak-learners-versus-strong-learners",
    "title": "Lecture 17: Boosting",
    "section": "Weak Learners versus Strong Learners",
    "text": "Weak Learners versus Strong Learners\nBefore we dive into the boosting algorithm, let’s first define what we mean by “weak learners” and “strong learners”.\n\nDefinitions\n\nA strong learner is a predictive model that can achieve arbitrarily low error on any learning problem, given enough data. In other words, a strong learner can learn any target function to arbitrary accuracy.\n\nExamples: Non-parametric models, deep decision trees, etc.\n\nA weak learner is a predictive model that can only achieve slightly better than random guessing on any learning problem. Specifically, for a binary classification problem where random guessing would achieve 50% accuracy, a weak learner can achieve \\(50\\% + \\epsilon\\) accuracy for some small \\(\\epsilon &gt; 0\\).\n\nExamples: Depth-1 decision trees (stumps).\n\nA weak learner is limited in its ability to fit complex functions (e.g. high bias), while a strong learner is flexible enough to fit any function (e.g. low bias, and low variance given enough data).\n\n\n\nCan We Turn Weak Learners into Strong Learners?\n\nIn 1988, Michael Kearns posed the fundamental question: “Can a set of weak learners be combined to create a single strong learner?”\nIn 1990, Robert Schapire proved that the answer to this question is “yes,” by introducing the boosting algorithm and proving that it yields strong learners from weak learners.\n\n\n\nHigh-Level Idea\nLet’s consider a simple 1D example to illustrate how boosting can combine weak learners into a strong learner:\n\nSuppose our weak (high-bias) learner is a “step function” with a single learnable parameter: \\(t\\) (the location of the step):\n\\[\n\\hat h(x) = \\begin{cases}\n0 & x &lt; t \\\\\n1 & x \\geq t\n\\end{cases}\n\\]\nWith just a single step function, we have no hope of approximating any complex target function (high bias).\nHowever, a linear combination of multiple step functions \\(\\hat h_1, \\ldots, \\hat h_m\\) will yield a piecewise constant function. Assuming the functions are ordered by their step locations \\(t_1 &lt; t_2 &lt; \\ldots &lt; t_m\\), we get:\n\\[\n\\sum_{j=1}^m \\alpha_j \\hat h_j(x) = \\begin{cases}\n0 & x &lt; t_1 \\\\\n\\alpha_1 & t_1 \\leq x &lt; t_2 \\\\\n\\alpha_1 + \\alpha_2 & t_2 \\leq x &lt; t_3 \\\\\n\\vdots \\\\\n\\sum_{j=1}^m \\alpha_j & x \\geq t_m\n\\end{cases}\n\\]\nwhere \\(\\alpha_j\\) are the linear combination weights. As \\(m \\to \\infty\\), we have enough step locations to approximate any continuous function arbitrarily well!\nBoosting will learn the step locations \\(t_j\\) / weights \\(\\alpha_j\\) in a sequential manner, where each new step function is trained to correct the errors made by the current ensemble.\n\n\n\nIntuition: A “Weak” Golfer\n\nImagine you are a golfer who is not very skilled (a weak learner).\nYour goal is to hit the golf ball into the target \\((Y_1, Y_2)\\) from your current location: \\((0, 0)\\)\nYou begin by taking your first swing in the direction \\(t_1\\) and you hit the ball with strength \\(\\alpha_1\\), landing at location \\(( \\hat Y^{(1)}_1, \\hat Y^{(1)}_2 )\\).\nIf we recenter our coordinate system to this new location, our new task is to hit the ball from \\((0, 0)\\) to the new target \\((Y_1 - \\hat Y^{(1)}_1, Y_2 - \\hat Y^{(1)}_2)\\).\nWe take our second swing in direction \\(t_2\\) with strength \\(\\alpha_2\\), landing at location \\(( \\hat Y^{(2)}_1, \\hat Y^{(2)}_2 )\\) in our new coordinate system.\nAgain, if we recenter our coordinate system to this new location, our new task is to hit the ball from \\((0, 0)\\) to the new target \\((Y_1 - (\\hat Y^{(1)}_1 + \\hat Y^{(2)}_1), Y_2 - (\\hat Y^{(1)}_2 + \\hat Y^{(2)}_2))\\).\nIf we repeat this process enough times, we will eventually hit the target exactly!"
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#algorithm-1-forward-stagewise-additive-modelling",
    "href": "schedule/lectures/lecture_17_boosting.html#algorithm-1-forward-stagewise-additive-modelling",
    "title": "Lecture 17: Boosting",
    "section": "Algorithm 1: Forward Stagewise Additive Modelling",
    "text": "Algorithm 1: Forward Stagewise Additive Modelling\nLet’s take this golf analogy and translate it into a mathematical algorithm. We are going to sequentially learn a series of weak learners \\(\\hat h^{(1)}, \\hat h^{(2)}, \\ldots, \\hat h^{(m)}\\) and corresponding weights \\(\\alpha^{(1)}, \\alpha^{(2)}, \\ldots, \\alpha^{(m)}\\) to form an ensemble predictor:\n\\[\\hat f^{(m)}(X) = \\sum_{t=1}^m \\alpha^{(t)} \\hat h^{(t)}(X) \\]\n\nBoosting works by training weak learners sequentially, where each new weak learner is trained to correct the errors made by the previous ensemble.\n\n\nThe Core Algorithm\nWe will use a recursive procedure to sequentially learn the ensemble models \\(\\hat h^{(t)}\\) and weights \\(\\alpha^{(t)}\\).\n\nRegression Problems\n\nLet \\(\\hat h^{(t)}\\) be a weak learner (e.g. our step function from before) and \\(\\alpha^{(t)}\\) be a corresponding weight term.\nTo have \\(\\alpha^{(t)} \\hat h^{(t)}\\) correct the errors of the previous ensemble \\(\\hat f^{(t-1)}\\), we can train \\(\\alpha^{(t)} \\hat h^{(t)}\\) to predict the residuals (errors) of the current ensemble on the training data:\n\\[\nR_i^{(t)} = Y_i - \\hat f^{(t-1)}(X_i) = Y_i - \\sum_{j=1}^{t-1} \\alpha^{(j)} \\hat h^{(j)}(X_i)\n\\]\nWe then simultaneously select \\(\\alpha^{(t)}\\) and \\(\\hat h^{(t)}\\) to minimize the squared error on these residuals\n\\[\n\\hat h^{(t)}, \\alpha^{(t)} = \\arg\\min_{h, \\alpha} \\sum_{i=1}^n (R_i^{(t)} - \\alpha h(X_i))^2\n\\]\nThis is similar to empirical risk minimization (ERM), except we are minimizing the loss on the residuals rather than the original response variables \\(Y_i\\).\n\n\n\n\n\n\n\nNoteWhat is the Purpose of the Weight \\(\\alpha^{(t)}\\)?\n\n\n\n\n\n\nWe could incorporate the weight \\(\\alpha^{(t)}\\) into the weak learner \\(\\hat h^{(t)}\\) itself, e.g. by defining our weak learner to be scaled step functions with two learned parameters: \\(t\\) (step location) and \\(\\alpha\\) (step height).\n\\[\\hat h(x) = \\begin{cases}\n0 & x &lt; t \\\\\n\\alpha & x \\geq t\n\\end{cases}.\\]\nIn practice, it is not always natural to incorporate the weight into the weak learner itself. For example, decision trees typically do not have a natural scaling parameter.\nMore importantly, having an explicit weight term allows us to control the contribution of each weak learner to the ensemble. If we simply add each weak learner’s prediction to the ensemble without any weighting, the ensemble might oscillate or diverge rather than converging to a good solution.\nIn the golf example, we want our swings to get lighter (i.e. \\(\\alpha^{(t)}\\) to get smaller) as we get closer to the target. Otherwise, we might overshoot the target and end up further away than before.\n\n\n\n\n\n\nGeneric Problems\n\nThe above procedure can be generalized to arbitrary loss functions \\(\\ell(Y, \\hat f(X))\\) (not just squared error).\nFor the squared error, note that:\n\\[\n\\begin{aligned}\n\\left( \\underbrace{R_i^{(t)}}_{Y_i - \\hat f^{(t-1)}(X_i)} - \\alpha^{(t)} \\hat h^{(t)}(X_i) \\right)^2\n&= \\left(Y_i - \\left( \\hat f^{(t-1)}(X_i) - \\alpha^{(t)} \\hat h^{(t)}(X_i) \\right) \\right)^2 \\\\\n&= \\ell( Y_i, \\hat f^{(t-1)}(X_i) + \\alpha^{(t)} \\hat h^{(t)}(X_i) )\n\\end{aligned}\n\\]\nTherefore, we can train the weak learners to minimize the loss on the updated ensemble predictions directly:\n\\[\n(\\hat h^{(t)}, \\alpha^{(t)}) = \\arg\\min_{h, \\alpha} \\sum_{i=1}^n \\ell( Y_i, \\hat f^{(t-1)}(X_i) + \\alpha h(X_i) )\n\\]\nThis procedure yields the following generic boosting algorithm:\n\n\n\n\n\n\n\nNoteForward Stagewise Additive Modelling\n\n\n\nGiven:\n\nA training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\) for some regression problem\nA weak learning algorithm (e.g., depth-1 decision trees)\nThe number of boosting rounds \\(m\\)\n\nInitialize the ensemble predictor:\n\\[ \\hat f^{(0)}(X) = 0 \\quad \\text{for all } X\\]\nFor \\(t = 1, 2, \\ldots, m\\):\n\nChoose a weak learner \\(\\hat h^{(t)}\\) that fits the current residuals:\n\\[\n\\hat h^{(t)}, \\alpha^{(t)} = \\arg\\min_{h, \\alpha} \\sum_{i=1}^n \\ell \\left( Y_i, \\hat f^{(t-1)}(X_i) + \\alpha h(X_i) \\right)\n\\]\nUpdate the ensemble predictor:\n\\[\n\\hat f^{(t)}(X) = \\hat f^{(t-1)}(X) + \\alpha^{(t)} \\hat h^{(t)}(X)\n\\]\n\n\n\n\n\n\nForward Stagewise Additive Modelling on 1D Data\n\n\n\nIn general, the simultaneous optimization over \\(\\hat h^{(t)}\\) and \\(\\alpha^{(t)}\\) can be computationally expensive, depending on the weak learner and loss function used.\nFortunately, there are specific settings where this optimization can be done efficiently. One such setting is binary classification with exponential loss, which leads to the popular AdaBoost algorithm."
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#adaboost-adaptive-boosting-for-binary-classification",
    "href": "schedule/lectures/lecture_17_boosting.html#adaboost-adaptive-boosting-for-binary-classification",
    "title": "Lecture 17: Boosting",
    "section": "AdaBoost: Adaptive Boosting for Binary Classification",
    "text": "AdaBoost: Adaptive Boosting for Binary Classification\n\nAdaBoost (Adaptive Boosting) was introduced by Freund and Schapire in 1996.\nIt is designed for binary classification problems and uses exponential loss, and it is a rare example where the optimization problem above can be solved efficiently in closed form.\nAdaBoost has remarkable theoretical properties: it can drive training error to zero in \\(O(\\log n)\\) iterations.\n\n\nDeriving AdaBoost\n\nAssume that \\(Y_i \\in \\{-1, +1\\}\\) for all \\(i\\) (binary classification), and assume that each weak learner \\(\\hat h^{(t)}\\) also outputs predictions in \\(\\{-1, +1\\}\\).\nThe exponential loss is given by:\n\\[ \\ell(Y, \\hat Y) = \\exp(-Y \\hat Y) \\]\n\nIf \\(Y\\) and \\(\\hat Y\\) have the same sign (correct classification), then the term inside the exponential is negative (low loss)\nIf \\(Y\\) and \\(\\hat Y\\) have opposite signs (incorrect classification), then the term inside the exponential is positive (high loss)\nWe can view the exponential loss as an approximation to the 0-1 loss that really penalizes large/incorrect \\(\\hat Y\\) values.\n\nLet’s apply this loss (and the fact that \\(\\hat h^{(t)}(X_i) \\in \\{-1, +1\\}\\)) to the generic boosting optimization problem:\n\\[\n\\begin{align*}\n(\\hat h^{(t)}, \\alpha^{(t)}) &= \\arg\\min_{h, \\alpha} \\sum_{i=1}^n \\ell \\left( Y_i, \\hat f^{(t-1)}(X_i) + \\alpha h(X_i) \\right)\n\\\\\n&= \\arg\\min_{h, \\alpha} \\sum_{i=1}^n \\exp\\left( -Y_i \\left( \\hat f^{(t-1)}(X_i) + \\alpha h(X_i) \\right) \\right)\n\\\\\n&= \\arg\\min_{h, \\alpha} \\sum_{i=1}^n \\underbrace{\\exp\\left( -Y_i \\hat f^{(t-1)}(X_i) \\right)}_{W_i} \\exp\\left( -\\alpha Y_i h(X_i) \\right)\n\\end{align*}\n\\]\nWe can interpret the \\(W_i = \\exp\\left( -Y_i \\hat f^{(t-1)}(X_i) \\right)\\) terms as weights on each training example \\(i\\), which depend on how well the current ensemble \\(\\hat f^{(t-1)}\\) classifies that example. Misclassified points will have larger weights, while correctly classified points will have smaller weights.\n\n\n\n\n\n\nTipInterpreting the Weights\n\n\n\n\n\nThe optimization problem\n\\[\\min_{h, \\alpha} \\sum_{i=1}^n W_i \\exp\\left( -\\alpha Y_i h(X_i) \\right) \\]\ncan be interpreted as training the weak learner \\(h\\) to minimize a weighted version of the exponential loss.\n\nIf \\(W_i = 1\\) for all \\(i\\), then this is just the ERM problem for exponential loss.\nHowever, if some \\(W_i\\) are larger than others, then the weak learner will focus more on correctly classifying those points with larger weights.\nBy setting \\(W_i\\) to be proportional to how poorly the current ensemble classifies point \\(i\\), we are effectively training the weak learner to focus on the “hard” points that the current ensemble misclassifies.\n\n\n\n\nNote that, for any fixed \\(\\alpha\\) and \\(h\\), the rightmost term can take on only two possible values:\n\\[\n\\exp\\left( -\\alpha Y_i h(X_i) \\right) = \\begin{cases}\n\\exp(-\\alpha) & \\text{if } Y_i = h(X_i) \\text{ (correct classification)} \\\\\n\\exp(\\alpha) & \\text{if } Y_i \\neq h(X_i) \\text{ (incorrect classification)}\n\\end{cases}\n\\]\nWe can thus subdivide the above summation into points that \\(h\\) correctly classifies and points that it misclassifies:\n\\[\n\\begin{align*}\n(\\hat h^{(t)}, \\alpha^{(t)}) &= \\arg\\min_{h, \\alpha} \\left[ \\sum_{i: Y_i = h(X_i)} W_i \\exp \\left( -\\alpha \\underbrace{Y_i h(X_i)}_{=+1} \\right) + \\sum_{i: Y_i \\neq h(X_i)} W_i \\exp \\left( -\\alpha \\underbrace{Y_i h(X_i)}_{=-1} \\right) \\right] \\\\\n&= \\arg\\min_{h, \\alpha} \\left[ \\exp(-\\alpha) \\sum_{i: Y_i = h(X_i)} W_i + \\exp(\\alpha) \\sum_{i: Y_i \\neq h(X_i)} W_i \\right]\n\\end{align*}\n\\]\nBy inspection (and assuming that \\(\\alpha &gt; 0\\)), we can see that the optimal weak learner \\(\\hat h^{(t)}\\) is the one that minimizes the weighted classification error:\n\\[\n\\hat h^{(t)} = \\arg\\min_h \\sum_{i=1}^n W_i \\mathbb{I}(Y_i \\neq h(X_i))\n\\]\nregardless of the value of \\(\\alpha\\).\nMoreover, for any fixed \\(h\\), we can find the optimal \\(\\alpha\\) by differentiating the above expression, which yields:\n\\[\n\\alpha^{(t)} = \\frac{1}{2} \\log \\left( \\frac{\\sum_{i: Y_i = \\hat h^{(t)}(X_i)} W_i}{\\sum_{i: Y_i \\neq \\hat h^{(t)}(X_i)} W_i} \\right)\n\\]\n\n\n\n\n\n\n\nNoteThe AdaBoost Algorithm\n\n\n\nGiven: - A training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\) with \\(Y_i \\in \\{-1, +1\\}\\) - A weak learning algorithm that outputs predictions in \\(\\{-1, +1\\}\\) - The number of boosting rounds \\(m\\)\nInitialize weights: Set \\(W_i = 1\\) for all \\(i = 1, \\ldots, n\\)\nFor \\(t = 1, 2, \\ldots, m\\):\n\nTrain a weak learner \\(\\hat h^{(t)}\\) to minimize the weighted classification error:\n\\[\n\\hat h^{(t)} = \\arg\\min_h \\sum_{i=1}^n W_i \\mathbb{I}(Y_i \\neq h(X_i))\n\\]\nCompute the weight for the weak learner:\n\\[\n\\alpha^{(t)} = \\frac{1}{2} \\log \\left( \\frac{\\sum_{i: Y_i = \\hat h^{(t)}(X_i)} W_i}{\\sum_{i: Y_i \\neq \\hat h^{(t)}(X_i)} W_i} \\right)\n\\]\nUpdate the ensemble:\n\\[\n\\hat f^{(t)}(X) = \\hat f^{(t-1)}(X) + \\alpha^{(t)} \\hat h^{(t)}(X)\n\\]\nUpdate the training example weights:\n\\[\nW_i \\gets W_i \\exp\\left( -\\alpha^{(t)} Y_i \\hat h^{(t)}(X_i) \\right) = \\exp\\left( -Y_i \\hat f^{(t)}(X_i) \\right)\n\\]\n\n\n\nAdaBoost will decrease the training error exponentially fast in the number of boosting rounds \\(m\\), regardless of how “weak” the base learners are. We can see an example of this convergence in the figure below, which applies boosted decision stumps (depth=1) to the mobility dataset. The resulting boosted ensemble is better than an optimally-tuned single decision tree after just a few boosting rounds!\n\n\n\nBoosted Decision Stumps on Mobility Data"
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#gradient-boosting-a-more-general-framework",
    "href": "schedule/lectures/lecture_17_boosting.html#gradient-boosting-a-more-general-framework",
    "title": "Lecture 17: Boosting",
    "section": "Gradient Boosting: A More General Framework",
    "text": "Gradient Boosting: A More General Framework\nWhat about other settings beyond binary classification with exponential loss? It turns out that a slight relaxation to the forward stagewise additive modelling framework, known as gradient boosting, provides a computationally efficient boosting approach that can be applied to essentially any (differentiable) loss function and weak learner. Its generality and efficiency make it the most widely used boosting algorithm in practice today.\nApplying the gradient boosting framework to decision stumps (i.e. depth-1 or depth-2 decision trees) is one of the most popular machine learning algorithms in practice today. A popular implementation is XGBoost, which has been used to win a non-trivial portion of machine learning competitions on Kaggle.\n\n\n\n\n\n\nWarningAdvanced Material Below!\n\n\n\nYou will not have to know the details of gradient boosting for this course/the final exam. You will only need to know of its existence, and that it is a more general/computationally efficient boosting algorithm than the forward stagewise additive modelling framework.\n\n\n\nThe Core Idea\n\nRecall that in the basic boosting algorithm, at each iteration we train a weak learner to predict the residuals \\(R_i^{(t)} = Y_i - \\hat f^{(t-1)}(X_i)\\).\nThese residuals can be viewed as the negative gradient of the squared loss function: \\[\nR_i^{(t)} = Y_i - \\hat f^{(t-1)}(X_i) = -\\frac{\\partial}{\\partial \\hat f^{(t-1)}(X_i)} \\frac{1}{2}(Y_i - \\hat f^{(t-1)}(X_i))^2\n\\]\nGradient boosting generalizes this idea to any differentiable loss function \\(\\ell (Y, \\hat f(X))\\) by training each weak learner to predict the negative gradient of the loss: \\[\nG_i^{(t)} = -\\frac{\\partial \\ell(Y_i, \\hat f(X_i))}{\\partial \\hat f(X_i)} \\bigg|_{\\hat f = \\hat f^{(t-1)}}\n\\]\n\n\n\nThe Gradient Boosting Algorithm\n\n\n\n\n\n\nNoteThe Gradient Boosting Algorithm\n\n\n\nGiven:\n\nA training sample \\(\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^n\\)\nA differentiable loss function \\(\\ell(Y, \\hat f(X))\\)\nA weak learning algorithm (e.g., depth-2 decision trees)\nThe number of boosting rounds \\(m\\)\n\nInitialize: Set \\(\\hat f^{(0)}(X) = \\arg\\min_c \\sum_{i=1}^n \\ell(Y_i, c)\\) (constant initial prediction)\nFor \\(t = 1, 2, \\ldots, m\\):\n\nCompute the negative gradient (pseudo-residuals) for each training example: \\[\nG_i^{(t)} = -\\frac{\\partial \\ell(Y_i, \\hat f^{(t-1)}(X_i))}{\\partial \\hat f^{(t-1)}(X_i)}\n\\]\nTrain a weak learner \\(\\hat h^{(t)}\\) to predict the pseudo-residuals: \\[\n\\hat h^{(t)} = \\arg\\min_h \\sum_{i=1}^n \\ell (G_i^{(t)}, h(X_i))\n\\]\nFind the optimal weight for the weak learner: \\[\n\\alpha^{(t)} = \\arg\\min_\\alpha \\sum_{i=1}^n \\ell(Y_i, \\hat f^{(t-1)}(X_i) + \\alpha \\hat h^{(t)}(X_i))\n\\]\nUpdate the ensemble: \\[\n\\hat f^{(t)}(X) = \\hat f^{(t-1)}(X) + \\alpha^{(t)} \\hat h^{(t)}(X)\n\\]\n\nReturn the final predictor: \\(\\hat f^{(m)}(X)\\)\n\n\n\n\nWhy “Gradient” Boosting?\n\nLet \\(\\boldsymbol G^{(t)} \\in \\mathbb R^n\\) be the concatenation of the gradients and let \\(\\boldsymbol{\\hat h}^{(t)} \\in \\mathbb R^n\\) be the predictions of the weak learner on the training data.\n\\[ \\boldsymbol G^{(t)} = \\begin{bmatrix} G_1^{(t)} \\\\ G_2^{(t)} \\\\ \\vdots \\\\ G_n^{(t)} \\end{bmatrix} \\quad , \\quad \\boldsymbol{\\hat h}^{(t)} = \\begin{bmatrix} \\hat h^{(t)}(X_1) \\\\ \\hat h^{(t)}(X_2) \\\\ \\vdots \\\\ \\hat h^{(t)}(X_n) \\end{bmatrix} \\]\nIn Step 2 of the algorithm, our goal is to minimize the loss between the gradients and the weak learner predictions:\n\\[ \\hat h^{(t)} = \\arg\\min_h \\sum_{i=1}^n \\left( G_i^{(t)} - h(X_i) \\right)^2, \\]\ni.e. we want to make each \\(\\hat h^{(t)}(X_i)\\) as close as possible to \\(G_i^{(t)}\\).\nWe can view this idea as wanting the \\(\\boldsymbol{\\hat h}^{(t)}\\) vector to point in the opposite direction as the \\(\\boldsymbol G^{(t)}\\) vector. In other words, we want to minimize the cosine similarity between these two vectors:\n\\[ \\min_{\\hat h^{(t)}} \\frac{\\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle}{\\|\\boldsymbol G^{(t)}\\|_2 \\|\\boldsymbol{\\hat h}^{(t)}\\|_2}, \\]\nas the cosine similarity is minimized (at \\(-1\\)) when the two vectors point in opposite directions.\nSince we will scale the predictions of the weak learner by \\(\\alpha^{(t)}\\) in Step 4, we can assume without loss of generality that \\(\\|\\boldsymbol{\\hat h}^{(t)}\\|_2 = 1\\).\nMinimizing the cosine similarity is thus equivalent to:\n\\[\n\\begin{align*}\n\\min_{\\hat h^{(t)}} \\frac{\\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle}{\\|\\boldsymbol G^{(t)}\\|_2 \\|\\boldsymbol{\\hat h}^{(t)}\\|_2}\n&= \\min_{\\hat h^{(t)}} \\frac{\\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle}{\\|\\boldsymbol G^{(t)}\\|_2 }\n\\\\\n&= \\min_{\\hat h^{(t)}} \\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle\n\\\\\n&= \\min_{\\hat h^{(t)}} \\sum_{i=1}^n \\ell(Y_i, \\hat f^{(t-1)}(X_i)) + \\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle\n\\end{align*}\n\\]\nwhere the last two equalities hold because neither \\(\\Vert \\boldsymbol G^{(t)} \\Vert\\) nor \\(\\sum_{i=1}^n \\ell(Y_i, \\hat f^{(t-1)}(X_i))\\) depend on \\(\\hat h^{(t)}\\).\nIn other words, Step 2 is equivalent to finding \\(\\hat h^{(t)}\\) that minimizes the first-order Taylor approximation of the loss around the current ensemble predictions:\n\\[\n\\sum_{i=1}^n \\ell(Y_i, \\hat f^{(t-1)}(X_i) + \\hat h^{(t)}(X_i)) \\approx \\sum_{i=1}^n \\ell(Y_i, \\hat f^{(t-1)}(X_i)) + \\langle \\boldsymbol G^{(t)}, \\boldsymbol{\\hat h}^{(t)} \\rangle.\n\\]\n\n\n\n\n\n\n\nTipConnection to Gradient-Based Optimization\n\n\n\nFor those of you who want to take a mathematical trip, we can view gradient boosting as performing gradient descent in function space. This idea is quite advanced, but if we view the ensemble prediction function \\(\\hat f^{(t)} : \\mathbb R^d \\to \\mathbb R\\) as an infinite-dimensional parameter vector, at each iteration we are updating this parameter vector with \\(\\hat h^{(t)} : \\mathbb R^d \\to \\mathbb R\\), which is another infinite-dimensional parameter vector that is aligned with the negative gradient of the loss."
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#properties-of-boosting",
    "href": "schedule/lectures/lecture_17_boosting.html#properties-of-boosting",
    "title": "Lecture 17: Boosting",
    "section": "Properties of Boosting",
    "text": "Properties of Boosting\n\nBias and Variance\n\nDecreases bias: Boosting combines many weak (high-bias) learners into a strong (low-bias) learner. As we add more boosting rounds, the ensemble becomes more flexible and can fit more complex functions. The training error typically decreases monotonically (even exponentially fast for AdaBoost) as we add more rounds.\nCan increase variance, but not much if done properly: Unlike bagging, boosting does not explicitly aim to reduce variance. In fact, boosting can increase variance if we use too many rounds, as the ensemble starts to overfit the training data. However, by using small weak learners and adding them gradually, we can control the variance and prevent overfitting.\n\n\n\nAdvantages and Disadvantages\n\nMuch more efficient than bagging/random forests: Because boosted ensembles use small weak learners (e.g., depth-2 trees) rather than full-depth trees, each component model is much faster to train. A depth-2 tree has at most 4 leaf nodes, while a full-depth tree on a dataset of size \\(n\\) has \\(n\\) leaf nodes!\nGood for anytime learning: Boosting is inherently sequential, but this can be an advantage for “anytime learning” scenarios. For a given test point, we don’t have to feed the test point through all \\(m\\) weak learners in the ensemble. Instead, we can truncate the ensemble prediction after \\(k &lt; m\\) models to get a quick prediction. Because the weak learners are trained on sequentially smaller residuals, the first few models in the ensemble will typically capture most of the signal, and thus may yield a reasonable prediction.\n(“Anytime learning” is a popular approach for early ML systems on resource-constrained devices. It was first used &lt;20 years ago to detect faces on pre-smartphone digital cameras!)\nSequential nature is a limitation: Unlike bagging/random forests, where each tree can be trained independently (and thus in parallel), boosting requires sequential training.\nNo free uncertainty quantification/risk estimation. Bagged ensembles were built upon bootstrapped training samples, which allowed us to inherit all the advantages of bootstrapping (e.g. variance estimates/confidence intervals, risk estimates on out-of-bag samples, etc.). Boosted ensembles do not have this property, since all weak learners are trained on the same training data.\n\n\n\nRisk of Overfitting\n\nBoosting can overfit if we use too many rounds or if the weak learners are too complex.\nHowever, in practice most boosting algorithms have been observed to be relatively resistant to overfitting (sometimes continuing to improve test error even after training error reaches zero), though this is not guaranteed and depends on the dataset and noise characteristics."
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#boosting-vs-bagging",
    "href": "schedule/lectures/lecture_17_boosting.html#boosting-vs-bagging",
    "title": "Lecture 17: Boosting",
    "section": "Boosting vs Bagging",
    "text": "Boosting vs Bagging\nBoosting and bagging are both ensemble methods, and frequently methods used to ensemble decision trees. However, they are not interchangeable!\n\nBagging reduces variance, and so it is best suited to ensemble low-bias/high-variance models (e.g., full-depth decision trees).\nBoosting reduces bias, and so it is best suited to ensemble high-bias/low-variance models (e.g., shallow decision trees).\n\n\nCan Boosting Rely on Independent Weak Learners?\n\nIn bagging/random forests, we relied on the fact that the component models made independent errors (due to being trained on different bootstrap samples) to achieve variance reduction through averaging.\nFor boosting high-bias models, can we similarly rely on independent weak learners?\n\n\n\n\n\n\nTipIntuitive Answer\n\n\n\n\n\nNo. If each weak learner makes systematic errors (e.g., always underpredicting in certain regions), then averaging many independent weak learners will not eliminate these systematic errors. The ensemble will still have high bias.\nTo reduce bias, we need the weak learners to correct each other’s errors in a coordinated way. This requires that the weak learners be trained sequentially, not independently.\n\n\n\n\n\n\n\n\n\nTipStatistical Justification\n\n\n\n\n\nRecall that bias is a systematic error: \\(\\text{Bias}[\\hat f(X)] = \\mathbb{E}[\\hat f(X)] - f(X)\\).\nIf we have \\(m\\) independent weak learners \\(\\hat h^{(1)}, \\ldots, \\hat h^{(m)}\\), then the ensemble predictor is:\n\\[\n\\hat f_\\text{ens}(X) = \\frac{1}{m} \\sum_{j=1}^m \\hat h^{(j)}(X)\n\\]\nThe bias of the ensemble is:\n\\[\n\\begin{aligned}\n\\text{Bias}[\\hat f_\\text{ens}(X)] &= \\mathbb{E}[\\hat f_\\text{ens}(X)] - \\mathbb E[Y \\mid X] \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m \\left( \\mathbb{E}[\\hat h^{(j)}(X)] - \\mathbb E[Y \\mid X] \\right) \\\\\n&= \\mathbb{E}[\\hat h^{(j)}(X)] - \\mathbb E[Y \\mid X] = \\text{Bias}[\\hat h^{(j)}(X)]\n\\end{aligned}\n\\]\nThus, the bias of the ensemble is the same as the bias of any individual weak learner! Averaging independent weak learners does not reduce bias.\n\n\n\n\n\n\nWhen to Use Boosting vs. Bagging/Random Forests\n\nBoosting (with small trees) is generally preferred when:\n\nComputational efficiency is important (faster training, smaller models)\nOur computational resources are sequential, not parallel\n\nBagging/Random Forests (with large trees) are generally preferred when:\n\nUncertainty quantification is important\nWe want risk estimates without cross-validation\nWe have access to computational resources\n\nIn practice, both methods are extremely powerful, and the choice often depends on the specific problem and priorities (accuracy vs. uncertainty vs. computational cost)."
  },
  {
    "objectID": "schedule/lectures/lecture_17_boosting.html#summary",
    "href": "schedule/lectures/lecture_17_boosting.html#summary",
    "title": "Lecture 17: Boosting",
    "section": "Summary",
    "text": "Summary\n\nBoosting is an ensemble method that combines many weak learners (high-bias models) into a strong learner (low-bias model).\nThe key insight is to train weak learners sequentially, where each new weak learner corrects the errors of the previous ensemble. This is fundamentally different from bagging, which trains models independently to reduce variance.\nAdaBoost is a specific boosting algorithm for binary classification that uses exponential loss and adaptive sample weighting. It has strong theoretical guarantees, driving training error to zero in \\(O(\\log n)\\) iterations.\nGradient boosting is a more general framework that can be applied to any differentiable loss function by training each weak learner to predict the negative gradient of the loss. This can be viewed as gradient descent in function space.\nBoosting reduces bias (unlike bagging, which reduces variance) and is computationally efficient, but does not provide uncertainty quantification or out-of-bag error estimates."
  }
]