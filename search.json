[
  {
    "objectID": "schedule/handouts/lab00-git.html",
    "href": "schedule/handouts/lab00-git.html",
    "title": "Lab 00 Git",
    "section": "",
    "text": "Check your Canvas profile settings to ensure the email associated with your Canvas account is correct.\nReview your Canvas notification settings and decide what you want to be notified about.\nVisit the Course website. In particular, as you might expect, this course requires computing. We will use R and RStudio as well as Git and GitHub. See the Computing tab.\nIf you have never used GitHub before, go to https://github.com/ create an account. You should be aware that this data is stored on US servers. Please exercise caution whenever using personal information. You may wish to use a pseudonym to protect your privacy if you have concerns.\n\nIf you haven’t already, visit Computing and follow the instructions to set up your computer.\n\nClone your labs-&lt;username&gt; repo.\n\nNavigate to the Course GitHub using the link in the Navbar.\nThen go to your labs-&lt;username&gt;\nClick the Green “Code” button, and copy the url by clicking the two overlapping squares.\nThen in RStudio, choose “New project” &gt; “Version Control” &gt; “Git” and paste the address.\nChoose a location on your machine where you want all your labs to be.\nSelect “Create Project”."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "href": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "title": "Lab 00 Git",
    "section": "Scenario 1. You do work on the wrong branch.",
    "text": "Scenario 1. You do work on the wrong branch.\nMake sure that you are on main. Remember that the actual submission is on the lab00-git branch.\nIn the R code chunk below, fit a linear model to the data and print the estimated coefficients, rounded to 2 decimal places.\n\nlibrary(tibble)\nset.seed(12345)\ndat &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = 2 + 3 * x1 - x2 + rnorm(100)\n)\n\nNow, stage the .Rmd. Commit with the message “on the wrong branch” and push.\nYou likely see an error like:\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThat’s because you’re on main. Ugh! But I did some work, and now I need to be on a different branch!\nSo let’s fix it. We want the stuff we just did on main to be on lab00-git. Note that everything you did is saved! Here are the steps:\nGet our changes onto the correct branch\n\nUse the dropdown to switch branches to lab00-git.\nGo to the Terminal (next to console).\nType git merge main.\n\nThat should copy all your changes in the .Rmd that you made on main into the correct place. Did it?\nIf you do this and you ever see stuff like\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the stuff that is currently on this branch.\n=======\nThis is stuff that got added on the other branch.\nWhile someone else changed stuff on this branch!\nI (git) don't know which to keep!?\nYou have to decide for me.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict\nThis means that there were conflicts between the two versions. The stuff above ====== was in your current branch. The stuff below is what you’re trying to merge in. You decide what to keep, the top, the bottom, or both (or neither). Just be sure to delete the junk lines with &lt;, &gt;, or =.\n\nOK. So now we have our changes in the right spot. Commit and Push the .Rmd (only). Let’s clean up main so we don’t have problems later. Switch back to main.\nUndo mistakes on the wrong branch.\nIn the Terminal, type git log. You should see some commits, one with the message “on the wrong branch”. There’s a bunch of other text that I won’t try to explain, but look at the stuff before (meaning below) that message. That’s the commit from before you did work on the wrong branch. You should see something like:\ncommit 1851c738984690b039a79a04e070f766e19993d5\nThat long string is what we’re after. It’s called a hash, and it uniquely identifies the commit. Take note of the first characters (like 5). That’s usually enough to get away with.\nType q to exit the log viewer.`\nNow type git reset --hard 1851c (replacing 1851c with the numbers from your unique hash). This command will undo any changes after that commmit, but only for this branch.\nTo recap, now the work we want is in the right place (on the other branch), and the mess on main is cleaned up. Boom."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "href": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "title": "Lab 00 Git",
    "section": "Scenario 2. You did something you shouldn’t have",
    "text": "Scenario 2. You did something you shouldn’t have\nSwitch your branch back to lab00-git (or whatever you named it).\nOpen the file lab01.Rmd. Select everything after # Instructions and delete it. Save. Then Knit (producing a pdf). Commit both files with a message “did the wrong lab, and built a pdf”. Push your commits with the Green up arrow.\nTake a look at the PR on GitHub now. There’s a bunch of crud that shouldn’t be there.\nWe’ve done 3 things here that we shouldn’t have.\n\nWe built a .pdf that we don’t want at all. It needs to go away.\nWe bollixed up the lab01.Rmd file. We don’t want that or it will screw up the lab next week.\nWe pushed it all into our submission for this week.\n\nThe first instinct is to Delete both files, commit, and push. This is VERY BAD. That will further screw up everything. Basically, you’re telling git “I don’t want these files at all” when you mean “I don’t want changes to these files in this branch”. The difference is subtle but important. Because you DO want these files (without the changes) at some point, but you don’t want them here.\nLet’s fix these issues.\n\nFirst, we want to “get rid of” the pdf. In the Terminal type\ngit reset HEAD^ -- lab01.pdf\nClick the little “Refresh” arrow ↩︎️ in the Git panel. You should now see lab01.pdf twice, once with a red D that is checked and once with two yellow question marks that is NOT checked. This is what we want. Don’t click any other boxes.\nCommit exactly as is. Use a message like “remove the stray pdf” and Push. Now, take a look at the PR on GitHub. It should be gone from the list of files in the PR.\nThere’s still that annoying two-yellow-question-mark version in the Git panel. Don’t click the check box (that will just redo everything we undid). Instead, highlight the file by clicking the file name, click the Gear Icon Dropdown ⛭, and then select “Revert”. Now it’s gone, and the pdf should disappear from your filesystem.\n\nSecond, let’s “undo” the deletion in the .Rmd. This is easy, and a useful pattern to remember.\nIn the Terminal, type\ngit checkout main -- lab01.Rmd\nWhat this does is grabs the version on main that isn’t messed up and puts it here, overwriting your changes. This isn’t the only way to fix your problem (you could have done the same thing we did with the pdf), but it’s pretty easy.\nStage, commit, and push. Now look at the PR on GitHub. Even though you made two changes (one deleting everything, and one restoring everything) to the lab01.Rmd, it should be “gone” from the PR now. That’s because the version on this branch looks just like the version on main, so there are no changes to be made into the main branch. This is just what we want.\n\nNow we’ve also fixed the third error already. None of those bogus changes to lab01 are in our PR for this week anymore."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#lets-just-recap-the-right-way.",
    "href": "schedule/handouts/lab00-git.html#lets-just-recap-the-right-way.",
    "title": "Lab 00 Git",
    "section": "Let’s just recap THE RIGHT WAY.",
    "text": "Let’s just recap THE RIGHT WAY.\n\nFor HW or Labs, always start on main.\nPull in the remote ⬇️ just to be sure everything is up-to-date.\nCreate a branch for your HW/Lab and switch to it. The name doesn’t matter, but it’s good practice to name in something meaningful (rather than something like stat406-lab-1 when you’re doing lab 4).\nOpen the HW/Lab .Rmd and click Knit. Make sure it works.\nDo the work, saving regularly. When you complete a section, Commit the file with a useful message (Push or Not).\nOnce you’re done, make sure that you have done the minimum number of Commits, push ⬆️ your .Rmd and the knitted .pdf.\nOpen a PR on GitHub and respond to the questions.\nMake sure that only the .Rmd and the .pdf for this HW/Lab are there. And Create Pull Request.\nOn your machine, switch the branch to main to prepare for the next HW/Lab.\n\nIf the TA asks for changes, just switch to the branch for this assignment, and make the requested changes. It’s all on your machine (even if the pdf disappears when you switch)."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#meta-lecture",
    "href": "schedule/slides/12-why-smooth.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "12 To(o) smooth or not to(o) smooth?",
    "text": "12 To(o) smooth or not to(o) smooth?\nStat 406\nDaniel J. McDonald\nLast modified – 02 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#last-time",
    "href": "schedule/slides/12-why-smooth.html#last-time",
    "title": "UBC Stat406 2023W",
    "section": "Last time…",
    "text": "Last time…\nWe’ve been discussing smoothing methods in 1-dimension:\n\\[\\Expect{Y\\given X=x} = f(x),\\quad x\\in\\R\\]\nWe looked at basis expansions, e.g.:\n\\[f(x) \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_k x^k\\]\nWe looked at local methods, e.g.:\n\\[f(x_i) \\approx  s_i^\\top \\y\\]\n\nWhat if \\(x \\in \\R^p\\) and \\(p&gt;1\\)?\n\n\n\nNote that \\(p\\) means the dimension of \\(x\\), not the dimension of the space of the polynomial basis or something else. That’s why I put \\(k\\) above."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#kernels-and-interactions",
    "href": "schedule/slides/12-why-smooth.html#kernels-and-interactions",
    "title": "UBC Stat406 2023W",
    "section": "Kernels and interactions",
    "text": "Kernels and interactions\nIn multivariate nonparametric regression, you estimate a surface over the input variables.\nThis is trying essentially to find \\(\\widehat{f}(x_1,\\ldots,x_p)\\).\nTherefore, this function by construction includes interactions, handles categorical data, etc. etc.\nThis is in contrast with explicit linear models which need you to specify these things.\nThis extra complexity (automatically including interactions, as well as other things) comes with tradeoffs.\n\nMore complicated functions (smooth Kernel regressions vs. linear models) tend to have lower bias but higher variance."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1",
    "href": "schedule/slides/12-why-smooth.html#issue-1",
    "title": "UBC Stat406 2023W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\n\nyou don’t need to memorize these formulas but you should know the intuition"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-1",
    "href": "schedule/slides/12-why-smooth.html#issue-1-1",
    "title": "UBC Stat406 2023W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nRecall, this decomposition is squared bias + variance + irreducible error\n\nIt depends on the choice of \\(h\\)\n\n\\[\\textrm{MSE}(\\hat{f}) = C_1 h^4 + \\frac{C_2}{nh} + \\sigma^2\\]\n\nUsing \\(h = cn^{-1/5}\\) balances squared bias and variance, leads to the above rate. (That balance minimizes the MSE)"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-2",
    "href": "schedule/slides/12-why-smooth.html#issue-1-2",
    "title": "UBC Stat406 2023W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nIntuition:\nas you collect data, use a smaller bandwidth and the MSE (on future data) decreases"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-3",
    "href": "schedule/slides/12-why-smooth.html#issue-1-3",
    "title": "UBC Stat406 2023W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nHow does this compare to just using a linear model?\nBias\n\nThe bias of using a linear model if the truth nonlinear is a number \\(b &gt; 0\\) which doesn’t depend on \\(n\\).\nThe bias of using kernel regression is \\(C_1/n^{4/5}\\). This goes to 0 as \\(n\\rightarrow\\infty\\).\n\nVariance\n\nThe variance of using a linear model is \\(C/n\\) no matter what\nThe variance of using kernel regression is \\(C_2/n^{4/5}\\)."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-4",
    "href": "schedule/slides/12-why-smooth.html#issue-1-4",
    "title": "UBC Stat406 2023W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nTo conclude:\n\nbias of kernels goes to zero, bias of lines doesn’t (unless the truth is linear).\nbut variance of lines goes to zero faster than for kernels.\n\nIf the linear model is right, you win.\nBut if it’s wrong, you (eventually) lose as \\(n\\) grows.\nHow do you know if you have enough data?\nCompare of the kernel version with CV-selected tuning parameter with the estimate of the risk for the linear model."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-2",
    "href": "schedule/slides/12-why-smooth.html#issue-2",
    "title": "UBC Stat406 2023W",
    "section": "Issue 2",
    "text": "Issue 2\nFor \\(p&gt;1\\), there is more trouble.\nFirst, lets look again at \\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nThat is for \\(p=1\\). It’s not that much slower than \\(C/n\\), the variance for linear models.\nIf \\(p&gt;1\\) similar calculations show,\n\\[\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .\\]"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-2-1",
    "href": "schedule/slides/12-why-smooth.html#issue-2-1",
    "title": "UBC Stat406 2023W",
    "section": "Issue 2",
    "text": "Issue 2\n\\[\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .\\]\nWhat if \\(p\\) is big (and \\(n\\) is really big)?\n\nThen \\((C_1 + C_2) / n^{4/(4+p)}\\) is still big.\nBut \\(Cp / n\\) is small.\nSo unless \\(b\\) is big, we should use the linear model.\n\nHow do you tell? Do model selection to decide.\nA very, very questionable rule of thumb: if \\(p&gt;\\log(n)\\), don’t do smoothing."
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#meta-lecture",
    "href": "schedule/slides/10-basis-expansions.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "10 Basis expansions",
    "text": "10 Basis expansions\nStat 406\nDaniel J. McDonald\nLast modified – 27 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#what-about-nonlinear-things",
    "href": "schedule/slides/10-basis-expansions.html#what-about-nonlinear-things",
    "title": "UBC Stat406 2023W",
    "section": "What about nonlinear things",
    "text": "What about nonlinear things\n\\[\\Expect{Y \\given X=x} = \\sum_{j=1}^p x_j\\beta_j\\]\nNow we relax this assumption of linearity:\n\\[\\Expect{Y \\given X=x} = f(x)\\]\nHow do we estimate \\(f\\)?\n\nFor this lecture, we use \\(x \\in \\R\\) (1 dimensional)\nHigher dimensions are possible, but complexity grows exponentially.\nWe’ll see some special techniques for \\(x\\in\\R^p\\) later this Module."
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#start-simple",
    "href": "schedule/slides/10-basis-expansions.html#start-simple",
    "title": "UBC Stat406 2023W",
    "section": "Start simple",
    "text": "Start simple\nFor any \\(f : \\R \\rightarrow [0,1]\\)\n\\[f(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2 + \\frac{1}{3!}f'''(x_0)(x-x_0)^3 + R_3(x-x_0)\\]\nSo we can linearly regress \\(y_i = f(x_i)\\) on the polynomials.\nThe more terms we use, the smaller \\(R\\).\n\n\nCode\nset.seed(406406)\ndata(arcuate, package = \"Stat406\") \narcuate &lt;- arcuate |&gt; slice_sample(n = 220)\narcuate %&gt;% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) +\n  geom_smooth(color = orange, formula = y ~ poly(x, 3), method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#same-thing-different-orders",
    "href": "schedule/slides/10-basis-expansions.html#same-thing-different-orders",
    "title": "UBC Stat406 2023W",
    "section": "Same thing, different orders",
    "text": "Same thing, different orders\n\n\nCode\narcuate %&gt;% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) + \n  geom_smooth(aes(color = \"a\"), formula = y ~ poly(x, 4), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"b\"), formula = y ~ poly(x, 7), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"c\"), formula = y ~ poly(x, 25), method = \"lm\", se = FALSE) +\n  scale_color_manual(name = \"Taylor order\",\n    values = c(green, red, orange), labels = c(\"4 terms\", \"7 terms\", \"25 terms\"))"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#still-a-linear-smoother",
    "href": "schedule/slides/10-basis-expansions.html#still-a-linear-smoother",
    "title": "UBC Stat406 2023W",
    "section": "Still a “linear smoother”",
    "text": "Still a “linear smoother”\nReally, this is still linear regression, just in a transformed space.\nIt’s not linear in \\(x\\), but it is linear in \\((x,x^2,x^3)\\) (for the 3rd-order case)\nSo, we’re still doing OLS with\n\\[\\X=\\begin{bmatrix}1& x_1 & x_1^2 & x_1^3 \\\\ \\vdots&&&\\vdots\\\\1& x_n & x_n^2 & x_n^3\\end{bmatrix}\\]\nSo we can still use our nice formulas for LOO-CV, GCV, Cp, AIC, etc.\n\nmax_deg &lt;- 20\ncv_nice &lt;- function(mdl) mean( residuals(mdl)^2 / (1 - hatvalues(mdl))^2 ) \ncvscores &lt;- map_dbl(seq_len(max_deg), ~ cv_nice(lm(fa ~ poly(position, .), data = arcuate)))"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section",
    "href": "schedule/slides/10-basis-expansions.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Code\nlibrary(cowplot)\ng1 &lt;- ggplot(tibble(cvscores, degrees = seq(max_deg)), aes(degrees, cvscores)) +\n  geom_point(colour = blue) +\n  geom_line(colour = blue) + \n  labs(ylab = 'LOO-CV', xlab = 'polynomial degree') +\n  geom_vline(xintercept = which.min(cvscores), linetype = \"dotted\") \ng2 &lt;- ggplot(arcuate, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_smooth(\n    colour = orange, \n    formula = y ~ poly(x, which.min(cvscores)), \n    method = \"lm\", \n    se = FALSE\n  )\nplot_grid(g1, g2, ncol = 2)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#other-bases",
    "href": "schedule/slides/10-basis-expansions.html#other-bases",
    "title": "UBC Stat406 2023W",
    "section": "Other bases",
    "text": "Other bases\n\nPolynomials\n\n\\(x \\mapsto \\left(1,\\ x,\\ x^2, \\ldots, x^p\\right)\\) (technically, not quite this, they are orthogonalized)\n\nLinear splines\n\n\\(x \\mapsto \\bigg(1,\\ x,\\ (x-k_1)_+,\\ (x-k_2)_+,\\ldots, (x-k_p)_+\\bigg)\\) for some choices \\(\\{k_1,\\ldots,k_p\\}\\)\n\nCubic splines\n\n\\(x \\mapsto \\bigg(1,\\ x,\\ x^2,\\ x^3,\\ (x-k_1)^3_+,\\ (x-k_2)^3_+,\\ldots, (x-k_p)^3_+\\bigg)\\) for some choices \\(\\{k_1,\\ldots,k_p\\}\\)\n\nFourier series\n\n\\(x \\mapsto \\bigg(1,\\ \\cos(2\\pi x),\\ \\sin(2\\pi x),\\ \\cos(2\\pi 2 x),\\ \\sin(2\\pi 2 x), \\ldots, \\cos(2\\pi p x),\\ \\sin(2\\pi p x)\\bigg)\\)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#how-do-you-choose",
    "href": "schedule/slides/10-basis-expansions.html#how-do-you-choose",
    "title": "UBC Stat406 2023W",
    "section": "How do you choose?",
    "text": "How do you choose?\nProcedure 1:\n\nPick your favorite basis. This is not as easy as it sounds. For instance, if \\(f\\) is a step function, linear splines will do well with good knots, but polynomials will be terrible unless you have lots of terms.\nPerform OLS on different orders.\nUse model selection criterion to choose the order.\n\nProcedure 2:\n\nUse a bunch of high-order bases, say Linear splines and Fourier series and whatever else you like.\nUse Lasso or Ridge regression or elastic net. (combining bases can lead to multicollinearity, but we may not care)\nUse model selection criteria to choose the tuning parameter."
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#try-both-procedures",
    "href": "schedule/slides/10-basis-expansions.html#try-both-procedures",
    "title": "UBC Stat406 2023W",
    "section": "Try both procedures",
    "text": "Try both procedures\n\nSplit arcuate into 75% training data and 25% testing data.\nEstimate polynomials up to 20 as before and choose best order.\nDo ridge, lasso and elastic net \\(\\alpha=.5\\) on 20th order polynomials, B splines with 20 knots, and Fourier series with \\(p=20\\). Choose tuning parameter (using lambda.1se).\nRepeat 1-3 10 times (different splits)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section-1",
    "href": "schedule/slides/10-basis-expansions.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "library(glmnet)\nmapto01 &lt;- function(x, pad = .005) (x - min(x) + pad) / (max(x) - min(x) + 2 * pad)\nx &lt;- mapto01(arcuate$position)\nXmat &lt;- cbind(\n  poly(x, 20), \n  splines::bs(x, df = 20), \n  cos(2 * pi * outer(x, 1:20)), sin(2 * pi * outer(x, 1:20))\n)\ny &lt;- arcuate$fa\nrmse &lt;- function(z, s) sqrt(mean( (z - s)^2 ))\nnzero &lt;- function(x) with(x, nzero[match(lambda.1se, lambda)])\nsim &lt;- function(maxdeg = 20, train_frac = 0.75) {\n  n &lt;- nrow(arcuate)\n  train &lt;- as.logical(rbinom(n, 1, train_frac))\n  test &lt;- !train # not precisely 25%, but on average\n  polycv &lt;- map_dbl(seq(maxdeg), ~ cv_nice(lm(y ~ Xmat[,seq(.)], subset = train))) # figure out which order to use\n  bpoly &lt;- lm(y[train] ~ Xmat[train, seq(which.min(polycv))]) # now use it\n  lasso &lt;- cv.glmnet(Xmat[train, ], y[train])\n  ridge &lt;- cv.glmnet(Xmat[train, ], y[train], alpha = 0)\n  elnet &lt;- cv.glmnet(Xmat[train, ], y[train], alpha = .5)\n  tibble(\n    methods = c(\"poly\", \"lasso\", \"ridge\", \"elnet\"),\n    rmses = c(\n      rmse(y[test], cbind(1, Xmat[test, 1:which.min(polycv)]) %*% coef(bpoly)),\n      rmse(y[test], predict(lasso, Xmat[test,])),\n      rmse(y[test], predict(ridge, Xmat[test,])),\n      rmse(y[test], predict(elnet, Xmat[test,]))\n    ),\n    nvars = c(which.min(polycv), nzero(lasso), nzero(ridge), nzero(elnet))\n  )\n}\nset.seed(12345)\nsim_results &lt;- map(seq(20), sim) |&gt; list_rbind() # repeat it 20 times"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section-2",
    "href": "schedule/slides/10-basis-expansions.html#section-2",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Code\nsim_results |&gt;  \n  pivot_longer(-methods) |&gt; \n  ggplot(aes(methods, value, fill = methods)) + \n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_y\") + \n  ylab(\"\") +\n  theme(legend.position = \"none\") + \n  xlab(\"\") +\n  scale_fill_viridis_d(begin = .2, end = 1)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#common-elements",
    "href": "schedule/slides/10-basis-expansions.html#common-elements",
    "title": "UBC Stat406 2023W",
    "section": "Common elements",
    "text": "Common elements\nIn all these cases, we transformed \\(x\\) to a higher-dimensional space\nUsed \\(p+1\\) dimensions with polynomials\nUsed \\(p+4\\) dimensions with cubic splines\nUsed \\(2p+1\\) dimensions with Fourier basis"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#featurization",
    "href": "schedule/slides/10-basis-expansions.html#featurization",
    "title": "UBC Stat406 2023W",
    "section": "Featurization",
    "text": "Featurization\nEach case applied a feature map to \\(x\\), call it \\(\\Phi\\)\nWe used new “features” \\(\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)\\)\nNeural networks (coming in module 4) use this idea\nYou’ve also probably seen it in earlier courses when you added interaction terms or other transformations.\n\nSome methods (notably Support Vector Machines and Ridge regression) allow \\(k=\\infty\\)\nSee [ISLR] 9.3.2 for baby overview or [ESL] 5.8 (note 😱)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#meta-lecture",
    "href": "schedule/slides/08-ridge-regression.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "08 Ridge regression",
    "text": "08 Ridge regression\nStat 406\nDaniel J. McDonald\nLast modified – 27 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#recap",
    "href": "schedule/slides/08-ridge-regression.html#recap",
    "title": "UBC Stat406 2023W",
    "section": "Recap",
    "text": "Recap\nSo far, we have emphasized model selection as\nDecide which predictors we would like to use in our linear model\nOr similarly:\nDecide which of a few linear models to use\nTo do this, we used a risk estimate, and chose the “model” with the lowest estimate\n\nMoving forward, we need to generalize this to\nDecide which of possibly infinite prediction functions \\(f\\in\\mathcal{F}\\) to use\nThankfully, this isn’t really any different. We still use those same risk estimates.\nRemember: We were choosing models that balance bias and variance (and hence have low prediction risk).\n\\[\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#regularization",
    "href": "schedule/slides/08-ridge-regression.html#regularization",
    "title": "UBC Stat406 2023W",
    "section": "Regularization",
    "text": "Regularization\n\nAnother way to control bias and variance is through regularization or shrinkage.\nRather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\nI mean ALL.\nBut, make your estimates of \\(\\beta\\) “smaller”"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#brief-aside-on-optimization",
    "href": "schedule/slides/08-ridge-regression.html#brief-aside-on-optimization",
    "title": "UBC Stat406 2023W",
    "section": "Brief aside on optimization",
    "text": "Brief aside on optimization\n\nAn optimization problem has 2 components:\n\nThe “Objective function”: e.g. \\(\\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\).\nThe “constraint”: e.g. “fewer than 5 non-zero entries in \\(\\beta\\)”.\n\nA constrained minimization problem is written\n\n\\[\\min_\\beta f(\\beta)\\;\\; \\mbox{ subject to }\\;\\; C(\\beta)\\]\n\n\\(f(\\beta)\\) is the objective function\n\\(C(\\beta)\\) is the constraint"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-constrained-version",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-constrained-version",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression (constrained version)",
    "text": "Ridge regression (constrained version)\nOne way to do this for regression is to solve (say): \\[\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\sum_j \\beta^2_j &lt; s\n\\] for some \\(s&gt;0\\).\n\nThis is called “ridge regression”.\nCall the minimizer of this problem \\(\\brt\\)\n\n\nCompare this to ordinary least squares:\n\\[\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\beta \\in \\R^p\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#geometry-of-ridge-regression-contours",
    "href": "schedule/slides/08-ridge-regression.html#geometry-of-ridge-regression-contours",
    "title": "UBC Stat406 2023W",
    "section": "Geometry of ridge regression (contours)",
    "text": "Geometry of ridge regression (contours)\n\n\nCode\nlibrary(mvtnorm)\nnorm_ball &lt;- function(q = 1, len = 1000) {\n  tg &lt;- seq(0, 2 * pi, length = len)\n  out &lt;- tibble(x = cos(tg), b = (1 - abs(x)^q)^(1 / q), bm = -b) |&gt;\n    pivot_longer(-x, values_to = \"y\")\n  out$lab &lt;- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipse_data &lt;- function(\n  n = 75, xlim = c(-2, 3), ylim = c(-2, 3),\n  mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  expand_grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)) |&gt;\n    rowwise() |&gt;\n    mutate(z = dmvnorm(c(x, y), mean, Sigma))\n}\n\nlballmax &lt;- function(ed, q = 1, tol = 1e-6, niter = 20) {\n  ed &lt;- filter(ed, x &gt; 0, y &gt; 0)\n  feasible &lt;- (ed$x^q + ed$y^q)^(1 / q) &lt;= 1\n  best &lt;- ed[feasible, ]\n  best[which.max(best$z), ]\n}\n\n\nnb &lt;- norm_ball(2)\ned &lt;- ellipse_data()\nbols &lt;- data.frame(x = 1, y = 1)\nbhat &lt;- lballmax(ed, 2)\nggplot(nb, aes(x, y)) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal() +\n  geom_label(\n    data = bols,\n    mapping = aes(label = bquote(\"hat(beta)[ols]\")),\n    parse = TRUE, \n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  theme_bw(base_size = 24) +\n  geom_label(\n    data = bhat,\n    mapping = aes(label = bquote(\"hat(beta)[s]^R\")),\n    parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#brief-aside-on-norms",
    "href": "schedule/slides/08-ridge-regression.html#brief-aside-on-norms",
    "title": "UBC Stat406 2023W",
    "section": "Brief aside on norms",
    "text": "Brief aside on norms\nRecall, for a vector \\(z \\in \\R^p\\)\n\\[\\snorm{z}_2 = \\sqrt{z_1^2 + z_2^2 + \\cdots + z^2_p} = \\sqrt{\\sum_{j=1}^p z_j^2}\\]\nSo,\n\\[\\snorm{z}^2_2 = z_1^2 + z_2^2 + \\cdots + z^2_p = \\sum_{j=1}^p z_j^2.\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#other-norms-we-should-remember",
    "href": "schedule/slides/08-ridge-regression.html#other-norms-we-should-remember",
    "title": "UBC Stat406 2023W",
    "section": "Other norms we should remember:",
    "text": "Other norms we should remember:\n\n\\(\\ell_q\\)-norm\n\n\\(\\left(\\sum_{j=1}^p |z_j|^q\\right)^{1/q}\\)\n\n\\(\\ell_1\\)-norm (special case)\n\n\\(\\sum_{j=1}^p |z_j|\\)\n\n\\(\\ell_0\\)-norm\n\n\\(\\sum_{j=1}^p I(z_j \\neq 0 ) = \\lvert \\{j : z_j \\neq 0 \\}\\rvert\\)\n\n\\(\\ell_\\infty\\)-norm\n\n\\(\\max_{1\\leq j \\leq p} |z_j|\\)\n\n\n\n\nRecall what a norm is: https://en.wikipedia.org/wiki/Norm_(mathematics)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression",
    "text": "Ridge regression\nAn equivalent way to write\n\\[\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\]\nis in the Lagrangian form\n\\[\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2.\\]\nFor every \\(\\lambda\\) there is a unique \\(s\\) (and vice versa) that makes\n\\[\\brt = \\brl\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-1",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-1",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\(\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\)\n\\(\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2\\)\nObserve:\n\n\\(\\lambda = 0\\) (or \\(s = \\infty\\)) makes \\(\\brl = \\bls\\)\nAny \\(\\lambda &gt; 0\\) (or \\(s &lt;\\infty\\)) penalizes larger values of \\(\\beta\\), effectively shrinking them.\n\n\\(\\lambda\\) and \\(s\\) are known as tuning parameters"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#visualizing-ridge-regression-2-coefficients",
    "href": "schedule/slides/08-ridge-regression.html#visualizing-ridge-regression-2-coefficients",
    "title": "UBC Stat406 2023W",
    "section": "Visualizing ridge regression (2 coefficients)",
    "text": "Visualizing ridge regression (2 coefficients)\n\n\nCode\nb &lt;- c(1, 1)\nn &lt;- 1000\nlams &lt;- c(1, 5, 10)\nols_loss &lt;- function(b1, b2) colMeans((y - X %*% rbind(b1, b2))^2) / 2\npen &lt;- function(b1, b2, lambda = 1) lambda * (b1^2 + b2^2) / 2\ngr &lt;- expand_grid(\n  b1 = seq(b[1] - 0.5, b[1] + 0.5, length.out = 100),\n  b2 = seq(b[2] - 0.5, b[2] + 0.5, length.out = 100)\n)\n\nX &lt;- mvtnorm::rmvnorm(n, c(0, 0), sigma = matrix(c(1, .3, .3, .5), nrow = 2))\ny &lt;- drop(X %*% b + rnorm(n))\n\nbols &lt;- coef(lm(y ~ X - 1))\nbridge &lt;- coef(MASS::lm.ridge(y ~ X - 1, lambda = lams * sqrt(n)))\n\npenalties &lt;- lams |&gt;\n  set_names(~ paste(\"lam =\", .)) |&gt;\n  map(~ pen(gr$b1, gr$b2, .x)) |&gt;\n  as_tibble()\ngr &lt;- gr |&gt;\n  mutate(loss = ols_loss(b1, b2)) |&gt;\n  bind_cols(penalties)\n\ng1 &lt;- ggplot(gr, aes(b1, b2)) +\n  geom_raster(aes(fill = loss)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = 0.5))\n\ng2 &lt;- gr |&gt;\n    pivot_longer(starts_with(\"lam\")) |&gt;\n    mutate(name = factor(name, levels = paste(\"lam =\", lams))) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"penalty\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ng3 &lt;- gr |&gt; \n  mutate(across(starts_with(\"lam\"), ~ loss + .x)) |&gt;\n  pivot_longer(starts_with(\"lam\")) |&gt;\n  mutate(name = factor(name, levels = paste(\"lam =\", lams))) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"loss + pen\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ncowplot::plot_grid(g1, g2, g3, rel_widths = c(2, 1, 1), nrow = 1)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#the-effect-on-the-estimates",
    "href": "schedule/slides/08-ridge-regression.html#the-effect-on-the-estimates",
    "title": "UBC Stat406 2023W",
    "section": "The effect on the estimates",
    "text": "The effect on the estimates\n\n\nCode\ngr |&gt; \n  mutate(z = ols_loss(b1, b2) + max(lams) * pen(b1, b2)) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = z)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = tibble(\n    b1 = c(bols[1], bridge[,1]),\n    b2 = c(bols[2], bridge[,2]),\n    estimate = factor(c(\"ols\", paste0(\"ridge = \", lams)), \n                      levels = c(\"ols\", paste0(\"ridge = \", lams)))\n  ),\n  aes(shape = estimate), size = 3) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2]), colour = orange, size = 4)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#example-data",
    "href": "schedule/slides/08-ridge-regression.html#example-data",
    "title": "UBC Stat406 2023W",
    "section": "Example data",
    "text": "Example data\nprostate data from [ESL]\n\ndata(prostate, package = \"ElemStatLearn\")\nprostate |&gt; as_tibble()\n\n# A tibble: 97 × 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;lgl&gt;\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ℹ 87 more rows\n\n\n\nUse lpsa as response."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-path",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-path",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression path",
    "text": "Ridge regression path\n\nY &lt;- prostate$lpsa\nX &lt;- model.matrix(~ ., data = prostate |&gt; dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge &lt;- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\n\n\n\n\n\nplot(ridge, xvar = \"lambda\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\nModel selection here:\n\nmeans choose some \\(\\lambda\\)\nA value of \\(\\lambda\\) is a vertical line.\nThis graphic is a “path” or “coefficient trace”\nCoefficients for varying \\(\\lambda\\)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#solving-the-minimization",
    "href": "schedule/slides/08-ridge-regression.html#solving-the-minimization",
    "title": "UBC Stat406 2023W",
    "section": "Solving the minimization",
    "text": "Solving the minimization\n\nOne nice thing about ridge regression is that it has a closed-form solution (like OLS)\n\n\\[\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nThis is easy to calculate in R for any \\(\\lambda\\).\nHowever, computations and interpretation are simplified if we examine the Singular Value Decomposition of \\(\\X = \\mathbf{UDV}^\\top\\).\nRecall: any matrix has an SVD.\nHere \\(\\mathbf{D}\\) is diagonal and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}\\)."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#solving-the-minization",
    "href": "schedule/slides/08-ridge-regression.html#solving-the-minization",
    "title": "UBC Stat406 2023W",
    "section": "Solving the minization",
    "text": "Solving the minization\n\\[\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nNote that \\(\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top\\).\nThen,\n\n\\[\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.\\]\n\nFor computations, now we only need to invert \\(\\mathbf{D}\\)."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#comparing-with-ols",
    "href": "schedule/slides/08-ridge-regression.html#comparing-with-ols",
    "title": "UBC Stat406 2023W",
    "section": "Comparing with OLS",
    "text": "Comparing with OLS\n\n\\(\\mathbf{D}\\) is a diagonal matrix\n\n\\[\\bls = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y\\]\n\\[\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.\\]\n\nNotice that \\(\\bls\\) depends on \\(d_j/d_j^2\\) while \\(\\brl\\) depends on \\(d_j/(d_j^2 + \\lambda)\\).\nRidge regression makes the coefficients smaller relative to OLS.\nBut if \\(\\X\\) has small singular values, ridge regression compensates with \\(\\lambda\\) in the denominator."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-and-multicollinearity",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-and-multicollinearity",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression and multicollinearity",
    "text": "Ridge regression and multicollinearity\nMulticollinearity: a linear combination of predictor variables is nearly equal to another predictor variable.\nSome comments:\n\nA better phrase: \\(\\X\\) is ill-conditioned\nAKA “(numerically) rank-deficient”.\n\\(\\X = \\mathbf{U D V}^\\top\\) ill-conditioned \\(\\Longleftrightarrow\\) some elements of \\(\\mathbf{D} \\approx 0\\)\n\\(\\bls= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y\\), so small entries of \\(\\mathbf{D}\\) \\(\\Longleftrightarrow\\) huge elements of \\(\\mathbf{D}^{-1}\\)\nMeans huge variance: \\(\\Var{\\bls} = \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top\\)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-and-ill-posed-x",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-and-ill-posed-x",
    "title": "UBC Stat406 2023W",
    "section": "Ridge regression and ill-posed \\(\\X\\)",
    "text": "Ridge regression and ill-posed \\(\\X\\)\nRidge Regression fixes this problem by preventing the division by a near-zero number\n\nConclusion\n\n\\((\\X^{\\top}\\X)^{-1}\\) can be really unstable, while \\((\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}\\) is not.\n\nAside\n\nEngineering approach to solving linear systems is to always do this with small \\(\\lambda\\). The thinking is about the numerics rather than the statistics.\n\n\nWhich \\(\\lambda\\) to use?\n\nComputational\n\nUse CV and pick the \\(\\lambda\\) that makes this smallest.\n\nIntuition (bias)\n\nAs \\(\\lambda\\rightarrow\\infty\\), bias ⬆\n\nIntuition (variance)\n\nAs \\(\\lambda\\rightarrow\\infty\\), variance ⬇\n\n\nYou should think about why."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds",
    "href": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds",
    "title": "UBC Stat406 2023W",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\nTo recap:\n\nDeciding which predictors to include, adding quadratic terms, or interactions is model selection (more precisely variable selection within a linear model).\nRidge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.\nIf the LM is true,\n\nOLS is unbiased, but Variance depends on \\(\\mathbf{D}^{-2}\\). Can be big.\nRidge is biased (can you find the bias?). But Variance is smaller than OLS.\n\nRidge regression does not perform variable selection.\nBut picking \\(\\lambda=3.7\\) and thereby deciding to predict with \\(\\widehat{\\beta}^R_{3.7}\\) is model selection."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds-1",
    "href": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds-1",
    "title": "UBC Stat406 2023W",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\n\nRidge regression\n\n\\(\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_2^2 \\leq s\\)\n\nBest (in-sample) linear regression model of size \\(s\\)\n\n\\(\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_0 \\leq s\\)\n\n\n\\(||\\beta||_0\\) is the number of nonzero elements in \\(\\beta\\)\nFinding the best in-sample linear model (of size \\(s\\), among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\nRidge regression is convex (easy to solve), but doesn’t do variable selection\nCan we somehow “interpolate” to get both?\nNote: selecting \\(\\lambda\\) is still model selection, but we’ve included all the variables."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#meta-lecture",
    "href": "schedule/slides/06-information-criteria.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "06 Information criteria",
    "text": "06 Information criteria\nStat 406\nDaniel J. McDonald\nLast modified – 26 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#generalized-cv",
    "href": "schedule/slides/06-information-criteria.html#generalized-cv",
    "title": "UBC Stat406 2023W",
    "section": "Generalized CV",
    "text": "Generalized CV\nLast time we saw a nice trick, that works some of the time (OLS, Ridge regression,…)\n\\[\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-h_{ii})^2} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\widehat{e}_i^2}{(1-h_{ii})^2}.\\]\n\n\\(\\widehat{\\y} = \\widehat{f}(\\mathbf{X}) = \\mathbf{H}\\mathbf{y}\\) for some matrix \\(\\mathbf{H}\\).\nA technical thing.\n\n\\[\\newcommand{\\H}{\\mathbf{H}}\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#this-is-another-nice-trick.",
    "href": "schedule/slides/06-information-criteria.html#this-is-another-nice-trick.",
    "title": "UBC Stat406 2023W",
    "section": "This is another nice trick.",
    "text": "This is another nice trick.\nIdea: replace \\(h_{ii}\\) with \\(\\frac{1}{n}\\sum_{i=1}^n h_{ii} = \\frac{1}{n}\\textrm{tr}(\\mathbf{H})\\)\nLet’s call \\(\\textrm{tr}(\\mathbf{H})\\) the degrees-of-freedom (or just df)\n\\[\\textrm{GCV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\widehat{e}_i^2}{(1-\\textrm{df}/n)^2} = \\frac{\\textrm{MSE}}{(1-\\textrm{df}/n)^2}\\]\nWhere does this stuff come from?"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#what-are-hatvalues",
    "href": "schedule/slides/06-information-criteria.html#what-are-hatvalues",
    "title": "UBC Stat406 2023W",
    "section": "What are hatvalues?",
    "text": "What are hatvalues?\n\ncv_nice &lt;- function(mdl) mean((residuals(mdl) / (1 - hatvalues(mdl)))^2)\n\nIn OLS, \\(\\widehat{\\y} = \\X\\widehat{\\beta} = \\X(\\X^\\top \\X)^{-1}\\X^\\top \\y\\)\nWe often call \\(\\mathbf{H} = \\X(\\X^\\top \\X)^{-1}\\X^\\top\\) the Hat matrix, because it puts the hat on \\(\\y\\)\nGCV uses \\(\\textrm{tr}(\\mathbf{H})\\).\nFor lm(), this is just p, the number of predictors (Why?)\nThis is one way of understanding the name degrees-of-freedom"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#alternative-interpretation",
    "href": "schedule/slides/06-information-criteria.html#alternative-interpretation",
    "title": "UBC Stat406 2023W",
    "section": "Alternative interpretation:",
    "text": "Alternative interpretation:\nSuppose, \\(Y_i\\) is independent from some distribution with mean \\(\\mu_i\\) and variance \\(\\sigma^2\\)\n(remember: in the linear model \\(\\Expect{Y_i} = x_i^\\top \\beta = \\mu_i\\) )\nLet \\(\\widehat{\\mathbf{Y}}\\) be an estimator of \\(\\mu\\) (all \\(i=1,\\ldots,n\\) elements of the vector).\n\n\\[\\begin{aligned}\n& \\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-\\mu_i)^2} \\\\\n&= \\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-Y_i + Y_i -\\mu_i)^2}\\\\\n&= \\frac{1}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)^2} + \\frac{1}{n}\\Expect{\\sum (Y_i-\\mu_i)^2} + \\frac{2}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)(Y_i-\\mu_i)}\\\\\n&= \\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2} + \\sigma^2 + \\frac{2}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)(Y_i-\\mu_i)} = \\cdots =\\\\\n&= \\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2} - \\sigma^2 + \\frac{2}{n}\\sum\\Cov{Y_i}{\\widehat Y_i}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#alternative-interpretation-1",
    "href": "schedule/slides/06-information-criteria.html#alternative-interpretation-1",
    "title": "UBC Stat406 2023W",
    "section": "Alternative interpretation:",
    "text": "Alternative interpretation:\n\\[\\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-\\mu_i)^2} = \\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2} - \\sigma^2 + \\frac{2}{n}\\sum\\Cov{Y_i}{\\widehat Y_i}\\]\nNow, if \\(\\widehat{\\mathbf{Y}} = \\H \\mathbf{Y}\\) for some matrix \\(\\H\\),\n\\(\\sum\\Cov{Y_i}{\\widehat Y_i} = \\Expect{\\mathbf{Y}^\\top \\H \\mathbf{Y}} = \\sigma^2 \\textrm{tr}(\\H)\\)\nThis gives Mallow’s \\(C_p\\) aka Stein’s Unbiased Risk Estimator:\n\\(MSE + 2\\hat{\\sigma}^2\\textrm{df}/n\\)\n\n\n\n\n\n\nImportant\n\n\nUnfortunately, df may be difficult or impossible to calculate for complicated prediction methods. But one can often estimate it well. This idea is beyond the level of this course."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#aic-and-bic",
    "href": "schedule/slides/06-information-criteria.html#aic-and-bic",
    "title": "UBC Stat406 2023W",
    "section": "AIC and BIC",
    "text": "AIC and BIC\nThese have a very similar flavor to \\(C_p\\), but their genesis is different.\nWithout going into too much detail, they look like\n\\(\\textrm{AIC}/n = -2\\textrm{loglikelihood}/n + 2\\textrm{df}/n\\)\n\\(\\textrm{BIC}/n = -2\\textrm{loglikelihood}/n + 2\\log(n)\\textrm{df}/n\\)\n\nIn the case of a linear model with Gaussian errors and \\(p\\) predictors\n\\[\\begin{aligned}\n\\textrm{AIC}/n &= \\log(2\\pi) + \\log(RSS/n) + 2(p+1)/n \\\\\n&\\propto \\log(RSS) + 2(p+1)/n\n\\end{aligned}\\]\n( \\(p+1\\) because of the unknown variance, intercept included in \\(p\\) or not)\n\n\n\n\n\n\n\n\nImportant\n\n\nUnfortunately, different books/software/notes define these differently. Even different R packages. This is super annoying.\nForms above are in [ESL] eq. (7.29) and (7.35). [ISLR] gives special cases in Section 6.1.3. Remember the generic form here."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#over-fitting-vs.-under-fitting",
    "href": "schedule/slides/06-information-criteria.html#over-fitting-vs.-under-fitting",
    "title": "UBC Stat406 2023W",
    "section": "Over-fitting vs. Under-fitting",
    "text": "Over-fitting vs. Under-fitting\n\nOver-fitting means estimating a really complicated function when you don’t have enough data.\n\nThis is likely a low-bias / high-variance situation.\n\nUnder-fitting means estimating a really simple function when you have lots of data.\n\nThis is likely a high-bias / low-variance situation.\nBoth of these outcomes are bad (they have high risk \\(=\\) big \\(R_n\\) ).\nThe best way to avoid them is to use a reasonable estimate of prediction risk to choose how complicated your model should be."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#recommendations",
    "href": "schedule/slides/06-information-criteria.html#recommendations",
    "title": "UBC Stat406 2023W",
    "section": "Recommendations",
    "text": "Recommendations\n\nWhen comparing models, choose one criterion: CV / AIC / BIC / Cp / GCV.\nCV is usually easiest to make sense of and doesn’t depend on other unknown parameters.\nBut, it requires refitting the model.\nAlso, it can be strange in cases with discrete predictors, time series, repeated measurements, graph structures, etc."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#high-level-intuition-of-these",
    "href": "schedule/slides/06-information-criteria.html#high-level-intuition-of-these",
    "title": "UBC Stat406 2023W",
    "section": "High-level intuition of these:",
    "text": "High-level intuition of these:\n\nGCV tends to choose “dense” models.\nTheory says AIC chooses the “best predicting model” asymptotically.\nTheory says BIC should choose the “true model” asymptotically, tends to select fewer predictors.\nIn some special cases, AIC = Cp = SURE \\(\\approx\\) LOO-CV\nAs a technical point, CV (or validation set) is estimating error on new data, unseen \\((X_0, Y_0)\\), while AIC / CP are estimating error on new Y at the observed \\(x_1,\\ldots,x_n\\). This is subtle.\n\n\n\nFor more information: see [ESL] Chapter 7. This material is more challenging than the level of this course, and is easily and often misunderstood."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#a-few-more-caveats",
    "href": "schedule/slides/06-information-criteria.html#a-few-more-caveats",
    "title": "UBC Stat406 2023W",
    "section": "A few more caveats",
    "text": "A few more caveats\nIt is often tempting to “just compare” risk estimates from vastly different models.\nFor example,\n\ndifferent transformations of the predictors,\ndifferent transformations of the response,\nPoisson likelihood vs. Gaussian likelihood in glm()\n\nThis is not always justified.\n\nThe “high-level intuition” is for “nested” models.\nDifferent likelihoods aren’t comparable.\nResiduals / response variables on different scales aren’t directly comparable.\n\n“Validation set” is easy, because you’re always comparing to the “right” thing. But it has lots of drawbacks."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#meta-lecture",
    "href": "schedule/slides/04-bias-variance.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "04 Bias and variance",
    "text": "04 Bias and variance\nStat 406\nDaniel J. McDonald\nLast modified – 18 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section",
    "href": "schedule/slides/04-bias-variance.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "We just talked about\n\nVariance of an estimator.\nIrreducible error when making predictions.\nThese are 2 of the 3 components of the “Prediction Risk” \\(R_n\\)"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#component-3-the-bias",
    "href": "schedule/slides/04-bias-variance.html#component-3-the-bias",
    "title": "UBC Stat406 2023W",
    "section": "Component 3, the Bias",
    "text": "Component 3, the Bias\nWe need to be specific about what we mean when we say bias.\nBias is neither good nor bad in and of itself.\nA very simple example: let \\(Z_1,\\ \\ldots,\\ Z_n \\sim N(\\mu, 1)\\). - We don’t know \\(\\mu\\), so we try to use the data (the \\(Z_i\\)’s) to estimate it.\n\nI propose 3 estimators:\n\n\\(\\widehat{\\mu}_1 = 12\\),\n\\(\\widehat{\\mu}_2=Z_6\\),\n\\(\\widehat{\\mu}_3=\\overline{Z}\\).\n\nThe bias (by definition) of my estimator is \\(E[\\widehat{\\mu_i}]-\\mu\\).\n\n\nCalculate the bias and variance of each estimator."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#regression-in-general",
    "href": "schedule/slides/04-bias-variance.html#regression-in-general",
    "title": "UBC Stat406 2023W",
    "section": "Regression in general",
    "text": "Regression in general\nIf I want to predict \\(Y\\) from \\(X\\), it is almost always the case that\n\\[\n\\mu(x) = \\Expect{Y\\given X=x} \\neq x^{\\top}\\beta\n\\]\nSo the bias of using a linear model is not zero.\n\nWhy? Because\n\\[\n\\Expect{Y\\given X=x}-x^\\top\\beta \\neq \\Expect{Y\\given X=x} - \\mu(x) = 0.\n\\]\nWe can include as many predictors as we like,\nbut this doesn’t change the fact that the world is non-linear."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#continuation-predicting-new-ys",
    "href": "schedule/slides/04-bias-variance.html#continuation-predicting-new-ys",
    "title": "UBC Stat406 2023W",
    "section": "(Continuation) Predicting new Y’s",
    "text": "(Continuation) Predicting new Y’s\nSuppose we want to predict \\(Y\\),\nwe know \\(E[Y]= \\mu \\in \\mathbb{R}\\) and \\(\\textrm{Var}[Y] = 1\\).\nOur data is \\(\\{y_1,\\ldots,y_n\\}\\)\nWe have considered estimating \\(\\mu\\) in various ways, and using \\(\\widehat{Y} = \\widehat{\\mu}\\)\n\n\nLet’s try one more: \\(\\widehat Y_a = a\\overline{Y}_n\\) for some \\(a \\in (0,1]\\)."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#one-can-show-wait-for-the-proof",
    "href": "schedule/slides/04-bias-variance.html#one-can-show-wait-for-the-proof",
    "title": "UBC Stat406 2023W",
    "section": "One can show… (wait for the proof)",
    "text": "One can show… (wait for the proof)\n\\(\\widehat Y_a = a\\overline{Y}_n\\) for some \\(a \\in (0,1]\\)\n\\[\nR_n(\\widehat Y_a) = \\Expect{(\\widehat Y_a-Y)^2} = (1 - a)^2\\mu^2 +\n\\frac{a^2}{n} +1\n\\]\n\nWe can minimize this in \\(a\\) to get the best possible prediction risk for an estimator of the form \\(\\widehat Y_a\\):\n\\[\n\\argmin_{a} R_n(\\widehat Y_a) = \\left(\\frac{\\mu^2}{\\mu^2 + 1/n} \\right)\n\\]\n\n\nWhat happens if \\(\\mu \\ll 1\\)?"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section-1",
    "href": "schedule/slides/04-bias-variance.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Important\n\n\n\nWait a minute! I’m saying there is a better estimator than \\(\\overline{Y}_n\\)!"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-estimating-the-mean",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-estimating-the-mean",
    "title": "UBC Stat406 2023W",
    "section": "Bias-variance tradeoff: Estimating the mean",
    "text": "Bias-variance tradeoff: Estimating the mean\n\\[\nR_n(\\widehat Y_a) = (a - 1)^2\\mu^2 +  \\frac{a^2}{n} + \\sigma^2\n\\]\n\nmu = 1; n = 5; sig = 1"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#to-restate",
    "href": "schedule/slides/04-bias-variance.html#to-restate",
    "title": "UBC Stat406 2023W",
    "section": "To restate",
    "text": "To restate\nIf \\(\\mu=\\) 1 and \\(n=\\) 5\nthen it is better to predict with 0.83 \\(\\overline{Y}_5\\)\nthan with \\(\\overline{Y}_5\\) itself.\n\nFor this \\(a =\\) 0.83 and \\(n=5\\)\n\n\\(R_5(\\widehat{Y}_a) =\\) 1.17\n\\(R_5(\\overline{Y}_5)=\\) 1.2"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#prediction-risk",
    "href": "schedule/slides/04-bias-variance.html#prediction-risk",
    "title": "UBC Stat406 2023W",
    "section": "Prediction risk",
    "text": "Prediction risk\n(Now using generic prediction function \\(f\\))\n\\[\nR_n(f) = \\Expect{(Y - f(X))^2}\n\\]\nWhy should we care about \\(R_n(f)\\)?\n👍 Measures predictive accuracy on average.\n👍 How much confidence should you have in \\(f\\)’s predictions.\n👍 Compare with other predictors: \\(R_n(f)\\) vs \\(R_n(g)\\)\n🤮 This is hard: Don’t know the distribution of the data (if I knew the truth, this would be easy)"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-decomposition",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-decomposition",
    "title": "UBC Stat406 2023W",
    "section": "Bias-variance decomposition",
    "text": "Bias-variance decomposition\n\\[R_n(\\widehat{Y}_a)=(a - 1)^2\\mu^2 + \\frac{a^2}{n} + 1\\]\n\nprediction risk = \\(\\textrm{bias}^2\\) + variance + irreducible error\nestimation risk = \\(\\textrm{bias}^2\\) + variance\n\nWhat is \\(R_n(\\widehat{Y}_a)\\) for our estimator \\(\\widehat{Y}_a=a\\overline{Y}_n\\)?\n\\[\\begin{aligned}\n\\textrm{bias}(\\widehat{Y}_a) &= \\Expect{a\\overline{Y}_n} - \\mu=(a-1)\\mu\\\\\n\\textrm{var}(\\widehat f(x)) &= \\Expect{ \\left(a\\overline{Y}_n - \\Expect{a\\overline{Y}_n}\\right)^2}\n=a^2\\Expect{\\left(\\overline{Y}_n-\\mu\\right)^2}=\\frac{a^2}{n} \\\\\n\\sigma^2 &= \\Expect{(Y-\\mu)^2}=1\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#this-decomposition-holds-generally",
    "href": "schedule/slides/04-bias-variance.html#this-decomposition-holds-generally",
    "title": "UBC Stat406 2023W",
    "section": "This decomposition holds generally",
    "text": "This decomposition holds generally\n\\[\\begin{aligned}\nR_n(\\hat{Y})\n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu + \\mu - \\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} +\n2\\Expect{(Y-\\mu)(\\mu-\\hat{Y})}\\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}] + E[\\hat{Y}] - \\hat{Y})^2}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 2\\Expect{(\\mu-E[\\hat{Y}])(E[\\hat{Y}] - \\hat{Y})}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-decomposition-1",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-decomposition-1",
    "title": "UBC Stat406 2023W",
    "section": "Bias-variance decomposition",
    "text": "Bias-variance decomposition\n\\[\\begin{aligned}\nR_n(\\hat{Y})\n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\\]\n\n\n\n\n\n\nImportant\n\n\n\nImplication: prediction risk is proportional to estimation risk. However, defining estimation risk requires stronger assumptions.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIn order to make good predictions, we want our prediction risk to be small. This means that we want to “balance” the bias and variance."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section-2",
    "href": "schedule/slides/04-bias-variance.html#section-2",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Code\ncols = c(blue, red, green, orange)\npar(mfrow = c(2, 2), bty = \"n\", ann = FALSE, xaxt = \"n\", yaxt = \"n\", \n    family = \"serif\", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))\nlibrary(mvtnorm)\nmv &lt;- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)\nva &lt;- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)\n\nfor (i in 1:4) {\n  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, \n       col = blue, ann = FALSE, pty = \"s\")\n  points(0, 0, pch = 19, cex = 30, col = \"white\")\n  points(0, 0, pch = 19, cex = 18, col = green)\n  points(0, 0, pch = 19, cex = 6, col = orange)\n  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)\n  switch(i,\n    \"1\" = {\n      mtext(\"low variance\", 3, cex = 2)\n      mtext(\"low bias\", 2, cex = 2)\n    },\n    \"2\" = mtext(\"high variance\", 3, cex = 2),\n    \"3\" = mtext(\"high bias\", 2, cex = 2)\n  )\n}"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-overview",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-overview",
    "title": "UBC Stat406 2023W",
    "section": "Bias-variance tradeoff: Overview",
    "text": "Bias-variance tradeoff: Overview\nbias: how well does \\(\\widehat{f}(x)\\) approximate the truth \\(\\Expect{Y\\given X=x}\\)\n\nIf we allow more complicated possible \\(\\widehat{f}\\), lower bias. Flexibility \\(\\Rightarrow\\) Expressivity\nBut, more flexibility \\(\\Rightarrow\\) larger variance\nComplicated models are hard to estimate precisely for fixed \\(n\\)\nIrreducible error\n\n\n\nSadly, that whole exercise depends on knowing the truth to evaluate \\(E\\ldots\\)"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#meta-lecture",
    "href": "schedule/slides/02-lm-example.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "02 Linear model example",
    "text": "02 Linear model example\nStat 406\nDaniel J. McDonald\nLast modified – 06 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\]"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#economic-mobility",
    "href": "schedule/slides/02-lm-example.html#economic-mobility",
    "title": "UBC Stat406 2023W",
    "section": "Economic mobility",
    "text": "Economic mobility\n\ndata(\"mobility\", package = \"Stat406\")\nmobility\n\n# A tibble: 741 × 43\n      ID Name        Mobility State Population Urban Black Seg_racial Seg_income\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1   100 Johnson Ci…   0.0622 TN        576081     1 0.021      0.09       0.035\n 2   200 Morristown    0.0537 TN        227816     1 0.02       0.093      0.026\n 3   301 Middlesbor…   0.0726 TN         66708     0 0.015      0.064      0.024\n 4   302 Knoxville     0.0563 TN        727600     1 0.056      0.21       0.092\n 5   401 Winston-Sa…   0.0448 NC        493180     1 0.174      0.262      0.072\n 6   402 Martinsvil…   0.0518 VA         92753     0 0.224      0.137      0.024\n 7   500 Greensboro    0.0474 NC       1055133     1 0.218      0.22       0.068\n 8   601 North Wilk…   0.0517 NC         90016     0 0.032      0.114      0.012\n 9   602 Galax         0.0796 VA         64676     0 0.029      0.131      0.005\n10   700 Spartanburg   0.0431 SC        354533     1 0.207      0.139      0.045\n# ℹ 731 more rows\n# ℹ 34 more variables: Seg_poverty &lt;dbl&gt;, Seg_affluence &lt;dbl&gt;, Commute &lt;dbl&gt;,\n#   Income &lt;dbl&gt;, Gini &lt;dbl&gt;, Share01 &lt;dbl&gt;, Gini_99 &lt;dbl&gt;, Middle_class &lt;dbl&gt;,\n#   Local_tax_rate &lt;dbl&gt;, Local_gov_spending &lt;dbl&gt;, Progressivity &lt;dbl&gt;,\n#   EITC &lt;dbl&gt;, School_spending &lt;dbl&gt;, Student_teacher_ratio &lt;dbl&gt;,\n#   Test_scores &lt;dbl&gt;, HS_dropout &lt;dbl&gt;, Colleges &lt;dbl&gt;, Tuition &lt;dbl&gt;,\n#   Graduation &lt;dbl&gt;, Labor_force_participation &lt;dbl&gt;, Manufacturing &lt;dbl&gt;, …\n\n\n\nNote how many observations and predictors it has.\nWe’ll use Mobility as the response"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#a-linear-model",
    "href": "schedule/slides/02-lm-example.html#a-linear-model",
    "title": "UBC Stat406 2023W",
    "section": "A linear model",
    "text": "A linear model\n\\[\\mbox{Mobility}_i = \\beta_0 + \\beta_1 \\, \\mbox{State}_i + \\beta_2 \\, \\mbox{Urban}_i + \\cdots + \\epsilon_i\\]\nor equivalently\n\\[E \\left[ \\biggl. \\mbox{mobility} \\, \\biggr| \\, \\mbox{State}, \\mbox{Urban},\n    \\ldots \\right]  = \\beta_0 + \\beta_1 \\, \\mbox{State} +\n    \\beta_2 \\, \\mbox{Urban} + \\cdots\\]"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#analysis",
    "href": "schedule/slides/02-lm-example.html#analysis",
    "title": "UBC Stat406 2023W",
    "section": "Analysis",
    "text": "Analysis\n\nRandomly split into a training (say 3/4) and a test set (1/4)\nUse training set to fit a model\nFit the “full” model\n“Look” at the fit\n\n\n\nset.seed(20220914)\nmob &lt;- mobility[complete.cases(mobility), ]\nn &lt;- nrow(mob)\nmob &lt;- mob |&gt; select(-Name, -ID, -State)\nset &lt;- sample.int(n, floor(n * .75), FALSE)\ntrain &lt;- mob[set, ]\ntest &lt;- mob[setdiff(1:n, set), ]\nfull &lt;- lm(Mobility ~ ., data = train)\n\n\nWhy don’t we include Name or ID?"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#results",
    "href": "schedule/slides/02-lm-example.html#results",
    "title": "UBC Stat406 2023W",
    "section": "Results",
    "text": "Results\n\nsummary(full)\n\n\nCall:\nlm(formula = Mobility ~ ., data = train)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.072092 -0.010256 -0.001452  0.009170  0.090428 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                1.849e-01  8.083e-02   2.288 0.022920 *  \nPopulation                 3.378e-09  2.478e-09   1.363 0.173916    \nUrban                      2.853e-03  3.892e-03   0.733 0.464202    \nBlack                      7.807e-02  2.859e-02   2.731 0.006735 ** \nSeg_racial                -5.626e-02  1.780e-02  -3.160 0.001754 ** \nSeg_income                 8.677e-01  9.355e-01   0.928 0.354453    \nSeg_poverty               -7.416e-01  5.014e-01  -1.479 0.140316    \nSeg_affluence             -2.224e-01  4.763e-01  -0.467 0.640874    \nCommute                    6.313e-02  2.838e-02   2.225 0.026915 *  \nIncome                     4.207e-07  6.997e-07   0.601 0.548112    \nGini                       3.592e+00  3.357e+00   1.070 0.285578    \nShare01                   -3.635e-02  3.357e-02  -1.083 0.279925    \nGini_99                   -3.657e+00  3.356e+00  -1.090 0.276704    \nMiddle_class               1.031e-01  4.835e-02   2.133 0.033828 *  \nLocal_tax_rate             2.268e-01  2.620e-01   0.866 0.387487    \nLocal_gov_spending         1.273e-07  3.016e-06   0.042 0.966374    \nProgressivity              4.983e-03  1.324e-03   3.764 0.000205 ***\nEITC                      -3.324e-04  4.528e-04  -0.734 0.463549    \nSchool_spending           -9.019e-04  2.272e-03  -0.397 0.691658    \nStudent_teacher_ratio     -1.639e-03  1.123e-03  -1.459 0.145748    \nTest_scores                2.487e-04  3.137e-04   0.793 0.428519    \nHS_dropout                -1.698e-01  9.352e-02  -1.816 0.070529 .  \nColleges                  -2.811e-02  7.661e-02  -0.367 0.713942    \nTuition                    3.459e-07  4.362e-07   0.793 0.428417    \nGraduation                -1.702e-02  1.425e-02  -1.194 0.233650    \nLabor_force_participation -7.850e-02  5.405e-02  -1.452 0.147564    \nManufacturing             -1.605e-01  2.816e-02  -5.700  3.1e-08 ***\nChinese_imports           -5.165e-04  1.004e-03  -0.514 0.607378    \nTeenage_labor             -1.019e+00  2.111e+00  -0.483 0.629639    \nMigration_in               4.490e-02  3.480e-01   0.129 0.897436    \nMigration_out             -4.475e-01  4.093e-01  -1.093 0.275224    \nForeign_born               9.137e-02  5.494e-02   1.663 0.097454 .  \nSocial_capital            -1.114e-03  2.728e-03  -0.408 0.683245    \nReligious                  4.570e-02  1.298e-02   3.520 0.000506 ***\nViolent_crime             -3.393e+00  1.622e+00  -2.092 0.037373 *  \nSingle_mothers            -3.590e-01  9.442e-02  -3.802 0.000177 ***\nDivorced                   1.707e-02  1.603e-01   0.107 0.915250    \nMarried                   -5.894e-02  7.246e-02  -0.813 0.416720    \nLongitude                 -4.239e-05  2.239e-04  -0.189 0.850001    \nLatitude                   6.725e-04  5.687e-04   1.182 0.238037    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02128 on 273 degrees of freedom\nMultiple R-squared:  0.7808,    Adjusted R-squared:  0.7494 \nF-statistic: 24.93 on 39 and 273 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#diagnostic-plots",
    "href": "schedule/slides/02-lm-example.html#diagnostic-plots",
    "title": "UBC Stat406 2023W",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\n\n\npar(mar = c(5, 3, 0, 0))\nplot(full, 1)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(full, 2)"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#section",
    "href": "schedule/slides/02-lm-example.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "(Those were plot methods for objects of class lm)\nSame thing in ggplot\n\n\nstuff &lt;- tibble(\n  residuals = residuals(full), \n  fitted = fitted(full),\n  stdresiduals = rstandard(full)\n)\nggplot(stuff, aes(fitted, residuals)) +\n  geom_point(colour = \"salmon\") +\n  geom_smooth(\n    se = FALSE, \n    colour = \"steelblue\", \n    linewidth = 2) +\n  ggtitle(\"Residuals vs Fitted\")\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(stuff, aes(sample = stdresiduals)) +\n  geom_qq(colour = \"purple\", size = 2) +\n  geom_qq_line(colour = \"peachpuff\", linewidth = 2) +\n  labs(\n    x = \"Theoretical quantiles\", \n    y = \"Standardized residuals\",\n    title = \"Normal Q-Q\")"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#fit-a-reduced-model",
    "href": "schedule/slides/02-lm-example.html#fit-a-reduced-model",
    "title": "UBC Stat406 2023W",
    "section": "Fit a reduced model",
    "text": "Fit a reduced model\n\nreduced &lt;- lm(\n  Mobility ~ Commute + Gini_99 + Test_scores + HS_dropout +\n    Manufacturing + Migration_in + Religious + Single_mothers, \n  data = train)\n\nsummary(reduced)$coefficients |&gt; as_tibble()\n\n# A tibble: 9 × 4\n   Estimate `Std. Error` `t value` `Pr(&gt;|t|)`\n      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1  0.166        0.0178        9.36   1.83e-18\n2  0.0637       0.0149        4.27   2.62e- 5\n3 -0.109        0.0390       -2.79   5.64e- 3\n4  0.000500     0.000256      1.95   5.19e- 2\n5 -0.216        0.0820       -2.64   8.81e- 3\n6 -0.159        0.0202       -7.89   5.65e-14\n7 -0.389        0.172        -2.26   2.42e- 2\n8  0.0436       0.0105        4.16   4.08e- 5\n9 -0.286        0.0466       -6.15   2.44e- 9\n\nreduced |&gt; broom::glance() |&gt; print(width = 120)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared  sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.718         0.711 0.0229      96.9 5.46e-79     8   743. -1466. -1429.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1    0.159         304   313"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#diagnostic-plots-for-reduced-model",
    "href": "schedule/slides/02-lm-example.html#diagnostic-plots-for-reduced-model",
    "title": "UBC Stat406 2023W",
    "section": "Diagnostic plots for reduced model",
    "text": "Diagnostic plots for reduced model\n\n\nplot(reduced, 1)\n\n\n\n\n\n\n\n\n\n\n\n\nplot(reduced, 2)"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#how-do-we-decide-which-model-is-better",
    "href": "schedule/slides/02-lm-example.html#how-do-we-decide-which-model-is-better",
    "title": "UBC Stat406 2023W",
    "section": "How do we decide which model is better?",
    "text": "How do we decide which model is better?\n\n\n\nGoodness of fit versus prediction power\n\n\nmap( # smaller AIC is better\n  list(full = full, reduced = reduced), \n  ~ c(aic = AIC(.x), rsq = summary(.x)$r.sq))\n\n$full\n          aic           rsq \n-1482.5981023     0.7807509 \n\n$reduced\n         aic          rsq \n-1466.088492     0.718245 \n\n\n\nUse both models to predict Mobility\nCompare both sets of predictions\n\n\n\n\nmses &lt;- function(preds, obs) {\n  round(mean((obs - preds)^2), 5)\n}\nc(\n  full = mses(\n    predict(full, newdata = test), \n    test$Mobility),\n  reduced = mses(\n    predict(reduced, newdata = test), \n    test$Mobility)\n)\n\n   full reduced \n0.00072 0.00084 \n\n\n\n\nCode\ntest$full &lt;- predict(full, newdata = test)\ntest$reduced &lt;- predict(reduced, newdata = test)\ntest |&gt; \n  select(Mobility, full, reduced) |&gt;\n  pivot_longer(-Mobility) |&gt;\n  ggplot(aes(Mobility, value)) + \n  geom_point(color = \"orange\") + \n  facet_wrap(~name, 2) +\n  xlab('observed mobility') + \n  ylab('predicted mobility') +\n  geom_abline(slope = 1, intercept = 0, col = \"darkblue\")"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#meta-lecture",
    "href": "schedule/slides/00-version-control.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "00 Git, Github, and Slack",
    "text": "00 Git, Github, and Slack\nStat 406\nDaniel J. McDonald\nLast modified – 11 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#course-communication",
    "href": "schedule/slides/00-version-control.html#course-communication",
    "title": "UBC Stat406 2023W",
    "section": "Course communication",
    "text": "Course communication\nWebsite:\nhttps://ubc-stat.github.io/stat-406/\n\nHosted on Github.\nLinks to slides and all materials\nSyllabus is there. Be sure to read it."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#course-communication-1",
    "href": "schedule/slides/00-version-control.html#course-communication-1",
    "title": "UBC Stat406 2023W",
    "section": "Course communication",
    "text": "Course communication\nSlack:\n\nLink to join on Canvas. This is our discussion board.\nNote that this data is hosted on servers outside of Canada. You may wish to use a pseudonym to protect your privacy.\nAnything super important will be posted to Slack and Canvas.\nBe sure you get Canvas email.\nIf I am sick, I will cancel class or arrange a substitute."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#course-communication-2",
    "href": "schedule/slides/00-version-control.html#course-communication-2",
    "title": "UBC Stat406 2023W",
    "section": "Course communication",
    "text": "Course communication\nGitHub organization\n\nLinked from the website.\nThis is where you complete / submit assignments / projects / in-class-work\nThis is also hosted on Servers outside Canada https://github.com/stat-406-2023/"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-these",
    "href": "schedule/slides/00-version-control.html#why-these",
    "title": "UBC Stat406 2023W",
    "section": "Why these?",
    "text": "Why these?\n\nYes, some data is hosted on servers in the US.\nBut in the real world, no one uses Canvas / Piazza, so why not learn things they do use?\nMuch easier to communicate, “mark” or comment on your work\nMuch more DS friendly\nNote that MDS uses both of these, the Stat and CS departments use both, many faculty use them, Google / Amazon / Facebook use things like these, etc.\n\n\nSlack help from MDS features and rules"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control",
    "href": "schedule/slides/00-version-control.html#why-version-control",
    "title": "UBC Stat406 2023W",
    "section": "Why version control?",
    "text": "Why version control?\n\nMuch of this lecture is based on material from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control-1",
    "href": "schedule/slides/00-version-control.html#why-version-control-1",
    "title": "UBC Stat406 2023W",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWords of wisdom\n\n\nYour closest collaborator is you six months ago, but you don’t reply to emails.\n– Paul Wilson"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-git",
    "href": "schedule/slides/00-version-control.html#why-git",
    "title": "UBC Stat406 2023W",
    "section": "Why Git",
    "text": "Why Git\n\n\n\nYou could use something like Box or Dropbox\nThese are poor-man’s version control\nGit is much more appropriate\nIt works with large groups\nIt’s very fast\nIt’s much better at fixing mistakes\nTech companies use it (so it’s in your interest to have some experience)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis will hurt, but what doesn’t kill you, makes you stronger."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#overview",
    "href": "schedule/slides/00-version-control.html#overview",
    "title": "UBC Stat406 2023W",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "href": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "title": "UBC Stat406 2023W",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#typical-workflow",
    "href": "schedule/slides/00-version-control.html#typical-workflow",
    "title": "UBC Stat406 2023W",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\ngit clone https://github.com/stat550-2021/lecture-slides.git\n\nCreate a branch\n\ngit branch &lt;branchname&gt;\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\ngit add &lt;name/of/tracked/file&gt;\n\nCommit your changes\n\ngit commit -m \"Some explanatory message\"\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to GitHub\n\ngit push\ngit push -u origin &lt;branchname&gt;"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "title": "UBC Stat406 2023W",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nDefinitely\n\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\n\n\n\nPossibly\n\nlogs, jupyter notebooks, images (that won’t change), …\n\n\n\n\nQuestionable\n\nprocessed data, static pdfs, …\n\n\n\n\nDefinitely not\n\nfull data, continually updated pdfs, other things compiled from source code, …"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-things-to-track",
    "href": "schedule/slides/00-version-control.html#what-things-to-track",
    "title": "UBC Stat406 2023W",
    "section": "What things to track",
    "text": "What things to track\n\nYou decide what is “versioned”.\nA file called .gitignore tells git files or types to never track\n\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n\nShortcut to track everything (use carefully):\n\ngit add ."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#rules",
    "href": "schedule/slides/00-version-control.html#rules",
    "title": "UBC Stat406 2023W",
    "section": "Rules",
    "text": "Rules\nHomework and Labs\n\nYou each have your own repo\nYou make a branch\nDO NOT rename files\nMake enough commits (3 for labs, 5 for HW).\nPush your changes (at anytime) and make a PR against main when done.\nTAs review your work.\nOn HW, if you want to revise, make changes in response to feedback and push to the same branch. Then “re-request review”."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr",
    "href": "schedule/slides/00-version-control.html#whats-a-pr",
    "title": "UBC Stat406 2023W",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "href": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "title": "UBC Stat406 2023W",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "href": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "title": "UBC Stat406 2023W",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThere are guardrails in place. But those won’t stop a bulldozer.\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 9 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#the-maindevelopbranch-workflow",
    "href": "schedule/slides/00-version-control.html#the-maindevelopbranch-workflow",
    "title": "UBC Stat406 2023W",
    "section": "The main/develop/branch workflow",
    "text": "The main/develop/branch workflow\n\nWhen working on your own\n\nDon’t NEED branches (but you should use them, really)\nI make a branch if I want to try a modification without breaking what I have.\n\nWhen working on a large team with production grade software\n\nmain is protected, released version of software (maybe renamed to release)\ndevelop contains things not yet on main, but thoroughly tested\nOn a schedule (once a week, once a month) develop gets merged to main\nYou work on a feature branch off develop to build your new feature\nYou do a PR against develop. Supervisors review your contributions\n\n\n\nI and many DS/CS/Stat faculty use this workflow with my lab."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#protection",
    "href": "schedule/slides/00-version-control.html#protection",
    "title": "UBC Stat406 2023W",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#guardrails",
    "href": "schedule/slides/00-version-control.html#guardrails",
    "title": "UBC Stat406 2023W",
    "section": "Guardrails",
    "text": "Guardrails\n\nThe .github directory contains interactions with GitHub\n\nActions: On push / PR / other GitHub does something on their server (builds a website, runs tests on code)\nPR templates: Little admonitions when you open a PR\nBranch protection: prevent you from doing stuff\n\nIn this course, I protect main so that you can’t push there\n\n\n\n\n\n\n\nWarning\n\n\nIf you try to push to main, it will give an error like\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThe fix is: make a new branch, then push that."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "href": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "title": "UBC Stat406 2023W",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\nCovers:\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\nCommand line versions (of the same)\ngit add &lt;name/of/file&gt;\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b &lt;name/of/branch&gt;"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "href": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "title": "UBC Stat406 2023W",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\ngit config user.name --global \"Daniel J. McDonald\"\ngit config user.email --global \"daniel@stat.ubc.ca\"\ngit config core.editor --global nano \n# or emacs or ... (default is vim)\nStaging\ngit add name/of/file # stage 1 file\ngit add . # stage all\nCommitting\n# stage/commit simultaneously\ngit commit -am \"message\" \n\n# open editor to write long commit message\ngit commit \nPushing\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n\n\nBranching\n# switch to branchname, error if uncommitted changes\ngit checkout branchname \n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\nCheck the status\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#conflicts",
    "href": "schedule/slides/00-version-control.html#conflicts",
    "title": "UBC Stat406 2023W",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\n\nThey look like this:\n\nHere are lines that are either unchanged from\nthe common ancestor, or cleanly resolved \nbecause only one side changed.\n\nBut below we have some troubles\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs:sample.txt\n\nAnd here is another line that is cleanly \nresolved or unmodified.\n\n\nYou get to decide, do you want to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nBut always delete the &lt;&lt;&lt;&lt;&lt;, ======, and &gt;&gt;&gt;&gt;&gt; lines.\nOnce you’re satisfied, committing resolves the conflict."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-other-pointers",
    "href": "schedule/slides/00-version-control.html#some-other-pointers",
    "title": "UBC Stat406 2023W",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "href": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "title": "UBC Stat406 2023W",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\n\nIn RStudio, select the file and click   then select  Revert…\n\n# grab the previously committed version\ngit checkout -- README.md \n2. Staged but not committed\n\nIn RStudio, uncheck the box by the file, then use the method above.\n\n# unstage\ngit reset HEAD README.md\ngit checkout -- README.md\n\n\n3. Committed\n\nNot easy to do in RStudio…\n\n# check the log to see where you made the chg, \ngit log\n# go one step before that (eg to 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n\n# alternatively\n# if it happens to also be on another branch\ngit checkout otherbranch -- README.md"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#recovering-from-things",
    "href": "schedule/slides/00-version-control.html#recovering-from-things",
    "title": "UBC Stat406 2023W",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main, Tried to Push but got refused\n\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# find out where to go to\ngit log\n# undo everything after ace2193\ngit reset --hard ace2193\ngit checkout newbranch\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\ngit checkout main\ngit branch -d badbranch\n\nAnything more complicated, either post to Slack or LMGTFY\nIn the Lab next week, you’ll practice\n\nDoing it right.\nRecovering from some mistakes."
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#meta-lecture",
    "href": "schedule/slides/00-quiz-0-wrap.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "00 Quiz 0 fun",
    "text": "00 Quiz 0 fun\nStat 406\nDaniel J. McDonald\nLast modified – 13 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#why-this-class",
    "href": "schedule/slides/00-quiz-0-wrap.html#why-this-class",
    "title": "UBC Stat406 2023W",
    "section": "Why this class?",
    "text": "Why this class?\n\nMost say requirements.\nInterest in ML/Stat learning\nExpressions of love/affection for Stats/CS/ML\nEnjoyment of past similar classes"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#why-this-class-1",
    "href": "schedule/slides/00-quiz-0-wrap.html#why-this-class-1",
    "title": "UBC Stat406 2023W",
    "section": "Why this class?",
    "text": "Why this class?\nMore idiosyncratic:\n\n\n“Professor received Phd from CMU, must be an awesome  researcher.”\n“Learn strategies.”\n(paraphrase) “Course structure with less weight on exam helps with anxiety”\n(paraphrase) “I love coding in R and want more of it”\n“Emmmmmmmmmmmmmmmm, to learn some skills from Machine Learning and finish my minor🙃.”\n“destiny”\n“challenges from ChatGPT”\n“I thought Daniel Mcdonald is a cool prof…”\n“I have heard this is the most useful stat course in UBC.”"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#syllabus-q",
    "href": "schedule/slides/00-quiz-0-wrap.html#syllabus-q",
    "title": "UBC Stat406 2023W",
    "section": "Syllabus Q",
    "text": "Syllabus Q"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#programming-languages",
    "href": "schedule/slides/00-quiz-0-wrap.html#programming-languages",
    "title": "UBC Stat406 2023W",
    "section": "Programming languages",
    "text": "Programming languages"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#matrix-inversion",
    "href": "schedule/slides/00-quiz-0-wrap.html#matrix-inversion",
    "title": "UBC Stat406 2023W",
    "section": "Matrix inversion",
    "text": "Matrix inversion\n\nlibrary(MASS)\nX &lt;- matrix(c(5, 3, 1, -1), nrow = 2)\nX\n\n     [,1] [,2]\n[1,]    5    1\n[2,]    3   -1\n\nsolve(X)\n\n      [,1]   [,2]\n[1,] 0.125  0.125\n[2,] 0.375 -0.625\n\nginv(X)\n\n      [,1]   [,2]\n[1,] 0.125  0.125\n[2,] 0.375 -0.625\n\nX^(-1)\n\n          [,1] [,2]\n[1,] 0.2000000    1\n[2,] 0.3333333   -1"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#linear-models",
    "href": "schedule/slides/00-quiz-0-wrap.html#linear-models",
    "title": "UBC Stat406 2023W",
    "section": "Linear models",
    "text": "Linear models\n\ny &lt;- X %*% c(2, -1) + rnorm(2)\ncoefficients(lm(y ~ X))\n\n(Intercept)          X1          X2 \n  4.8953718   0.9380314          NA \n\ncoef(lm(y ~ X))\n\n(Intercept)          X1          X2 \n  4.8953718   0.9380314          NA \n\nsolve(t(X) %*% X) %*% t(X) %*% y\n\n          [,1]\n[1,]  2.161874\n[2,] -1.223843\n\nsolve(crossprod(X), crossprod(X, y))\n\n          [,1]\n[1,]  2.161874\n[2,] -1.223843\n\n\n\nX \\ y # this is Matlab\n\nError: &lt;text&gt;:1:3: unexpected '\\\\'\n1: X \\\n      ^"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#pets-and-plans",
    "href": "schedule/slides/00-quiz-0-wrap.html#pets-and-plans",
    "title": "UBC Stat406 2023W",
    "section": "Pets and plans",
    "text": "Pets and plans"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#grade-predictions",
    "href": "schedule/slides/00-quiz-0-wrap.html#grade-predictions",
    "title": "UBC Stat406 2023W",
    "section": "Grade predictions",
    "text": "Grade predictions\n\n\n4 people say 100%\n24 say 90%\n25 say 85%\n27 say 80%\nLots of clumping\n\n\n1 said 35, and 1 said 50. Woof!"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#prediction-accuracy-last-year",
    "href": "schedule/slides/00-quiz-0-wrap.html#prediction-accuracy-last-year",
    "title": "UBC Stat406 2023W",
    "section": "Prediction accuracy (last year)",
    "text": "Prediction accuracy (last year)"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#prediction-accuracy-last-year-1",
    "href": "schedule/slides/00-quiz-0-wrap.html#prediction-accuracy-last-year-1",
    "title": "UBC Stat406 2023W",
    "section": "Prediction accuracy (last year)",
    "text": "Prediction accuracy (last year)\n\nsummary(lm(actual ~ predicted - 1, data = acc))\n\n\nCall:\nlm(formula = actual ~ predicted - 1, data = acc)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-63.931  -2.931   1.916   6.052  21.217 \n\nCoefficients:\n          Estimate Std. Error t value Pr(&gt;|t|)    \npredicted  0.96590    0.01025   94.23   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.2 on 137 degrees of freedom\n  (8 observations deleted due to missingness)\nMultiple R-squared:  0.9848,    Adjusted R-squared:  0.9847 \nF-statistic:  8880 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nUBC Stat 406 - 2023"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#meta-lecture",
    "href": "schedule/slides/00-cv-for-many-models.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "00 CV for many models",
    "text": "00 CV for many models\nStat 406\nDaniel J. McDonald\nLast modified – 19 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#some-data-and-4-models",
    "href": "schedule/slides/00-cv-for-many-models.html#some-data-and-4-models",
    "title": "UBC Stat406 2023W",
    "section": "Some data and 4 models",
    "text": "Some data and 4 models\n\ndata(\"mobility\", package = \"Stat406\")\n\nModel 1: Lasso on all predictors, use CV min\nModel 2: Ridge on all predictors, use CV min\nModel 3: OLS on all predictors (no tuning parameters)\nModel 4: (1) Lasso on all predictors, then (2) OLS on those chosen at CV min\n\nHow do I decide between these 4 models?"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#cv-functions",
    "href": "schedule/slides/00-cv-for-many-models.html#cv-functions",
    "title": "UBC Stat406 2023W",
    "section": "CV functions",
    "text": "CV functions\n\nkfold_cv &lt;- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  fold_labels &lt;- sample(rep(seq_len(kfolds), length.out = nrow(data)))\n  errors &lt;- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows &lt;- fold_labels == fold\n    train &lt;- data[!test_rows, ]\n    test &lt;- data[test_rows, ]\n    current_model &lt;- estimator(train)\n    test$.preds &lt;- predictor(current_model, test)\n    errors[fold] &lt;- error_fun(test)\n  }\n  mean(errors)\n}\n\nloo_cv &lt;- function(dat) {\n  mdl &lt;- lm(Mobility ~ ., data = dat)\n  mean( abs(residuals(mdl)) / abs(1 - hatvalues(mdl)) ) # MAE version\n}"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#experiment-setup",
    "href": "schedule/slides/00-cv-for-many-models.html#experiment-setup",
    "title": "UBC Stat406 2023W",
    "section": "Experiment setup",
    "text": "Experiment setup\n\n# prepare our data\n# note that mob has only continuous predictors, otherwise could be trouble\nmob &lt;- mobility[complete.cases(mobility), ] |&gt; select(-ID, -State, -Name)\n# avoid doing this same operation a bunch\nxmat &lt;- function(dat) dat |&gt; select(!Mobility) |&gt; as.matrix()\n\n# set up our model functions\nlibrary(glmnet)\nmod1 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, type.measure = \"mae\", ...)\nmod2 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, alpha = 0, type.measure = \"mae\", ...)\nmod3 &lt;- function(dat, ...) glmnet(xmat(dat), dat$Mobility, lambda = 0, ...) # just does lm()\nmod4 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, relax = TRUE, gamma = 1, type.measure = \"mae\", ...)\n\n# this will still \"work\" on mod3, because there's only 1 s\npredictor &lt;- function(mod, dat) drop(predict(mod, newx = xmat(dat), s = \"lambda.min\"))\n\n# chose mean absolute error just 'cause\nerror_fun &lt;- function(testdata) mean(abs(testdata$Mobility - testdata$.preds))"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#run-the-experiment",
    "href": "schedule/slides/00-cv-for-many-models.html#run-the-experiment",
    "title": "UBC Stat406 2023W",
    "section": "Run the experiment",
    "text": "Run the experiment\n\nall_model_funs &lt;- lst(mod1, mod2, mod3, mod4)\nall_fits &lt;- map(all_model_funs, .f = exec, dat = mob)\n\n# unfortunately, does different splits for each method, so we use 10, \n# it would be better to use the _SAME_ splits\nten_fold_cv &lt;- map_dbl(all_model_funs, ~ kfold_cv(mob, .x, predictor, error_fun, 10)) \n\nin_sample_cv &lt;- c(\n  mod1 = min(all_fits[[1]]$cvm),\n  mod2 = min(all_fits[[2]]$cvm),\n  mod3 = loo_cv(mob),\n  mod4 = min(all_fits[[4]]$cvm)\n)\n\ntib &lt;- bind_rows(in_sample_cv, ten_fold_cv)\ntib$method = c(\"in_sample\", \"out_of_sample\")\ntib\n\n# A tibble: 2 × 5\n    mod1   mod2   mod3   mod4 method       \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        \n1 0.0159 0.0161 0.0164 0.0156 in_sample    \n2 0.0158 0.0161 0.0165 0.0161 out_of_sample\n\n\n\n\nUBC Stat 406 - 2023"
  },
  {
    "objectID": "course-setup.html",
    "href": "course-setup.html",
    "title": "Guide for setting up the course infrastructure",
    "section": "",
    "text": "Version 2023\nThis guide (hopefully) gives enough instructions for recreating new iterations of Stat 406."
  },
  {
    "objectID": "course-setup.html#github-org",
    "href": "course-setup.html#github-org",
    "title": "Guide for setting up the course infrastructure",
    "section": "Github Org",
    "text": "Github Org\n\nCreate a GitHub.com organization\n\nThis is free for faculty with instructor credentials.\nAllows more comprehensive GitHub actions, PR templates and CODEOWNER behaviour than the UBC Enterprise version\nDownside is getting students added (though we include R scripts for this)\n\nOnce done, go to https://github.com/watching. Click the Red Down arrow “Unwatch all”. Then select this Org. The TAs should do the same.\n\n\nPermissions and structure\nSettings &gt; Member Privileges\nWe list only the important ones.\n\nBase Permissions: No Permission\nRepository creation: None\nRepo forking: None\nPages creation: None\nTeam creation rules: No\n\nBe sure to click save in each area after making changes.\nSettings &gt; Actions &gt; General\nAll repositories: Allow all actions and reusable workflows.\nWorkflow permissions: Read and write permissions.\n\n\nTeams\n\n2 teams, one for the TAs and one for the students\nYou must then manually add the teams to any repos they should access\n\nI generally give the TAs “Write” permission, and the students “Read” permission with some exceptions. See the Repos section below."
  },
  {
    "objectID": "course-setup.html#repos",
    "href": "course-setup.html#repos",
    "title": "Guide for setting up the course infrastructure",
    "section": "Repos",
    "text": "Repos\nThere are typically about 10 repositories. Homeworks and Labs each have 3 with very similar behaviours.\nBe careful copying directories. All of them have hidden files and folders, e.g. .git. Of particular importance are the .github directories which contain PR templates and GitHub Actions. Also relevant are the .Rprofile files which try to override Student Language settings and avoid unprintible markdown characters.\n\nHomeworks\n\nhomework-solutions\nThis is where most of the work happens. My practice is to create the homework solutions first. I edit these (before school starts) until I’m happy. I then duplicate the file and remove the answers. The result is hwxx-instructions.Rmd. The .gitignore file should ignore all of the solutions and commmit only the instructions. Then, about 1 week after the deadline, I adjust the .gitignore and push the solution files.\n\nStudents have Read permission.\nTAs have Write permission.\nThe preamble.tex file is common to HWs and Labs. It creates a lavender box where the solution will go. This makes life easy for the TAs.\n\n\n\nhomework-solutions-private\nExactly the same as homework-solutions except that all solutions are available from the beginning for TA access. To create this, after I’m satisfied with homework-solutions I copy all files (not the directory) into a new directory, git init then upload to the org. The students never have permission here.\n\n\nhomework-template\nThis is a “template repo” used for creating student specific homework-studentgh repos (using the setup scripts).\nVery Important: copy the hwxx-instructions files over to a new directory. Do NOT copy the directory or you’ll end up with the solutions visible to the students.\nThen rename hwxx-instructions.Rmd to hwxx.Rmd. Now the students have a .pdf with instructions, and a template .Rmd to work on.\nOther important tasks: * The .gitignore is more elaborate in an attempt to avoid students pushing junk into these repos. * The .github directory contains 3 files: CODEOWNERS begins as an empty doc which will be populated with the assigned grader later; pull_request_template.md is used for all HW submission PRs; workflows contains a GH-action to comment on the PR with the date+time when the PR is opened. * Under Settings &gt; General, select “Template repository”. This makes it easier to duplicate to the student repos.\n\n\n\nLabs\nThe three Labs repos operate exactly as the analogous homework repos.\n\nlabs-solutions\nDo any edits here before class begins.\n\n\nlabs-solutions-private\nSame as with the homeworks\n\n\nlabs-template\nSame as with the homeworks\n\n\n\nclicker-solutions\nThis contains the complete set of clicker questions.\nAnswers are hidden in comments on the presentation.\nI release them incrementally after each module (copying over from my clicker deck).\n\n\nopen-pr-log\nThis contains a some GitHub actions to automatically keep track of open PRs for the TAs.\nIt’s still in testing phase, but should work properly. It will create two markdown docs, 1 for labs and 1 for homework. Each shows the assigned TA, the date the PR was opened, and a link to the PR. If everything is configured properly, it should run automatically at 3am every night.\n\nOnly the TAs should have access.\nUnder Settings &gt; Secrets and Variables &gt; Actions you must add a “Repository Secret”. This should be a GitHub Personal Access Token created in your account (Settings &gt; Developer settings &gt; Tokens (classic)). It needs Repo, Workflow, and Admin:Org permissions. I set it to expire at the end of the course. I use it only for this purpose (rather than my other tokens for typical logins).\n\n\n\n.github / .github-private\nThese contains a README that gives some basic information about the available repos and the course. It’s visible Publically, and appears on the Org homepage for all to see. The .github-private has the same function, but applies only to Org members.\n\n\nbakeoff-bakeoff\nThis is for the bonus for HW4. Both TAs and Students have access. I put the TA team as CODEOWNERS and protect the main branch (Settings &gt; Branches &gt; Branch Protection Rules). Here, we “Require approvals” and “Require Review from Code Owners”."
  },
  {
    "objectID": "course-setup.html#r-package",
    "href": "course-setup.html#r-package",
    "title": "Guide for setting up the course infrastructure",
    "section": "R package",
    "text": "R package\nThis is hosted at https://github.com/ubc-stat/stat-406-rpackage/. The main purposes are:\n\nDocumentation of datasets used in class, homework, and labs (if not in other R packages)\nProvide a few useful functions.\nInstall all the packages the students need at once, and try to compile LaTeX.\n\nPackage requirements are done manually, unfortunately. Typically, I’ll open the various projects in RStudio and run sort(unique(renv::dependencies()$Package)). It’s not infallible, but works well.\nAll necessary packages should go in “Suggests:” in the DESCRIPTION. This avoids build errors. Note that install via remotes::install_github() then requires dependencies = TRUE."
  },
  {
    "objectID": "course-setup.html#worksheets",
    "href": "course-setup.html#worksheets",
    "title": "Guide for setting up the course infrastructure",
    "section": "Worksheets",
    "text": "Worksheets\nThese are derived from Matías’s Rmd notes from 2018. They haven’t been updated much.\nThey are hosted at https://github.com/ubc-stat/stat-406-worksheets/.\nI tried requiring them one year. The model was to distribute the R code for the chapters with some random lines removed. Then the students could submit the completed code for small amounts of credit. It didn’t seem to move the needle much and was hard to grade (autograding would be nice here).\nNote that there is a GHaction that automatically renders the book from source and pushes to the gh-pages branch. So local build isn’t necessary and derivative files should not be checked in to version control."
  },
  {
    "objectID": "course-setup.html#course-website-lectures",
    "href": "course-setup.html#course-website-lectures",
    "title": "Guide for setting up the course infrastructure",
    "section": "Course website / lectures",
    "text": "Course website / lectures"
  },
  {
    "objectID": "course-setup.html#ghclass-package",
    "href": "course-setup.html#ghclass-package",
    "title": "Guide for setting up the course infrastructure",
    "section": "{ghclass} package",
    "text": "{ghclass} package"
  },
  {
    "objectID": "course-setup.html#canvas",
    "href": "course-setup.html#canvas",
    "title": "Guide for setting up the course infrastructure",
    "section": "Canvas",
    "text": "Canvas\nI use a the shell provided by FoS.\nNothing else goes here, but you have to update all the links.\nTwo Canvas Quizzes: * Quiz 0 collects GitHub accounts, ensures that students read the syllabus. Due in Week 1. * Final Exam is the final * I usually record lectures (automatically) using the classroom tech that automatically uploads. * Update the various links on the Homepage."
  },
  {
    "objectID": "course-setup.html#slack",
    "href": "course-setup.html#slack",
    "title": "Guide for setting up the course infrastructure",
    "section": "Slack",
    "text": "Slack\n\nSet up a free Org. Invite link gets posted to Canvas.\nI add @students.ubc.ca, @ubc.ca, @stat.ubc.ca to the whitelist.\nI also post the invite on Canvas.\nCreate channels before people join. That way you can automatically add everyone to channels all at once. I do one for each module, 1 for code/github, 1 for mechanics. + 1 for the TAs (private)\nClick through all the settings. It’s useful to adjust these a bit."
  },
  {
    "objectID": "course-setup.html#clickers",
    "href": "course-setup.html#clickers",
    "title": "Guide for setting up the course infrastructure",
    "section": "Clickers",
    "text": "Clickers\nSee https://lthub.ubc.ca/guides/iclicker-cloud-instructor-guide/\nI only use “Polling” no “Quizzing” and no “Attendance”\n\nIn clicker Settings &gt; Polling &gt; Sharing. Turn off the Sending (to avoid students doing it at home)\nNo participation points.\n2 points for correct, 2 for answering.\nIntegrations &gt; Set this up with Canvas. Sync the roster. You’ll likely have to repeat this near the Add/Drop Deadline.\nI only sync the total, since I’ll recalibrate later."
  },
  {
    "objectID": "computing/windows.html",
    "href": "computing/windows.html",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#installation-notes",
    "href": "computing/windows.html#installation-notes",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#terminal",
    "href": "computing/windows.html#terminal",
    "title": " Windows",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works."
  },
  {
    "objectID": "computing/windows.html#github",
    "href": "computing/windows.html#github",
    "title": " Windows",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/windows.html#git-bash-and-windows-terminal",
    "href": "computing/windows.html#git-bash-and-windows-terminal",
    "title": " Windows",
    "section": "Git, Bash, and Windows Terminal",
    "text": "Git, Bash, and Windows Terminal\nAlthough these three are separate programs, we are including them in the same section here since they are packaged together in the same installer on Windows. Briefly, we will be using the Bash shell to interact with our computers via a command line interface, Git to keep a version history of our files and upload to/download from to GitHub, and Windows Terminal to run the both Bash and Git.\nGo to https://git-scm.com/download/win and download the windows version of git. After the download has finished, run the installer and accept the default configuration for all pages except for the following:\n\nOn the Select Components page, add a Git Bash profile to Windows Terminal.\n\n\nTo install windows terminal visit this link and click Get to open it in Windows Store. Inside the Store, click Get again and then click Install. After installation, click Launch to start Windows Terminal. In the top of the window, you will see the tab bar with one open tab, a plus sign, and a down arrow. Click the down arrow and select Settings (or type the shortcut Ctrl + ,). In the Startup section, click the dropdown menu under Default profile and select Git Bash.\n\nYou can now launch the Windows terminal from the start menu or pin it to the taskbar like any other program (you can read the rest of the article linked above for additional tips if you wish). To make sure everything worked, close down Windows Terminal, and open it again. Git Bash should open by default, the text should be green and purple, and the tab should read MINGW64:/c/Users/$USERNAME (you should also see /c/Users/$USERNAME if you type pwd into the terminal). This screenshot shows what it should look like:\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we refer to “the terminal” in these installation instructions, we want you to use the Windows Terminal that you just installed with the Git Bash profile. Do not use Windows PowerShell, CMD, or anything else unless explicitly instructed to do so.\n\n\nTo open a new tab you can click the plus sign or use Ctrl + Shift + t (you can close a tab with Ctrl + Shift + w). To copy text from the terminal, you can highlight it with the mouse and then click Ctrl + Shift + c. To paste text you use Ctrl + Shift + v, try it by pasting the following into the terminal to check which version of Bash you just installed:\nbash --version\nThe output should look similar to this:\nGNU bash, version 4.4.23(1)-release (x86_64-pc-sys)\nCopyright (C) 2019 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nNote\n\n\n\nIf there is a newline (the enter character) in the clipboard when you are pasting into the terminal, you will be asked if you are sure you want to paste since this newline will act as if you pressed enter and run the command. As a guideline you can press Paste anyway unless you are sure you don’t want this to happen.\n\n\nLet’s also check which version of git was installed:\ngit --version\ngit version 2.32.0.windows.2\n\n\n\n\n\n\nNote\n\n\n\nSome of the Git commands we will use are only available since Git 2.23, so make sure your if your Git is at least this version.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/windows.html#latex",
    "href": "computing/windows.html#latex",
    "title": " Windows",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nNote that you might see two error messages regarding lua during the installation, you can safely ignore these, the installation will complete successfully after clicking “OK”.\nIf it still fails, proceed with the instructions\n\nIn RStudio, run the following commands to install the tinytex package and setup tinytex:\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nIn order for Git Bash to be able to find the location of TinyTex, you will need to sign out of Windows and back in again. After doing that, you can check that the installation worked by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2021/W32TeX)\nkpathsea version 6.3.3\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/windows.html#github-pat",
    "href": "computing/windows.html#github-pat",
    "title": " Windows",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/windows.html#post-installation-notes",
    "href": "computing/windows.html#post-installation-notes",
    "title": " Windows",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/windows.html#attributions",
    "href": "computing/windows.html#attributions",
    "title": " Windows",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/mac_x86.html",
    "href": "computing/mac_x86.html",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#installation-notes",
    "href": "computing/mac_x86.html#installation-notes",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#terminal",
    "href": "computing/mac_x86.html#terminal",
    "title": " MacOS x86",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_x86.html#github",
    "href": "computing/mac_x86.html#github",
    "title": " MacOS x86",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_x86.html#git",
    "href": "computing/mac_x86.html#git",
    "title": " MacOS x86",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_x86.html#latex",
    "href": "computing/mac_x86.html#latex",
    "title": " MacOS x86",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_x86.html#github-pat",
    "href": "computing/mac_x86.html#github-pat",
    "title": " MacOS x86",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_x86.html#post-installation-notes",
    "href": "computing/mac_x86.html#post-installation-notes",
    "title": " MacOS x86",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_x86.html#attributions",
    "href": "computing/mac_x86.html#attributions",
    "title": " MacOS x86",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/index.html",
    "href": "computing/index.html",
    "title": " Computing",
    "section": "",
    "text": "In order to participate in this class, we will require the use of R, and encourage the use of RStudio. Both are free, and you likely already have both.\nYou also need Git, Github and Slack.\nBelow are instructions for installation. These are edited and simplified from the DSCI 310 Setup Instructions. If you took DSCI 310 last year, you may be good to go, with the exception of the R package."
  },
  {
    "objectID": "computing/index.html#laptop-requirements",
    "href": "computing/index.html#laptop-requirements",
    "title": " Computing",
    "section": "Laptop requirements",
    "text": "Laptop requirements\n\nRuns one of the following operating systems: Ubuntu 20.04, macOS (version 11.4.x or higher), Windows 10 (version 2004, 20H2, 21H1 or higher).\n\nWhen installing Ubuntu, checking the box “Install third party…” will (among other things) install proprietary drivers, which can be helpful for wifi and graphics cards.\n\nCan connect to networks via a wireless connection for on campus work\nHas at least 30 GB disk space available\nHas at least 4 GB of RAM\nUses a 64-bit CPU\nIs at most 6 years old (4 years old or newer is recommended)\nUses English as the default language. Using other languages is possible, but we have found that it often causes problems in the homework. We’ve done our best to fix them, but we may ask you to change it if you are having trouble.\nStudent user has full administrative access to the computer."
  },
  {
    "objectID": "computing/index.html#software-installation-instructions",
    "href": "computing/index.html#software-installation-instructions",
    "title": " Computing",
    "section": "Software installation instructions",
    "text": "Software installation instructions\nPlease click the appropriate link below to view the installation instructions for your operating system:\n\nmacOS x86 or macOS arm\nUbuntu\nWindows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 406",
    "section": "",
    "text": "Jump to Schedule Syllabus\n\n\nAt the end of the course, you will be able to:\n\nAssess the prediction properties of the supervised learning methods covered in class;\nCorrectly use regularization to improve predictions from linear models, and also to identify important explanatory variables;\nExplain the practical difference between predictions obtained with parametric and non-parametric methods, and decide in specific applications which approach should be used;\nSelect and construct appropriate ensembles to obtain improved predictions in different contexts;\nUse and interpret principal components and other dimension reduction techniques;\nEmploy reasonable coding practices and understand basic R syntax and function.\nWrite reports and use proper version control; engage with standard software."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Complete readings before the material is covered in class, and then review again afterwards.\nParticipate actively in class. If you don’t understand something, I can guarantee no one else does either. I have a Ph.D., and I’ve been doing this for more than 10 years. It’s hard for me to remember what it’s like to be you and what you don’t know. Say something! I want you to learn this stuff, and I love to explain more carefully.\nCome to office hours. Again, I like explaining things.\nTry the Labs again without the help of your classmates.\nRead the examples at the end of the [ISLR] chapters. Try the exercises.\nDo not procrastinate — don’t let a module go by with unanswered questions as it will just make the following module’s material even more difficult to follow.\nDo the Worksheets."
  },
  {
    "objectID": "faq.html#how-do-i-succeed-in-this-class",
    "href": "faq.html#how-do-i-succeed-in-this-class",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Complete readings before the material is covered in class, and then review again afterwards.\nParticipate actively in class. If you don’t understand something, I can guarantee no one else does either. I have a Ph.D., and I’ve been doing this for more than 10 years. It’s hard for me to remember what it’s like to be you and what you don’t know. Say something! I want you to learn this stuff, and I love to explain more carefully.\nCome to office hours. Again, I like explaining things.\nTry the Labs again without the help of your classmates.\nRead the examples at the end of the [ISLR] chapters. Try the exercises.\nDo not procrastinate — don’t let a module go by with unanswered questions as it will just make the following module’s material even more difficult to follow.\nDo the Worksheets."
  },
  {
    "objectID": "faq.html#git-and-github",
    "href": "faq.html#git-and-github",
    "title": " Frequently asked questions",
    "section": "Git and Github",
    "text": "Git and Github\n\nHomework/Labs workflow\nRstudio version (uses the Git tab. Usually near Environment/History in the upper right)\n\nMake sure you are on main. Pull in remote changes. Click .\nCreate a new branch by clicking the think that looks kinda like .\nWork on your documents and save frequently.\nStage your changes by clicking the check boxes.\nCommit your changes by clicking Commit.\nRepeat 3-5 as necessary.\nPush to Github \nWhen done, go to Github and open a PR.\nUse the dropdown menu to go back to main and avoid future headaches.\n\nCommand line version\n\n(Optional, but useful. Pull in any remote changes.) git pull\nCreate a new branch git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes git add &lt;name-of-document1&gt; repeat for each changed document. git add . stages all changed documents.\nCommit your changes git commit -m \"some message that is meaningful\"\nRepeat 3-5 as necessary.\nPush to Github git push. It may suggest a longer form of this command, obey.\nWhen done, go to Github and open a PR.\nSwitch back to main to avoid future headaches. git checkout main.\n\n\n\nAsking for a HW regrade.\n\n\n\n\n\n\nTo be eligible\n\n\n\n\nYou must have received &gt;3 points of deductions to be eligible.\nAnd they must have been for “content”, not penalties.\nIf you fix the errors, you can raise your grade to 7/10.\nYou must make revisions and re-request review within 1 week of your initial review.\n\n\n\n\nGo to the your local branch for this HW. If you don’t remember the right name, you can check the PRs in your repo on GitHub by clicking “Pull Requests” tab. It might be closed.\nMake any changes you need to make to the files, commit and push. Make sure to rerender the .pdf if needed.\nGo to GitHub.com and find the original PR for this assignment. There should now be additional commits since the previous Review.\nAdd a comment to the TA describing the changes you’ve made. Be concise and clear.\nUnder “Reviewers” on the upper right of the screen, you should see a 🔁 button. Once you click that, the TA will be notified to review your changes.\n\n\n\nFixing common problems\n\nmaster/main\n“master” has some pretty painful connotations. So as part of an effort to remove racist names from code, the default branch is now “main” on new versions of GitHub. But old versions (like the UBC version) still have “master”. Below, I’ll use “main”, but if you see “master” on what you’re doing, that’s the one to use.\n\n\nStart from main\nBranches should be created from the main branch, not the one you used for the last assignment.\ngit checkout main\nThis switches to main. Then pull and start the new assignment following the workflow above. (In Rstudio, use the dropdown menu.)\n\n\nYou forgot to work on a new branch\nUgh, you did some labs before realizing you forgot to create a new branch. Don’t stress. There are some things below to try. But if you’re confused ASK. We’ve had practice with this, and soon you will too!\n(1) If you started from main and haven’t made any commits (but you SAVED!!):\ngit branch -b &lt;new-branch-name&gt;\nThis keeps everything you have and puts you on a new branch. No problem. Commit and proceed as usual.\n(2) If you are on main and made some commits:\ngit branch &lt;new-branch-name&gt;\ngit log\nThe first line makes a new branch with all the stuff you’ve done. Then we look at the log. Locate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit reset ac2a8 --hard\nThis rolls main back to that commit. You don’t need the whole string, just the first few characters. Finally\ngit checkout &lt;new-branch-name&gt;\nand continue working.\n(3) If you started work on &lt;some-old-branch&gt; for work you already submitted:\nThis one is harder, and I would suggest getting in touch with the TAs. Here’s the procedure.\ngit commit -am \"uhoh, I need to be on a different branch\"\ngit branch &lt;new-branch-name&gt;\nCommit your work with a dumb message, then create a new branch. It’s got all your stuff.\ngit log\nLocate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit rebase --onto main ac2a8 &lt;new-branch-name&gt;\ngit checkout &lt;new-branch-name&gt;\nThis makes the new branch look like main but without the differences from main that are on ac2a8 and WITH all the work you did after ac2a8. It’s pretty cool. And should work. Finally, we switch to our new branch.\n\n\n\nHow can I get better at R?\nI get this question a lot. The answer is almost never “go read the book How to learn R fast” or “watch the video on FreeRadvice.com”. To learn programming, the only thing to do is to program. Do your tutorialls. Redo your tutorials. Run through the code in the textbook. Ask yourself why we used one function instead of another. Ask questions. Play little coding games. If you find yourself wondering how some bit of code works, run through it step by step. Print out the results and see what it’s doing. If you take on these kinds of tasks regularly, you will improve rapidly.\nCoding is an active activity just like learning Spanish. You have to practice constantly. For the same reasons that it is difficult/impossible to learn Spanish just from reading a textbook, it is difficult/impossible to learn R just from reading/watching.\nWhen I took German in 7th grade, I remember my teacher saying “to learn a language, you have to constantly tell lies”. What he meant was, you don’t just say “yesterday I went to the gym”. You say “yesterday I went to the market”, “yesterday I went to the movies”, “today she’s going to the gym”, etc. The point is to internalize conjugation, vocabulary, and the inner workings of the language. The same is true when coding. Do things different ways. Try automating regular tasks.\nRecommended resources\n\nData Science: A first introduction This is the course textbook for UBC’s DSCI 100\nR4DS written by Hadley Wickham and Garrett Grolemund\nDSCI 310 Coursenotes by Tiffany A. Timbers, Joel Ostblom, Florencia D’Andrea, and Rodolfo Lourenzutti\nHappy Git with R by Jenny Bryan\nModern Dive: Statistical Inference via Data Science\nStat545\nGoogle\n\n\n\nMy code doesn’t run. What do I do?\nThis is a constant issue with code, and it happens to everyone. The following is a general workflow for debugging stuck code.\n\nIf the code is running, but not doing what you want, see below.\nRead the Error message. It will give you some important hints. Sometimes these are hard to parse, but that’s ok.\n\n\nset.seed(12345)\ny &lt;- rnorm(10)\nx &lt;- matrix(rnorm(20), 2)\nlinmod &lt;- lm(y ~ x)\n## Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE): variable lengths differ (found for 'x')\n\nThis one is a little difficult. The first stuff before the colon is telling me where the error happened, but I didn’t use a function called model.frame.default. Nonetheless, after the colon it says variable lengths differ. Well y is length 10 and x has 10 rows right? Oh wait, how many rows does x have?\n\nRead the documentation for the function in the error message. For the above, I should try ?matrix.\nGoogle!! If the first few steps didn’t help, copy the error message into Google. This almost always helps. Best to remove any overly specific information first.\nAsk your classmates Slack. In order to ask most effectively, you should probably provide them some idea of how the error happened. See the section on MWEs for how to do this.\nSee me or the TA. Note that it is highly likely that I will ask if you did the above steps first. And I will want to see your minimal working example (MWE).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you meet with me, be prepared to show me your code! Or message me your MWE. Or both. But not neither.\n\n\nIf the error cannot be reproduced in my presence, it is very unlikely that I can fix it.\n\n\nMinimal working examples\nAn MWE is a small bit of code which will work on anyone’s machine and reproduce the error that you are getting. This is a key component of getting help debugging. When you do your homework, there’s lots of stuff going on that will differ from most other students. To allow them (or me, or the TA) to help you, you need to be able to get their machine to reproduce your error (and only your error) without much hassle.\nI find that, in the process of preparing an MWE, I can often answer my own question. So it is a useful exercise even if you aren’t ready to call in the experts yet. The process of stripping your problem down to its bare essence often reveals where the root issue lies. My above code is an MWE: I set a seed, so we both can use exactly the same data, and it’s only a few lines long without calling any custom code that you don’t have.\nFor a good discussion of how to do this, see the R Lecture or stackexchange.\n\n\nHow to write good code\nThis is covered in much greater detail in the lectures, so see there. Here is my basic advice.\n\nWrite script files (which you save) and source them. Don’t do everything in the console. R (and python and Matlab and SAS) is much better as a scripting language than as a calculator.\nDon’t write anything more than once. This has three corollaries:\n\nIf you are tempted to copy/paste, don’t.\nDon’t use magic numbers. Define all constants at the top of the script.\nWrite functions.\n\nThe third is very important. Functions are easy to test. You give different inputs and check whether the output is as expected. This helps catch mistakes.\nThere are two kinds of errors: syntax and function.\n\nThe first R can find (missing close parenthesis, wrong arguments, etc.)\n\nThe second you can only catch by thorough testing\n\nDon’t use magic numbers.\nUse meaningful names. Don’t do this:\n\ndata(\"ChickWeight\")\nout &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\n\nComment things that aren’t clear from the (meaningful) names.\nComment long formulas that don’t immediately make sense:\n\ngarbage &lt;- with(\n  ChickWeight, \n  by(weight, Chick, function(x) (x^2 + 23) / length(x))\n) ## WTF???"
  },
  {
    "objectID": "computing/mac_arm.html",
    "href": "computing/mac_arm.html",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#installation-notes",
    "href": "computing/mac_arm.html#installation-notes",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#terminal",
    "href": "computing/mac_arm.html#terminal",
    "title": " MacOS ARM",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_arm.html#github",
    "href": "computing/mac_arm.html#github",
    "title": " MacOS ARM",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_arm.html#git",
    "href": "computing/mac_arm.html#git",
    "title": " MacOS ARM",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_arm.html#latex",
    "href": "computing/mac_arm.html#latex",
    "title": " MacOS ARM",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_arm.html#github-pat",
    "href": "computing/mac_arm.html#github-pat",
    "title": " MacOS ARM",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_arm.html#post-installation-notes",
    "href": "computing/mac_arm.html#post-installation-notes",
    "title": " MacOS ARM",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_arm.html#attributions",
    "href": "computing/mac_arm.html#attributions",
    "title": " MacOS ARM",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/ubuntu.html",
    "href": "computing/ubuntu.html",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#installation-notes",
    "href": "computing/ubuntu.html#installation-notes",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#ubuntu-software-settings",
    "href": "computing/ubuntu.html#ubuntu-software-settings",
    "title": " Ubuntu",
    "section": "Ubuntu software settings",
    "text": "Ubuntu software settings\nTo ensure that you are installing the right version of the software in this guide, open “Software & Updates” and make sure that the boxes in the screenshot are checked (this is the default configuration)."
  },
  {
    "objectID": "computing/ubuntu.html#github",
    "href": "computing/ubuntu.html#github",
    "title": " Ubuntu",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/ubuntu.html#git",
    "href": "computing/ubuntu.html#git",
    "title": " Ubuntu",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, so if your Git is older than this version, we ask you to update it using the following commands:\nsudo apt update\nsudo apt install git\nYou can check your git version with the following command:\ngit --version\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Linux section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/ubuntu.html#latex",
    "href": "computing/ubuntu.html#latex",
    "title": " Ubuntu",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it still fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/ubuntu.html#github-pat",
    "href": "computing/ubuntu.html#github-pat",
    "title": " Ubuntu",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/ubuntu.html#post-installation-notes",
    "href": "computing/ubuntu.html#post-installation-notes",
    "title": " Ubuntu",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/ubuntu.html#attributions",
    "href": "computing/ubuntu.html#attributions",
    "title": " Ubuntu",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "Term 2023 Winter 1: 05 Sep - 07 Dec 2023"
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": " Syllabus",
    "section": "Course info",
    "text": "Course info\nInstructor:\nDaniel McDonald\nOffice: Earth Sciences Building 3106\nWebsite: https://dajmcdon.github.io/\nEmail: daniel@stat.ubc.ca\nSlack: @prof-daniel\nOffice hours:\nMonday (TA), 2-3pm ESB 1045\nThursday/Tuesday (DJM), 10-11am ESB 4182 (the first Tuesday of each month will be moved to Thursday)\nThursday (TA), 3-4pm ESB 3174\nFriday (TA/DJM), 10-11am Zoom (link on Canvas)\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGithub: https://github.com/stat-406-2023\nSee also Canvas\nLectures:\nTue/Thu 0800h - 0930h\n(In person) Earth Sciences Building (ESB) 1012\nTextbooks:\n[ISLR]\n[ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": " Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\nThis is a course in statistical learning methods. Based on the theory of linear models covered in Stat 306, this course will focus on applying many techniques of data analysis to interesting datasets.\nThe course combines analysis with methodology and computational aspects. It treats both the “art” of understanding unfamiliar data and the “science” of analyzing that data in terms of statistical properties. The focus will be on practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own lives.\nThis is not a “how to program” course, nor a “tour of machine learning methods”. Rather, this course is about how to understand some ML methods. STAT 306 tends to give background in many of the tools of understanding as well as working with already-written R packages. On the other hand, CPSC 340 introduces many methods with a focus on “from-scratch” implementation (in Julia or Python). This course will try to bridge the gap between these approaches. Depending on which course you took, you may be more or less skilled in some aspects than in others. That’s OK and expected.\n\nLearning outcomes\n\nAssess the prediction properties of the supervised learning methods covered in class;\nCorrectly use regularization to improve predictions from linear models, and also to identify important explanatory variables;\nExplain the practical difference between predictions obtained with parametric and non-parametric methods, and decide in specific applications which approach should be used;\nSelect and construct appropriate ensembles to obtain improved predictions in different contexts;\nUse and interpret principal components and other dimension reduction techniques;\nEmploy reasonable coding practices and understand basic R syntax and function.\nWrite reports and use proper version control; engage with standard software."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": " Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nRequired:\nAn Introduction to Statistical Learning, James, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: https://www.statlearning.com\n\n\nOptional (but excellent):\nThe Elements of Statistical Learning, Hastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\nThis second book is a more advanced treatment of a superset of the topics we will cover. If you want to learn more and understand the material more deeply, this is the book for you. All readings from [ESL] are optional."
  },
  {
    "objectID": "syllabus.html#course-assessment-opportunities",
    "href": "syllabus.html#course-assessment-opportunities",
    "title": " Syllabus",
    "section": "Course assessment opportunities",
    "text": "Course assessment opportunities\n\nEffort-based component\nLabs: [0, 20]\nHomework assignments: [0, 50]\nClickers: [0, 10]\nTotal: min(65, Labs + Homework + Clickers)\n\n\nLabs\nThese are intended to keep you on track. They are to be submitted via pull requests in your personal labs-&lt;username&gt; repo (see the computing tab for descriptions on how to do this).\nLabs typically have a few questions for you to answer or code to implement. These are to be done during lab periods. But you can do them on your own as well. These are worth 2 points each up to a maximum of 20 points. They are due at 2300 on the day of your assigned lab section.\nIf you attend lab, you may share a submission with another student (with acknowledgement on the PR). If you do not attend lab, you must work on your own (subject to the collaboration instructions for Assignments below).\n\nRules.\nYou must submit via PR by the deadline. Your PR must include at least 3 commits. After lab 2, failure to include at least 3 commits will result in a maximum score of 1.\n\n\n\n\n\n\nTip\n\n\n\nIf you attend your lab section, you may work in pairs, submitting a single document to one of your Repos. Be sure to put both names on the document, and mention the collaboration on your PR. You still have until 11pm to submit.\n\n\n\n\nMarking.\nThe overriding theme here is “if you put in the effort, you’ll get all the points.” Grading scheme:\n\n2 if basically all correct\n\n1 if complete but with some major errors, or mostly complete and mostly correct\n\n0 otherwise\n\nYou may submit as many labs as you wish up to 20 total points.\nThere are no appeals on grades.\nIt’s important here to recognize just how important active participation in these activities is. You learn by doing, and this is your opportunity to learn in a low-stakes environment. One thing you’ll learn, for example, is that all animals urinate in 21 seconds.1\n\n\n\nAssignments\nThere will be 5 assignments. These are submitted via pull request similar to the labs but to the homework-&lt;username&gt; repo. Each assignment is worth up to 10 points. They are due by 2300 on the deadline. You must make at least 5 commits. Failure to have at least 5 commits will result in a 25% deduction on HW1 and a 50% deduction thereafter. No exceptions.\nAssignments are typically lightly marked. The median last year was 8/10. But they are not easy. Nor are they short. They often involve a combination of coding, writing, description, and production of statistical graphics.\nAfter receiving a mark and feedback, if you score less than 7, you may make corrections to bring your total to 7. This means, if you fix everything that you did wrong, you get 7. Not 10. The revision must be submitted within 1 week of getting your mark. Only 1 revision per assignment. The TA decision is final. Note that the TAs will only regrade parts you missed, but if you somehow make it worse, they can deduct more points.\nThe revision allowance applies only if you got 3 or more points of “content” deductions. If you missed 3 points for content and 2 more for “penalties” (like insufficient commits, code that runs off the side of the page, etc), then you are ineligible.\n\nPolicy on collaboration on assignments\nDiscussing assignments with your classmates is allowed and encouraged, but it is important that every student get practice working on these problems. This means that all the work you turn in must be your own. The general policy on homework collaboration is:\n\nYou must first make a serious effort to solve the problem.\nIf you are stuck after doing so, you may ask for help from another student. You may discuss strategies to solve the problem, but you may not look at their code, nor may they spell out the solution to you step-by-step.\nOnce you have gotten help, you must write your own solution individually. You must disclose, in your GitHub pull request, the names of anyone from whom you got help.\nThis also applies in reverse: if someone approaches you for help, you must not provide it unless they have already attempted to solve the problem, and you may not share your code or spell out the solution step-by-step.\n\n\n\n\n\n\n\nWarning\n\n\n\nAdherence to the above policy means that identical answers, or nearly identical answers, cannot occur. Thus, such occurrences are violations of the Course’s Academic honesty policy.\n\n\nThese rules also apply to getting help from other people such as friends not in the course (try the problem first, discuss strategies, not step-by-step solutions, acknowledge those from whom you received help).\nYou may not use homework help websites, ChatGPT, Stack Overflow, and so on under any circumstances. The purpose here is to learn. Good faith efforts toward learning are rewarded.\nYou can always, of course, ask me for help on Slack. And public Slack questions are allowed and encouraged.\nYou may also use external sources (books, websites, papers, …) to\n\nLook up programming language documentation, find useful packages, find explanations for error messages, or remind yourself about the syntax for some feature. I do this all the time in the real world. Wikipedia is your friend.\nRead about general approaches to solving specific problems (e.g. a guide to dynamic programming or a tutorial on unit testing in your programming language), or\nClarify material from the course notes or assignments.\n\nBut external sources must be used to support your solution, not to obtain your solution. You may not use them to\n\nFind solutions to the specific problems assigned as homework (in words or in code)—you must independently solve the problem assigned, not translate a solution presented online or elsewhere.\nFind course materials or solutions from this or similar courses from previous years, or\nCopy text or code to use in your submissions without attribution.\n\nIf you use code from online or other sources, you must include code comments identifying the source. It must be clear what code you wrote and what code is from other sources. This rule also applies to text, images, and any other material you submit.\nPlease talk to me if you have any questions about this policy. Any form of plagiarism or cheating will result in sanctions to be determined by me, including grade penalties (such as negative points for the assignment or reductions in letter grade) or course failure. I am obliged to report violations to the appropriate University authorities. See also the text below.\n\n\n\nClickers\nThese are short multiple choice and True / False questions. They happen in class. For each question, correct answers are worth 4, incorrect answers are worth 2. You get 0 points for not answering.\nSuppose there are N total clicker questions, and you have x points. Your final score for this component is\nmax(0, min(5 * x / N - 5, 10)).\nNote that if your average is less than 1, you get 0 points in this component.\n\n\n\n\n\n\nImportant\n\n\n\nIn addition, your final grade in this course will be reduced by 1 full letter grade.\n\n\nThis means that if you did everything else and get a perfect score on the final exam, you will get a 79. Two people did this last year. They were sad.\n\n\n\n\n\n\nWarning\n\n\n\nDON’T DO THIS!!\n\n\nThis may sound harsh, but think about what is required for such a penalty. You’d have to skip more than 50% of class meetings and get every question wrong when you are in class. This is an in-person course. It is not possible to get an A without attending class on a regular basis.\nTo compensate, I will do my best to post recordings of lectures. Past experience has shown 2 things:\n\nYou learn better by attending class than by skipping and “watching”.\nSometimes the technology messes up. So there’s no guarantee that these will be available.\n\nThe purpose is to let you occasionally miss class for any reason with minimal consequences. See also below. If for some reason you need to miss longer streches of time, please contact me or discuss your situation with your Academic Advisor as soon as possible. Don’t wait until December.\n\n\n\nYour score on HW, Labs, and Clickers\nThe total you can accumulate across these 3 components is 65 points. But you can get there however you want. The total available is 80 points. The rest is up to you. But with choice, comes responsibility.\nRules:\n\nNothing dropped.\nNo extensions.\nIf you miss a lab or a HW deadline, then you miss it.\nMake up for missed work somewhere else.\nIf you isolate due to Covid, fine. You miss a few clickers and maybe a lab (though you can do it remotely).\nIf you have a job interview and can’t complete an assignment on time, then skip it.\n\nWe’re not going to police this stuff. You don’t need to let me know. There is no reason that every single person enrolled in this course shouldn’t get &gt; 65 in this class.\nIllustrative scenarios:\n\nDoing 80% on 5 homeworks, coming to class and getting 50% correct, get 2 points on 8 labs gets you 65 points.\nDoing 90% on 5 homeworks, getting 50% correct on all the clickers, averaging 1/2 on all the labs gets you 65 points.\nGoing to all the labs and getting 100%, 100% on 4 homeworks, plus being wrong on every clicker gets you 65 points\n\nChoose your own adventure. Note that the biggest barrier to getting to 65 is skipping the assignments.\n\n\n\n\nFinal exam\n35 points\n\n\nAll multiple choice, T/F, matching.\nThe clickers are the best preparation.\nQuestions may ask you to understand or find mistakes in code.\nNo writing code.\n\nThe Final is very hard. By definition, it cannot be effort-based.\nIt is intended to separate those who really understand the material from those who don’t. Last year, the median was 50%. But if you put in the work (do all the effort points) and get 50%, you get an 83 (an A-). If you put in the work (do all the effort points) and skip the final, you get a 65. You do not have to pass the final to pass the course. You don’t even have to take the final.\nThe point of this scheme is for those who work hard to do well. But only those who really understand the material will get 90+."
  },
  {
    "objectID": "syllabus.html#health-issues-and-considerations",
    "href": "syllabus.html#health-issues-and-considerations",
    "title": " Syllabus",
    "section": "Health issues and considerations",
    "text": "Health issues and considerations\n\nCovid Safety in the Classroom\n\n\n\n\n\n\nImportant\n\n\n\nIf you think you’re sick, stay home no matter what.\n\n\nMasks. Masks are recommended. For our in-person meetings in this class, it is important that all of us feel as comfortable as possible engaging in class activities while sharing an indoor space. Masks are a primary tool to make it harder for Covid-19 to find a new host. Please feel free to wear one or not given your own personal circumstances. Note that there are some people who cannot wear a mask. These individuals are equally welcome in our class.\nVaccination. If you have not yet had a chance to get vaccinated against Covid-19, vaccines are available to you, free. See http://www.vch.ca/covid-19/covid-19-vaccine for help finding an appointment. Boosters will be available later this term. The higher the rate of vaccination in our community overall, the lower the chance of spreading this virus. You are an important part of the UBC community. Please arrange to get vaccinated if you have not already done so. The same goes for Flu.\n\n\nYour personal health\n\n\n\n\n\n\nWarning\n\n\n\nIf you are sick, it’s important that you stay home – no matter what you think you may be sick with (e.g., cold, flu, other).\n\n\n\nDo not come to class if you have Covid symptoms, have recently tested positive for Covid, or are required to quarantine. You can check this website to find out if you should self-isolate or self-monitor: http://www.bccdc.ca/health-info/diseases-conditions/covid-19/self-isolation#Who.\nYour precautions will help reduce risk and keep everyone safer. In this class, the marking scheme is intended to provide flexibility so that you can prioritize your health and still be able to succeed. All work can be completed outside of class with reasonable time allowances.\nIf you do miss class because of illness:\n\nMake a connection early in the term to another student or a group of students in the class. You can help each other by sharing notes. If you don’t yet know anyone in the class, post on the discussion forum to connect with other students.\nConsult the class resources on here and on Canvas. We will post all the slides, readings, and recordings for each class day.\nUse Slack for help.\nCome to virtual office hours.\nSee the marking scheme for reassurance about what flexibility you have. No part of your final grade will be directly impacted by missing class.\n\nIf you are sick on final exam day, do not attend the exam. You must follow up with your home faculty’s advising office to apply for deferred standing. Students who are granted deferred standing write the final exam at a later date. If you’re a Science student, you must apply for deferred standing (an academic concession) through Science Advising no later than 48 hours after the missed final exam/assignment. Learn more and find the application online. For additional information about academic concessions, see the UBC policy here.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease talk with me if you have any concerns or ask me if you are worried about falling behind."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": " Syllabus",
    "section": "University policies",
    "text": "University policies\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here.\n\nAcademic honesty and standards\nUBC Vancouver Statement\nAcademic honesty is essential to the continued functioning of the University of British Columbia as an institution of higher learning and research. All UBC students are expected to behave as honest and responsible members of an academic community. Breach of those expectations or failure to follow the appropriate policies, principles, rules, and guidelines of the University with respect to academic honesty may result in disciplinary action.\nFor the full statement, please see the 2022/23 Vancouver Academic Calendar\nCourse specific\nSeveral commercial services have approached students regarding selling class notes/study guides to their classmates. Please be advised that selling a faculty member’s notes/study guides individually or on behalf of one of these services using UBC email or Canvas, violates both UBC information technology and UBC intellectual property policy. Selling the faculty member’s notes/study guides to fellow students in this course is not permitted. Violations of this policy will be considered violations of UBC Academic Honesty and Standards and will be reported to the Dean of Science as a violation of course rules. Sanctions for academic misconduct may include a failing grade on the assignment for which the notes/study guides are being sold, a reduction in your final course grade, a failing grade in the course, among other possibilities. Similarly, contracting with any service that results in an individual other than the enrolled student providing assistance on quizzes or exams or posing as an enrolled student is considered a violation of UBC’s academic honesty standards.\nSome of the problems that are assigned are similar or identical to those assigned in previous years by me or other instructors for this or other courses. Using proofs or code from anywhere other than the textbooks, this year’s course notes, or the course website is not only considered cheating (as described above), it is easily detectable cheating. Such behavior is strictly forbidden.\nIn previous years, I have caught students cheating on the exams or assignments. I did not enforce any penalty because the action did not help. Cheating, in my experience, occurs because students don’t understand the material, so the result is usually a failing grade even before I impose any penalty and report the incident to the Dean’s office. I carefully structure exams and assignments to make it so that I can catch these issues. I will catch you, and it does not help. Do your own work, and use the TAs and me as resources. If you are struggling, we are here to help.\n\n\n\n\n\n\nCaution\n\n\n\nIf I suspect cheating, your case will be forwarded to the Dean’s office. No questions asked.\n\n\nGenerative AI\nTools to help you code more quickly are rapidly becoming more prevalent. I use them regularly myself. The point of this course is not to “complete assignments” but to learn coding (and other things). With that goal in mind, I recommend you avoid the use of Generative AI. It is unlikely to contribute directly to your understanding of the material. Furthermore, I have experimented with certain tools on the assignments for this course and have found the results underwhelming.\nThe material in this course is best learned through trial and error. Avoiding this mechanism (with generative AI or by copying your friend) is a short-term solution at best. I have tried to structure this course to discourage these types of short cuts, and minimize the pressure you may feel to take them.\n\n\nAcademic Concessions\nThese are handled according to UBC policy. Please see\n\nUBC student services\nUBC Vancouver Academic Calendar\nFaculty of Science Concessions\n\n\n\nMissed final exam\nStudents who miss the final exam must report to their Faculty advising office within 72 hours of the missed exam, and must supply supporting documentation. Only your Faculty Advising office can grant deferred standing in a course. You must also notify your instructor prior to (if possible) or immediately after the exam. Your instructor will let you know when you are expected to write your deferred exam. Deferred exams will ONLY be provided to students who have applied for and received deferred standing from their Faculty.\n\n\nTake care of yourself\nCourse work at this level can be intense, and I encourage you to take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. I struggle with these issues too, and I try hard to set aside time for things that make me happy (cooking, playing/listening to music, exercise, going for walks).\nAll of us benefit from support during times of struggle. If you are having any problems or concerns, do not hesitate to speak with me. There are also many resources available on campus that can provide help and support. Asking for support sooner rather than later is almost always a good idea.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support. UBC Counseling Services is here to help: call 604 822 3811 or visit their website. Consider also reaching out to a friend, faculty member, or family member you trust to help get you the support you need.\n\nA dated PDF is available at this link."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": " Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA careful reading of this paper with the provocative title “Law of Urination: all mammals empty their bladders over the same duration” reveals that the authors actually mean something far less precise. In fact, their claim is more accurately stated as “mammals over 3kg in body weight urinate in 21 seconds with a standard deviation of 13 seconds”. But the accurate characterization is far less publicity-worthy.↩︎"
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": " Schedule",
    "section": "",
    "text": "Required readings and lecture videos are listed below for each module. Readings from [ISLR] are always required while those from [ESL] are optional and supplemental."
  },
  {
    "objectID": "schedule/index.html#introduction-and-review",
    "href": "schedule/index.html#introduction-and-review",
    "title": " Schedule",
    "section": "0 Introduction and Review",
    "text": "0 Introduction and Review\nRequired reading below is meant to reengage brain cells which have no doubt forgotten all the material that was covered in STAT 306 or CPSC 340. We don’t presume that you remember all these details, but that, upon rereading, they at least sound familiar. If this all strikes you as completely foreign, this class may not be for you.\n\nRequired reading\n\n[ISLR] 2.1, 2.2, and Chapter 3 (this material is review)\n\nOptional reading\n\n[ESL] 2.4 and 2.6\n\nHandouts\n\nProgramming in R .Rmd, .pdf\n\n\nUsing in RMarkdown .Rmd, .pdf\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n05 Sep 23\n(no class, Imagine UBC)\n\n\n\n07 Sep 23\nIntro to class, Git\n(Quiz 0 due tomorrow)\n\n\n12 Sep 23\nUnderstanding R / Rmd\nLab 00, (Labs begin)\n\n\n14 Sep 23\nLM review, LM Example"
  },
  {
    "objectID": "schedule/index.html#model-accuracy",
    "href": "schedule/index.html#model-accuracy",
    "title": " Schedule",
    "section": "1 Model Accuracy",
    "text": "1 Model Accuracy\n\nTopics\n\nModel selection; cross validation; information criteria; stepwise regression\n\nRequired reading\n\n[ISLR] Ch 2.2 (not 2.2.3), 5.1 (not 5.1.5), 6.1, 6.4\n\nOptional reading\n\n[ESL] 7.1-7.5, 7.10\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n19 Sep 23\nRegression function, Bias and Variance\n\n\n\n21 Sep 23\nRisk estimation, Info Criteria\n\n\n\n26 Sep 23\nGreedy selection\n\n\n\n28 Sep 23\n\nHW 1 due"
  },
  {
    "objectID": "schedule/index.html#regularization-smoothing-and-trees",
    "href": "schedule/index.html#regularization-smoothing-and-trees",
    "title": " Schedule",
    "section": "2 Regularization, smoothing, and trees",
    "text": "2 Regularization, smoothing, and trees\n\nTopics\n\nRidge regression, lasso, and related; linear smoothers (splines, kernels); kNN\n\nRequired reading\n\n[ISLR] Ch 6.2, 7.1-7.7.1, 8.1, 8.1.1, 8.1.3, 8.1.4\n\nOptional reading\n\n[ESL] 3.4, 3.8, 5.4, 6.3\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n3 Oct 23\nRidge, Lasso\n\n\n\n5 Oct 23\nCV for comparison, NP 1\n\n\n\n10 Oct 23\nNP 2, Why smoothing?\n\n\n\n12 Oct 23\nNo class (Makeup Monday)\n\n\n\n17 Oct 23\nOther\nHW 2 due"
  },
  {
    "objectID": "schedule/index.html#classification",
    "href": "schedule/index.html#classification",
    "title": " Schedule",
    "section": "3 Classification",
    "text": "3 Classification\n\nTopics\n\nlogistic regression; LDA/QDA; naive bayes; trees\n\nRequired reading\n\n[ISLR] Ch 2.2.3, 5.1.5, 4-4.5, 8.1.2\n\nOptional reading\n\n[ESL] 4-4.4, 9.2, 13.3\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n19 Oct 23\nClassification, LDA and QDA\n\n\n\n24 Oct 23\nLogistic regression\n\n\n\n26 Oct 23\nGradient descent, Other losses\n\n\n\n31 Oct 23\nNonlinear"
  },
  {
    "objectID": "schedule/index.html#modern-techniques",
    "href": "schedule/index.html#modern-techniques",
    "title": " Schedule",
    "section": "4 Modern techniques",
    "text": "4 Modern techniques\n\nTopics\n\nbagging; boosting; random forests; neural networks\n\nRequired reading\n\n[ISLR] 5.2, 8.2, 10.1, 10.2, 10.6, 10.7\n\nOptional reading\n\n[ESL] 10.1-10.10 (skip 10.7), 11.1, 11.3, 11.4, 11.7\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n2 Nov 23\nThe bootstrap\nHW 3 due\n\n\n7 Nov 23\nBagging and random forests, Boosting\n\n\n\n9 Nov 23\nIntro to neural nets\n\n\n\n14 Nov 23\nNo class. (Midterm break)\n\n\n\n16 Nov 23\nEstimating neural nets\n\n\n\n21 Nov 23\nNeural nets wrapup\nHW 4 due"
  },
  {
    "objectID": "schedule/index.html#unsupervised-learning",
    "href": "schedule/index.html#unsupervised-learning",
    "title": " Schedule",
    "section": "5 Unsupervised learning",
    "text": "5 Unsupervised learning\n\nTopics\n\ndimension reduction and clustering\n\nRequired reading\n\n[ISLR] 12\n\nOptional reading\n\n[ESL] 8.5, 13.2, 14.3, 14.5.1, 14.8, 14.9\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n23 Nov 23\nIntro to PCA, Issues with PCA\n\n\n\n28 Nov 23\nPCA v KPCA\n\n\n\n30 Nov 23\nK means clustering\n\n\n\n5 Dec 23\nHierarchical clustering\n\n\n\n7 Dec 23\n\nHW 5 due"
  },
  {
    "objectID": "schedule/index.html#f-final-exam",
    "href": "schedule/index.html#f-final-exam",
    "title": " Schedule",
    "section": "F Final exam",
    "text": "F Final exam\nDate and time TBD.\n\n\n\n\n\n\nImportant\n\n\n\nDo not make any plans to leave Vancouver before the final exam date is announced.\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou must bring your computer as the exam will be given through Canvas\nPlease arrange to borrow one from the library if you do not have your own. Let me know ASAP if this may pose a problem.\nYou may bring 2 sheets of front/back 8.5x11 paper with any notes you want to use. No other materials will be allowed.\nThere will be no required coding, but I may show code or output and ask questions about it.\nIt will be entirely multiple choice / True-False / matching, etc. Delivered on Canvas."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#meta-lecture",
    "href": "schedule/slides/00-intro-to-class.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "00 Intro to class",
    "text": "00 Intro to class\nStat 406\nDaniel J. McDonald\nLast modified – 17 August 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#about-me",
    "href": "schedule/slides/00-intro-to-class.html#about-me",
    "title": "UBC Stat406 2023W",
    "section": "About me",
    "text": "About me\n\n\n\nDaniel J. McDonald\ndaniel@stat.ubc.ca\nhttp://dajmcdon.github.io/\nAssociate Professor, Department of Statistics"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#philosophy-of-the-class",
    "href": "schedule/slides/00-intro-to-class.html#philosophy-of-the-class",
    "title": "UBC Stat406 2023W",
    "section": "Philosophy of the class",
    "text": "Philosophy of the class\nI and the TAs are here to help you learn. Ask questions.\nWe encourage engagement, curiosity and generosity\nWe favour steady work through the Term (vs. sleeping until finals)\n\nThe assessments attempt to reflect this ethos."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#more-philosophy",
    "href": "schedule/slides/00-intro-to-class.html#more-philosophy",
    "title": "UBC Stat406 2023W",
    "section": "More philosophy",
    "text": "More philosophy\nWhen the term ends, I want\n\nYou to be better at coding.\nYou to have an understanding of the variety of methods available to do prediction and data analysis.\nYou to articulate their strengths and weaknesses.\nYou to be able to choose between different methods using your intuition and the data.\n\n\nI do not want\n\nYou to be under undo stress\nYou to feel the need to cheat, plagiarize, or drop the course\nYou to feel treated unfairly."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#section",
    "href": "schedule/slides/00-intro-to-class.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "I promise\n\nTo grade/mark fairly. Good faith effort will be rewarded\nTo be flexible. This semester (like the last 4) is different for everyone.\nTo understand and adapt to issues.\n\n\nI do not promise that you will all get the grade you want."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#on-covid",
    "href": "schedule/slides/00-intro-to-class.html#on-covid",
    "title": "UBC Stat406 2023W",
    "section": "On COVID",
    "text": "On COVID\n\n\n\nI work on COVID a lot.\nStatistics is hugely important.\n\nPolicies (TL; DR)\n\nI encourage you to wear a mask\nDo NOT come to class if you are possibly sick\nBe kind and considerate to others\nThe Marking scheme is flexible enough to allow some missed classes"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#section-1",
    "href": "schedule/slides/00-intro-to-class.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "We’ll talk about lots of ML models\nBut our focus is on how to “understand” everything in this diagram.\nHow do we interpret? Evaluate? Choose a model?\nWhat are the implications / assumptions implied by our choices?\nDeep understanding of statistics helps with intuition."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#predictive-models",
    "href": "schedule/slides/00-intro-to-class.html#predictive-models",
    "title": "UBC Stat406 2023W",
    "section": "Predictive models",
    "text": "Predictive models\n\n1. Preprocessing\ncentering / scaling / factors-to-dummies / basis expansion / missing values / dimension reduction / discretization / transformations\n2. Model fitting\nWhich box do you use?\n3. Prediction\nRepeat all the preprocessing on new data. But be careful.\n4. Postprocessing, interpretation, and evaluation"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#section-5",
    "href": "schedule/slides/00-intro-to-class.html#section-5",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Source: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#modules",
    "href": "schedule/slides/00-intro-to-class.html#modules",
    "title": "UBC Stat406 2023W",
    "section": "6 modules",
    "text": "6 modules\n\n\n\nReview (today and next week)\nModel accuracy and selection\nRegularization, smoothing, trees\nClassifiers\nModern techniques (classification and regression)\nUnsupervised learning\n\n\n\nEach module is approximately 2 weeks long\nEach module is based on a collection of readings and lectures\nEach module (except the review) has a homework assignment"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#assessments",
    "href": "schedule/slides/00-intro-to-class.html#assessments",
    "title": "UBC Stat406 2023W",
    "section": "Assessments",
    "text": "Assessments\nEffort-based\nTotal across three components: 65 points, any way you want\n\nLabs, up to 20 points (2 each)\nAssignments, up to 50 points (10 each)\nClickers, up to 10 points\n\n\nKnowledge-based\nFinal Exam, 35 points"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#why-this-scheme",
    "href": "schedule/slides/00-intro-to-class.html#why-this-scheme",
    "title": "UBC Stat406 2023W",
    "section": "Why this scheme?",
    "text": "Why this scheme?\n\nYou stay on top of the material\nYou come to class and participate\nYou gain coding practice in the labs\nYou work hard on the assignments\n\n\n\n\n\n\n\nMost of this is Effort Based\n\n\nwork hard, guarantee yourself 65%"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#time-expectations-per-week",
    "href": "schedule/slides/00-intro-to-class.html#time-expectations-per-week",
    "title": "UBC Stat406 2023W",
    "section": "Time expectations per week:",
    "text": "Time expectations per week:\n\nComing to class – 3 hours\nReading the book – 1 hour\nLabs – 1 hour\nHomework – 4 hours\nStudy / thinking / playing – 1 hour\n\n\nShow the course website https://ubc-stat.github.io/stat-406/"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#labs-assignments",
    "href": "schedule/slides/00-intro-to-class.html#labs-assignments",
    "title": "UBC Stat406 2023W",
    "section": "Labs / Assignments",
    "text": "Labs / Assignments\nThe goal is to “Do the work”\n\n\nAssignments\n\nNot easy, especially the first 2, especially if you are unfamiliar with R / Rmarkdown / ggplot\nYou may revise to raise your score to 7/10, see Syllabus. Only if you get lose 3+ for content (penalties can’t be redeemed).\nDon’t leave these for the last minute\n\n\nLabs\n\nLabs should give you practice, allow for questions with the TAs.\nThey are due at 2300 on the day of your lab, lightly graded.\nYou may do them at home, but you must submit individually (in lab, you may share submission)\nLabs are lightly graded"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#clickers",
    "href": "schedule/slides/00-intro-to-class.html#clickers",
    "title": "UBC Stat406 2023W",
    "section": "Clickers",
    "text": "Clickers\n\nQuestions are similar to the Final\n0 points for skipping, 2 points for trying, 4 points for correct\n\nAverage of 3 = 10 points (the max)\nAverage of 2 = 5 points\nAverage of 1 = 0 points\ntotal = max(0, min(5 * points / N - 5, 10))\n\nBe sure to sync your device in Canvas.\n\n\n\n\n\n\n\nDon’t do this!\n\n\nAverage &lt; 1 drops your Final Mark 1 letter grade.\nA- becomes B-, C+ becomes D."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#final-exam",
    "href": "schedule/slides/00-intro-to-class.html#final-exam",
    "title": "UBC Stat406 2023W",
    "section": "Final Exam",
    "text": "Final Exam\n\nScheduled by the university.\nIt is hard\nThe median last year was 50% \\(\\Rightarrow\\) A-\n\nPhilosophy:\n\nIf you put in the effort, you’re guaranteed a C+.\nBut to get an A+, you should really deeply understand the material.\n\nNo penalty for skipping the final.\nIf you’re cool with C+ and hate tests, then that’s fine."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#advice",
    "href": "schedule/slides/00-intro-to-class.html#advice",
    "title": "UBC Stat406 2023W",
    "section": "Advice",
    "text": "Advice\n\nSkipping HW makes it difficult to get to 65\nCome to class!\nYes it’s at 8am. I hate it too.\nTo compensate, I will record the class and post to Canvas.\nIn terms of last year’s class, attendance in lecture and active engagement (asking questions, coming to office hours, etc.) is the best predictor of success.\n\n\nQuestions?"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#textbooks",
    "href": "schedule/slides/00-intro-to-class.html#textbooks",
    "title": "UBC Stat406 2023W",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\nJames, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: http://statlearning.com/\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning\n\n\nHastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\nIt’s worth your time to read.\nIf you need more practice, read the Worksheets."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#computer",
    "href": "schedule/slides/00-intro-to-class.html#computer",
    "title": "UBC Stat406 2023W",
    "section": "Computer",
    "text": "Computer\n\n\n\n\n\nAll coding in R\nSuggest you use RStudio IDE\nSee https://ubc-stat.github.io/stat-406/ for instructions\nIt tells you how to install what you will need, hopefully all at once, for the whole Term.\nWe will use R and we assume some background knowledge.\nLinks to useful supplementary resources are available on the website.\n\n\n\n\n\n\nThis course is not an intro to R / python / MongoDB / SQL."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#other-resources",
    "href": "schedule/slides/00-intro-to-class.html#other-resources",
    "title": "UBC Stat406 2023W",
    "section": "Other resources",
    "text": "Other resources\n\nCanvas\n\nGrades, links to videos from class\n\nCourse website\n\nAll the material (slides, extra worksheets) https://ubc-stat.github.io/stat-406\n\nSlack\n\nDiscussion board, questions.\n\nGithub\n\nHomework / Lab submission\n\n\n\n\n\nAll lectures will be recorded and posted\nI cannot guarantee that they will all work properly (sometimes I mess it up)"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#some-more-words",
    "href": "schedule/slides/00-intro-to-class.html#some-more-words",
    "title": "UBC Stat406 2023W",
    "section": "Some more words",
    "text": "Some more words\n\nLectures are hard. It’s 8am, everyone’s tired.\nCoding is hard. I hope you’ll get better at it.\nI strongly urge you to get up at the same time everyday. My plan is to go to the gym on MWF. It’s really hard to sleep in until 10 on MWF and make class at 8 on T/Th.\n\n\n\nLet’s be kind and understanding to each other.\nI have to give you a grade, but I want that grade to reflect your learning and effort, not other junk.\n\n\n\nIf you need help, please ask."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#meta-lecture",
    "href": "schedule/slides/00-r-review.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour",
    "text": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour\nStat 406\nDaniel J. McDonald\nLast modified – 11 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "href": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "title": "UBC Stat406 2023W",
    "section": "Tour of Rstudio",
    "text": "Tour of Rstudio\nThings to note\n\nConsole\nTerminal\nScripts, .Rmd, Knit\nFiles, Projects\nGetting help\nEnvironment, Git"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#simple-stuff",
    "href": "schedule/slides/00-r-review.html#simple-stuff",
    "title": "UBC Stat406 2023W",
    "section": "Simple stuff",
    "text": "Simple stuff\n\n\nVectors:\n\nx &lt;- c(1, 3, 4)\nx[1]\n\n[1] 1\n\nx[-1]\n\n[1] 3 4\n\nrev(x)\n\n[1] 4 3 1\n\nc(x, x)\n\n[1] 1 3 4 1 3 4\n\n\n\n\n\nMatrices:\n\nx &lt;- matrix(1:25, nrow = 5, ncol = 5)\nx[1,]\n\n[1]  1  6 11 16 21\n\nx[,-1]\n\n     [,1] [,2] [,3] [,4]\n[1,]    6   11   16   21\n[2,]    7   12   17   22\n[3,]    8   13   18   23\n[4,]    9   14   19   24\n[5,]   10   15   20   25\n\nx[c(1,3),  2:3]\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    8   13"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#simple-stuff-1",
    "href": "schedule/slides/00-r-review.html#simple-stuff-1",
    "title": "UBC Stat406 2023W",
    "section": "Simple stuff",
    "text": "Simple stuff\n\n\nLists\n\n(l &lt;- list(\n  a = letters[1:2], \n  b = 1:4, \n  c = list(a = 1)))\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] 1 2 3 4\n\n$c\n$c$a\n[1] 1\n\nl$a\n\n[1] \"a\" \"b\"\n\nl$c$a\n\n[1] 1\n\nl[\"b\"] # compare to l[[\"b\"]] == l$b\n\n$b\n[1] 1 2 3 4\n\n\n\n\nData frames\n\n(dat &lt;- data.frame(\n  z = 1:5, \n  b = 6:10, \n  c = letters[1:5]))\n\n  z  b c\n1 1  6 a\n2 2  7 b\n3 3  8 c\n4 4  9 d\n5 5 10 e\n\nclass(dat)\n\n[1] \"data.frame\"\n\ndat$b\n\n[1]  6  7  8  9 10\n\ndat[1,]\n\n  z b c\n1 1 6 a\n\n\n\n\nData frames are sort-of lists and sort-of matrices"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tibbles",
    "href": "schedule/slides/00-r-review.html#tibbles",
    "title": "UBC Stat406 2023W",
    "section": "Tibbles",
    "text": "Tibbles\nThese are {tidyverse} data frames\n\n(dat2 &lt;- tibble(z = 1:5, b = z + 5, c = letters[z]))\n\n# A tibble: 5 × 3\n      z     b c    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     6 a    \n2     2     7 b    \n3     3     8 c    \n4     4     9 d    \n5     5    10 e    \n\nclass(dat2)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe’ll return to classes in a moment. A tbl_df is a “subclass” of data.frame.\nAnything that data.frame can do, tbl_df can do (better).\nFor instance, the printing is more informative.\nAlso, you can construct one by referencing previously constructed columns."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#understanding-signatures",
    "href": "schedule/slides/00-r-review.html#understanding-signatures",
    "title": "UBC Stat406 2023W",
    "section": "Understanding signatures",
    "text": "Understanding signatures\n\n\nCode\nsig &lt;- sig::sig\n\n\n\nsig(lm)\n\nfn &lt;- function(formula, data, subset, weights, na.action, method = \"qr\", model\n  = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts =\n  NULL, offset, ...)\n\nsig(`+`)\n\nfn &lt;- function(e1, e2)\n\nsig(dplyr::filter)\n\nfn &lt;- function(.data, ..., .by = NULL, .preserve = FALSE)\n\nsig(stats::filter)\n\nfn &lt;- function(x, filter, method = c(\"convolution\", \"recursive\"), sides = 2,\n  circular = FALSE, init = NULL)\n\nsig(rnorm)\n\nfn &lt;- function(n, mean = 0, sd = 1)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "href": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "title": "UBC Stat406 2023W",
    "section": "These are all the same",
    "text": "These are all the same\n\nset.seed(12345)\nrnorm(3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(3, 0, 1)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(sd = 1, n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\nFunctions can have default values.\nYou may, but don’t have to, name the arguments\nIf you name them, you can pass them out of order (but you shouldn’t)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#write-lots-of-functions.-i-cant-emphasize-this-enough.",
    "href": "schedule/slides/00-r-review.html#write-lots-of-functions.-i-cant-emphasize-this-enough.",
    "title": "UBC Stat406 2023W",
    "section": "Write lots of functions. I can’t emphasize this enough.",
    "text": "Write lots of functions. I can’t emphasize this enough.\n\n\n\nf &lt;- function(arg1, arg2, arg3 = 12, ...) {\n  stuff &lt;- arg1 * arg3\n  stuff2 &lt;- stuff + arg2\n  plot(arg1, stuff2, ...)\n  return(stuff2)\n}\nx &lt;- rnorm(100)\n\n\n\n\ny1 &lt;- f(x, 3, 15, col = 4, pch = 19)\n\n\n\n\n\n\n\nstr(y1)\n\n num [1:100] -3.8 12.09 -24.27 12.45 -1.14 ..."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "title": "UBC Stat406 2023W",
    "section": "Outputs vs. Side effects",
    "text": "Outputs vs. Side effects\n\n\n\nSide effects are things a function does, outputs can be assigned to variables\nA good example is the hist function\nYou have probably only seen the side effect which is to plot the histogram\n\n\nmy_histogram &lt;- hist(rnorm(1000))\n\n\n\n\n\n\n\n\n\n\n\nstr(my_histogram)\n\nList of 6\n $ breaks  : num [1:14] -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 ...\n $ counts  : int [1:13] 4 21 41 89 142 200 193 170 74 38 ...\n $ density : num [1:13] 0.008 0.042 0.082 0.178 0.284 0.4 0.386 0.34 0.148 0.076 ...\n $ mids    : num [1:13] -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ...\n $ xname   : chr \"rnorm(1000)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\nclass(my_histogram)\n\n[1] \"histogram\""
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "href": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "title": "UBC Stat406 2023W",
    "section": "When writing functions, program defensively, ensure behaviour",
    "text": "When writing functions, program defensively, ensure behaviour\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  x + 1\n}\n  \nincrementer(2)\n\n[1] 3\n\nincrementer(1:4)\n\n[1] 2 3 4 5\n\nincrementer(\"a\")\n\nError in x + 1: non-numeric argument to binary operator\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  stopifnot(is.numeric(x))\n  return(x + 1)\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): is.numeric(x) is not TRUE\n\n\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + 1\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): `x` must be numeric\n\nincrementer(2, -3) ## oops!\n\n[1] 3\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + inc_by\n}\nincrementer(2, -3)\n\n[1] -1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#how-to-keep-track",
    "href": "schedule/slides/00-r-review.html#how-to-keep-track",
    "title": "UBC Stat406 2023W",
    "section": "How to keep track",
    "text": "How to keep track\n\n\n\nlibrary(testthat)\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  if (!is.numeric(inc_by)) {\n    stop(\"`inc_by` must be numeric\")\n  }\n  x + inc_by\n}\nexpect_error(incrementer(\"a\"))\nexpect_equal(incrementer(1:3), 2:4)\nexpect_equal(incrementer(2, -3), -1)\nexpect_error(incrementer(1, \"b\"))\nexpect_identical(incrementer(1:3), 2:4)\n\nError: incrementer(1:3) not identical to 2:4.\nObjects equal but not identical\n\n\n\n\n\nis.integer(2:4)\n\n[1] TRUE\n\nis.integer(incrementer(1:3))\n\n[1] FALSE\n\nexpect_identical(incrementer(1:3, 1L), 2:4)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIf you copy something, write a function.\nValidate your arguments.\nTo ensure proper functionality, write tests to check if inputs result in predicted outputs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#classes",
    "href": "schedule/slides/00-r-review.html#classes",
    "title": "UBC Stat406 2023W",
    "section": "Classes",
    "text": "Classes\n\n\nWe saw some of these earlier:\n\ntib &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100), \n  y = x1 + 2 * x2 + rnorm(100)\n)\nmdl &lt;- lm(y ~ ., data = tib )\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(mdl)\n\n[1] \"lm\"\n\n\nThe class allows for the use of “methods”\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n    -0.1742       1.0454       2.0470  \n\n\n\n\n\nR “knows what to do” when you print() an object of class \"lm\".\nprint() is called a “generic” function.\nYou can create “methods” that get dispatched.\nFor any generic, R looks for a method for the class.\nIf available, it calls that function."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "href": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "title": "UBC Stat406 2023W",
    "section": "Viewing the dispatch chain",
    "text": "Viewing the dispatch chain\n\nsloop::s3_dispatch(print(incrementer))\n\n=&gt; print.function\n * print.default\n\nsloop::s3_dispatch(print(tib))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(print(mdl))\n\n=&gt; print.lm\n * print.default"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-geeky-but-important",
    "href": "schedule/slides/00-r-review.html#r-geeky-but-important",
    "title": "UBC Stat406 2023W",
    "section": "R-Geeky But Important",
    "text": "R-Geeky But Important\nThere are lots of generic functions in R\nCommon ones are print(), summary(), and plot().\nAlso, lots of important statistical modelling concepts: residuals() coef()\n(In python, these work the opposite way: obj.residuals. The dot after the object accesses methods defined for that type of object. But the dispatch behaviour is less robust.)\n\nThe convention is that the specialized function is named method.class(), e.g., summary.lm().\nIf no specialized function is defined, R will try to use method.default().\n\nFor this reason, R programmers try to avoid . in names of functions or objects."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#wherefore-methods",
    "href": "schedule/slides/00-r-review.html#wherefore-methods",
    "title": "UBC Stat406 2023W",
    "section": "Wherefore methods?",
    "text": "Wherefore methods?\n\nThe advantage is that you don’t have to learn a totally new syntax to grab residuals or plot things\nYou just use residuals(mdl) whether mdl has class lm could have been done two centuries ago, or a Batrachian Emphasis Machine which won’t be invented for another five years.\nThe one draw-back is the help pages for the generic methods tend to be pretty vague\nCompare ?summary with ?summary.lm."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#different-environments",
    "href": "schedule/slides/00-r-review.html#different-environments",
    "title": "UBC Stat406 2023W",
    "section": "Different environments",
    "text": "Different environments\n\nThese are often tricky, but are very common.\nMost programming languages have this concept in one way or another.\nIn R code run in the Console produces objects in the “Global environment”\nYou can see what you create in the “Environment” tab.\nBut there’s lots of other stuff.\nMany packages are automatically loaded at startup, so you have access to the functions and data inside\n\nFor example mean(), lm(), plot(), iris (technically iris is lazy-loaded, meaning it’s not in memory until you call it, but it is available)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section",
    "href": "schedule/slides/00-r-review.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Other packages require you to load them with library(pkg) before their functions are available.\nBut, you can call those functions by prefixing the package name ggplot2::ggplot().\nYou can also access functions that the package developer didn’t “export” for use with ::: like dplyr:::as_across_fn_call()\n\n\nThat is all about accessing “objects in package environments”"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "title": "UBC Stat406 2023W",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nAs one might expect, functions create an environment inside the function.\n\nz &lt;- 1\nfun &lt;- function(x) {\n  z &lt;- x\n  print(z)\n  invisible(z)\n}\nfun(14)\n\n[1] 14\n\n\nNon-trivial cases are data-masking environments.\n\ntib &lt;- tibble(x1 = rnorm(100),  x2 = rnorm(100),  y = x1 + 2 * x2)\nmdl &lt;- lm(y ~ x2, data = tib)\nx2\n\nError in eval(expr, envir, enclos): object 'x2' not found\n\n\n\nlm() looks “inside” the tib to find y and x2\nThe data variables are added to the lm() environment"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "title": "UBC Stat406 2023W",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nWhen Knit, .Rmd files run in their OWN environment.\nThey are run from top to bottom, with code chunks depending on previous\nThis makes them reproducible.\nJupyter notebooks don’t do this. 😱\nObjects in your local environment are not available in the .Rmd\nObjects in the .Rmd are not available locally.\n\n\n\n\n\n\nTip\n\n\nThe most frequent error I see is:\n\nrunning chunks individually, 1-by-1, and it works\nKnitting, and it fails\n\nThe reason is almost always that the chunks refer to objects in the Environment that don’t exist in the .Rmd"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-1",
    "href": "schedule/slides/00-r-review.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "This error also happens because:\n\nlibrary() calls were made globally but not in the .Rmd\n\nso the packages aren’t loaded\n\npaths to data or other objects are not relative to the .Rmd in your file system\n\nthey must be\n\nCarefully keeping Labs / Assignments in their current location will help to avoid some of these."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#how-to-fix-code",
    "href": "schedule/slides/00-r-review.html#how-to-fix-code",
    "title": "UBC Stat406 2023W",
    "section": "How to fix code",
    "text": "How to fix code\n\nIf you’re using a function in a package, start with ?function to see the help\n\nMake sure you’re calling the function correctly.\nTry running the examples.\npaste the error into Google (if you share the error on Slack, I often do this first)\nGo to the package website if it exists, and browse around\n\nIf your .Rmd won’t Knit\n\nDid you make the mistake on the last slide?\nDid it Knit before? Then the bug is in whatever you added.\nDid you never Knit it? Why not?\nCall rstudioapi::restartSession(), then run the Chunks 1-by-1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-2",
    "href": "schedule/slides/00-r-review.html#section-2",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "Adding browser()\n\nOnly useful with your own functions.\nOpen the script with the function, and add browser() to the code somewhere\nThen call your function.\nThe execution will Stop where you added browser() and you’ll have access to the local environment to play around"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#reproducible-examples",
    "href": "schedule/slides/00-r-review.html#reproducible-examples",
    "title": "UBC Stat406 2023W",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\n\n\n\n\n\nQuestion I get on Slack that I hate:\n\n\n“I ran the code like you had on Slide 39, but it didn’t work.”\n\n\n\n\nIf you want to ask me why the code doesn’t work, you need to show me what’s wrong.\n\n\n\n\n\n\n\nDon’t just paste a screenshot!\n\n\nUnless you get lucky, I won’t be able to figure it out from that. And we’ll both get frustrated.\n\n\n\nWhat you need is a Reproducible Example or reprex.\n\nThis is a small chunk of code that\n\nruns in it’s own environment\nand produces the error."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#reproducible-examples-how-it-works",
    "href": "schedule/slides/00-r-review.html#reproducible-examples-how-it-works",
    "title": "UBC Stat406 2023W",
    "section": "Reproducible examples, How it works",
    "text": "Reproducible examples, How it works\n\nOpen a new .R script.\nPaste your buggy code in the file (no need to save)\nEdit your code to make sure it’s “enough to produce the error” and nothing more. (By rerunning the code a few times.)\nCopy your code.\nCall reprex::reprex(venue = \"r\") from the console. This will run your code in a new environment and show the result in the Viewer tab. Does it create the error you expect?\nIf it creates other errors, that may be the problem. You may fix the bug on your own!\nIf it doesn’t have errors, then your global environment is Farblunget.\nThe Output is now on your clipboard. Go to Slack and paste it in a message. Then press Cmd+Shift+Enter (on Mac) or Ctrl+Shift+Enter (Windows/Linux). Under Type, select R.\nSend the message, perhaps with more description and an SOS emoji.\n\n\n\n\n\n\n\nNote\n\n\nBecause Reprex runs in it’s own environment, it doesn’t have access to any of the libraries you loaded or the stuff in your global environment. You’ll have to load these things in the script."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "href": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "title": "UBC Stat406 2023W",
    "section": "{tidyverse} is huge",
    "text": "{tidyverse} is huge\nCore tidyverse is nearly 30 different R packages, but we’re going to just talk about a few of them.\nFalls roughly into a few categories:\n\nConvenience functions: {magrittr} and many many others.\nData processing: {dplyr} and many others.\nGraphing: {ggplot2} and some others like {scales}.\nUtilities\n\n\n\nWe’re going to talk quickly about some of it, but ignore much of 2.\nThere’s a lot that’s great about these packages, especially ease of data processing.\nBut it doesn’t always jive with base R (it’s almost a separate proglang at this point)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#piping",
    "href": "schedule/slides/00-r-review.html#piping",
    "title": "UBC Stat406 2023W",
    "section": "Piping",
    "text": "Piping\nThis was introduced by {magrittr} as %&gt;%,\nbut is now in base R (&gt;=4.1.0) as |&gt;.\nNote: there are other pipes in {magrittr} (e.g. %$% and %T%) but I’ve never used them.\nI’ve used the old version for so long, that it’s hard for me to adopt the new one.\nThe point of the pipe is to logically sequence nested operations"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#example",
    "href": "schedule/slides/00-r-review.html#example",
    "title": "UBC Stat406 2023W",
    "section": "Example",
    "text": "Example\n\n\n\nmse1 &lt;- print(\n  sum(\n    residuals(\n      lm(y~., data = mutate(\n        tib, \n        x3 = x1^2,\n        x4 = log(x2 + abs(min(x2)) + 1)\n      )\n      )\n    )^2\n  )\n)\n\n[1] 6.469568e-29\n\n\n\n\nmse2 &lt;- tib |&gt;\n  mutate(\n    x3 = x1^2, \n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() |&gt;\n  print()\n\n[1] 6.469568e-29"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-4",
    "href": "schedule/slides/00-r-review.html#section-4",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "It may seem like we should push this all the way\n\ntib |&gt;\n  mutate(\n    x3 = x1^2, \n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() -&gt;\n  mse3\n\nThis works, but it’s really annoying."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#a-new-one",
    "href": "schedule/slides/00-r-review.html#a-new-one",
    "title": "UBC Stat406 2023W",
    "section": "A new one…",
    "text": "A new one…\nJust last week, I learned\n\nlibrary(magrittr)\ntib &lt;- tibble(x = 1:5, z = 6:10)\ntib &lt;- tib |&gt; mutate(b = x + z)\ntib\n\n# A tibble: 5 × 3\n      x     z     b\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1     6     7\n2     2     7     9\n3     3     8    11\n4     4     9    13\n5     5    10    15\n\n# start over\ntib &lt;- tibble(x = 1:5, z = 6:10)\ntib %&lt;&gt;% mutate(b = x + z)\ntib\n\n# A tibble: 5 × 3\n      x     z     b\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1     1     6     7\n2     2     7     9\n3     3     8    11\n4     4     9    13\n5     5    10    15"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "href": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "title": "UBC Stat406 2023W",
    "section": "Data processing in {dplyr}",
    "text": "Data processing in {dplyr}\nThis package has all sorts of things. And it interacts with {tibble} generally.\nThe basic idea is “tibble in, tibble out”.\nSatisfies data masking which means you can refer to columns by name or use helpers like ends_with(\"_rate\")\nMajorly useful operations:\n\nselect() (chooses columns to keep)\nmutate() (showed this already)\ngroup_by()\npivot_longer() and pivot_wider()\nleft_join() and full_join()\nsummarise()\n\n\n\n\n\n\n\nNote\n\n\nfilter() and select() are functions in Base R.\nSometimes you get 🐞 because it called the wrong version.\nTo be sure, prefix it like dplyr::select()."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "href": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "title": "UBC Stat406 2023W",
    "section": "A useful data frame",
    "text": "A useful data frame\n\nlibrary(epidatr)\ncovid &lt;- covidcast(\n  source = \"jhu-csse\",\n  signals = \"confirmed_7dav_incidence_prop,deaths_7dav_incidence_prop\",\n  time_type = \"day\",\n  geo_type = \"state\",\n  time_values = epirange(20220801, 20220821),\n  geo_values = \"ca,wa\") |&gt;\n  fetch() |&gt;\n  select(geo_value, time_value, signal, value)\n\ncovid\n\n# A tibble: 84 × 4\n   geo_value time_value signal                        value\n   &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;\n 1 ca        2022-08-01 confirmed_7dav_incidence_prop  45.4\n 2 wa        2022-08-01 confirmed_7dav_incidence_prop  27.7\n 3 ca        2022-08-02 confirmed_7dav_incidence_prop  44.9\n 4 wa        2022-08-02 confirmed_7dav_incidence_prop  27.7\n 5 ca        2022-08-03 confirmed_7dav_incidence_prop  44.5\n 6 wa        2022-08-03 confirmed_7dav_incidence_prop  26.6\n 7 ca        2022-08-04 confirmed_7dav_incidence_prop  42.3\n 8 wa        2022-08-04 confirmed_7dav_incidence_prop  26.6\n 9 ca        2022-08-05 confirmed_7dav_incidence_prop  40.7\n10 wa        2022-08-05 confirmed_7dav_incidence_prop  34.6\n# ℹ 74 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples",
    "href": "schedule/slides/00-r-review.html#examples",
    "title": "UBC Stat406 2023W",
    "section": "Examples",
    "text": "Examples\nRename the signal to something short.\n\ncovid &lt;- covid |&gt; \n  mutate(signal = case_when(\n    str_starts(signal, \"confirmed\") ~ \"case_rate\", \n    TRUE ~ \"death_rate\"\n  ))\n\nSort by time_value then geo_value\n\ncovid &lt;- covid |&gt; arrange(time_value, geo_value)\n\nCalculate grouped medians\n\ncovid |&gt; \n  group_by(geo_value, signal) |&gt;\n  summarise(med = median(value), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  geo_value signal        med\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 ca        case_rate  33.2  \n2 ca        death_rate  0.112\n3 wa        case_rate  23.2  \n4 wa        death_rate  0.178"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples-1",
    "href": "schedule/slides/00-r-review.html#examples-1",
    "title": "UBC Stat406 2023W",
    "section": "Examples",
    "text": "Examples\nSplit the data into two tibbles by signal\n\ncases &lt;- covid |&gt; \n  filter(signal == \"case_rate\") |&gt;\n  rename(case_rate = value) |&gt; select(-signal)\ndeaths &lt;- covid |&gt; \n  filter(signal == \"death_rate\") |&gt;\n  rename(death_rate = value) |&gt; select(-signal)\n\nJoin them together\n\njoined &lt;- full_join(cases, deaths, by = c(\"geo_value\", \"time_value\"))\n\nDo the same thing by pivoting\n\ncovid |&gt; pivot_wider(names_from = signal, values_from = value)\n\n# A tibble: 42 × 4\n   geo_value time_value case_rate death_rate\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ca        2022-08-01      45.4      0.105\n 2 wa        2022-08-01      27.7      0.169\n 3 ca        2022-08-02      44.9      0.106\n 4 wa        2022-08-02      27.7      0.169\n 5 ca        2022-08-03      44.5      0.107\n 6 wa        2022-08-03      26.6      0.173\n 7 ca        2022-08-04      42.3      0.112\n 8 wa        2022-08-04      26.6      0.173\n 9 ca        2022-08-05      40.7      0.116\n10 wa        2022-08-05      34.6      0.225\n# ℹ 32 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "href": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "title": "UBC Stat406 2023W",
    "section": "Plotting with {ggplot2}",
    "text": "Plotting with {ggplot2}\n\nEverything you can do with ggplot(), you can do with plot(). But the defaults are much prettier.\nIt’s also much easier to adjust by aesthetics / panels by factors.\nIt also uses “data masking”: data goes into ggplot(data = mydata), then the columns are available to the rest.\nIt (sort of) pipes, but by adding layers with +\nIt strongly prefers “long” data frames over “wide” data frames.\n\n\nI’ll give a very fast overview of some confusing bits."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#meta-lecture",
    "href": "schedule/slides/01-lm-review.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "01 Linear model review",
    "text": "01 Linear model review\nStat 406\nDaniel J. McDonald\nLast modified – 30 August 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#the-normal-linear-model",
    "href": "schedule/slides/01-lm-review.html#the-normal-linear-model",
    "title": "UBC Stat406 2023W",
    "section": "The normal linear model",
    "text": "The normal linear model\nAssume that\n\\[\ny_i = x_i^\\top \\beta + \\epsilon_i.\n\\]\n\n\nWhat is the mean of \\(y_i\\)?\nWhat is the distribution of \\(\\epsilon_i\\)?\nWhat is the notation \\(\\mathbf{X}\\) or \\(\\mathbf{y}\\)?"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#drawing-a-sample",
    "href": "schedule/slides/01-lm-review.html#drawing-a-sample",
    "title": "UBC Stat406 2023W",
    "section": "Drawing a sample",
    "text": "Drawing a sample\n\\[\ny_i = x_i^\\top \\beta + \\epsilon_i.\n\\]\nHow would I create data from this model (draw a sample)?\n\nSet up parameters\n\np &lt;- 3\nn &lt;- 100\nsigma &lt;- 2\n\n\n\nCreate the data\n\nepsilon &lt;- rnorm(n, sd = sigma) # this is random\nX &lt;- matrix(runif(n * p), n, p) # treat this as fixed, but I need numbers\nbeta &lt;- (p + 1):1 # parameter, also fixed, but I again need numbers\nY &lt;- cbind(1, X) %*% beta + epsilon # epsilon is random, so this is\n## Equiv: Y &lt;- beta[1] + X %*% beta[-1] + epsilon"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#how-do-we-estimate-beta",
    "href": "schedule/slides/01-lm-review.html#how-do-we-estimate-beta",
    "title": "UBC Stat406 2023W",
    "section": "How do we estimate beta?",
    "text": "How do we estimate beta?\n\nGuess.\nOrdinary least squares (OLS).\nMaximum likelihood.\nDo something more creative."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ols",
    "href": "schedule/slides/01-lm-review.html#method-2.-ols",
    "title": "UBC Stat406 2023W",
    "section": "Method 2. OLS",
    "text": "Method 2. OLS\nI want to find an estimator \\(\\widehat\\beta\\) that makes small errors on my data.\nI measure errors with the difference between predictions \\(\\mathbf{X}\\widehat\\beta\\) and the responses \\(\\mathbf{y}\\).\n\n\nDon’t care if the differences are positive or negative\n\\[\\sum_{i=1}^n \\left\\lvert y_i - x_i^\\top \\widehat\\beta \\right\\rvert.\\]\nThis is hard to minimize (what is the derivative of \\(|\\cdot|\\)?)\n\\[\\sum_{i=1}^n ( y_i - x_i^\\top \\widehat\\beta )^2.\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ols-solution",
    "href": "schedule/slides/01-lm-review.html#method-2.-ols-solution",
    "title": "UBC Stat406 2023W",
    "section": "Method 2. OLS solution",
    "text": "Method 2. OLS solution\nWe write this as\n\\[\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n\nFind the \\(\\beta\\) which minimizes the sum of squared errors.\n\n\nNote that this is the same as\n\\[\\widehat\\beta = \\argmin_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n\nFind the beta which minimizes the mean squared error."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ok-do-it",
    "href": "schedule/slides/01-lm-review.html#method-2.-ok-do-it",
    "title": "UBC Stat406 2023W",
    "section": "Method 2. Ok, do it",
    "text": "Method 2. Ok, do it\nWe differentiate and set to zero\n\\[\\begin{aligned}\n& \\frac{\\partial}{\\partial \\beta} \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2\\\\\n&= -\\frac{2}{n}\\sum_{i=1}^n x_i (y_i - x_i^\\top\\beta)\\\\\n&= \\frac{2}{n}\\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n0 &\\equiv \\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n&\\Rightarrow \\sum_{i=1}^n x_i x_i^\\top \\beta = \\sum_{i=1}^n x_i y_i\\\\\n&\\Rightarrow \\beta = \\left(\\sum_{i=1}^n x_i x_i^\\top\\right)^{-1}\\sum_{i=1}^n x_i y_i\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#in-matrix-notation",
    "href": "schedule/slides/01-lm-review.html#in-matrix-notation",
    "title": "UBC Stat406 2023W",
    "section": "In matrix notation…",
    "text": "In matrix notation…\n…this is\n\\[\\hat\\beta = ( \\mathbf{X}^\\top  \\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}.\\]\nThe \\(\\beta\\) which “minimizes the sum of squared errors”\nAKA, the SSE."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood",
    "href": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood",
    "title": "UBC Stat406 2023W",
    "section": "Method 3: maximum likelihood",
    "text": "Method 3: maximum likelihood\nMethod 2 didn’t use anything about the distribution of \\(\\epsilon\\).\nBut if we know that \\(\\epsilon\\) has a normal distribution, we can write down the joint distribution of \\(\\mathbf{y}=(y_1,\\ldots,y_n)^\\top\\):\n\\[\\begin{aligned}\nf_Y(\\mathbf{y} ; \\beta) &= \\prod_{i=1}^n f_{y_i ; \\beta}(y_i)\\\\\n  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (y_i-x_i^\\top \\beta)^2\\right)\\\\\n  &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood-1",
    "href": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood-1",
    "title": "UBC Stat406 2023W",
    "section": "Method 3: maximum likelihood",
    "text": "Method 3: maximum likelihood\n\\[\nf_Y(\\mathbf{y} ; \\beta) = \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\]\nIn probability courses, we think of \\(f_Y\\) as a function of \\(\\mathbf{y}\\) with \\(\\beta\\) fixed:\n\nIf we integrate over \\(\\mathbf{y}\\), it’s \\(1\\).\nIf we want the probability of \\((a,b)\\), we integrate from \\(a\\) to \\(b\\).\netc."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#turn-it-around",
    "href": "schedule/slides/01-lm-review.html#turn-it-around",
    "title": "UBC Stat406 2023W",
    "section": "Turn it around…",
    "text": "Turn it around…\n…instead, think of it as a function of \\(\\beta\\).\nWe call this “the likelihood” of beta: \\(\\mathcal{L}(\\beta)\\).\nGiven some data, we can evaluate the likelihood for any value of \\(\\beta\\) (assuming \\(\\sigma\\) is known).\nIt won’t integrate to 1 over \\(\\beta\\).\nBut it is “convex”,\nmeaning we can maximize it (the second derivative wrt \\(\\beta\\) is everywhere negative)."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#so-lets-maximize",
    "href": "schedule/slides/01-lm-review.html#so-lets-maximize",
    "title": "UBC Stat406 2023W",
    "section": "So let’s maximize",
    "text": "So let’s maximize\nThe derivative of this thing is kind of ugly.\nBut if we’re trying to maximize over \\(\\beta\\), we can take an increasing transformation without changing anything.\nI choose \\(\\log_e\\).\n\\[\\begin{aligned}\n\\mathcal{L}(\\beta) &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\\\\\n\\ell(\\beta) &=-\\frac{n}{2}\\log (2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\n\\end{aligned}\\]\nBut we can ignore constants, so this gives\n\\[\\widehat\\beta = \\argmax_\\beta -\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\]\nThe same as before!"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#meta-lecture",
    "href": "schedule/slides/03-regression-function.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "03 The regression function",
    "text": "03 The regression function\nStat 406\nDaniel J. McDonald\nLast modified – 18 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#mean-squared-error-mse",
    "href": "schedule/slides/03-regression-function.html#mean-squared-error-mse",
    "title": "UBC Stat406 2023W",
    "section": "Mean squared error (MSE)",
    "text": "Mean squared error (MSE)\nLast time… Ordinary Least Squares\n\\[\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n“Find the \\(\\beta\\) which minimizes the sum of squared errors.”\n\\[\\widehat\\beta = \\arg\\min_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n“Find the beta which minimizes the mean squared error.”"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#forget-all-that",
    "href": "schedule/slides/03-regression-function.html#forget-all-that",
    "title": "UBC Stat406 2023W",
    "section": "Forget all that…",
    "text": "Forget all that…\nThat’s “stuff that seems like a good idea”\nAnd it is for many reasons\nThis class is about those reasons, and the “statistics” behind it\n\n\n\nMethods for “Statistical” Learning\nStarts with “what is a model?”"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#what-is-a-model",
    "href": "schedule/slides/03-regression-function.html#what-is-a-model",
    "title": "UBC Stat406 2023W",
    "section": "What is a model?",
    "text": "What is a model?\nIn statistics, “model” has a mathematical meaning.\nDistinct from “algorithm” or “procedure”.\nDefining a model often leads to a procedure/algorithm with good properties.\nSometimes procedure/algorithm \\(\\Rightarrow\\) a specific model.\n\nStatistics (the field) tells me how to understand when different procedures are desirable and the mathematical guarantees that they satisfy.\n\nWhen are certain models appropriate?\n\nOne definition of “Statistical Learning” is the “statistics behind the procedure”."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-101",
    "href": "schedule/slides/03-regression-function.html#statistical-models-101",
    "title": "UBC Stat406 2023W",
    "section": "Statistical models 101",
    "text": "Statistical models 101\nWe observe data \\(Z_1,\\ Z_2,\\ \\ldots,\\ Z_n\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\n\nA statistical model is a set of distributions \\(\\mathcal{P}\\).\n\nSome examples:\n\n\\(\\P = \\{ 0 &lt; p &lt; 1 : P(z=1)=p,\\ P(z=0)=1-p\\}\\).\n\\(\\P = \\{ \\beta \\in \\R^p,\\ \\sigma&gt;0 : Y \\sim N(X^\\top\\beta,\\sigma^2),\\  X\\mbox{ fixed}\\}\\).\n\\(\\P = \\{\\mbox{all CDF's }F\\}\\).\n\\(\\P = \\{\\mbox{all smooth functions } f: \\R^p \\rightarrow \\R : Z_i = (X_i, Y_i),\\ E[Y_i] = f(X_i) \\}\\)"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models",
    "href": "schedule/slides/03-regression-function.html#statistical-models",
    "title": "UBC Stat406 2023W",
    "section": "Statistical models",
    "text": "Statistical models\nWe want to use the data to select a distribution \\(P\\) that probably generated the data.\n\nMy model:\n\\[\n\\P = \\{ P(z=1)=p,\\ P(z=0)=1-p,\\ 0 &lt; p &lt; 1 \\}\n\\]\n\nTo completely characterize \\(P\\), I just need to estimate \\(p\\).\nNeed to assume that \\(P \\in \\P\\).\nThis assumption is mostly empty: need independent, can’t see \\(z=12\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-1",
    "href": "schedule/slides/03-regression-function.html#statistical-models-1",
    "title": "UBC Stat406 2023W",
    "section": "Statistical models",
    "text": "Statistical models\nWe observe data \\(Z_i=(Y_i,X_i)\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\n\nMy model\n\\[\n\\P = \\{ \\beta \\in \\R^p, \\sigma&gt;0 : Y_i \\given X_i=x_i \\sim N(x_i^\\top\\beta,\\ \\sigma^2) \\}.\n\\]\n\nTo completely characterize \\(P\\), I just need to estimate \\(\\beta\\) and \\(\\sigma\\).\nNeed to assume that \\(P\\in\\P\\).\nThis time, I have to assume a lot more: (conditional) Linearity, independence, conditional Gaussian noise, no ignored variables, no collinearity, etc."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-unfamiliar-example",
    "href": "schedule/slides/03-regression-function.html#statistical-models-unfamiliar-example",
    "title": "UBC Stat406 2023W",
    "section": "Statistical models, unfamiliar example",
    "text": "Statistical models, unfamiliar example\nWe observe data \\(Z_i \\in \\R\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\nMy model\n\\[\n\\P = \\{ Z_i \\textrm{ has a density function } f \\}.\n\\]\n\nTo completely characterize \\(P\\), I need to estimate \\(f\\).\nIn fact, we can’t hope to do this.\n\nRevised Model 1 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; M \\}\\)\nRevised Model 2 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; K &lt; M \\}\\)\nRevised Model 3 - \\(\\P=\\{ Z_i \\textrm{ has a density function } f : \\int |f'| dx &lt; M \\}\\)\n\nEach of these suggests different ways of estimating \\(f\\)"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#assumption-lean-regression",
    "href": "schedule/slides/03-regression-function.html#assumption-lean-regression",
    "title": "UBC Stat406 2023W",
    "section": "Assumption Lean Regression",
    "text": "Assumption Lean Regression\nImagine \\(Z = (Y, \\mathbf{X}) \\sim P\\) with \\(Y \\in \\R\\) and \\(\\mathbf{X} = (1, X_1, \\ldots, X_p)^\\top\\).\nWe are interested in the conditional distribution \\(P_{Y|\\mathbf{X}}\\)\nSuppose we think that there is some function of interest which relates \\(Y\\) and \\(X\\).\nLet’s call this function \\(\\mu(\\mathbf{X})\\) for the moment. How do we estimate \\(\\mu\\)? What is \\(\\mu\\)?\n\n\nTo make this precise, we\n\nHave a model \\(\\P\\).\nNeed to define a “good” functional \\(\\mu\\).\nLet’s loosely define “good” as\n\n\nGiven a new (random) \\(Z\\), \\(\\mu(\\mathbf{X})\\) is “close” to \\(Y\\).\n\n\n\nSee Berk et al. Assumption Lean Regression."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#evaluating-close",
    "href": "schedule/slides/03-regression-function.html#evaluating-close",
    "title": "UBC Stat406 2023W",
    "section": "Evaluating “close”",
    "text": "Evaluating “close”\nWe need more functions.\nChoose some loss function \\(\\ell\\) that measures how close \\(\\mu\\) and \\(Y\\) are.\n\n\n\nSquared-error:\n\\(\\ell(y,\\ \\mu) = (y-\\mu)^2\\)\nAbsolute-error:\n\\(\\ell(y,\\ \\mu) = |y-\\mu|\\)\nZero-One:\n\\(\\ell(y,\\ \\mu) = I(y\\neq\\mu)=\\begin{cases} 0 & y=\\mu\\\\1 & \\mbox{else}\\end{cases}\\)\nCauchy:\n\\(\\ell(y,\\ \\mu) = \\log(1 + (y - \\mu)^2)\\)\n\n\n\n\n\nCode\nggplot() +\n  xlim(-2, 2) +\n  geom_function(fun = ~log(1+.x^2), colour = 'purple', linewidth = 2) +\n  geom_function(fun = ~.x^2, colour = tertiary, linewidth = 2) +\n  geom_function(fun = ~abs(.x), colour = primary, linewidth = 2) +\n  geom_line(\n    data = tibble(x = seq(-2, 2, length.out = 100), y = as.numeric(x != 0)), \n    aes(x, y), colour = orange, linewidth = 2) +\n  geom_point(data = tibble(x = 0, y = 0), aes(x, y), \n             colour = orange, pch = 16, size = 3) +\n  ylab(bquote(\"\\u2113\" * (y - mu))) + xlab(bquote(y - mu))"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#start-with-expected-squared-error",
    "href": "schedule/slides/03-regression-function.html#start-with-expected-squared-error",
    "title": "UBC Stat406 2023W",
    "section": "Start with (Expected) Squared Error",
    "text": "Start with (Expected) Squared Error\nLet’s try to minimize the expected squared error (MSE).\nClaim: \\(\\mu(X) = \\Expect{Y\\ \\vert\\ X}\\) minimizes MSE.\nThat is, for any \\(r(X)\\), \\(\\Expect{(Y - \\mu(X))^2} \\leq \\Expect{(Y-r(X))^2}\\).\n\nProof of Claim:\n\\[\\begin{aligned}\n\\Expect{(Y-r(X))^2}\n&= \\Expect{(Y- \\mu(X) + \\mu(X) - r(X))^2}\\\\\n&= \\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2\\Expect{(Y- \\mu(X))(\\mu(X) - r(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2(\\mu(X) - r(X))\\Expect{(Y- \\mu(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} + 0\\\\\n&\\geq \\Expect{(Y- \\mu(X))^2}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#the-regression-function",
    "href": "schedule/slides/03-regression-function.html#the-regression-function",
    "title": "UBC Stat406 2023W",
    "section": "The regression function",
    "text": "The regression function\nSometimes people call this solution:\n\\[\\mu(X) = \\Expect{Y \\ \\vert\\  X}\\]\nthe regression function. (But don’t forget that it depended on \\(\\ell\\).)\nIf we assume that \\(\\mu(x) = \\Expect{Y \\ \\vert\\  X=x} = x^\\top \\beta\\), then we get back exactly OLS.\n\nBut why should we assume \\(\\mu(x) = x^\\top \\beta\\)?"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#brief-aside",
    "href": "schedule/slides/03-regression-function.html#brief-aside",
    "title": "UBC Stat406 2023W",
    "section": "Brief aside",
    "text": "Brief aside\nSome notation / terminology\n\n“Hats” on things mean “estimates”, so \\(\\widehat{\\mu}\\) is an estimate of \\(\\mu\\)\nParameters are “properties of the model”, so \\(f_X(x)\\) or \\(\\mu\\) or \\(\\Var{Y}\\)\nRandom variables like \\(X\\), \\(Y\\), \\(Z\\) may eventually become data, \\(x\\), \\(y\\), \\(z\\), once observed.\n“Estimating” means “using observations to estimate parameters”\n“Predicting” means “using observations to predict future data”\nOften, there is a parameter whose estimate will provide a prediction.\n\n\n\nThis last point can lead to confusion."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#the-regression-function-1",
    "href": "schedule/slides/03-regression-function.html#the-regression-function-1",
    "title": "UBC Stat406 2023W",
    "section": "The regression function",
    "text": "The regression function\nIn mathematics: \\(\\mu(x) = \\Expect{Y \\ \\vert\\  X=x}\\).\nIn words:\nRegression with squared-error loss is really about estimating the (conditional) mean.\n\nIf \\(Y\\sim \\textrm{N}(\\mu,\\ 1)\\), our best guess for a new \\(Y\\) is \\(\\mu\\).\nFor regression, we let the mean \\((\\mu)\\) depend on \\(X\\).\n\nThink of \\(Y\\sim \\textrm{N}(\\mu(X),\\ 1)\\), then conditional on \\(X=x\\), our best guess for a new \\(Y\\) is \\(\\mu(x)\\)\n\n[whatever this function \\(\\mu\\) is]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#anything-strange",
    "href": "schedule/slides/03-regression-function.html#anything-strange",
    "title": "UBC Stat406 2023W",
    "section": "Anything strange?",
    "text": "Anything strange?\nFor any two variables \\(Y\\) and \\(X\\), we can always write\n\\[Y = E[Y\\given X] + (Y - E[Y\\given X]) = \\mu(X) + \\eta(X)\\]\nsuch that \\(\\Expect{\\eta(X)}=0\\).\n\n\nSuppose, \\(\\mu(X)=\\mu_0\\) (constant in \\(X\\)), are \\(Y\\) and \\(X\\) independent?\n\n\n\n\nSuppose \\(Y\\) and \\(X\\) are independent, is \\(\\mu(X)=\\mu_0\\)?\n\n\n\n\nFor more practice on this see the Fun Worksheet on Theory and solutions\nIn this course, I do not expect you to be able to create this math, but understanding and explaining it is important."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#what-do-we-mean-by-good-predictions",
    "href": "schedule/slides/03-regression-function.html#what-do-we-mean-by-good-predictions",
    "title": "UBC Stat406 2023W",
    "section": "What do we mean by good predictions?",
    "text": "What do we mean by good predictions?\nWe make observations and then attempt to “predict” new, unobserved data.\nSometimes this is the same as estimating the (conditional) mean.\nMostly, we observe \\((y_1,x_1),\\ \\ldots,\\ (y_n,x_n)\\), and we want some way to predict \\(Y\\) from \\(X\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#expected-test-mse",
    "href": "schedule/slides/03-regression-function.html#expected-test-mse",
    "title": "UBC Stat406 2023W",
    "section": "Expected test MSE",
    "text": "Expected test MSE\nFor regression applications, we will use squared-error loss:\n\\(R_n(\\widehat{\\mu}) = \\Expect{(Y-\\widehat{\\mu}(X))^2}\\)\n\nI’m giving this a name, \\(R_n\\) for ease.\nDifferent than text.\nThis is expected test MSE."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#example-estimatingpredicting-the-conditional-mean",
    "href": "schedule/slides/03-regression-function.html#example-estimatingpredicting-the-conditional-mean",
    "title": "UBC Stat406 2023W",
    "section": "Example: Estimating/Predicting the (conditional) mean",
    "text": "Example: Estimating/Predicting the (conditional) mean\nSuppose we know that we want to predict a quantity \\(Y\\),\nwhere \\(\\Expect{Y}= \\mu \\in \\mathbb{R}\\) and \\(\\Var{Y} = 1\\).\nOur data is \\(\\{y_1,\\ldots,y_n\\}\\)\nClaim: We want to estimate \\(\\mu\\).\n\nWhy?"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#estimating-the-mean",
    "href": "schedule/slides/03-regression-function.html#estimating-the-mean",
    "title": "UBC Stat406 2023W",
    "section": "Estimating the mean",
    "text": "Estimating the mean\n\nLet \\(\\widehat{Y}=\\overline{Y}_n\\) be the sample mean.\n\nWe can ask about the estimation risk (since we’re estimating \\(\\mu\\)):\n\n\n\n\\[\\begin{aligned}\n    E[(\\overline{Y}_n-\\mu)^2]\n    &= E[\\overline{Y}_n^2]\n    -2\\mu E[\\overline{Y}_n] + \\mu^2 \\\\\n    &= \\mu^2 + \\frac{1}{n} - 2\\mu^2 +\n    \\mu^2\\\\ &= \\frac{1}{n}\n\\end{aligned}\\]\n\n\nUseful trick\nFor any \\(Z\\),\n\\(\\Var{Z} = \\Expect{Z^2} - \\Expect{Z}^2\\).\nTherefore:\n\\(\\Expect{Z^2} = \\Var{Z} + \\Expect{Z}^2\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys",
    "title": "UBC Stat406 2023W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\n\nLet \\(\\widehat{Y}=\\overline{Y}_n\\) be the sample mean.\n\nWhat is the prediction risk of \\(\\overline{Y}\\)?\n\n\n\n\\[\\begin{aligned}\n  R_n(\\overline{Y}_n)\n  &= \\E[(\\overline{Y}_n-Y)^2]\\\\\n  &= \\E[\\overline{Y}_{n}^{2}] -2\\E[\\overline{Y}_n Y] + \\E[Y^2] \\\\\n  &= \\mu^2 + \\frac{1}{n} - 2\\mu^2 + \\mu^2 + 1 \\\\\n  &= 1 + \\frac{1}{n}\n\\end{aligned}\\]\n\n\nTricks:\nUsed the variance thing again.\nIf \\(X\\) and \\(Z\\) are independent, then \\(\\Expect{XZ} = \\Expect{X}\\Expect{Z}\\)"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys-1",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys-1",
    "title": "UBC Stat406 2023W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\n\nWhat is the prediction risk of guessing \\(Y=0\\)?\nYou can probably guess that this is a stupid idea.\nLet’s show why it’s stupid.\n\n\\[\\begin{aligned}\n        R_n(0) &= \\E[(0-Y)^2] = 1 + \\mu^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys-2",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys-2",
    "title": "UBC Stat406 2023W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\n\nWhat is the prediction risk of guessing \\(Y=\\mu\\)?\nThis is a great idea, but we don’t know \\(\\mu\\).\nLet’s see what happens anyway.\n\n\\[\\begin{aligned}\n        R_n(\\mu) &= \\E[(Y-\\mu)^2]= 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#risk-relations",
    "href": "schedule/slides/03-regression-function.html#risk-relations",
    "title": "UBC Stat406 2023W",
    "section": "Risk relations",
    "text": "Risk relations\nPrediction risk: \\(R_n(\\overline{Y}_n) = 1 + \\frac{1}{n}\\)\nEstimation risk: \\(E[(\\overline{Y}_n - \\mu)^2] = \\frac{1}{n}\\)\nThere is actually a nice interpretation here:\n\nThe common \\(1/n\\) term is \\(\\Var{\\overline{Y}_n}\\)\n\nThe extra factor of \\(1\\) in the prediction risk is irreducible error\n\n\\(Y\\) is a random variable, and hence noisy.\nWe can never eliminate it’s intrinsic variance.\n\nIn other words, even if we knew \\(\\mu\\), we could never get closer than \\(1\\), on average.\n\n\nIntuitively, \\(\\overline{Y}_n\\) is the obvious thing to do.\nBut what about unintuitive things…"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#meta-lecture",
    "href": "schedule/slides/05-estimating-test-mse.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "05 Estimating test MSE",
    "text": "05 Estimating test MSE\nStat 406\nDaniel J. McDonald\nLast modified – 18 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#estimating-prediction-risk",
    "href": "schedule/slides/05-estimating-test-mse.html#estimating-prediction-risk",
    "title": "UBC Stat406 2023W",
    "section": "Estimating prediction risk",
    "text": "Estimating prediction risk\nLast time, we saw\n\\(R_n(\\widehat{f}) = E[(Y-\\widehat{f}(X))^2]\\)\nprediction risk = \\(\\textrm{bias}^2\\) + variance + irreducible error\nWe argued that we want procedures that produce \\(\\widehat{f}\\) with small \\(R_n\\).\n\nHow do we estimate \\(R_n\\)?"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error",
    "href": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error",
    "title": "UBC Stat406 2023W",
    "section": "Don’t use training error",
    "text": "Don’t use training error\nThe training error in regression is\n\\[\\widehat{R}_n(\\widehat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nHere, the \\(n\\) is doubly used (annoying, but simple): \\(n\\) observations to create \\(\\widehat{f}\\) and \\(n\\) terms in the sum.\n\n\n\n\n\n\nImportant\n\n\nTraining error is a bad estimator for \\(R_n(\\widehat{f})\\).\n\n\n\nSo we should never use it."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#these-all-have-the-same-r2-and-training-error",
    "href": "schedule/slides/05-estimating-test-mse.html#these-all-have-the-same-r2-and-training-error",
    "title": "UBC Stat406 2023W",
    "section": "These all have the same \\(R^2\\) and Training Error",
    "text": "These all have the same \\(R^2\\) and Training Error\n\n\n\n\nCode\nans &lt;- anscombe |&gt;\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), \n               names_pattern = \"(.)(.)\")\nggplot(ans, aes(x, y)) + \n  geom_point(colour = orange, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = blue, linewidth = 2) +\n  facet_wrap(~set, labeller = label_both)\n\n\n\n\n\n\n\n\n\n\n\n\nans %&gt;% \n  group_by(set) |&gt; \n  summarise(\n    R2 = summary(lm(y ~ x))$r.sq, \n    train_error = mean((y - predict(lm(y ~ x)))^2)\n  ) |&gt;\n  kableExtra::kable(digits = 2)\n\n\n\n\nset\nR2\ntrain_error\n\n\n\n\n1\n0.67\n1.25\n\n\n2\n0.67\n1.25\n\n\n3\n0.67\n1.25\n\n\n4\n0.67\n1.25"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#adding-junk-predictors-increases-r2-and-decreases-training-error",
    "href": "schedule/slides/05-estimating-test-mse.html#adding-junk-predictors-increases-r2-and-decreases-training-error",
    "title": "UBC Stat406 2023W",
    "section": "Adding “junk” predictors increases \\(R^2\\) and decreases Training Error",
    "text": "Adding “junk” predictors increases \\(R^2\\) and decreases Training Error\n\nn &lt;- 100\np &lt;- 10\nq &lt;- 0:30\nx &lt;- matrix(rnorm(n * (p + max(q))), nrow = n)\ny &lt;- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)\n\nregress_on_junk &lt;- function(q) {\n  x &lt;- x[, 1:(p + q)]\n  mod &lt;- lm(y ~ x)\n  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))\n}\n\n\n\nCode\nmap(q, regress_on_junk) |&gt; \n  list_rbind() |&gt;\n  mutate(q = q) |&gt;\n  pivot_longer(-q) |&gt;\n  ggplot(aes(q, value, colour = name)) +\n  geom_line(linewidth = 2) + xlab(\"train_error\") +\n  scale_colour_manual(values = c(blue, orange), guide = \"none\") +\n  facet_wrap(~ name, scales = \"free_y\")"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#other-things-you-cant-use",
    "href": "schedule/slides/05-estimating-test-mse.html#other-things-you-cant-use",
    "title": "UBC Stat406 2023W",
    "section": "Other things you can’t use",
    "text": "Other things you can’t use\nYou should not use anova\nor the \\(p\\)-values from the lm output for this purpose.\n\nThese things are to determine whether those parameters are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.\n\nNot the same as “are they useful for prediction = do they help me get smaller \\(R_n\\)?”"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#risk-of-risk",
    "href": "schedule/slides/05-estimating-test-mse.html#risk-of-risk",
    "title": "UBC Stat406 2023W",
    "section": "Risk of Risk",
    "text": "Risk of Risk\nWhile it’s crummy, Training Error is an estimator of \\(R_n(\\hat{f})\\)\nRecall, \\(R_n(\\hat{f})\\) is a parameter (a property of the data distribution)\nSo we can ask “is \\(\\widehat{R}(\\hat{f})\\) a good estimator for \\(R_n(\\hat{f})\\)?”\nBoth are just numbers, so perhaps a good way to measure is\n\\[\nE[(R_n - \\widehat{R})^2]\n= \\cdots\n= (R_n - E[\\widehat{R}])^2 + \\Var{\\widehat{R}}\n\\]\nChoices you make determine how good this is.\nWe can try to balance it’s bias and variance…"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#held-out-sets",
    "href": "schedule/slides/05-estimating-test-mse.html#held-out-sets",
    "title": "UBC Stat406 2023W",
    "section": "Held out sets",
    "text": "Held out sets\nOne option is to have a separate “held out” or “validation set”.\n👍 Estimates the test error\n👍 Fast computationally\n🤮 Estimate is random\n🤮 Estimate has high variance (depends on 1 choice of split)\n🤮 Estimate has some bias because we only used some of the data"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#aside",
    "href": "schedule/slides/05-estimating-test-mse.html#aside",
    "title": "UBC Stat406 2023W",
    "section": "Aside",
    "text": "Aside\nIn my experience, CS has particular definitions of “training”, “validation”, and “test” data.\nI think these are not quite the same as in Statistics.\n\nTest data - Hypothetical data you don’t get to see, ever. Infinite amounts drawn from the population.\n\nExpected test error or Risk is an expected value over this distribution. It’s not a sum over some data kept aside.\n\nSometimes I’ll give you “test data”. You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.\nTraining data - This is data that you get to touch.\nValidation set - Often, we need to choose models. One way to do this is to split off some of your training data and pretend that it’s like a “Test Set”.\n\nWhen and how you split your training data can be very important."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#intuition-for-cv",
    "href": "schedule/slides/05-estimating-test-mse.html#intuition-for-cv",
    "title": "UBC Stat406 2023W",
    "section": "Intuition for CV",
    "text": "Intuition for CV\nOne reason that \\(\\widehat{R}_n(\\widehat{f})\\) is bad is that we are using the same data to pick \\(\\widehat{f}\\) AND to estimate \\(R_n\\).\n“Validation set” fixes this, but holds out a particular, fixed block of data we pretend mimics the “test data”\n\nWhat if we set aside one observation, say the first one \\((y_1, x_1)\\).\nWe estimate \\(\\widehat{f}^{(1)}\\) without using the first observation.\nThen we test our prediction:\n\\[\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.\\]\n(why the notation \\(\\widetilde{R}_1\\)? Because we’re estimating the risk with 1 observation. )"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#keep-going",
    "href": "schedule/slides/05-estimating-test-mse.html#keep-going",
    "title": "UBC Stat406 2023W",
    "section": "Keep going",
    "text": "Keep going\nBut that was only one data point \\((y_1, x_1)\\). Why stop there?\nDo the same with \\((y_2, x_2)\\)! Get an estimate \\(\\widehat{f}^{(2)}\\) without using it, then\n\\[\\widetilde{R}_1(\\widehat{f}^{(2)}) = (y_2 -\\widehat{f}^{(2)}(x_2))^2.\\]\nWe can keep doing this until we try it for every data point.\nAnd then average them! (Averages are good)\n\\[\\mbox{LOO-CV} = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{R}_1(\\widehat{f}^{(i)}) = \\frac{1}{n}\\sum_{i=1}^n\n(y_i - \\widehat{f}^{(i)}(x_i))^2\\]\n\nThis is leave-one-out cross validation"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#problems-with-loo-cv",
    "href": "schedule/slides/05-estimating-test-mse.html#problems-with-loo-cv",
    "title": "UBC Stat406 2023W",
    "section": "Problems with LOO-CV",
    "text": "Problems with LOO-CV\n🤮 Each held out set is small \\((n=1)\\). Therefore, the variance of the Squared Error of each prediction is high.\n🤮 The training sets overlap. This is bad.\n\nUsually, averaging reduces variance: \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.\\)\nBut only if the variables are independent. If not, then \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.\\)\nSince the training sets overlap a lot, that covariance can be pretty big.\n\n🤮 We have to estimate this model \\(n\\) times.\n🎉 Bias is low because we used almost all the data to fit the model: \\(E[\\mbox{LOO-CV}] = R_{n-1}\\)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#k-fold-cv",
    "href": "schedule/slides/05-estimating-test-mse.html#k-fold-cv",
    "title": "UBC Stat406 2023W",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nTo alleviate some of these problems, people usually use \\(K\\)-fold cross validation.\nThe idea of \\(K\\)-fold is\n\nDivide the data into \\(K\\) groups.\nLeave a group out and estimate with the rest.\nTest on the held-out group. Calculate an average risk over these \\(\\sim n/K\\) data.\nRepeat for all \\(K\\) groups.\nAverage the average risks.\n\n\n\n🎉 Less overlap, smaller covariance.\n🎉 Larger hold-out sets, smaller variance.\n🎉 Less computations (only need to estimate \\(K\\) times)\n🤮 LOO-CV is (nearly) unbiased for \\(R_n\\)\n🤮 K-fold CV is unbiased for \\(R_{n(1-1/K)}\\)\nThe risk depends on how much data you use to estimate the model. \\(R_n\\) depends on \\(n\\)."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#a-picture",
    "href": "schedule/slides/05-estimating-test-mse.html#a-picture",
    "title": "UBC Stat406 2023W",
    "section": "A picture",
    "text": "A picture\n\n\nCode\npar(mar = c(0, 0, 0, 0))\nplot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = \"n\", yaxt = \"n\", xaxt = \"n\")\nrect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)\nrect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), \n     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)\npoints(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)\ntext(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c(\"1\", \"2\", \"3\", \"K\"), cex = 3, \n     col = red)\ntext(6, 4.5, \"Training data\", cex = 3, col = blue)\ntext(2, 1.5, \"Validation data\", cex = 3, col = red)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#code",
    "href": "schedule/slides/05-estimating-test-mse.html#code",
    "title": "UBC Stat406 2023W",
    "section": "Code",
    "text": "Code\n\n#' @param data The full data set\n#' @param estimator Function. Has 1 argument (some data) and fits a model. \n#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions\n#' @param error_fun Function. Has one arg: the test data, with fits added.\n#' @param kfolds Integer. The number of folds.\nkfold_cv &lt;- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  n &lt;- nrow(data)\n  fold_labels &lt;- sample(rep(1:kfolds, length.out = n))\n  errors &lt;- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows &lt;- fold_labels == fold\n    train &lt;- data[!test_rows, ]\n    test &lt;- data[test_rows, ]\n    current_model &lt;- estimator(train)\n    test$.preds &lt;- predictor(current_model, test)\n    errors[fold] &lt;- error_fun(test)\n  }\n  mean(errors)\n}\n\n\n\nsomedata &lt;- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))\nest &lt;- function(dataset) lm(z ~ ., data = dataset)\npred &lt;- function(mod, dataset) predict(mod, newdata = dataset)\nerror_fun &lt;- function(testdata) mutate(testdata, errs = (z - .preds)^2) |&gt; pull(errs) |&gt; mean()\nkfold_cv(somedata, est, pred, error_fun, 5)\n\n[1] 0.9532271"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#trick",
    "href": "schedule/slides/05-estimating-test-mse.html#trick",
    "title": "UBC Stat406 2023W",
    "section": "Trick",
    "text": "Trick\nFor a certain “nice” models, one can show\n(after pages of tedious algebra which I wouldn’t wish on my worst enemy, but might, in a fit of rage assign as homework to belligerent students)\n\\[\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-h_{ii})^2} = \\frac{1}{n} \\sum_{i=1}^n \\frac{\\widehat{e}_i^2}{(1-h_{ii})^2}.\\]\n\nThis trick means that you only have to fit the model once rather than \\(n\\) times!\nYou still have to calculate this for each model!\n\n\ncv_nice &lt;- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#trick-1",
    "href": "schedule/slides/05-estimating-test-mse.html#trick-1",
    "title": "UBC Stat406 2023W",
    "section": "Trick",
    "text": "Trick\n\ncv_nice &lt;- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )\n\n“Nice” requires:\n\n\\(\\widehat{y}_i = h_i(\\mathbf{X})^\\top \\mathbf{y}\\) for some vector \\(h_i\\)\n\\(e^{(i)} = \\frac{\\widehat{e}_i}{(1-h_{ii})}\\)"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#meta-lecture",
    "href": "schedule/slides/07-greedy-selection.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "07 Greedy selection",
    "text": "07 Greedy selection\nStat 406\nDaniel J. McDonald\nLast modified – 18 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#recap",
    "href": "schedule/slides/07-greedy-selection.html#recap",
    "title": "UBC Stat406 2023W",
    "section": "Recap",
    "text": "Recap\nModel Selection means select a family of distributions for your data.\nIdeally, we’d do this by comparing the \\(R_n\\) for one family with that for another.\nWe’d use whichever has smaller \\(R_n\\).\nBut \\(R_n\\) depends on the truth, so we estimate it with \\(\\widehat{R}\\).\nThen we use whichever has smaller \\(\\widehat{R}\\)."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#example",
    "href": "schedule/slides/07-greedy-selection.html#example",
    "title": "UBC Stat406 2023W",
    "section": "Example",
    "text": "Example\nThe truth:\n\ndat &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100),\n  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)\n)\n\nModel 1: y ~ x1 + x2\nModel 2: y ~ x1 + x2 + x1*x2\nModel 3: y ~ x2 + sin(x1 * x2)\n\n(What are the families for each of these?)"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#fit-each-model-and-estimate-r_n",
    "href": "schedule/slides/07-greedy-selection.html#fit-each-model-and-estimate-r_n",
    "title": "UBC Stat406 2023W",
    "section": "Fit each model and estimate \\(R_n\\)",
    "text": "Fit each model and estimate \\(R_n\\)\n\nforms &lt;- list(\"y ~ x1 + x2\", \"y ~ x1 * x2\", \"y ~ x2 + sin(x1*x2)\") |&gt; \n  map(as.formula)\nfits &lt;- map(forms, ~ lm(.x, data = dat))\nmap(fits, ~ tibble(\n  R2 = summary(.x)$r.sq,\n  training_error = mean(residuals(.x)^2),\n  loocv = mean( (residuals(.x) / (1 - hatvalues(.x)))^2 ),\n  AIC = AIC(.x),\n  BIC = BIC(.x)\n)) |&gt; list_rbind()\n\n# A tibble: 3 × 5\n     R2 training_error loocv   AIC   BIC\n  &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.589           21.3  22.9  598.  608.\n2 0.595           21.0  23.4  598.  611.\n3 0.586           21.4  23.0  598.  609."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#model-selection-vs.-variable-selection",
    "href": "schedule/slides/07-greedy-selection.html#model-selection-vs.-variable-selection",
    "title": "UBC Stat406 2023W",
    "section": "Model Selection vs. Variable Selection",
    "text": "Model Selection vs. Variable Selection\nModel selection is very comprehensive\nYou choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.\nVariable selection is a subset of this. It means\n\nchoosing which predictors to include in a predictive model\n\nEliminating a predictor, means removing it from the model.\nSome procedures automatically search predictors, and eliminate some.\nWe call this variable selection. But the procedure is implicitly selecting a model as well.\n\nMaking this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#selecting-variables-predictors-with-linear-methods",
    "href": "schedule/slides/07-greedy-selection.html#selecting-variables-predictors-with-linear-methods",
    "title": "UBC Stat406 2023W",
    "section": "Selecting variables / predictors with linear methods",
    "text": "Selecting variables / predictors with linear methods\n\n\nSuppose we have a pile of predictors.\nWe estimate models with different subsets of predictors and use CV / Cp / AIC / BIC to decide which is preferred.\nSometimes you might have a few plausible subsets. Easy enough to choose with our criterion.\nSometimes you might just have a bunch of predictors, then what do you do?\n\n\n\nAll subsets\n\nestimate model based on every possible subset of size \\(|\\mathcal{S}| \\leq \\min\\{n, p\\}\\), use one with lowest risk estimate\n\nForward selection\n\nstart with \\(\\mathcal{S}=\\varnothing\\), add predictors greedily\n\nBackward selection\n\nstart with \\(\\mathcal{S}=\\{1,\\ldots,p\\}\\), remove greedily\n\nHybrid\n\ncombine forward and backward smartly"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#costs-and-benefits",
    "href": "schedule/slides/07-greedy-selection.html#costs-and-benefits",
    "title": "UBC Stat406 2023W",
    "section": "Costs and benefits",
    "text": "Costs and benefits\n\nAll subsets\n\n👍 estimates each subset\n💣 takes \\(2^p\\) model fits when \\(p&lt;n\\). If \\(p=50\\), this is about \\(10^{15}\\) models.\n\nForward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n\nBackward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n💣 doesn’t work if \\(p&gt;n\\)\n\nHybrid\n\n👍 visits more models than forward/backward\n💣 slower"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#synthetic-example",
    "href": "schedule/slides/07-greedy-selection.html#synthetic-example",
    "title": "UBC Stat406 2023W",
    "section": "Synthetic example",
    "text": "Synthetic example\n\nset.seed(123)\nn &lt;- 406\ndf &lt;- tibble( # like data.frame, but columns can be functions of preceding\n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  x5 = x1 + rnorm(n, sd = .1), # correlated with x1\n  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)\n  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only\n)\n\n\n\\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) are the true predictors\nBut the rest are correlated with them"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#full-model",
    "href": "schedule/slides/07-greedy-selection.html#full-model",
    "title": "UBC Stat406 2023W",
    "section": "Full model",
    "text": "Full model\n\nfull &lt;- lm(y ~ ., data = df)\nsummary(full)\n\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7739 -1.4283 -0.0929  1.4257  7.5869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  0.03383    0.27700   0.122  0.90287   \nx1           6.70481    2.06743   3.243  0.00128 **\nx2          -0.43945    1.71650  -0.256  0.79807   \nx3           1.37293    1.11524   1.231  0.21903   \nx4          -1.19911    1.17850  -1.017  0.30954   \nx5          -0.53918    1.07089  -0.503  0.61490   \nx6          -1.88547    1.21652  -1.550  0.12196   \nx7          -1.25245    1.10743  -1.131  0.25876   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.231 on 398 degrees of freedom\nMultiple R-squared:  0.6411,    Adjusted R-squared:  0.6347 \nF-statistic: 101.5 on 7 and 398 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#true-model",
    "href": "schedule/slides/07-greedy-selection.html#true-model",
    "title": "UBC Stat406 2023W",
    "section": "True model",
    "text": "True model\n\ntruth &lt;- lm(y ~ x1 + x2, data = df)\nsummary(truth)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4519 -1.3873 -0.1941  1.3498  7.5533 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1676     0.2492   0.673   0.5015    \nx1            3.0316     0.1146  26.447   &lt;2e-16 ***\nx2            0.2447     0.1109   2.207   0.0279 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.233 on 403 degrees of freedom\nMultiple R-squared:  0.6357,    Adjusted R-squared:  0.6339 \nF-statistic: 351.6 on 2 and 403 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#all-subsets",
    "href": "schedule/slides/07-greedy-selection.html#all-subsets",
    "title": "UBC Stat406 2023W",
    "section": "All subsets",
    "text": "All subsets\n\nlibrary(leaps)\ntrythemall &lt;- regsubsets(y ~ ., data = df)\nsummary(trythemall)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df)\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp",
    "title": "UBC Stat406 2023W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(trythemall)$bic, \n  Cp = summary(trythemall)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) + \n  geom_point() + \n  geom_line() + \n  facet_wrap(~name, scales = \"free_y\") + \n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#forward-stepwise",
    "href": "schedule/slides/07-greedy-selection.html#forward-stepwise",
    "title": "UBC Stat406 2023W",
    "section": "Forward stepwise",
    "text": "Forward stepwise\n\nstepup &lt;- regsubsets(y ~ ., data = df, method = \"forward\")\nsummary(stepup)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"forward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: forward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp-1",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp-1",
    "title": "UBC Stat406 2023W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(stepup)$bic,\n  Cp = summary(stepup)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange),\n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#backward-selection",
    "href": "schedule/slides/07-greedy-selection.html#backward-selection",
    "title": "UBC Stat406 2023W",
    "section": "Backward selection",
    "text": "Backward selection\n\nstepdown &lt;- regsubsets(y ~ ., data = df, method = \"backward\")\nsummary(stepdown)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"backward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: backward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp-2",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp-2",
    "title": "UBC Stat406 2023W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(stepdown)$bic,\n  Cp = summary(stepdown)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#section",
    "href": "schedule/slides/07-greedy-selection.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "somehow, for this seed, everything is the same"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#randomness-and-prediction-error",
    "href": "schedule/slides/07-greedy-selection.html#randomness-and-prediction-error",
    "title": "UBC Stat406 2023W",
    "section": "Randomness and prediction error",
    "text": "Randomness and prediction error\nAll of that was for one data set.\nDoesn’t say which procedure is better generally.\nIf we want to know how they compare generally, we should repeat many times\n\nGenerate training data\nEstimate with different algorithms\nPredict held-out set data\nExamine prediction MSE (on held-out set)\n\n\nI’m not going to do all subsets, just the truth, forward selection, backward, and the full model\nFor forward/backward selection, I’ll use Cp to choose the final size"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#code-for-simulation",
    "href": "schedule/slides/07-greedy-selection.html#code-for-simulation",
    "title": "UBC Stat406 2023W",
    "section": "Code for simulation",
    "text": "Code for simulation\n… Annoyingly, no predict method for regsubsets, so we make one.\n\npredict.regsubsets &lt;- function(object, newdata, risk_estimate = c(\"cp\", \"bic\"), ...) {\n  risk_estimate &lt;- match.arg(risk_estimate)\n  chosen &lt;- coef(object, which.min(summary(object)[[risk_estimate]]))\n  predictors &lt;- names(chosen)\n  if (object$intercept) predictors &lt;- predictors[-1]\n  X &lt;- newdata[, predictors]\n  if (object$intercept) X &lt;- cbind2(1, X)\n  drop(as.matrix(X) %*% chosen)\n}"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#section-1",
    "href": "schedule/slides/07-greedy-selection.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "simulate_and_estimate_them_all &lt;- function(n = 406) {\n  N &lt;- 2 * n # generate 2x the amount of data (half train, half test)\n  df &lt;- tibble( # generate data\n    x1 = rnorm(N), \n    x2 = rnorm(N, mean = 2), \n    x3 = rexp(N),\n    x4 = x2 + rnorm(N, sd = .1), \n    x5 = x1 + rnorm(N, sd = .1),\n    x6 = x1 - x2 + rnorm(N, sd = .1), \n    x7 = x1 + x3 + rnorm(N, sd = .1),\n    y = x1 * 3 + x2 / 3 + rnorm(N, sd = 2.2)\n  )\n  train &lt;- df[1:n, ] # half the data for training\n  test &lt;- df[(n + 1):N, ] # half the data for evaluation\n  \n  oracle &lt;- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs\n  full &lt;- lm(y ~ ., data = train)\n  stepup &lt;- regsubsets(y ~ ., data = train, method = \"forward\")\n  stepdown &lt;- regsubsets(y ~ ., data = train, method = \"backward\")\n  \n  tibble(\n    y = test$y,\n    oracle = predict(oracle, newdata = test),\n    full = predict(full, newdata = test),\n    stepup = predict(stepup, newdata = test),\n    stepdown = predict(stepdown, newdata = test),\n    truth = drop(as.matrix(test[, c(\"x1\", \"x2\")]) %*% c(3, 1/3))\n  )\n}\n\nset.seed(12345)\nour_sim &lt;- map(1:50, ~ simulate_and_estimate_them_all(406)) |&gt;\n  list_rbind(names_to = \"sim\")"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#what-is-oracle",
    "href": "schedule/slides/07-greedy-selection.html#what-is-oracle",
    "title": "UBC Stat406 2023W",
    "section": "What is “Oracle”",
    "text": "What is “Oracle”"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#results",
    "href": "schedule/slides/07-greedy-selection.html#results",
    "title": "UBC Stat406 2023W",
    "section": "Results",
    "text": "Results\n\n\nour_sim |&gt; \n  group_by(sim) %&gt;%\n  summarise(\n    across(oracle:truth, ~ mean((y - .)^2)), \n    .groups = \"drop\"\n  ) %&gt;%\n  transmute(across(oracle:stepdown, ~ . / truth - 1)) |&gt; \n  pivot_longer(\n    everything(), \n    names_to = \"method\", \n    values_to = \"mse\"\n  ) |&gt; \n  ggplot(aes(method, mse, fill = method)) +\n  geom_boxplot(notch = TRUE) +\n  geom_hline(yintercept = 0, linewidth = 2) +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(\n    labels = scales::label_percent()\n  ) +\n  ylab(\"% increase in mse relative\\n to the truth\")"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#meta-lecture",
    "href": "schedule/slides/09-l1-penalties.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "09 L1 penalties",
    "text": "09 L1 penalties\nStat 406\nDaniel J. McDonald\nLast modified – 02 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#last-time",
    "href": "schedule/slides/09-l1-penalties.html#last-time",
    "title": "UBC Stat406 2023W",
    "section": "Last time",
    "text": "Last time\n\nRidge regression\n\n\\(\\min \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2 \\st \\snorm{\\beta}_2^2 \\leq s\\)\n\nBest (in sample) linear regression model of size \\(s\\)\n\n\\(\\min \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 \\st \\snorm{\\beta}_0 \\leq s\\)\n\n\n\\(\\snorm{\\beta}_0\\) is the number of nonzero elements in \\(\\beta\\)\nFinding the “best” linear model (of size \\(s\\), among these predictors, in sample) is a nonconvex optimization problem (In fact, it is NP-hard)\nRidge regression is convex (easy to solve), but doesn’t do variable selection\nCan we somehow “interpolate” to get both?"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#geometry-of-convexity",
    "href": "schedule/slides/09-l1-penalties.html#geometry-of-convexity",
    "title": "UBC Stat406 2023W",
    "section": "Geometry of convexity",
    "text": "Geometry of convexity\n\n\nCode\nlibrary(mvtnorm)\nnormBall &lt;- function(q = 1, len = 1000) {\n  tg &lt;- seq(0, 2 * pi, length = len)\n  out &lt;- data.frame(x = cos(tg)) %&gt;%\n    mutate(b = (1 - abs(x)^q)^(1 / q), bm = -b) %&gt;%\n    gather(key = \"lab\", value = \"y\", -x)\n  out$lab &lt;- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipseData &lt;- function(n = 100, xlim = c(-2, 3), ylim = c(-2, 3),\n                        mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  df &lt;- expand.grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)\n  )\n  df$z &lt;- dmvnorm(df, mean, Sigma)\n  df\n}\n\nlballmax &lt;- function(ed, q = 1, tol = 1e-6) {\n  ed &lt;- filter(ed, x &gt; 0, y &gt; 0)\n  for (i in 1:20) {\n    ff &lt;- abs((ed$x^q + ed$y^q)^(1 / q) - 1) &lt; tol\n    if (sum(ff) &gt; 0) break\n    tol &lt;- 2 * tol\n  }\n  best &lt;- ed[ff, ]\n  best[which.max(best$z), ]\n}\n\nnbs &lt;- list()\nnbs[[1]] &lt;- normBall(0, 1)\nqs &lt;- c(.5, .75, 1, 1.5, 2)\nfor (ii in 2:6) nbs[[ii]] &lt;- normBall(qs[ii - 1])\nnbs &lt;- bind_rows(nbs)\nnbs$lab &lt;- factor(nbs$lab, levels = unique(nbs$lab))\nseg &lt;- data.frame(\n  lab = levels(nbs$lab)[1],\n  x0 = c(-1, 0), x1 = c(1, 0), y0 = c(0, -1), y1 = c(0, 1)\n)\nlevels(seg$lab) &lt;- levels(nbs$lab)\nggplot(nbs, aes(x, y)) +\n  geom_path(size = 1.2) +\n  facet_wrap(~lab, labeller = label_parsed) +\n  geom_segment(data = seg, aes(x = x0, xend = x1, y = y0, yend = y1), size = 1.2) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  coord_equal() +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  scale_y_continuous(breaks = c(-1, 0, 1)) +\n  geom_vline(xintercept = 0, size = .5) +\n  geom_hline(yintercept = 0, size = .5) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2]))"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#the-best-of-both-worlds",
    "href": "schedule/slides/09-l1-penalties.html#the-best-of-both-worlds",
    "title": "UBC Stat406 2023W",
    "section": "The best of both worlds",
    "text": "The best of both worlds\n\n\nCode\nnb &lt;- normBall(1)\ned &lt;- ellipseData()\nbols &lt;- data.frame(x = 1, y = 1)\nbhat &lt;- lballmax(ed, 1)\nggplot(nb, aes(x, y)) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  geom_label(\n    data = bols, mapping = aes(label = bquote(\"hat(beta)[ols]\")), parse = TRUE,\n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  geom_label(\n    data = bhat, mapping = aes(label = bquote(\"hat(beta)[s]^L\")), parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )\n\n\n\nThis regularization set…\n\n… is convex (computationally efficient)\n… has corners (performs variable selection)"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#ell_1-regularized-regression",
    "href": "schedule/slides/09-l1-penalties.html#ell_1-regularized-regression",
    "title": "UBC Stat406 2023W",
    "section": "\\(\\ell_1\\)-regularized regression",
    "text": "\\(\\ell_1\\)-regularized regression\nKnown as\n\n“lasso”\n“basis pursuit”\n\nThe estimator satisfies\n\\[\\blt = \\argmin_{ \\snorm{\\beta}_1 \\leq s}  \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2\\]\nIn its corresponding Lagrangian dual form:\n\\[\\bll = \\argmin_{\\beta} \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1\\]"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#lasso",
    "href": "schedule/slides/09-l1-penalties.html#lasso",
    "title": "UBC Stat406 2023W",
    "section": "Lasso",
    "text": "Lasso\nWhile the ridge solution can be easily computed\n\\[\\brl = \\argmin_{\\beta} \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_2^2 = (\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1} \\X^{\\top}\\y\\]\nthe lasso solution\n\\[\\bll = \\argmin_{\\beta} \\frac 1n\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1 = \\; ??\\]\ndoesn’t have a closed-form solution.\nHowever, because the optimization problem is convex, there exist efficient algorithms for computing it\n\n\nThe best are Iterative Soft Thresholding or Coordinate Descent. Gradient Descent doesn’t work very well in practice."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#coefficient-path-ridge-vs-lasso",
    "href": "schedule/slides/09-l1-penalties.html#coefficient-path-ridge-vs-lasso",
    "title": "UBC Stat406 2023W",
    "section": "Coefficient path: ridge vs lasso",
    "text": "Coefficient path: ridge vs lasso\n\n\nCode\nlibrary(glmnet)\ndata(prostate, package = \"ElemStatLearn\")\nX &lt;- prostate |&gt; dplyr::select(-train, -lpsa) |&gt;  as.matrix()\nY &lt;- prostate$lpsa\nlasso &lt;- glmnet(x = X, y = Y) # alpha = 1 by default\nridge &lt;- glmnet(x = X, y = Y, alpha = 0)\nop &lt;- par()\n\n\n\npar(mfrow = c(1, 2), mar = c(5, 3, 5, .1))\nplot(lasso, main = \"Lasso\")\nplot(ridge, main = \"Ridge\")"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#same-but-against-lambda",
    "href": "schedule/slides/09-l1-penalties.html#same-but-against-lambda",
    "title": "UBC Stat406 2023W",
    "section": "Same but against Lambda",
    "text": "Same but against Lambda\n\npar(mfrow = c(1, 2), mar = c(5, 3, 5, .1))\nplot(lasso, main = \"Lasso\", xvar = \"lambda\")\nplot(ridge, main = \"Ridge\", xvar = \"lambda\")"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#additional-intuition-for-why-lasso-selects-variables",
    "href": "schedule/slides/09-l1-penalties.html#additional-intuition-for-why-lasso-selects-variables",
    "title": "UBC Stat406 2023W",
    "section": "Additional intuition for why Lasso selects variables",
    "text": "Additional intuition for why Lasso selects variables\nSuppose, for a particular \\(\\lambda\\), I have solutions for \\(\\widehat{\\beta}_k\\), \\(k = 1,\\ldots,j-1, j+1,\\ldots,p\\).\nLet \\(\\widehat{\\y}_{-j} = \\X_{-j}\\widehat{\\beta}_{-j}\\), and assume WLOG \\(\\overline{\\X}_k = 0\\), \\(\\X_k^\\top\\X_k = 1\\ \\forall k\\)\nOne can show that:\n\\[\n\\widehat{\\beta}_j = S\\left(\\mathbf{X}^\\top_j(\\y - \\widehat{\\y}_{-j}),\\ \\lambda\\right).\n\\]\n\\[\nS(z, \\gamma) = \\textrm{sign}(z)(|z| - \\gamma)_+ = \\begin{cases} z - \\gamma & z &gt; \\gamma\\\\\nz + \\gamma & z &lt; -\\gamma \\\\ 0 & |z| \\leq \\gamma \\end{cases}\n\\]\n\nIterating over this is called coordinate descent and gives the solution\n\n\n\n\nIf I were told all the other coefficient estimates.\nThen to find this one, I’d shrink when the gradient is big, or set to 0 if it gets too small.\n\n\n\nSee for example, https://doi.org/10.18637/jss.v033.i01"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#packages",
    "href": "schedule/slides/09-l1-penalties.html#packages",
    "title": "UBC Stat406 2023W",
    "section": "Packages",
    "text": "Packages\nThere are two main R implementations for finding lasso\n{glmnet}: lasso = glmnet(X, Y, alpha=1).\n\nSetting alpha = 0 gives ridge regression (as does lm.ridge in the MASS package)\nSetting alpha \\(\\in (0,1)\\) gives a method called the “elastic net” which combines ridge regression and lasso, more on that next lecture.\nIf you don’t specify alpha, it does lasso\n\n{lars}: lars = lars(X, Y)\n\nlars() also does other things called “Least angle” and “forward stagewise” in addition to “forward stepwise” regression\nThe path returned by lars() is more useful than that returned by glmnet().\n\n\nBut you should use {glmnet}."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#choosing-the-lambda",
    "href": "schedule/slides/09-l1-penalties.html#choosing-the-lambda",
    "title": "UBC Stat406 2023W",
    "section": "Choosing the \\(\\lambda\\)",
    "text": "Choosing the \\(\\lambda\\)\nYou have to choose \\(\\lambda\\) in lasso or in ridge regression\nlasso selects variables (by setting coefficients to zero), but the value of \\(\\lambda\\) determines how many/which.\nAll of these packages come with CV built in.\nHowever, the way to do it differs from package to package"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "href": "schedule/slides/09-l1-penalties.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "title": "UBC Stat406 2023W",
    "section": "{glmnet} version (same procedure for lasso or ridge)",
    "text": "{glmnet} version (same procedure for lasso or ridge)\n\nlasso &lt;- cv.glmnet(X, Y) # estimate full model and CV no good reason to call glmnet() itself\n# 2. Look at the CV curve. If the dashed lines are at the boundaries, redo and adjust lambda\nlambda_min &lt;- lasso$lambda.min # the value, not the location (or use lasso$lambda.1se)\ncoeffs &lt;- coefficients(lasso, s = \"lambda.min\") # s can be string or a number\npreds &lt;- predict(lasso, newx = X, s = \"lambda.1se\") # must supply `newx`\n\n\n\\(\\widehat{R}_{CV}\\) is an estimator of \\(R_n\\), it has bias and variance\nBecause we did CV, we actually have 10 \\(\\widehat{R}\\) values, 1 per split.\nCalculate the mean (that’s what we’ve been using), but what about SE?"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#section",
    "href": "schedule/slides/09-l1-penalties.html#section",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "par(mfrow = c(1, 2), mar = c(5, 3, 3, 0))\nplot(lasso) # a plot method for the cv fit\nplot(lasso$glmnet.fit) # the glmnet.fit == glmnet(X,Y)\nabline(v = colSums(abs(coef(lasso$glmnet.fit)[-1, drop(lasso$index)])), lty = 2)"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#paths-with-chosen-lambda",
    "href": "schedule/slides/09-l1-penalties.html#paths-with-chosen-lambda",
    "title": "UBC Stat406 2023W",
    "section": "Paths with chosen lambda",
    "text": "Paths with chosen lambda\n\nridge &lt;- cv.glmnet(X, Y, alpha = 0, lambda.min.ratio = 1e-10) # added to get a minimum\npar(mfrow = c(1, 4))\nplot(ridge, main = \"Ridge\")\nplot(lasso, main = \"Lasso\")\nplot(ridge$glmnet.fit, main = \"Ridge\")\nabline(v = sum(abs(coef(ridge)))) # defaults to `lambda.1se`\nplot(lasso$glmnet.fit, main = \"Lasso\")\nabline(v = sum(abs(coef(lasso)))) # again, `lambda.1se` unless told otherwise"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#degrees-of-freedom",
    "href": "schedule/slides/09-l1-penalties.html#degrees-of-freedom",
    "title": "UBC Stat406 2023W",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nLasso is not a linear smoother. There is no matrix \\(S\\) such that \\(\\widehat{\\y} = \\mathbf{S}\\y\\) for the predicted values from lasso.\n\nWe can’t use cv_nice().\nWe don’t have \\(\\tr{\\mathbf{S}} = \\textrm{df}\\) because there is no \\(\\mathbf{S}\\).\n\nHowever,\n\nOne can show that \\(\\textrm{df}_\\lambda = \\E[\\#(\\widehat{\\beta}_\\lambda \\neq 0)] = \\E[||\\widehat{\\beta}_\\lambda||_0]\\)\nThe proof is PhD-level material\n\nNote that the \\(\\widehat{\\textrm{df}}_\\lambda\\) is shown on the CV plot and that lasso.glmnet$glmnet.fit$df contains this value for all \\(\\lambda\\)."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#other-flavours",
    "href": "schedule/slides/09-l1-penalties.html#other-flavours",
    "title": "UBC Stat406 2023W",
    "section": "Other flavours",
    "text": "Other flavours\n\nThe elastic net\n\ngenerally used for correlated variables that combines a ridge/lasso penalty. Use glmnet(..., alpha = a) (0 &lt; a &lt; 1).\n\nGrouped lasso\n\nwhere variables are included or excluded in groups. Required for factors (1-hot encoding)\n\nRelaxed lasso\n\nTakes the estimated model from lasso and fits the full least squares solution on the selected covariates (less bias, more variance). Use glmnet(..., relax = TRUE).\n\nDantzig selector\n\na slightly modified version of the lasso"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#lasso-cinematic-universe",
    "href": "schedule/slides/09-l1-penalties.html#lasso-cinematic-universe",
    "title": "UBC Stat406 2023W",
    "section": "Lasso cinematic universe",
    "text": "Lasso cinematic universe\n\n\n\nSCAD\n\na non-convex version of lasso that adds a more severe variable selection penalty\n\n\\(\\sqrt{\\textrm{lasso}}\\)\n\nclaims to be tuning parameter free (but isn’t). Uses \\(\\Vert\\cdot\\Vert_2\\) instead of \\(\\Vert\\cdot\\Vert_1\\) for the loss.\n\nGeneralized lasso\n\nAdds various additional matrices to the penalty term (e.g. \\(\\Vert D\\beta\\Vert_1\\)).\n\nArbitrary combinations\n\ncombine the above penalties in your favourite combinations"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#warnings-on-regularized-regression",
    "href": "schedule/slides/09-l1-penalties.html#warnings-on-regularized-regression",
    "title": "UBC Stat406 2023W",
    "section": "Warnings on regularized regression",
    "text": "Warnings on regularized regression\n\nThis isn’t a method unless you say how to choose \\(\\lambda\\).\nThe intercept is never penalized. Adds an extra degree-of-freedom.\nPredictor scaling is very important.\nDiscrete predictors need groupings.\nCentering the predictors is important\n(These all work with other likelihoods.)\n\n\nSoftware handles most of these automatically, but not always. (No Lasso with factor predictors.)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#meta-lecture",
    "href": "schedule/slides/11-kernel-smoothers.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "11 Local methods",
    "text": "11 Local methods\nStat 406\nDaniel J. McDonald\nLast modified – 27 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#last-time",
    "href": "schedule/slides/11-kernel-smoothers.html#last-time",
    "title": "UBC Stat406 2023W",
    "section": "Last time…",
    "text": "Last time…\nWe looked at feature maps as a way to do nonlinear regression.\nWe used new “features” \\(\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)\\)\nNow we examine an alternative\nSuppose I just look at the “neighbours” of some point (based on the \\(x\\)-values)\nI just average the \\(y\\)’s at those locations together"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#lets-use-3-neighbors",
    "href": "schedule/slides/11-kernel-smoothers.html#lets-use-3-neighbors",
    "title": "UBC Stat406 2023W",
    "section": "Let’s use 3 neighbors",
    "text": "Let’s use 3 neighbors\n\n\nCode\nlibrary(cowplot)\ndata(arcuate, package = \"Stat406\")\nset.seed(406406)\narcuate_unif &lt;- arcuate |&gt; slice_sample(n = 40) |&gt; arrange(position)\npt &lt;- 15\nnn &lt;-  3\nseq_range &lt;- function(x, n = 101) seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out = n)\nneibs &lt;- sort.int(abs(arcuate_unif$position - arcuate_unif$position[pt]), index.return = TRUE)$ix[1:nn]\narcuate_unif$neighbours = seq_len(40) %in% neibs\ng1 &lt;- ggplot(arcuate_unif, aes(position, fa, colour = neighbours)) + \n  geom_point() +\n  scale_colour_manual(values = c(blue, red)) + \n  geom_vline(xintercept = arcuate_unif$position[pt], colour = red) + \n  annotate(\"rect\", fill = red, alpha = .25, ymin = -Inf, ymax = Inf,\n           xmin = min(arcuate_unif$position[neibs]), \n           xmax = max(arcuate_unif$position[neibs])\n  ) +\n  theme(legend.position = \"none\")\ng2 &lt;- ggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(\n    data = tibble(\n      position = seq_range(arcuate_unif$position),\n      fa = FNN::knn.reg(\n        arcuate_unif$position, matrix(position, ncol = 1),\n        y = arcuate_unif$fa\n      )$pred\n    ),\n    colour = orange, linewidth = 2\n  )\nplot_grid(g1, g2, ncol = 2)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#knn",
    "href": "schedule/slides/11-kernel-smoothers.html#knn",
    "title": "UBC Stat406 2023W",
    "section": "KNN",
    "text": "KNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(arcuate, package = \"Stat406\")\nlibrary(FNN)\narcuate_unif &lt;- arcuate |&gt; \n  slice_sample(n = 40) |&gt; \n  arrange(position) \n\nnew_position &lt;- seq(\n  min(arcuate_unif$position), \n  max(arcuate_unif$position),\n  length.out = 101\n)\n\nknn3 &lt;- knn.reg(\n  train = arcuate_unif$position, \n  test = matrix(arcuate_unif$position, ncol = 1), \n  y = arcuate_unif$fa, \n  k = 3\n)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#this-method-is-k-nearest-neighbors.",
    "href": "schedule/slides/11-kernel-smoothers.html#this-method-is-k-nearest-neighbors.",
    "title": "UBC Stat406 2023W",
    "section": "This method is \\(K\\)-nearest neighbors.",
    "text": "This method is \\(K\\)-nearest neighbors.\nIt’s a linear smoother just like in previous lectures: \\(\\widehat{\\mathbf{y}} = \\mathbf{S} \\mathbf{y}\\) for some matrix \\(S\\).\nYou should imagine what \\(\\mathbf{S}\\) looks like.\nWhat is the effective degrees of freedom of KNN?\nKNN averages the neighbors with equal weight.\nBut some neighbors are “closer” than other neighbors."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#local-averages",
    "href": "schedule/slides/11-kernel-smoothers.html#local-averages",
    "title": "UBC Stat406 2023W",
    "section": "Local averages",
    "text": "Local averages\nInstead of choosing the number of neighbors to average, we can average any observations within a certain distance.\n\n\nThe boxes have width 30."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#what-is-a-kernel-smoother",
    "href": "schedule/slides/11-kernel-smoothers.html#what-is-a-kernel-smoother",
    "title": "UBC Stat406 2023W",
    "section": "What is a “kernel” smoother?",
    "text": "What is a “kernel” smoother?\n\nThe mathematics:\n\n\nA kernel is any function \\(K\\) such that for any \\(u\\), \\(K(u) \\geq 0\\), \\(\\int du K(u)=1\\) and \\(\\int uK(u)du=0\\).\n\n\nThe idea: a kernel is a nice way to take weighted averages. The kernel function gives the weights.\nThe previous example is called the boxcar kernel."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-with-the-boxcar",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-with-the-boxcar",
    "title": "UBC Stat406 2023W",
    "section": "Smoothing with the boxcar",
    "text": "Smoothing with the boxcar\n\n\nCode\ntestpts &lt;- seq(0, 200, length.out = 101)\ndmat &lt;- abs(outer(testpts, arcuate_unif$position, \"-\"))\nS &lt;- (dmat &lt; 15)\nS &lt;- S / rowSums(S)\nboxcar &lt;- tibble(position = testpts, fa = S %*% arcuate_unif$fa)\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(data = boxcar, colour = orange)\n\n\n\nThis one gives the same non-zero weight to all points within \\(\\pm 15\\) range."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#other-kernels",
    "href": "schedule/slides/11-kernel-smoothers.html#other-kernels",
    "title": "UBC Stat406 2023W",
    "section": "Other kernels",
    "text": "Other kernels\nMost of the time, we don’t use the boxcar because the weights are weird. (constant)\nA more common one is the Gaussian kernel:\n\n\nCode\ngaussian_kernel &lt;- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 7.5) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n\n\n\nFor the plot, I made \\(\\sigma=7.5\\).\nNow the weights “die away” for points farther from where we’re predicting. (but all nonzero!!)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#other-kernels-1",
    "href": "schedule/slides/11-kernel-smoothers.html#other-kernels-1",
    "title": "UBC Stat406 2023W",
    "section": "Other kernels",
    "text": "Other kernels\nWhat if I made \\(\\sigma=15\\)?\n\n\nCode\ngaussian_kernel &lt;- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 15) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n\n\n\nBefore, points far from \\(x_{15}\\) got very small weights, now they have more influence.\nFor the Gaussian kernel, \\(\\sigma\\) determines something like the “range” of the smoother."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#many-gaussians",
    "href": "schedule/slides/11-kernel-smoothers.html#many-gaussians",
    "title": "UBC Stat406 2023W",
    "section": "Many Gaussians",
    "text": "Many Gaussians\nThe following code creates \\(\\mathbf{S}\\) for Gaussian kernel smoothers with different \\(\\sigma\\)\n\ndmat &lt;- as.matrix(dist(x))\nSgauss &lt;- function(sigma) {\n  gg &lt;- dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg), \"/\") # make the rows sum to 1.\n}\n\n\n\nCode\nSgauss &lt;- function(sigma) {\n  gg &lt;-  dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg),'/') # make the rows sum to 1.\n}\nboxcar$S15 = with(arcuate_unif, Sgauss(15) %*% fa)\nboxcar$S08 = with(arcuate_unif, Sgauss(8) %*% fa)\nboxcar$S30 = with(arcuate_unif, Sgauss(30) %*% fa)\nbc = boxcar %&gt;% select(position, S15, S08, S30) %&gt;% \n  pivot_longer(-position, names_to = \"Sigma\")\nggplot(arcuate_unif, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_line(data = bc, aes(position, value, colour = Sigma), linewidth = 1.5) +\n  scale_colour_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#the-bandwidth",
    "href": "schedule/slides/11-kernel-smoothers.html#the-bandwidth",
    "title": "UBC Stat406 2023W",
    "section": "The bandwidth",
    "text": "The bandwidth\n\nChoosing \\(\\sigma\\) is very important.\nThis “range” parameter is called the bandwidth.\nIt is way more important than which kernel you use.\nThe default kernel in ksmooth() is something called ‘Epanechnikov’:\n\n\nepan &lt;- function(x) 3/4 * (1 - x^2) * (abs(x) &lt; 1)\nggplot(data.frame(x = c(-2, 2)), aes(x)) + stat_function(fun = epan, colour = green, linewidth = 2)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#choosing-the-bandwidth",
    "href": "schedule/slides/11-kernel-smoothers.html#choosing-the-bandwidth",
    "title": "UBC Stat406 2023W",
    "section": "Choosing the bandwidth",
    "text": "Choosing the bandwidth\nAs we have discussed, kernel smoothing (and KNN) are linear smoothers\n\\[\\widehat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}\\]\nThe degrees of freedom is \\(\\textrm{tr}(\\mathbf{S})\\)\nTherefore we can use our model selection criteria from before\n\nUnfortunately, these don’t satisfy the “technical condition”, so cv_nice() doesn’t give LOO-CV"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data",
    "title": "UBC Stat406 2023W",
    "section": "Smoothing the full Lidar data",
    "text": "Smoothing the full Lidar data\n\nar &lt;- arcuate |&gt; slice_sample(n = 200)\n\ngcv &lt;- function(y, S) {\n  yhat &lt;- S %*% y\n  mean( (y - yhat)^2 / (1 - mean(diag(S)))^2 )\n}\n\ndmat &lt;- as.matrix(dist(ar$position))\nsigmas &lt;- 10^(seq(log10(300), log10(.3), length = 100))\n\ngcvs &lt;- map_dbl(sigmas, ~ gcv(ar$fa, Sgauss(.x)))\nbest_s &lt;- sigmas[which.min(gcvs)]\n\nar$smoothed &lt;- Sgauss(best_s) %*% ar$fa"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data-1",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data-1",
    "title": "UBC Stat406 2023W",
    "section": "Smoothing the full Lidar data",
    "text": "Smoothing the full Lidar data\n\n\nCode\ng3 &lt;- ggplot(data.frame(sigma = sigmas, gcv = gcvs), aes(sigma, gcv)) +\n  geom_point(colour = blue) +\n  geom_vline(xintercept = best_s, colour = red) +\n  scale_x_log10() +\n  xlab(sprintf(\"Sigma, best is sig = %.2f\", best_s))\ng4 &lt;- ggplot(ar, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(aes(y = smoothed), colour = orange, linewidth = 2)\nplot_grid(g3, g4, nrow = 1)\n\n\n\nI considered \\(\\sigma \\in [0.3,\\ 300]\\) and used \\(3.97\\).\nIt’s too wiggly, to my eye. Typical for GCV."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-manually",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-manually",
    "title": "UBC Stat406 2023W",
    "section": "Smoothing manually",
    "text": "Smoothing manually\nI did Kernel Smoothing “manually”\n\nFor a fixed bandwidth\nCompute the smoothing matrix\nMake the predictions\nRepeat and compute GCV\n\nThe point is to “show how it works”. It’s also really easy."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#r-functions-packages",
    "href": "schedule/slides/11-kernel-smoothers.html#r-functions-packages",
    "title": "UBC Stat406 2023W",
    "section": "R functions / packages",
    "text": "R functions / packages\nThere are a number of other ways to do this in R\n\nloess()\nksmooth()\nKernSmooth::locpoly()\nmgcv::gam()\nnp::npreg()\n\nThese have tricks and ways of doing CV and other things automatically.\n\nNote\n\nAll I needed was the distance matrix dist(x).\n\n\nGiven ANY distance function\n\n\nsay, \\(d(\\mathbf{x}_i, \\mathbf{x}_j) = \\Vert\\mathbf{x}_i - \\mathbf{x}_j\\Vert_2 + I(x_{i,3} = x_{j,3})\\)\n\n\nI can use these methods."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#meta-lecture",
    "href": "schedule/slides/13-gams-trees.html#meta-lecture",
    "title": "UBC Stat406 2023W",
    "section": "13 GAMs and Trees",
    "text": "13 GAMs and Trees\nStat 406\nDaniel J. McDonald\nLast modified – 02 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#gams",
    "href": "schedule/slides/13-gams-trees.html#gams",
    "title": "UBC Stat406 2023W",
    "section": "GAMs",
    "text": "GAMs\nLast time we discussed smoothing in multiple dimensions.\nHere we introduce the concept of GAMs (Generalized Additive Models)\nThe basic idea is to imagine that the response is the sum of some functions of the predictors:\n\\[\\Expect{Y \\given X=x} = \\beta_0 + f_1(x_{1})+\\cdots+f_p(x_{p}).\\]\nNote that OLS is a GAM (take \\(f_j(x_{j})=\\beta_j x_{j}\\)):\n\\[\\Expect{Y \\given X=x} = \\beta_0 + \\beta_1 x_{1}+\\cdots+\\beta_p x_{p}.\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#gams-1",
    "href": "schedule/slides/13-gams-trees.html#gams-1",
    "title": "UBC Stat406 2023W",
    "section": "Gams",
    "text": "Gams\nThese work by estimating each \\(f_i\\) using basis expansions in predictor \\(i\\)\nThe algorithm for fitting these things is called “backfitting”:\n\nCenter \\(\\y\\) and \\(\\X\\).\nHold \\(f_k\\) for all \\(k\\neq j\\) fixed, and regress \\(f_j\\) on the partial residuals using your favorite smoother.\nRepeat for \\(1\\leq j\\leq p\\).\nRepeat steps 2 and 3 until the estimated functions “stop moving” (iterate)\nReturn the results."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example",
    "href": "schedule/slides/13-gams-trees.html#very-small-example",
    "title": "UBC Stat406 2023W",
    "section": "Very small example",
    "text": "Very small example\n\nlibrary(mgcv)\nset.seed(12345)\nn &lt;- 500\nsimple &lt;- tibble(\n  x1 = runif(n, 0, 2*pi),\n  x2 = runif(n),\n  y = 5 + 2 * sin(x1) + 8 * sqrt(x2) + rnorm(n, sd = .25)\n)\n\npivot_longer(simple, -y, names_to = \"predictor\", values_to = \"x\") |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(col = blue) +\n  facet_wrap(~predictor, scales = \"free_x\")"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example-1",
    "href": "schedule/slides/13-gams-trees.html#very-small-example-1",
    "title": "UBC Stat406 2023W",
    "section": "Very small example",
    "text": "Very small example\nSmooth each coordinate independently\n\nex_smooth &lt;- gam(y ~ s(x1) + s(x2), data = simple)\n# s(z) means \"smooth\" z, uses spline basis for each with ridge penalty, GCV\nplot(ex_smooth, pages = 1, scale = 0, shade = TRUE, \n     resid = TRUE, se = 2, las = 1)\n\nhead(coef(ex_smooth))\n\n(Intercept)     s(x1).1     s(x1).2     s(x1).3     s(x1).4     s(x1).5 \n 10.2070490  -4.5764100   0.7117161   0.4548928   0.5535001  -0.2092996 \n\nex_smooth$gcv.ubre\n\n    GCV.Cp \n0.06619721"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#wherefore-gams",
    "href": "schedule/slides/13-gams-trees.html#wherefore-gams",
    "title": "UBC Stat406 2023W",
    "section": "Wherefore GAMs?",
    "text": "Wherefore GAMs?\nIf\n\\(\\Expect{Y \\given X=x} = \\beta_0 + f_1(x_{1})+\\cdots+f_p(x_{p}),\\)\nthen\n\\(\\textrm{MSE}(\\hat f) = \\frac{Cp}{n^{4/5}} + \\sigma^2.\\)\n\nExponent no longer depends on \\(p\\). Converges faster. (If the truth is additive.)\nYou could also use the same methods to include “some” interactions like\n\n\\[\\begin{aligned}&\\Expect{Y \\given X=x}\\\\ &= \\beta_0 + f_{12}(x_{1},\\ x_{2})+f_3(x_3)+\\cdots+f_p(x_{p}),\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example-2",
    "href": "schedule/slides/13-gams-trees.html#very-small-example-2",
    "title": "UBC Stat406 2023W",
    "section": "Very small example",
    "text": "Very small example\nSmooth two coordinates together\n\nex_smooth2 &lt;- gam(y ~ s(x1, x2), data = simple)\nplot(ex_smooth2,\n  scheme = 2, scale = 0, shade = TRUE,\n  resid = TRUE, se = 2, las = 1\n)"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#regression-trees",
    "href": "schedule/slides/13-gams-trees.html#regression-trees",
    "title": "UBC Stat406 2023W",
    "section": "Regression trees",
    "text": "Regression trees\nTrees involve stratifying or segmenting the predictor space into a number of simple regions.\nTrees are simple and useful for interpretation.\nBasic trees are not great at prediction.\nModern methods that use trees are much better (Module 4)"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#regression-trees-1",
    "href": "schedule/slides/13-gams-trees.html#regression-trees-1",
    "title": "UBC Stat406 2023W",
    "section": "Regression trees",
    "text": "Regression trees\nRegression trees estimate piece-wise constant functions\nThe slabs are axis-parallel rectangles \\(R_1,\\ldots,R_K\\) based on \\(\\X\\)\nIn each region, we average the \\(y_i\\)’s: \\(\\hat\\mu_1,\\ldots,\\hat\\mu_k\\)\nMinimize \\(\\sum_{k=1}^K \\sum_{i=1}^n (y_i-\\mu_k)^2\\) over \\(R_k,\\mu_k\\) for \\(k\\in \\{1,\\ldots,K\\}\\)\n\nThis sounds more complicated than it is.\nThe minimization is performed greedily (like forward stepwise regression)."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#mobility-data",
    "href": "schedule/slides/13-gams-trees.html#mobility-data",
    "title": "UBC Stat406 2023W",
    "section": "Mobility data",
    "text": "Mobility data\n\nbigtree &lt;- tree(Mobility ~ ., data = mob)\nsmalltree &lt;- prune.tree(bigtree, k = .09)\ndraw.tree(smalltree, digits = 2)\n\n\nThis is called the dendrogram"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#partition-view",
    "href": "schedule/slides/13-gams-trees.html#partition-view",
    "title": "UBC Stat406 2023W",
    "section": "Partition view",
    "text": "Partition view\n\nmob$preds &lt;- predict(smalltree)\npar(mfrow = c(1, 2), mar = c(5, 3, 0, 0))\ndraw.tree(smalltree, digits = 2)\ncols &lt;- viridisLite::viridis(20, direction = -1)[cut(log(mob$Mobility), 20)]\nplot(mob$Black, mob$Commute,\n  pch = 19, cex = .4, bty = \"n\", las = 1, col = cols,\n  ylab = \"Commute time\", xlab = \"% Black\"\n)\npartition.tree(smalltree, add = TRUE, ordvars = c(\"Black\", \"Commute\"))\n\n\nWe predict all observations in a region with the same value.\n\\(\\bullet\\) The three regions correspond to the leaves of the tree."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#section-1",
    "href": "schedule/slides/13-gams-trees.html#section-1",
    "title": "UBC Stat406 2023W",
    "section": "",
    "text": "draw.tree(bigtree, digits = 2)\n\n\nTerminology\nWe call each split or end point a node. Each terminal node is referred to as a leaf.\nThe interior nodes lead to branches."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#advantages-and-disadvantages-of-trees",
    "href": "schedule/slides/13-gams-trees.html#advantages-and-disadvantages-of-trees",
    "title": "UBC Stat406 2023W",
    "section": "Advantages and disadvantages of trees",
    "text": "Advantages and disadvantages of trees\n🎉 Trees are very easy to explain (much easier than even linear regression).\n🎉 Some people believe that decision trees mirror human decision.\n🎉 Trees can easily be displayed graphically no matter the dimension of the data.\n🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.\n💩 Trees aren’t very good at prediction.\n💩 Full trees badly overfit, so we “prune” them using CV\n\nWe’ll talk more about trees next module for Classification."
  }
]