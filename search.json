[
  {
    "objectID": "schedule/slides/00-r-review.html#section",
    "href": "schedule/slides/00-r-review.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour",
    "text": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 11 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#housekeeping",
    "href": "schedule/slides/00-r-review.html#housekeeping",
    "title": "UBC Stat406 2024W",
    "section": "Housekeeping",
    "text": "Housekeeping\n\nIntroduction\nMy office hours will be in (room redacted) after class today only\n\nI’m trying to book this room for future OHs\n\nThis week’s lab is due Friday 23:00 (to help with setup issues)\n\nonly for this week. After that, 23:00 on night of the lab\n\nReminders\n\n4 students still haven’t done Quiz 0\n25 students need to accept the GitHub invite\n8 students need to enroll in a lab section"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "href": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "title": "UBC Stat406 2024W",
    "section": "Tour of Rstudio",
    "text": "Tour of Rstudio\nThings to note\n\nConsole\nTerminal\nScripts, .Rmd, Knit\nFiles, Projects\nGetting help\nEnvironment, Git"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "href": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "title": "UBC Stat406 2024W",
    "section": "R and the {tidyverse}",
    "text": "R and the {tidyverse}\n\n\n\n\nToday is going to be a whirlwind tour of R.\nIf you are new to R: read the first 4 chapters of Data Science: A First Introduction.\nIt’s available for free at https://datasciencebook.ca. It covers:\n\nData loading from .csv, Excel, database, and web sources\nData saving to .csv files\nData wrangling with tidyverse functions\nPlotting with ggplot"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures",
    "href": "schedule/slides/00-r-review.html#basic-data-structures",
    "title": "UBC Stat406 2024W",
    "section": "Basic data structures",
    "text": "Basic data structures\n\n\nVectors:\n\nx &lt;- c(1, 3, 4)\nx[1]\n\n[1] 1\n\nx[-1]\n\n[1] 3 4\n\nrev(x)\n\n[1] 4 3 1\n\nc(x, x)\n\n[1] 1 3 4 1 3 4\n\n\n\n\n\nMatrices:\n\nx &lt;- matrix(1:25, nrow = 5, ncol = 5)\nx[1,]\n\n[1]  1  6 11 16 21\n\nx[,-1]\n\n     [,1] [,2] [,3] [,4]\n[1,]    6   11   16   21\n[2,]    7   12   17   22\n[3,]    8   13   18   23\n[4,]    9   14   19   24\n[5,]   10   15   20   25\n\nx[c(1,3),  2:3]\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    8   13\n\n\n\nAll elements of a vector/matrix must be of the same type"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures-1",
    "href": "schedule/slides/00-r-review.html#basic-data-structures-1",
    "title": "UBC Stat406 2024W",
    "section": "Basic data structures",
    "text": "Basic data structures\n\n\nLists\n\n(l &lt;- list(\n  a = letters[1:2], \n  b = 1:4, \n  c = list(a = 1)))\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] 1 2 3 4\n\n$c\n$c$a\n[1] 1\n\nl$a\n\n[1] \"a\" \"b\"\n\nl$c$a\n\n[1] 1\n\nl[\"b\"] # compare to l[[\"b\"]] == l$b\n\n$b\n[1] 1 2 3 4\n\n\n\n\nData frames\n\n(dat &lt;- data.frame(\n  z = 1:5, \n  b = 6:10, \n  c = letters[1:5]))\n\n  z  b c\n1 1  6 a\n2 2  7 b\n3 3  8 c\n4 4  9 d\n5 5 10 e\n\nclass(dat)\n\n[1] \"data.frame\"\n\ndat$b\n\n[1]  6  7  8  9 10\n\ndat[1,]\n\n  z b c\n1 1 6 a\n\n\n\n\nLists can have multiple element types; data frames are lists of vectors"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tibbles",
    "href": "schedule/slides/00-r-review.html#tibbles",
    "title": "UBC Stat406 2024W",
    "section": "Tibbles",
    "text": "Tibbles\nThese are {tidyverse} data frames\n\n(dat2 &lt;- tibble(z = 1:5, b = z + 5, c = letters[z]))\n\n# A tibble: 5 × 3\n      z     b c    \n  &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n1     1     6 a    \n2     2     7 b    \n3     3     8 c    \n4     4     9 d    \n5     5    10 e    \n\nclass(dat2)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\n\nWe’ll return to classes in a moment. A tbl_df is a “subclass” of data.frame.\nAnything that data.frame can do, tbl_df can do (better).\nFor instance, the printing is more informative.\nAlso, you can construct one by referencing previously constructed columns."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#functions-in-r",
    "href": "schedule/slides/00-r-review.html#functions-in-r",
    "title": "UBC Stat406 2024W",
    "section": "Functions in R",
    "text": "Functions in R\nA function is a mapping from inputs to outputs, and is defined with the function keyword.\nThe function’s body is wrapped in curly braces, and its output is given by the return keyword (or the last evaluated statement)\n\nf &lt;- function(x, y){\n  x+y \n}\n\nf(3,5)\n\n[1] 8\n\n\n\nf &lt;- function(x, y){\n  return(x+y)\n}\n\nf(3,5)\n\n[1] 8"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#function-signatures",
    "href": "schedule/slides/00-r-review.html#function-signatures",
    "title": "UBC Stat406 2024W",
    "section": "Function Signatures",
    "text": "Function Signatures\n\n\nCode\nsig &lt;- sig::sig\n\n\n\nsig(lm)\n\nfn &lt;- function(formula, data, subset, weights, na.action, method = \"qr\", model\n  = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts =\n  NULL, offset, ...)\n\nsig(`+`)\n\nfn &lt;- function(e1, e2)\n\nsig(dplyr::filter)\n\nfn &lt;- function(.data, ..., .by = NULL, .preserve = FALSE)\n\nsig(stats::filter)\n\nfn &lt;- function(x, filter, method = c(\"convolution\", \"recursive\"), sides = 2,\n  circular = FALSE, init = NULL)\n\nsig(rnorm)\n\nfn &lt;- function(n, mean = 0, sd = 1)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "href": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "title": "UBC Stat406 2024W",
    "section": "These are all the same",
    "text": "These are all the same\n\nset.seed(12345)\nrnorm(3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(3, 0, 1)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(sd = 1, n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\nFunctions can have default values.\nYou may, but don’t have to, name the arguments\nIf you name them, you can pass them out of order (but you shouldn’t)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "title": "UBC Stat406 2024W",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nf &lt;- function(arg1, arg2, arg3 = 12, ...) {\n  stuff &lt;- arg1 * arg3\n  stuff2 &lt;- stuff + arg2\n  plot(arg1, stuff2, ...)\n  return(stuff2)\n}\nx &lt;- rnorm(100)\n\n\n\n\ny1 &lt;- f(x, 3, 15, col = 4, pch = 19)\n\n\n\n\n\n\n\nstr(y1)\n\n num [1:100] -3.8 12.09 -24.27 12.45 -1.14 ..."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "title": "UBC Stat406 2024W",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nSide effects are things a function changes in global scope\nOutputs can be assigned to variables\nA good example is the hist function\nYou have probably only seen the side effect which is to plot the histogram\n\n\nmy_histogram &lt;- hist(rnorm(1000))\n\n\n\n\n\n\n\n\n\n\n\nstr(my_histogram)\n\nList of 6\n $ breaks  : num [1:14] -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 ...\n $ counts  : int [1:13] 4 21 41 89 142 200 193 170 74 38 ...\n $ density : num [1:13] 0.008 0.042 0.082 0.178 0.284 0.4 0.386 0.34 0.148 0.076 ...\n $ mids    : num [1:13] -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ...\n $ xname   : chr \"rnorm(1000)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\nclass(my_histogram)\n\n[1] \"histogram\""
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "href": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "title": "UBC Stat406 2024W",
    "section": "When writing functions, program defensively, ensure behaviour",
    "text": "When writing functions, program defensively, ensure behaviour\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  x + 1\n}\n  \nincrementer(2)\n\n[1] 3\n\nincrementer(1:4)\n\n[1] 2 3 4 5\n\nincrementer(\"a\")\n\nError in x + 1: non-numeric argument to binary operator\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  stopifnot(is.numeric(x))\n  return(x + 1)\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): is.numeric(x) is not TRUE\n\n\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + 1\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): `x` must be numeric\n\nincrementer(2, -3) ## oops!\n\n[1] 3\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + inc_by\n}\nincrementer(2, -3)\n\n[1] -1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#unit-testing",
    "href": "schedule/slides/00-r-review.html#unit-testing",
    "title": "UBC Stat406 2024W",
    "section": "Unit Testing",
    "text": "Unit Testing\nWhen you write functions, test them!\nUse testthat: check a few usual values and corner cases\n\n\n\nlibrary(testthat)\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  if (!is.numeric(inc_by)) {\n    stop(\"`inc_by` must be numeric\")\n  }\n  x + inc_by\n}\nexpect_error(incrementer(\"a\"))\nexpect_equal(incrementer(1:3), 2:4)\nexpect_equal(incrementer(2, -3), -1)\nexpect_error(incrementer(1, \"b\"))\nexpect_identical(incrementer(1:3), 2:4)\n\nError: incrementer(1:3) not identical to 2:4.\nObjects equal but not identical\n\n\n\n\n\nis.integer(2:4)\n\n[1] TRUE\n\nis.integer(incrementer(1:3))\n\n[1] FALSE\n\nexpect_identical(incrementer(1:3, 1L), 2:4)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDon’t copy code; write a function. Validate your arguments. Write tests to check if inputs result in predicted outputs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#classes",
    "href": "schedule/slides/00-r-review.html#classes",
    "title": "UBC Stat406 2024W",
    "section": "Classes",
    "text": "Classes\n\n\nWe saw some of these earlier:\n\ntib &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100), \n  y = x1 + 2 * x2 + rnorm(100)\n)\nmdl &lt;- lm(y ~ ., data = tib )\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(mdl)\n\n[1] \"lm\"\n\n\nThe class allows for the use of “methods”\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n    -0.1742       1.0454       2.0470  \n\n\n\n\n\nR “knows what to do” when you print() an object of class \"lm\".\nprint() is called a “generic” function.\nYou can create “methods” that get dispatched.\nFor any generic, R looks for a method for the class.\nIf available, it calls that function."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "href": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "title": "UBC Stat406 2024W",
    "section": "Viewing the dispatch chain",
    "text": "Viewing the dispatch chain\n\nsloop::s3_dispatch(print(incrementer))\n\n=&gt; print.function\n * print.default\n\nsloop::s3_dispatch(print(tib))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(print(mdl))\n\n=&gt; print.lm\n * print.default"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#generic-methods",
    "href": "schedule/slides/00-r-review.html#generic-methods",
    "title": "UBC Stat406 2024W",
    "section": "Generic Methods",
    "text": "Generic Methods\nThere are lots of generic functions in R\nCommon ones are print(), summary(), and plot().\nAlso, lots of important statistical modelling concepts: residuals() coef()\n(In python, these work the opposite way: obj.residuals. The dot after the object accesses methods defined for that type of object. But the dispatch behaviour is less robust.)\n\nThe convention is that the specialized function is named method.class(), e.g., summary.lm().\nIf no specialized function is defined, R will try to use method.default().\n\nFor this reason, R programmers try to avoid . in names of functions or objects."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#wherefore-methods",
    "href": "schedule/slides/00-r-review.html#wherefore-methods",
    "title": "UBC Stat406 2024W",
    "section": "Wherefore methods?",
    "text": "Wherefore methods?\n\nThe advantage is that you don’t have to learn a totally new syntax to grab residuals or plot things\nYou just use residuals(mdl) whether mdl has class lm or any other class you expect to have residuals\nThe one draw-back is the help pages for the generic methods tend to be pretty vague\nCompare ?summary with ?summary.lm."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#different-environments",
    "href": "schedule/slides/00-r-review.html#different-environments",
    "title": "UBC Stat406 2024W",
    "section": "Different environments",
    "text": "Different environments\n(known as scope in other languages)\n\nThese are often tricky, but are very common.\nMost programming languages have this concept in one way or another.\nIn R code run in the Console produces objects in the “Global environment”\nYou can see what you create in the “Environment” tab.\nBut there’s lots of other stuff.\nMany packages are automatically loaded at startup, so you have access to the functions and data inside\n\nFor example mean(), lm(), plot(), iris (technically iris is lazy-loaded, meaning it’s not in memory until you call it, but it is available)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-1",
    "href": "schedule/slides/00-r-review.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Other packages require you to load them with library(pkg) before their functions are available.\nBut, you can call those functions by prefixing the package name ggplot2::ggplot().\nYou can also access functions that the package developer didn’t “export” for use with ::: like dplyr:::as_across_fn_call()\n\n\nThat is all about accessing “objects in package environments”"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "title": "UBC Stat406 2024W",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nAs one might expect, functions create an environment inside the function.\n\nz &lt;- 1\nfun &lt;- function(x) {\n  z &lt;- x\n  print(z)\n  invisible(z)\n}\nfun(14)\n\n[1] 14\n\n\n\nNon-trivial cases are data-masking environments.\n\ntib &lt;- tibble(x1 = rnorm(100),  x2 = rnorm(100),  y = x1 + 2 * x2)\nmdl &lt;- lm(y ~ x2, data = tib)\nx2\n\nError in eval(expr, envir, enclos): object 'x2' not found\n\n\n\nlm() looks “inside” the tib to find y and x2\nThe data variables are added to the lm() environment"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "title": "UBC Stat406 2024W",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nWhen Knit, .Rmd files run in their OWN environment.\nThey are run from top to bottom, with code chunks depending on previous\nThis makes them reproducible.\n\nObjects in your local environment are not available in the .Rmd\nObjects in the .Rmd are not available locally.\n\n\n\n\n\n\nTip\n\n\nThe most frequent error I see is:\n\nrunning chunks individually, 1-by-1, and it works\nKnitting, and it fails\n\nThe reason is almost always that the chunks refer to objects in the Environment that don’t exist in the .Rmd"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-2",
    "href": "schedule/slides/00-r-review.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "This error also happens because:\n\nlibrary() calls were made globally but not in the .Rmd\n\nso the packages aren’t loaded\n\npaths to data or other objects are not relative to the .Rmd in your file system\n\nthey must be\n\nCarefully keeping Labs / Assignments in their current location will help to avoid some of these.\n\n\n\n\n\n\n\nTip\n\n\nKnit frequently throughout your homework / lab so that you encounter environment errors early and often!"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#how-to-fix-code",
    "href": "schedule/slides/00-r-review.html#how-to-fix-code",
    "title": "UBC Stat406 2024W",
    "section": "How to fix code",
    "text": "How to fix code\n\nIf you’re using a function in a package, start with ?function to see the help\n\nMake sure you’re calling the function correctly.\nTry running the examples.\npaste the error into Google (if you share the error on Slack, I often do this first)\nGo to the package website if it exists, and browse around\n\nIf your .Rmd won’t Knit\n\nDid you make the mistake on the last slide?\nDid it Knit before? Then the bug is in whatever you added.\nDid you never Knit it? Why not?\nCall rstudioapi::restartSession(), then run the Chunks 1-by-1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#adding-browser",
    "href": "schedule/slides/00-r-review.html#adding-browser",
    "title": "UBC Stat406 2024W",
    "section": "Adding browser()",
    "text": "Adding browser()\n(known as a breakpoint in any other language)\n\nOnly useful with your own functions.\nOpen the script with the function, and add browser() to the code somewhere\nThen call your function.\nThe execution will Stop where you added browser() and you’ll have access to the local environment to play around"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#reproducible-examples",
    "href": "schedule/slides/00-r-review.html#reproducible-examples",
    "title": "UBC Stat406 2024W",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\n\n\n\n\n\nQuestion I frequently get:\n\n\n“I ran this code, but it didn’t work.”\n\n\n\n\nIf you want to ask me why the code doesn’t work, you need to show me what’s wrong.\n\n\n\n\n\n\n\nDon’t just paste a screenshot!\n\n\nUnless you get lucky, I won’t be able to figure it out from that. And we’ll both get frustrated.\n\n\n\nWhat you need is a Reproducible Example or reprex.\n\nThis is a small chunk of code that\n\nruns in it’s own environment\nand produces the error."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#the-reprex-package",
    "href": "schedule/slides/00-r-review.html#the-reprex-package",
    "title": "UBC Stat406 2024W",
    "section": "The {reprex} package",
    "text": "The {reprex} package\n\nOpen a new .R script.\nPaste your buggy code in the file (no need to save)\nEdit your code to make sure it’s “enough to produce the error” and nothing more. (By rerunning the code a few times.)\nCopy your code (so that it’s on the clipboard)\nCall reprex::reprex(venue = \"r\") from the console. This will run your code in a new environment and show the result in the Viewer tab. Does it create the error you expect?\nIf it creates other errors, that may be the problem. You may fix the bug on your own!\nIf it doesn’t have errors, then your global environment is Farblunget.\nThe Output is now on your clipboard. Go to Slack and paste it in a message. Then press Cmd+Shift+Enter (on Mac) or Ctrl+Shift+Enter (Windows/Linux). Under Type, select R.\nSend the message, perhaps with more description and an SOS emoji.\n\n\n\n\n\n\n\nNote\n\n\nBecause Reprex runs in it’s own environment, it doesn’t have access to any of the libraries you loaded or the stuff in your global environment. You’ll have to load these things in the script."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-pitfalls",
    "href": "schedule/slides/00-r-review.html#r-pitfalls",
    "title": "UBC Stat406 2024W",
    "section": "R Pitfalls",
    "text": "R Pitfalls\n\nR is very permissive, and this leads to frequent silent errors\n\nnonstandard evaluation of arguments, data masking\nallows dots in names (even though they mean something syntactically!)\nallows accessing attributes that don’t exist\npromotion of ints to floats, floats to strings 😱\n\nLots of unusual design decisions\n\nmany assignment operators (-&gt;, &lt;-, -&gt;&gt;, &lt;&lt;-, =)\nmany accessors (a$b is a[[\"b\"]] but not a[\"b\"])\nlacking basic data types (e.g., hash maps)\ninformal classes (class(x) &lt;- \"a weird new class!\")\ntonnes of functions/data/objects in the global namespace\n3 == \"3\" (evaluates to TRUE?!!?!)\n\nRscript executable treats code differently than the R REPL"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "href": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "title": "UBC Stat406 2024W",
    "section": "{tidyverse} is huge",
    "text": "{tidyverse} is huge\nCore tidyverse is ~30 different packages, but we’re going to just talk about a few.\nLoad all of them by calling library(tidyverse)\nPackages fall roughly into a few categories:\n\nConvenience functions: {magrittr} and many many others.\nData processing: {dplyr} and many others.\nGraphing: {ggplot2} and some others like {scales}.\nUtilities\n\n\n\nWe’re going to talk quickly about some of it, but ignore much of 2.\nThere’s a lot that’s great about these packages, especially ease of data processing.\nBut it doesn’t always jive with base R (it’s almost a separate proglang at this point)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-in-doubt",
    "href": "schedule/slides/00-r-review.html#when-in-doubt",
    "title": "UBC Stat406 2024W",
    "section": "When in doubt…",
    "text": "When in doubt…\n\n\n\n\nRead the first 4 chapters (especially 3 and 4!)\nhttps://datasciencebook.ca"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "href": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "title": "UBC Stat406 2024W",
    "section": "Piping with {magrittr}",
    "text": "Piping with {magrittr}\nThis was introduced by {magrittr} as %&gt;%,\nbut is now in base R (&gt;=4.1.0) as |&gt;.\nNote: there are other pipes in {magrittr} (e.g. %$% and %T%) but I’ve never used them.\nThe point of the pipe is to logically sequence nested operations\nThe pipe passes the left hand side as the first argument of the right hand side"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#example",
    "href": "schedule/slides/00-r-review.html#example",
    "title": "UBC Stat406 2024W",
    "section": "Example",
    "text": "Example\n\n\n\nselect(filter(mtcars, cyl == 6), mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse1 &lt;- print(\n  sum(\n    residuals(\n      lm(y~., data = mutate(\n        tib, \n        x3 = x1^2,\n        x4 = log(x2 + abs(min(x2)) + 1)\n      )\n      )\n    )^2\n  )\n)\n\n[1] 9.888005e-30\n\n\n\n\nmtcars |&gt; filter(cyl == 6) |&gt; select(mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse2 &lt;- tib |&gt;\n  mutate(\n    x3 = x1^2, \n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() |&gt;\n  print()\n\n[1] 9.888005e-30"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-3",
    "href": "schedule/slides/00-r-review.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "It may seem like we should push this all the way\n\ntib |&gt;\n  mutate(\n    x3 = x1^2, \n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() -&gt;\n  mse3\n\nThis technically works…but at a minimum it makes it hard to extend pipe sequences.\n\n\n\n\n\n\n\nNote\n\n\nOpinion zone: It’s also just weird. Don’t encourage the R devs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "href": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "title": "UBC Stat406 2024W",
    "section": "Data processing in {dplyr}",
    "text": "Data processing in {dplyr}\nThis package has all sorts of things. And it interacts with {tibble} generally.\nThe basic idea is “tibble in, tibble out”.\nSatisfies data masking which means you can refer to columns by name or use helpers like ends_with(\"_rate\")\nMajorly useful operations:\n\nselect() (chooses columns to keep)\nmutate() (showed this already)\ngroup_by()\npivot_longer() and pivot_wider()\nleft_join() and full_join()\nsummarise()\n\n\n\n\n\n\n\nNote\n\n\nfilter() and select() are functions in Base R.\nSometimes you get 🐞 because it called the wrong version.\nTo be sure, prefix it like dplyr::select()."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "href": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "title": "UBC Stat406 2024W",
    "section": "A useful data frame",
    "text": "A useful data frame\n\n7-day rolling avg COVID case/death counts for CA and WA from Aug 1-21, 2022 from Johns Hopkins\n\nlibrary(tidyverse)\ncovid &lt;- read_csv(\"data/covid.csv\") |&gt;\n  select(geo_value, time_value, signal, value)\n\ncovid\n\n# A tibble: 84 × 4\n   geo_value time_value signal                        value\n   &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;\n 1 ca        2022-08-01 confirmed_7dav_incidence_prop  45.4\n 2 wa        2022-08-01 confirmed_7dav_incidence_prop  27.7\n 3 ca        2022-08-02 confirmed_7dav_incidence_prop  44.9\n 4 wa        2022-08-02 confirmed_7dav_incidence_prop  27.7\n 5 ca        2022-08-03 confirmed_7dav_incidence_prop  44.5\n 6 wa        2022-08-03 confirmed_7dav_incidence_prop  26.6\n 7 ca        2022-08-04 confirmed_7dav_incidence_prop  42.3\n 8 wa        2022-08-04 confirmed_7dav_incidence_prop  26.6\n 9 ca        2022-08-05 confirmed_7dav_incidence_prop  40.7\n10 wa        2022-08-05 confirmed_7dav_incidence_prop  34.6\n# ℹ 74 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples",
    "href": "schedule/slides/00-r-review.html#examples",
    "title": "UBC Stat406 2024W",
    "section": "Examples",
    "text": "Examples\nRename the signal to something short.\n\ncovid &lt;- covid |&gt; \n  mutate(signal = case_when(\n    str_starts(signal, \"confirmed\") ~ \"case_rate\", \n    TRUE ~ \"death_rate\"\n  ))\n\nSort by time_value then geo_value\n\ncovid &lt;- covid |&gt; arrange(time_value, geo_value)\n\nCalculate grouped medians\n\ncovid |&gt; \n  group_by(geo_value, signal) |&gt;\n  summarise(med = median(value), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  geo_value signal        med\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 ca        case_rate  33.2  \n2 ca        death_rate  0.112\n3 wa        case_rate  23.2  \n4 wa        death_rate  0.178"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples-1",
    "href": "schedule/slides/00-r-review.html#examples-1",
    "title": "UBC Stat406 2024W",
    "section": "Examples",
    "text": "Examples\nSplit the data into two tibbles by signal\n\ncases &lt;- covid |&gt; \n  filter(signal == \"case_rate\") |&gt;\n  rename(case_rate = value) |&gt; select(-signal)\ndeaths &lt;- covid |&gt; \n  filter(signal == \"death_rate\") |&gt;\n  rename(death_rate = value) |&gt; select(-signal)\n\nJoin them together\n\njoined &lt;- full_join(cases, deaths, by = c(\"geo_value\", \"time_value\"))\n\nDo the same thing by pivoting\n\ncovid |&gt; pivot_wider(names_from = signal, values_from = value)\n\n# A tibble: 42 × 4\n   geo_value time_value case_rate death_rate\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ca        2022-08-01      45.4      0.105\n 2 wa        2022-08-01      27.7      0.169\n 3 ca        2022-08-02      44.9      0.106\n 4 wa        2022-08-02      27.7      0.169\n 5 ca        2022-08-03      44.5      0.107\n 6 wa        2022-08-03      26.6      0.173\n 7 ca        2022-08-04      42.3      0.112\n 8 wa        2022-08-04      26.6      0.173\n 9 ca        2022-08-05      40.7      0.116\n10 wa        2022-08-05      34.6      0.225\n# ℹ 32 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "href": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "title": "UBC Stat406 2024W",
    "section": "Plotting with {ggplot2}",
    "text": "Plotting with {ggplot2}\n\nEverything you can do with ggplot(), you can do with plot(). But the defaults are much prettier.\nIt’s also much easier to adjust by aesthetics / panels by factors.\nIt also uses “data masking”: data goes into ggplot(data = mydata), then the columns are available to the rest.\nIt (sort of) pipes, but by adding layers with +\nIt strongly prefers “long” data frames over “wide” data frames.\n\n\nI’ll give a very fast overview of some confusing bits."
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html",
    "href": "schedule/handouts/keras-nnet.html",
    "title": "Keras and Neural Networks",
    "section": "",
    "text": "Attribution: this Lab derives mainly from a Vignette in the R Keras package under the MIT License.\nThis proved to be more challenging than I anticipated…\nMy setup:\n\nMacOS M1 Processor\nR/Rstudio 4.3+\n\nThen, if not already installed, you’ll need 2 R packages\n\ninstall.packages(\"reticulate\")\nremotes::install_github(\"rstudio/tensorflow\")\ninstall.packages(\"keras\")\n\nNow make sure that python is installed on your system. If it isn’t (or if you haven’t used it in a while, or if it’s somewhere R can’t find) this may take a while.\n\nreticulate::install_python()\n\nNow restart R before proceeding.\nFinally, install the python keras package (which also installs tensorflow and some other things).\n\nkeras::install_keras()\n\nRestart R again.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#installation",
    "href": "schedule/handouts/keras-nnet.html#installation",
    "title": "Keras and Neural Networks",
    "section": "",
    "text": "Attribution: this Lab derives mainly from a Vignette in the R Keras package under the MIT License.\nThis proved to be more challenging than I anticipated…\nMy setup:\n\nMacOS M1 Processor\nR/Rstudio 4.3+\n\nThen, if not already installed, you’ll need 2 R packages\n\ninstall.packages(\"reticulate\")\nremotes::install_github(\"rstudio/tensorflow\")\ninstall.packages(\"keras\")\n\nNow make sure that python is installed on your system. If it isn’t (or if you haven’t used it in a while, or if it’s somewhere R can’t find) this may take a while.\n\nreticulate::install_python()\n\nNow restart R before proceeding.\nFinally, install the python keras package (which also installs tensorflow and some other things).\n\nkeras::install_keras()\n\nRestart R again.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#thoughts-on-r-vs.-python",
    "href": "schedule/handouts/keras-nnet.html#thoughts-on-r-vs.-python",
    "title": "Keras and Neural Networks",
    "section": "Thoughts on R vs. Python",
    "text": "Thoughts on R vs. Python\nI’m doing this in R because it’s easier to walk through an R notebook than a Jupyter notebook (for me).\nMost deep learning infrastructure is written in Python. So everything here is running python under the hood.\nOnce configured, it doesn’t matter which you use: do what you’re comfortable with.\nThere’s nothing special about Python (nor R). Consider this quote from Yann LeCun, head of AI at Facebook and one of the three fathers of deep learning (posted on Facebook on 26 October 2020):"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#overview",
    "href": "schedule/handouts/keras-nnet.html#overview",
    "title": "Keras and Neural Networks",
    "section": "Overview",
    "text": "Overview\nIn this guide, we will train a neural network model to classify images of clothing, like sneakers and shirts.\nThis guide uses the Fashion MNIST dataset which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 by 28 pixels), as seen here:\n\nFashion MNIST is intended as a drop-in replacement for the classic MNIST dataset. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we’ll use here. The original MNIST was curated by Yann LeCun, and he maintained a database of performance results for many years.\nHere, we use Fashion MNIST for variety, and because it’s a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They’re good starting points to test and debug code.\nWe will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from Keras.\n\nfashion_mnist &lt;- dataset_fashion_mnist()\n\nc(train_images, train_labels) %&lt;-% fashion_mnist$train\nc(test_images, test_labels) %&lt;-% fashion_mnist$test\n\nAt this point we have four arrays: The train_images and train_labels arrays are the training set — the data the model uses to learn. The model is tested against the test set: the test_images, and test_labels arrays.\nThe images each are 28 x 28 arrays, with pixel values ranging between 0 and 255. The labels are arrays of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\n\n\nDigit\nClass\n\n\n\n\n0\nT-shirt/top\n\n\n1\nTrouser\n\n\n2\nPullover\n\n\n3\nDress\n\n\n4\nCoat\n\n\n5\nSandal\n\n\n6\nShirt\n\n\n7\nSneaker\n\n\n8\nBag\n\n\n9\nAnkle boot\n\n\n\nEach image is mapped to a single label. Since the class names are not included with the dataset, we’ll store them in a vector to use later when plotting the images.\n\nclass_names &lt;- c(\n  \"T-shirt/top\",\n  \"Trouser\",\n  \"Pullover\",\n  \"Dress\",\n  \"Coat\",\n  \"Sandal\",\n  \"Shirt\",\n  \"Sneaker\",\n  \"Bag\",\n  \"Ankle boot\"\n)"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#explore-the-data",
    "href": "schedule/handouts/keras-nnet.html#explore-the-data",
    "title": "Keras and Neural Networks",
    "section": "Explore the data",
    "text": "Explore the data\nLet’s explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, with each image represented as 28 x 28 pixels:\n\ndim(train_images)\n\n[1] 60000    28    28\n\n\nLikewise, there are 60,000 labels in the training set:\n\ndim(train_labels)\n\n[1] 60000\n\n\nEach label is an integer between 0 and 9:\n\ntrain_labels[1:20]\n\n [1] 9 0 0 3 0 2 7 2 5 5 0 9 5 5 7 9 1 0 6 4\n\n\nThere are 10,000 images in the test set. Again, each image is represented as 28 x 28 pixels:\n\ndim(test_images)\n\n[1] 10000    28    28\n\n\nAnd the test set contains 10,000 images labels:\n\ndim(test_labels)\n\n[1] 10000"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#preprocess-the-data",
    "href": "schedule/handouts/keras-nnet.html#preprocess-the-data",
    "title": "Keras and Neural Networks",
    "section": "Preprocess the data",
    "text": "Preprocess the data\nThe data should be preprocessed before training the network. If you inspect the first image in the training set, you will see that the pixel values fall in the range of 0 to 255:\n\nimage1 &lt;- as.data.frame(train_images[1, , ])\ncolnames(image1) &lt;- seq_len(ncol(image1))\nimage1$y &lt;- seq_len(nrow(image1))\nimage1 &lt;- pivot_longer(image1, -y, names_to = \"x\")\nimage1$x &lt;- as.integer(image1$x)\n\nggplot(image1, aes(x, y, fill = value)) +\n  geom_raster() +\n  scale_fill_gradient(low = \"white\", high = \"#053b64\", na.value = NA) +\n  scale_y_reverse() +\n  theme_void() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nWe scale these values to a range of 0 to 1 before feeding to the neural network model. For this, we simply divide by 255. The main implications here are for starting values, learning rate, and regularization. The defaults like inputs in [0, 1].\nIt’s important that the training set and the testing set are preprocessed in the same way:\n\ntrain_images &lt;- train_images / 255\ntest_images &lt;- test_images / 255\n\nDisplay the first 25 images from the training set and display the class name above each image.\nVerify that the data is in the correct format and we’re ready to build and train the network.\n\nsample_clothes &lt;- map(1:25, ~ expand_grid(x = 1:28, y = 1:28)) |&gt;\n  list_rbind(names_to = \"idx\")\nimgs &lt;- train_images[1:25, , ]\nimgs &lt;- apply(imgs, 1, c)\ncn &lt;- class_names[train_labels[1:25] + 1]\nnames(cn) &lt;- 1:25\nsample_clothes$value &lt;- c(imgs)\nrm(imgs)\nggplot(sample_clothes, aes(x, y, fill = value)) +\n  geom_raster() +\n  scale_fill_gradient(low = \"white\", high = \"#053b64\", na.value = NA) +\n  scale_y_reverse() +\n  theme_void() +\n  facet_wrap(~idx, nrow = 5, ncol = 5, labeller = labeller(idx = cn)) +\n  theme()"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#build-the-model",
    "href": "schedule/handouts/keras-nnet.html#build-the-model",
    "title": "Keras and Neural Networks",
    "section": "Build the model",
    "text": "Build the model\nBuilding the neural network requires configuring the layers of the model, then compiling the model.\n\nSetup the layers\nThe basic building block of a neural network is the layer. Layers extract representations from the data fed into them. And, hopefully, these representations are more meaningful for the problem at hand.\nMost of deep learning consists of chaining together simple layers. Most layers, like layer_dense(), have parameters that are learned during training.\n\nmodel &lt;- keras_model_sequential()\nmodel |&gt;\n  layer_flatten(input_shape = c(28, 28)) %&gt;% # input\n  layer_dense(units = 128, activation = \"relu\") %&gt;% # hidden layer\n  layer_dense(units = 10, activation = \"softmax\") # output class\n\nThe first layer in this network, layer_flatten(), transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1d-array of 28 * 28 = 784 pixels. Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn; it only reformats the data.\nAfter the pixels are flattened, the network consists of a sequence of two ‘dense’ layers. These are densely-connected, or fully-connected, neural layers. The first dense layer has 128 nodes (or neurons). The second (and last) layer is a 10-node softmax layer—this returns an array of 10 probability scores that sum to 1. Each node contains a score that indicates the probability that the current image belongs to one of the 10 digit classes.\n\n\nCompile the model\nBefore the model is ready for training, it needs a few more settings. These are added during the model’s compile step:\n\nLoss function: This measures how accurate the model is during training. We want to minimize this function to “steer” the model in the right direction.\nOptimizer: This is how the model is updated based on the data it sees and its loss function.\nMetrics: Used to monitor the training and testing steps. The following example uses accuracy, the fraction of the images that are correctly classified.\n\n\nmodel |&gt; compile(\n  optimizer = \"adam\",\n  loss = \"sparse_categorical_crossentropy\",\n  metrics = c(\"accuracy\")\n)\n\n\n\nTrain the model\nTraining the neural network model requires the following steps:\n\nFeed the training data to the model — in this example, the train_images and train_labels arrays.\nThe model learns to associate images and labels.\nWe ask the model to make predictions about a test set — in this example, the test_images array. We verify that the predictions match the labels from the test_labels array.\n\nTo start training, call the fit() method — the model is “fit” to the training data (takes about a minute):\n\nmodel |&gt; fit(train_images, train_labels, epochs = 5)\n\nEpoch 1/5\n1875/1875 - 9s - loss: 0.5468 - accuracy: 0.8102 - 9s/epoch - 5ms/step\nEpoch 2/5\n1875/1875 - 8s - loss: 0.4775 - accuracy: 0.8356 - 8s/epoch - 4ms/step\nEpoch 3/5\n1875/1875 - 8s - loss: 0.4722 - accuracy: 0.8370 - 8s/epoch - 4ms/step\nEpoch 4/5\n1875/1875 - 9s - loss: 0.4732 - accuracy: 0.8406 - 9s/epoch - 5ms/step\nEpoch 5/5\n1875/1875 - 9s - loss: 0.4783 - accuracy: 0.8379 - 9s/epoch - 5ms/step\n\n\n\ntrain_score &lt;- model |&gt; evaluate(train_images, train_labels, verbose = 0)\ntrain_score\n\n     loss  accuracy \n0.4885503 0.8282500 \n\n\nAs the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 83% on the training data.\n\n\nEvaluate accuracy\nNext, compare how the model performs on the test dataset:\n\ntest_score &lt;- model |&gt; evaluate(test_images, test_labels, verbose = 0)\ntest_score\n\n     loss  accuracy \n0.5505691 0.8124000 \n\n\nIt turns out, the accuracy on the test data set is a little less than the accuracy on the training dataset.\n\n\nMake predictions\nWith the model trained, we can use it to make predictions about some images.\n\npredictions &lt;- model |&gt; predict(test_images)\n\n313/313 - 1s - 514ms/epoch - 2ms/step\n\n\nHere, the model has predicted the label for each image in the testing set. Let’s take a look at the first prediction:\n\nround(predictions[1, ], 3)\n\n [1] 0.000 0.000 0.000 0.000 0.000 0.035 0.000 0.027 0.000 0.938\n\n\nA prediction is an array of 10 numbers. These are the posterior probabilities for each of the 10 different articles of clothing. We can see which label has the highest confidence value:\n\nwhich.max(predictions[1, ])\n\n[1] 10\n\n\nAlternatively, we can also directly get the class prediction:\n\nclass_pred &lt;- model |&gt;\n  predict(test_images) |&gt;\n  k_argmax()\n\n313/313 - 0s - 453ms/epoch - 1ms/step\n\n\n\nas.vector(class_pred[1:20])\n\n [1] 9 2 1 1 6 1 4 6 5 7 2 5 5 3 4 1 2 6 8 0\n\n\nAs the labels are 0-based, this actually means a predicted label of 9 would correspond to the label found in class_names[10]. So the model is most confident that this image is an ankle boot. And we can check the test label to see this is correct:\n\ntest_labels[1]\n\n[1] 9\n\n\nLet’s plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are orange\n\npar(mfcol = c(5, 5))\npar(mar = c(0, 0, 1.5, 0), xaxs = \"i\", yaxs = \"i\")\nfor (i in 1:25) {\n  img &lt;- test_images[i, , ]\n  img &lt;- t(apply(img, 2, rev))\n  # subtract 1 as labels go from 0 to 9\n  predicted_label &lt;- which.max(predictions[i, ]) - 1\n  true_label &lt;- test_labels[i]\n  color &lt;- ifelse(predicted_label == true_label, \"#0b62a4\", \"#ff9200\")\n  image(1:28, 1:28, img,\n    col = gray((255:0) / 255),\n    xaxt = \"n\", yaxt = \"n\",\n    main = paste0(\n      class_names[predicted_label + 1], \" (\",\n      class_names[true_label + 1], \")\"\n    ),\n    col.main = color\n  )\n}"
  },
  {
    "objectID": "schedule/handouts/keras-nnet.html#what-about-random-forests",
    "href": "schedule/handouts/keras-nnet.html#what-about-random-forests",
    "title": "Keras and Neural Networks",
    "section": "What about random forests?",
    "text": "What about random forests?\nWhat if we just did random forests instead? Takes a good bit less effort.\n\nlibrary(ranger) # faster version of randomForests\ntrain_images &lt;- t(apply(train_images, 1, c)) # flatten\ntest_images &lt;- t(apply(test_images, 1, c))\ntrain_images &lt;- cbind(train_labels, train_images) |&gt; as_tibble()\n\nWarning: The `x` argument of `as_tibble.matrix()` must have unique column names if\n`.name_repair` is omitted as of tibble 2.0.0.\nℹ Using compatibility `.name_repair`.\n\ntest_images &lt;- cbind(test_labels, test_images) |&gt; as_tibble()\nnames(train_images) &lt;- c(\"cl\", paste0(\"x\", 1:(ncol(train_images) - 1)))\nnames(test_images) &lt;- names(train_images)\ntrain_images$cl &lt;- as.factor(train_images$cl)\ntest_images$cl &lt;- as.factor(test_images$cl)\nrf &lt;- ranger(cl ~ ., data = train_images, num.trees = 100)\npreds &lt;- predict(rf, data = test_images)\n\nThe Test Set accuracy from Random Forests is 88%.\nSlightly better than the Neural Net for my run, but reasonably close."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#section",
    "href": "schedule/slides/11-kernel-smoothers.html#section",
    "title": "UBC Stat406 2024W",
    "section": "11 Local methods",
    "text": "11 Local methods\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 09 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#last-time",
    "href": "schedule/slides/11-kernel-smoothers.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time…",
    "text": "Last time…\nWe looked at feature maps as a way to do nonlinear regression.\nWe used new “features” \\(\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)\\)\nNow we examine an alternative\nSuppose I just look at the “neighbours” of some point (based on the \\(x\\)-values)\nI just average the \\(y\\)’s at those locations together"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#lets-use-3-neighbours",
    "href": "schedule/slides/11-kernel-smoothers.html#lets-use-3-neighbours",
    "title": "UBC Stat406 2024W",
    "section": "Let’s use 3 neighbours",
    "text": "Let’s use 3 neighbours\n\n\nCode\nlibrary(cowplot)\ndata(arcuate, package = \"Stat406\")\nset.seed(406406)\narcuate_unif &lt;- arcuate |&gt; slice_sample(n = 40) |&gt; arrange(position)\npt &lt;- 15\nnn &lt;-  3\nseq_range &lt;- function(x, n = 101) seq(min(x, na.rm = TRUE), max(x, na.rm = TRUE), length.out = n)\nneibs &lt;- sort.int(abs(arcuate_unif$position - arcuate_unif$position[pt]), index.return = TRUE)$ix[1:nn]\narcuate_unif$neighbours = seq_len(40) %in% neibs\ng1 &lt;- ggplot(arcuate_unif, aes(position, fa, colour = neighbours)) + \n  geom_point() +\n  scale_colour_manual(values = c(blue, red)) + \n  geom_vline(xintercept = arcuate_unif$position[pt], colour = red) + \n  annotate(\"rect\", fill = red, alpha = .25, ymin = -Inf, ymax = Inf,\n           xmin = min(arcuate_unif$position[neibs]), \n           xmax = max(arcuate_unif$position[neibs])\n  ) +\n  theme(legend.position = \"none\")\ng2 &lt;- ggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(\n    data = tibble(\n      position = seq_range(arcuate_unif$position),\n      fa = FNN::knn.reg(\n        arcuate_unif$position, matrix(position, ncol = 1),\n        y = arcuate_unif$fa\n      )$pred\n    ),\n    colour = orange, linewidth = 2\n  )\nplot_grid(g1, g2, ncol = 2)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#knn",
    "href": "schedule/slides/11-kernel-smoothers.html#knn",
    "title": "UBC Stat406 2024W",
    "section": "KNN",
    "text": "KNN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndata(arcuate, package = \"Stat406\")\nlibrary(FNN)\narcuate_unif &lt;- arcuate |&gt; \n  slice_sample(n = 40) |&gt; \n  arrange(position) \n\nnew_position &lt;- seq(\n  min(arcuate_unif$position), \n  max(arcuate_unif$position),\n  length.out = 101\n)\n\nknn3 &lt;- knn.reg(\n  train = arcuate_unif$position, \n  test = matrix(arcuate_unif$position, ncol = 1), \n  y = arcuate_unif$fa, \n  k = 3\n)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#this-method-is-k-nearest-neighbours.",
    "href": "schedule/slides/11-kernel-smoothers.html#this-method-is-k-nearest-neighbours.",
    "title": "UBC Stat406 2024W",
    "section": "This method is \\(K\\)-nearest neighbours.",
    "text": "This method is \\(K\\)-nearest neighbours.\nIt’s a linear smoother just like in previous lectures: \\(\\widehat{\\mathbf{y}} = \\mathbf{S} \\mathbf{y}\\) for some matrix \\(S\\).\nYou should imagine what \\(\\mathbf{S}\\) looks like.\nWhat is the degrees of freedom of KNN?\nKNN averages the neighbours with equal weight.\nBut some neighbours are “closer” than other neighbours."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#local-averages",
    "href": "schedule/slides/11-kernel-smoothers.html#local-averages",
    "title": "UBC Stat406 2024W",
    "section": "Local averages",
    "text": "Local averages\nInstead of choosing the number of neighbours to average, we can average any observations within a certain distance.\n\n\nThe boxes have width 30."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#what-is-a-kernel-smoother",
    "href": "schedule/slides/11-kernel-smoothers.html#what-is-a-kernel-smoother",
    "title": "UBC Stat406 2024W",
    "section": "What is a “kernel” smoother?",
    "text": "What is a “kernel” smoother?\n\nThe mathematics:\n\n\nA kernel is any function \\(K\\) such that for any \\(u\\), \\(K(u) \\geq 0\\), \\(\\int du K(u)=1\\) and \\(\\int uK(u)du=0\\).\n\n\nThe idea: a kernel is a nice way to take weighted averages. The kernel function gives the weights.\nThe previous example is called the boxcar kernel."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-with-the-boxcar",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-with-the-boxcar",
    "title": "UBC Stat406 2024W",
    "section": "Smoothing with the boxcar",
    "text": "Smoothing with the boxcar\n\n\nCode\ntestpts &lt;- seq(0, 200, length.out = 101)\ndmat &lt;- abs(outer(testpts, arcuate_unif$position, \"-\"))\nS &lt;- (dmat &lt; 15)\nS &lt;- S / rowSums(S)\nboxcar &lt;- tibble(position = testpts, fa = S %*% arcuate_unif$fa)\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(data = boxcar, colour = orange)\n\n\n\nThis one gives the same non-zero weight to all points within \\(\\pm 15\\) range."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#other-kernels",
    "href": "schedule/slides/11-kernel-smoothers.html#other-kernels",
    "title": "UBC Stat406 2024W",
    "section": "Other kernels",
    "text": "Other kernels\nMost of the time, we don’t use the boxcar because the weights are weird. (constant)\nA more common one is the Gaussian kernel:\n\n\nCode\ngaussian_kernel &lt;- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 7.5) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n\n\n\nFor the plot, I made \\(\\sigma=7.5\\).\nNow the weights “die away” for points farther from where we’re predicting. (but all nonzero!!)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#other-kernels-1",
    "href": "schedule/slides/11-kernel-smoothers.html#other-kernels-1",
    "title": "UBC Stat406 2024W",
    "section": "Other kernels",
    "text": "Other kernels\nWhat if I made \\(\\sigma=15\\)?\n\n\nCode\ngaussian_kernel &lt;- function(x) dnorm(x, mean = arcuate_unif$position[15], sd = 15) * 3\nggplot(arcuate_unif, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_segment(aes(x = position[15], y = 0, xend = position[15], yend = fa[15]), colour = orange) +\n  stat_function(fun = gaussian_kernel, geom = \"area\", fill = orange)\n\n\n\nBefore, points far from \\(x_{15}\\) got very small weights, now they have more influence.\nFor the Gaussian kernel, \\(\\sigma\\) determines something like the “range” of the smoother."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#many-gaussians",
    "href": "schedule/slides/11-kernel-smoothers.html#many-gaussians",
    "title": "UBC Stat406 2024W",
    "section": "Many Gaussians",
    "text": "Many Gaussians\nThe following code creates \\(\\mathbf{S}\\) for Gaussian kernel smoothers with different \\(\\sigma\\)\n\ndmat &lt;- as.matrix(dist(x))\nSgauss &lt;- function(sigma) {\n  gg &lt;- dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg), \"/\") # make the rows sum to 1.\n}\n\n\n\nCode\nSgauss &lt;- function(sigma) {\n  gg &lt;-  dnorm(dmat, sd = sigma) # not an argument, uses the global dmat\n  sweep(gg, 1, rowSums(gg),'/') # make the rows sum to 1.\n}\nboxcar$S15 = with(arcuate_unif, Sgauss(15) %*% fa)\nboxcar$S08 = with(arcuate_unif, Sgauss(8) %*% fa)\nboxcar$S30 = with(arcuate_unif, Sgauss(30) %*% fa)\nbc = boxcar %&gt;% select(position, S15, S08, S30) %&gt;% \n  pivot_longer(-position, names_to = \"Sigma\")\nggplot(arcuate_unif, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_line(data = bc, aes(position, value, colour = Sigma), linewidth = 1.5) +\n  scale_colour_brewer(palette = \"Set1\")"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#the-bandwidth",
    "href": "schedule/slides/11-kernel-smoothers.html#the-bandwidth",
    "title": "UBC Stat406 2024W",
    "section": "The bandwidth",
    "text": "The bandwidth\n\nChoosing \\(\\sigma\\) is very important.\nThis “range” parameter is called the bandwidth.\nIt is way more important than which kernel you use.\nThe default kernel in ksmooth() is something called ‘Epanechnikov’:\n\n\nepan &lt;- function(x) 3/4 * (1 - x^2) * (abs(x) &lt; 1)\nggplot(data.frame(x = c(-2, 2)), aes(x)) + stat_function(fun = epan, colour = green, linewidth = 2)"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#choosing-the-bandwidth",
    "href": "schedule/slides/11-kernel-smoothers.html#choosing-the-bandwidth",
    "title": "UBC Stat406 2024W",
    "section": "Choosing the bandwidth",
    "text": "Choosing the bandwidth\nAs we have discussed, kernel smoothing (and KNN) are linear smoothers\n\\[\\widehat{\\mathbf{y}} = \\mathbf{S}\\mathbf{y}\\]\nThe degrees of freedom is \\(\\textrm{tr}(\\mathbf{S})\\)\nTherefore we can use our model selection criteria from before\n\nUnfortunately, these don’t satisfy the “technical condition”, so cv_nice() doesn’t give LOO-CV"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data",
    "title": "UBC Stat406 2024W",
    "section": "Smoothing the full Lidar data",
    "text": "Smoothing the full Lidar data\n\nar &lt;- arcuate |&gt; slice_sample(n = 200)\n\ngcv &lt;- function(y, S) {\n  yhat &lt;- S %*% y\n  mean( (y - yhat)^2 / (1 - mean(diag(S)))^2 )\n}\n\nfake_loocv &lt;- function(y, S) {\n  yhat &lt;- S %*% y\n  mean( (y - yhat)^2 / (1 - diag(S))^2 )\n}\n\ndmat &lt;- as.matrix(dist(ar$position))\nsigmas &lt;- 10^(seq(log10(300), log10(.3), length = 100))\n\ngcvs &lt;- map_dbl(sigmas, ~ gcv(ar$fa, Sgauss(.x)))\nflcvs &lt;- map_dbl(sigmas, ~ fake_loocv(ar$fa, Sgauss(.x)))\nbest_s &lt;- sigmas[which.min(gcvs)]\nother_s &lt;- sigmas[which.min(flcvs)]\n\nar$smoothed &lt;- Sgauss(best_s) %*% ar$fa\nar$other &lt;- Sgauss(other_s) %*% ar$fa"
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data-1",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-the-full-lidar-data-1",
    "title": "UBC Stat406 2024W",
    "section": "Smoothing the full Lidar data",
    "text": "Smoothing the full Lidar data\n\n\nCode\ng3 &lt;- ggplot(data.frame(sigma = sigmas, gcv = gcvs), aes(sigma, gcv)) +\n  geom_point(colour = blue) +\n  geom_vline(xintercept = best_s, colour = red) +\n  scale_x_log10() +\n  xlab(sprintf(\"Sigma, best is sig = %.2f\", best_s))\ng4 &lt;- ggplot(ar, aes(position, fa)) +\n  geom_point(colour = blue) +\n  geom_line(aes(y = smoothed), colour = orange, linewidth = 2)\nplot_grid(g3, g4, nrow = 1)\n\n\n\nI considered \\(\\sigma \\in [0.3,\\ 300]\\) and used \\(3.97\\).\nIt’s too wiggly, to my eye. Typical for GCV."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#smoothing-manually",
    "href": "schedule/slides/11-kernel-smoothers.html#smoothing-manually",
    "title": "UBC Stat406 2024W",
    "section": "Smoothing manually",
    "text": "Smoothing manually\nI did Kernel Smoothing “manually”\n\nFor a fixed bandwidth\nCompute the smoothing matrix\nMake the predictions\nRepeat and compute GCV\n\nThe point is to “show how it works”. It’s also really easy."
  },
  {
    "objectID": "schedule/slides/11-kernel-smoothers.html#r-functions-packages",
    "href": "schedule/slides/11-kernel-smoothers.html#r-functions-packages",
    "title": "UBC Stat406 2024W",
    "section": "R functions / packages",
    "text": "R functions / packages\nThere are a number of other ways to do this in R\n\nloess()\nksmooth()\nKernSmooth::locpoly()\nmgcv::gam()\nnp::npreg()\n\nThese have tricks and ways of doing CV and other things automatically.\n\nNote\n\nAll I needed was the distance matrix dist(x).\n\n\nGiven ANY distance function\n\n\nsay, \\(d(\\mathbf{x}_i, \\mathbf{x}_j) = \\Vert\\mathbf{x}_i - \\mathbf{x}_j\\Vert_2 + I(x_{i,3} = x_{j,3})\\)\n\n\nI can use these methods."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#section",
    "href": "schedule/slides/09-l1-penalties.html#section",
    "title": "UBC Stat406 2024W",
    "section": "09 L1 penalties",
    "text": "09 L1 penalties\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 02 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#last-time",
    "href": "schedule/slides/09-l1-penalties.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\n\nRidge regression\n\n\\(\\min \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2 \\st \\snorm{\\beta}_2^2 \\leq s\\)\n\nBest (in sample) linear regression model of size \\(s\\)\n\n\\(\\min \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 \\st \\snorm{\\beta}_0 \\leq s\\)\n\n\n\\(\\snorm{\\beta}_0\\) is the number of nonzero elements in \\(\\beta\\)\nFinding the “best” linear model (of size \\(s\\), among these predictors, in sample) is a nonconvex optimization problem (In fact, it is NP-hard)\nRidge regression is convex (easy to solve), but doesn’t do variable selection\nCan we somehow “interpolate” to get both?"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#geometry-of-convexity",
    "href": "schedule/slides/09-l1-penalties.html#geometry-of-convexity",
    "title": "UBC Stat406 2024W",
    "section": "Geometry of convexity",
    "text": "Geometry of convexity\n\n\nCode\nlibrary(mvtnorm)\nnormBall &lt;- function(q = 1, len = 1000) {\n  tg &lt;- seq(0, 2 * pi, length = len)\n  out &lt;- data.frame(x = cos(tg)) %&gt;%\n    mutate(b = (1 - abs(x)^q)^(1 / q), bm = -b) %&gt;%\n    gather(key = \"lab\", value = \"y\", -x)\n  out$lab &lt;- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipseData &lt;- function(n = 100, xlim = c(-2, 3), ylim = c(-2, 3),\n                        mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  df &lt;- expand.grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)\n  )\n  df$z &lt;- dmvnorm(df, mean, Sigma)\n  df\n}\n\nlballmax &lt;- function(ed, q = 1, tol = 1e-6) {\n  ed &lt;- filter(ed, x &gt; 0, y &gt; 0)\n  for (i in 1:20) {\n    ff &lt;- abs((ed$x^q + ed$y^q)^(1 / q) - 1) &lt; tol\n    if (sum(ff) &gt; 0) break\n    tol &lt;- 2 * tol\n  }\n  best &lt;- ed[ff, ]\n  best[which.max(best$z), ]\n}\n\nnbs &lt;- list()\nnbs[[1]] &lt;- normBall(0, 1)\nqs &lt;- c(.5, .75, 1, 1.5, 2)\nfor (ii in 2:6) nbs[[ii]] &lt;- normBall(qs[ii - 1])\nnbs &lt;- bind_rows(nbs)\nnbs$lab &lt;- factor(nbs$lab, levels = unique(nbs$lab))\nseg &lt;- data.frame(\n  lab = levels(nbs$lab)[1],\n  x0 = c(-1, 0), x1 = c(1, 0), y0 = c(0, -1), y1 = c(0, 1)\n)\nlevels(seg$lab) &lt;- levels(nbs$lab)\nggplot(nbs, aes(x, y)) +\n  geom_path(size = 1.2) +\n  facet_wrap(~lab, labeller = label_parsed) +\n  geom_segment(data = seg, aes(x = x0, xend = x1, y = y0, yend = y1), size = 1.2) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  coord_equal() +\n  scale_x_continuous(breaks = c(-1, 0, 1)) +\n  scale_y_continuous(breaks = c(-1, 0, 1)) +\n  geom_vline(xintercept = 0, size = .5) +\n  geom_hline(yintercept = 0, size = .5) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2]))"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#the-best-of-both-worlds",
    "href": "schedule/slides/09-l1-penalties.html#the-best-of-both-worlds",
    "title": "UBC Stat406 2024W",
    "section": "The best of both worlds",
    "text": "The best of both worlds\n\n\nCode\nnb &lt;- normBall(1)\ned &lt;- ellipseData()\nbols &lt;- data.frame(x = 1, y = 1)\nbhat &lt;- lballmax(ed, 1)\nggplot(nb, aes(x, y)) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal(xlim = c(-2, 2), ylim = c(-2, 2)) +\n  theme_bw(base_family = \"\", base_size = 24) +\n  geom_label(\n    data = bols, mapping = aes(label = bquote(\"hat(beta)[ols]\")), parse = TRUE,\n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  geom_label(\n    data = bhat, mapping = aes(label = bquote(\"hat(beta)[s]^L\")), parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )\n\n\n\nThis regularization set…\n\n… is convex (computationally efficient)\n… has corners (performs variable selection)"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#ell_1-regularized-regression",
    "href": "schedule/slides/09-l1-penalties.html#ell_1-regularized-regression",
    "title": "UBC Stat406 2024W",
    "section": "\\(\\ell_1\\)-regularized regression",
    "text": "\\(\\ell_1\\)-regularized regression\nKnown as\n\n“lasso”\n“basis pursuit”\n\nThe estimator satisfies\n\\[\\blt = \\argmin_{ \\snorm{\\beta}_1 \\leq s}  \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2\\]\nIn its corresponding Lagrangian dual form:\n\\[\\bll = \\argmin_{\\beta} \\frac{1}{n}\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1\\]"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#lasso",
    "href": "schedule/slides/09-l1-penalties.html#lasso",
    "title": "UBC Stat406 2024W",
    "section": "Lasso",
    "text": "Lasso\nWhile the ridge solution can be easily computed\n\\[\\brl = \\argmin_{\\beta} \\frac 1n \\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_2^2 = (\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1} \\X^{\\top}\\y\\]\nthe lasso solution\n\\[\\bll = \\argmin_{\\beta} \\frac 1n\\snorm{\\y-\\X\\beta}_2^2 + \\lambda \\snorm{\\beta}_1 = \\; ??\\]\ndoesn’t have a closed-form solution.\nHowever, because the optimization problem is convex, there exist efficient algorithms for computing it\n\n\nThe best are Iterative Soft Thresholding or Coordinate Descent. Gradient Descent doesn’t work very well in practice."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#coefficient-path-ridge-vs-lasso",
    "href": "schedule/slides/09-l1-penalties.html#coefficient-path-ridge-vs-lasso",
    "title": "UBC Stat406 2024W",
    "section": "Coefficient path: ridge vs lasso",
    "text": "Coefficient path: ridge vs lasso\n\n\nCode\nlibrary(glmnet)\ndata(prostate, package = \"ElemStatLearn\")\nX &lt;- prostate |&gt; dplyr::select(-train, -lpsa) |&gt;  as.matrix()\nY &lt;- prostate$lpsa\nlasso &lt;- glmnet(x = X, y = Y) # alpha = 1 by default\nridge &lt;- glmnet(x = X, y = Y, alpha = 0)\nop &lt;- par()\n\n\n\npar(mfrow = c(1, 2), mar = c(5, 3, 5, .1))\nplot(lasso, main = \"Lasso\")\nplot(ridge, main = \"Ridge\")"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#same-but-against-lambda",
    "href": "schedule/slides/09-l1-penalties.html#same-but-against-lambda",
    "title": "UBC Stat406 2024W",
    "section": "Same but against Lambda",
    "text": "Same but against Lambda\n\npar(mfrow = c(1, 2), mar = c(5, 3, 5, .1))\nplot(lasso, main = \"Lasso\", xvar = \"lambda\")\nplot(ridge, main = \"Ridge\", xvar = \"lambda\")"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#additional-intuition-for-why-lasso-selects-variables",
    "href": "schedule/slides/09-l1-penalties.html#additional-intuition-for-why-lasso-selects-variables",
    "title": "UBC Stat406 2024W",
    "section": "Additional intuition for why Lasso selects variables",
    "text": "Additional intuition for why Lasso selects variables\nSuppose, for a particular \\(\\lambda\\), I have solutions for \\(\\widehat{\\beta}_k\\), \\(k = 1,\\ldots,j-1, j+1,\\ldots,p\\).\nLet \\(\\widehat{\\y}_{-j} = \\X_{-j}\\widehat{\\beta}_{-j}\\), and assume WLOG \\(\\overline{\\X}_k = 0\\), \\(\\X_k^\\top\\X_k = 1\\ \\forall k\\)\nOne can show that:\n\\[\n\\widehat{\\beta}_j = S\\left(\\mathbf{X}^\\top_j(\\y - \\widehat{\\y}_{-j}),\\ \\lambda\\right).\n\\]\n\\[\nS(z, \\gamma) = \\textrm{sign}(z)(|z| - \\gamma)_+ = \\begin{cases} z - \\gamma & z &gt; \\gamma\\\\\nz + \\gamma & z &lt; -\\gamma \\\\ 0 & |z| \\leq \\gamma \\end{cases}\n\\]\n\nIterating over this is called coordinate descent and gives the solution\n\n\n\n\nIf I were told all the other coefficient estimates.\nThen to find this one, I’d shrink when the gradient is big, or set to 0 if it gets too small.\n\n\n\nSee for example, https://doi.org/10.18637/jss.v033.i01"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#packages",
    "href": "schedule/slides/09-l1-penalties.html#packages",
    "title": "UBC Stat406 2024W",
    "section": "Packages",
    "text": "Packages\nThere are two main R implementations for finding lasso\n{glmnet}: lasso = glmnet(X, Y, alpha=1).\n\nSetting alpha = 0 gives ridge regression (as does lm.ridge in the MASS package)\nSetting alpha \\(\\in (0,1)\\) gives a method called the “elastic net” which combines ridge regression and lasso, more on that next lecture.\nIf you don’t specify alpha, it does lasso\n\n{lars}: lars = lars(X, Y)\n\nlars() also does other things called “Least angle” and “forward stagewise” in addition to “forward stepwise” regression\nThe path returned by lars() is more useful than that returned by glmnet().\n\n\nBut you should use {glmnet}."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#choosing-the-lambda",
    "href": "schedule/slides/09-l1-penalties.html#choosing-the-lambda",
    "title": "UBC Stat406 2024W",
    "section": "Choosing the \\(\\lambda\\)",
    "text": "Choosing the \\(\\lambda\\)\nYou have to choose \\(\\lambda\\) in lasso or in ridge regression\nlasso selects variables (by setting coefficients to zero), but the value of \\(\\lambda\\) determines how many/which.\nAll of these packages come with CV built in.\nHowever, the way to do it differs from package to package"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "href": "schedule/slides/09-l1-penalties.html#glmnet-version-same-procedure-for-lasso-or-ridge",
    "title": "UBC Stat406 2024W",
    "section": "{glmnet} version (same procedure for lasso or ridge)",
    "text": "{glmnet} version (same procedure for lasso or ridge)\n\nlasso &lt;- cv.glmnet(X, Y) # estimate full model and CV no good reason to call glmnet() itself\n# 2. Look at the CV curve. If the dashed lines are at the boundaries, redo and adjust lambda\nlambda_min &lt;- lasso$lambda.min # the value, not the location (or use lasso$lambda.1se)\ncoeffs &lt;- coefficients(lasso, s = \"lambda.min\") # s can be string or a number\npreds &lt;- predict(lasso, newx = X, s = \"lambda.1se\") # must supply `newx`\n\n\n\\(\\widehat{R}_{CV}\\) is an estimator of \\(R_n\\), it has bias and variance\nBecause we did CV, we actually have 10 \\(\\widehat{R}\\) values, 1 per split.\nCalculate the mean (that’s what we’ve been using), but what about SE?"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#section-1",
    "href": "schedule/slides/09-l1-penalties.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "par(mfrow = c(1, 2), mar = c(5, 3, 3, 0))\nplot(lasso) # a plot method for the cv fit\nplot(lasso$glmnet.fit) # the glmnet.fit == glmnet(X,Y)\nabline(v = colSums(abs(coef(lasso$glmnet.fit)[-1, drop(lasso$index)])), lty = 2)"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#paths-with-chosen-lambda",
    "href": "schedule/slides/09-l1-penalties.html#paths-with-chosen-lambda",
    "title": "UBC Stat406 2024W",
    "section": "Paths with chosen lambda",
    "text": "Paths with chosen lambda\n\nridge &lt;- cv.glmnet(X, Y, alpha = 0, lambda.min.ratio = 1e-10) # added to get a minimum\npar(mfrow = c(1, 4))\nplot(ridge, main = \"Ridge\")\nplot(lasso, main = \"Lasso\")\nplot(ridge$glmnet.fit, main = \"Ridge\")\nabline(v = sum(abs(coef(ridge)))) # defaults to `lambda.1se`\nplot(lasso$glmnet.fit, main = \"Lasso\")\nabline(v = sum(abs(coef(lasso)))) # again, `lambda.1se` unless told otherwise"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#degrees-of-freedom",
    "href": "schedule/slides/09-l1-penalties.html#degrees-of-freedom",
    "title": "UBC Stat406 2024W",
    "section": "Degrees of freedom",
    "text": "Degrees of freedom\nLasso is not a linear smoother. There is no matrix \\(S\\) such that \\(\\widehat{\\y} = \\mathbf{S}\\y\\) for the predicted values from lasso.\n\nWe can’t use cv_nice().\nWe don’t have \\(\\tr{\\mathbf{S}} = \\textrm{df}\\) because there is no \\(\\mathbf{S}\\).\n\nHowever,\n\nOne can show that \\(\\textrm{df}_\\lambda = \\E[\\#(\\widehat{\\beta}_\\lambda \\neq 0)] = \\E[||\\widehat{\\beta}_\\lambda||_0]\\)\nThe proof is PhD-level material\n\nNote that the \\(\\widehat{\\textrm{df}}_\\lambda\\) is shown on the CV plot and that lasso.glmnet$glmnet.fit$df contains this value for all \\(\\lambda\\)."
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#other-flavours",
    "href": "schedule/slides/09-l1-penalties.html#other-flavours",
    "title": "UBC Stat406 2024W",
    "section": "Other flavours",
    "text": "Other flavours\n\nThe elastic net\n\ngenerally used for correlated variables that combines a ridge/lasso penalty. Use glmnet(..., alpha = a) (0 &lt; a &lt; 1).\n\nGrouped lasso\n\nwhere variables are included or excluded in groups. Required for factors (1-hot encoding)\n\nRelaxed lasso\n\nTakes the estimated model from lasso and fits the full least squares solution on the selected covariates (less bias, more variance). Use glmnet(..., relax = TRUE).\n\nDantzig selector\n\na slightly modified version of the lasso"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#lasso-cinematic-universe",
    "href": "schedule/slides/09-l1-penalties.html#lasso-cinematic-universe",
    "title": "UBC Stat406 2024W",
    "section": "Lasso cinematic universe",
    "text": "Lasso cinematic universe\n\n\n\nSCAD\n\na non-convex version of lasso that adds a more severe variable selection penalty\n\n\\(\\sqrt{\\textrm{lasso}}\\)\n\nclaims to be tuning parameter free (but isn’t). Uses \\(\\Vert\\cdot\\Vert_2\\) instead of \\(\\Vert\\cdot\\Vert_1\\) for the loss.\n\nGeneralized lasso\n\nAdds various additional matrices to the penalty term (e.g. \\(\\Vert D\\beta\\Vert_1\\)).\n\nArbitrary combinations\n\ncombine the above penalties in your favourite combinations"
  },
  {
    "objectID": "schedule/slides/09-l1-penalties.html#warnings-on-regularized-regression",
    "href": "schedule/slides/09-l1-penalties.html#warnings-on-regularized-regression",
    "title": "UBC Stat406 2024W",
    "section": "Warnings on regularized regression",
    "text": "Warnings on regularized regression\n\nThis isn’t a method unless you say how to choose \\(\\lambda\\).\nThe intercept is never penalized. Adds an extra degree-of-freedom.\nPredictor scaling is very important.\nDiscrete predictors need groupings.\nCentering the predictors is important\n(These all work with other likelihoods.)\n\n\nSoftware handles most of these automatically, but not always. (No Lasso with factor predictors.)"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#section",
    "href": "schedule/slides/18-the-bootstrap.html#section",
    "title": "UBC Stat406 2024W",
    "section": "18 The bootstrap",
    "text": "18 The bootstrap\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 02 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#a-small-detour",
    "href": "schedule/slides/18-the-bootstrap.html#a-small-detour",
    "title": "UBC Stat406 2024W",
    "section": "A small detour…",
    "text": "A small detour…"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#in-statistics",
    "href": "schedule/slides/18-the-bootstrap.html#in-statistics",
    "title": "UBC Stat406 2024W",
    "section": "In statistics…",
    "text": "In statistics…\nThe “bootstrap” works. And well.\nIt’s good for “second-level” analysis.\n\n“First-level” analyses are things like \\(\\hat\\beta\\), \\(\\hat \\y\\), an estimator of the center (a median), etc.\n“Second-level” are things like \\(\\Var{\\hat\\beta}\\), a confidence interval for \\(\\hat \\y\\), or a median, etc.\n\n\nYou usually get these “second-level” properties from “the sampling distribution of an estimator”"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#in-statistics-1",
    "href": "schedule/slides/18-the-bootstrap.html#in-statistics-1",
    "title": "UBC Stat406 2024W",
    "section": "In statistics…",
    "text": "In statistics…\nThe “bootstrap” works. And well.\nIt’s good for “second-level” analysis.\n\n“First-level” analyses are things like \\(\\hat\\beta\\), \\(\\hat \\y\\), an estimator of the center (a median), etc.\n“Second-level” are things like \\(\\Var{\\hat\\beta}\\), a confidence interval for \\(\\hat \\y\\), or a median, etc.\n\n\nBut what if you don’t know the sampling distribution? Or you’re skeptical of the CLT argument?"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#in-statistics-2",
    "href": "schedule/slides/18-the-bootstrap.html#in-statistics-2",
    "title": "UBC Stat406 2024W",
    "section": "In statistics…",
    "text": "In statistics…\nThe “bootstrap” works. And well.\nIt’s good for “second-level” analysis.\n\n“First-level” analyses are things like \\(\\hat\\beta\\), \\(\\hat \\y\\), an estimator of the center (a median), etc.\n“Second-level” are things like \\(\\Var{\\hat\\beta}\\), a confidence interval for \\(\\hat \\y\\), or a median, etc.\n\n\nSampling distributions\n\nIf \\(X_i\\) are iid Normal \\((0,\\sigma^2)\\), then \\(\\Var{\\overline{X}} = \\sigma^2 / n\\).\nIf \\(X_i\\) are iid and \\(n\\) is big, then \\(\\Var{\\overline{X}} \\approx \\Var{X_1} / n\\).\nIf \\(X_i\\) are iid Binomial \\((m, p)\\), then \\(\\Var{\\overline{X}} = mp(1-p) / n\\)"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#example-of-unknown-sampling-distribution",
    "href": "schedule/slides/18-the-bootstrap.html#example-of-unknown-sampling-distribution",
    "title": "UBC Stat406 2024W",
    "section": "Example of unknown sampling distribution",
    "text": "Example of unknown sampling distribution\nI estimate a LDA on some data.\nI get a new \\(x_0\\) and produce \\(\\widehat{Pr}(y_0 =1 \\given x_0)\\).\nCan I get a 95% confidence interval for \\(Pr(y_0=1 \\given x_0)\\)?\n\nThe bootstrap gives this to you."
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#bootstrap-procedure",
    "href": "schedule/slides/18-the-bootstrap.html#bootstrap-procedure",
    "title": "UBC Stat406 2024W",
    "section": "Bootstrap procedure",
    "text": "Bootstrap procedure\n\nResample your training data w/ replacement.\nCalculate LDA on this sample.\nProduce a new prediction, call it \\(\\widehat{Pr}_b(y_0 =1 \\given x_0)\\).\nRepeat 1-3 \\(b = 1,\\ldots,B\\) times.\nCI: \\(\\left[2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(1-\\alpha/2),\\ 2\\widehat{Pr}(y_0 =1 \\given x_0) - \\widehat{F}_{boot}(\\alpha/2)\\right]\\)\n\n\n\\(\\hat{F}\\) is the “empirical” distribution of the bootstraps."
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#empirical-distribution",
    "href": "schedule/slides/18-the-bootstrap.html#empirical-distribution",
    "title": "UBC Stat406 2024W",
    "section": "Empirical distribution",
    "text": "Empirical distribution\n\n\nCode\nr &lt;- rexp(50, 1 / 5)\nggplot(tibble(r = r), aes(r)) + \n  stat_ecdf(colour = orange) +\n  geom_vline(xintercept = quantile(r, probs = c(.05, .95))) +\n  geom_hline(yintercept = c(.05, .95), linetype = \"dashed\") +\n  annotate(\n    \"label\", x = c(5, 12), y = c(.25, .75), \n    label = c(\"hat(F)[boot](.05)\", \"hat(F)[boot](.95)\"), \n    parse = TRUE\n  )"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#very-basic-example",
    "href": "schedule/slides/18-the-bootstrap.html#very-basic-example",
    "title": "UBC Stat406 2024W",
    "section": "Very basic example",
    "text": "Very basic example\n\nLet \\(X_i\\sim \\textrm{Exponential}(1/5)\\). The pdf is \\(f(x) = \\frac{1}{5}e^{-x/5}\\)\nI know if I estimate the mean with \\(\\bar{X}\\), then by the CLT (if \\(n\\) is big),\n\n\\[\\frac{\\sqrt{n}(\\bar{X}-E[X])}{s} \\approx N(0, 1).\\]\n\nThis gives me a 95% confidence interval like \\[\\bar{X} \\pm 2s/\\sqrt{n}\\]\nBut I don’t want to estimate the mean, I want to estimate the median."
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#section-2",
    "href": "schedule/slides/18-the-bootstrap.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Code\nggplot(data.frame(x = c(0, 12)), aes(x)) +\n  stat_function(fun = function(x) dexp(x, 1 / 5), color = orange) +\n  geom_vline(xintercept = 5, color = blue) + # mean\n  geom_vline(xintercept = qexp(.5, 1 / 5), color = red) + # median\n  annotate(\"label\",\n    x = c(2.5, 5.5, 10), y = c(.15, .15, .05),\n    label = c(\"median\", \"bar(x)\", \"pdf\"), parse = TRUE,\n    color = c(red, blue, orange), size = 6\n  )"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#now-what",
    "href": "schedule/slides/18-the-bootstrap.html#now-what",
    "title": "UBC Stat406 2024W",
    "section": "Now what…",
    "text": "Now what…\n\nI give you a sample of size 500, you give me the sample median.\nHow do you get a CI?\nYou can use the bootstrap!\n\n\nset.seed(406406406)\nx &lt;- rexp(n, 1 / 5)\n(med &lt;- median(x)) # sample median\n\n[1] 3.611615\n\nB &lt;- 100\nalpha &lt;- 0.05\nFhat &lt;- map_dbl(1:B, ~ median(sample(x, replace = TRUE))) # repeat B times, \"empirical distribution\"\nCI &lt;- 2 * med - quantile(Fhat, probs = c(1 - alpha / 2, alpha / 2))"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#section-3",
    "href": "schedule/slides/18-the-bootstrap.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Code\nggplot(data.frame(Fhat), aes(Fhat)) +\n  geom_density(color = orange) +\n  geom_vline(xintercept = CI, color = orange, linetype = 2) +\n  geom_vline(xintercept = med, col = blue) +\n  geom_vline(xintercept = qexp(.5, 1 / 5), col = red) +\n  annotate(\"label\",\n    x = c(3.15, 3.5, 3.75), y = c(.5, .5, 1),\n    color = c(orange, red, blue),\n    label = c(\"widehat(F)\", \"true~median\", \"widehat(median)\"),\n    parse = TRUE\n  ) +\n  xlab(\"x\") +\n  geom_rug(aes(2 * med - Fhat))"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#how-does-this-work",
    "href": "schedule/slides/18-the-bootstrap.html#how-does-this-work",
    "title": "UBC Stat406 2024W",
    "section": "How does this work?",
    "text": "How does this work?"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#approximations",
    "href": "schedule/slides/18-the-bootstrap.html#approximations",
    "title": "UBC Stat406 2024W",
    "section": "Approximations",
    "text": "Approximations"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#slightly-harder-example",
    "href": "schedule/slides/18-the-bootstrap.html#slightly-harder-example",
    "title": "UBC Stat406 2024W",
    "section": "Slightly harder example",
    "text": "Slightly harder example\n\n\n\nggplot(fatcats, aes(Bwt, Hwt)) +\n  geom_point(color = blue) +\n  xlab(\"Cat body weight (Kg)\") +\n  ylab(\"Cat heart weight (g)\")\n\n\n\n\n\n\n\n\n\n\n\ncats.lm &lt;- lm(Hwt ~ 0 + Bwt, data = fatcats)\nsummary(cats.lm)\n\n\nCall:\nlm(formula = Hwt ~ 0 + Bwt, data = fatcats)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2353  -0.7932  -0.1407   0.5968  11.1026 \n\nCoefficients:\n    Estimate Std. Error t value Pr(&gt;|t|)    \nBwt  3.95424    0.06294   62.83   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.089 on 143 degrees of freedom\nMultiple R-squared:  0.965, Adjusted R-squared:  0.9648 \nF-statistic:  3947 on 1 and 143 DF,  p-value: &lt; 2.2e-16\n\nconfint(cats.lm)\n\n       2.5 %   97.5 %\nBwt 3.829836 4.078652"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "href": "schedule/slides/18-the-bootstrap.html#when-we-fit-models-we-examine-diagnostics",
    "title": "UBC Stat406 2024W",
    "section": "When we fit models, we examine diagnostics",
    "text": "When we fit models, we examine diagnostics\n\n\n\nqqnorm(residuals(cats.lm), pch = 16, col = blue)\nqqline(residuals(cats.lm), col = orange, lwd = 2)\n\n\n\n\n\n\n\n\nThe tails are too fat. So I don’t believe that CI…\n\n\nWe bootstrap\n\nB &lt;- 500\nalpha &lt;- .05\nbhats &lt;- map_dbl(1:B, ~ {\n  newcats &lt;- fatcats |&gt;\n    slice_sample(prop = 1, replace = TRUE)\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n\n   97.5%     2.5% \n3.826735 4.084322 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %   97.5 %\nBwt 3.829836 4.078652"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#an-alternative",
    "href": "schedule/slides/18-the-bootstrap.html#an-alternative",
    "title": "UBC Stat406 2024W",
    "section": "An alternative",
    "text": "An alternative\n\nSo far, I didn’t use any information about the data-generating process.\nWe’ve done the non-parametric bootstrap\nThis is easiest, and most common for the methods in this module\n\n\nBut there’s another version\n\nYou could try a “parametric bootstrap”\nThis assumes knowledge about the DGP"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#same-data",
    "href": "schedule/slides/18-the-bootstrap.html#same-data",
    "title": "UBC Stat406 2024W",
    "section": "Same data",
    "text": "Same data\n\n\nNon-parametric bootstrap\nSame as before\n\nB &lt;- 500\nalpha &lt;- .05\nbhats &lt;- map_dbl(1:B, ~ {\n  newcats &lt;- fatcats |&gt;\n    slice_sample(prop = 1, replace = TRUE)\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # NP Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n\n   97.5%     2.5% \n3.832907 4.070232 \n\nconfint(cats.lm) # Original CI\n\n       2.5 %   97.5 %\nBwt 3.829836 4.078652\n\n\n\n\nParametric bootstrap\n\nAssume that the linear model is TRUE.\nThen, \\(\\texttt{Hwt}_i = \\widehat{\\beta}\\times \\texttt{Bwt}_i + \\widehat{e}_i\\), \\(\\widehat{e}_i \\approx \\epsilon_i\\)\nThe \\(\\epsilon_i\\) is random \\(\\longrightarrow\\) just resample \\(\\widehat{e}_i\\).\n\n\nB &lt;- 500\nbhats &lt;- double(B)\ncats.lm &lt;- lm(Hwt ~ 0 + Bwt, data = fatcats)\nr &lt;- residuals(cats.lm)\nbhats &lt;- map_dbl(1:B, ~ {\n  newcats &lt;- fatcats |&gt; mutate(\n    Hwt = predict(cats.lm) +\n      sample(r, n(), replace = TRUE)\n  )\n  coef(lm(Hwt ~ 0 + Bwt, data = newcats))\n})\n\n2 * coef(cats.lm) - # Parametric Bootstrap CI\n  quantile(bhats, probs = c(1 - alpha / 2, alpha / 2))\n\n   97.5%     2.5% \n3.815162 4.065045"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#bootstrap-error-sources",
    "href": "schedule/slides/18-the-bootstrap.html#bootstrap-error-sources",
    "title": "UBC Stat406 2024W",
    "section": "Bootstrap error sources",
    "text": "Bootstrap error sources\n\n\nSimulation error\n\nusing only \\(B\\) samples to estimate \\(F\\) with \\(\\hat{F}\\).\n\nStatistical error\n\nour data depended on a sample from the population. We don’t have the whole population so we make an error by using a sample (Note: this part is what always happens with data, and what the science of statistics analyzes.)\n\nSpecification error\n\nIf we use the parametric bootstrap, and our model is wrong, then we are overconfident."
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#types-of-intervals",
    "href": "schedule/slides/18-the-bootstrap.html#types-of-intervals",
    "title": "UBC Stat406 2024W",
    "section": "Types of intervals",
    "text": "Types of intervals\nLet \\(\\hat{\\theta}\\) be our sample statistic, \\(\\hat{\\Theta}\\) be the resamples\nOur interval is\n\\[\n[2\\hat{\\theta} - \\theta^*_{1-\\alpha/2},\\ 2\\hat{\\theta} - \\theta^*_{\\alpha/2}]\n\\]\nwhere \\(\\theta^*_q\\) is the \\(q\\) quantile of \\(\\hat{\\Theta}\\).\n\n\nCalled the “Pivotal Interval”\nHas the correct \\(1-\\alpha\\)% coverage under very mild conditions on \\(\\hat{\\theta}\\)"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#types-of-intervals-1",
    "href": "schedule/slides/18-the-bootstrap.html#types-of-intervals-1",
    "title": "UBC Stat406 2024W",
    "section": "Types of intervals",
    "text": "Types of intervals\nLet \\(\\hat{\\theta}\\) be our sample statistic, \\(\\hat{\\Theta}\\) be the resamples\n\\[\n[\\hat{\\theta} - z_{\\alpha/2}\\hat{s},\\ \\hat{\\theta} + z_{\\alpha/2}\\hat{s}]\n\\]\nwhere \\(\\hat{s} = \\sqrt{\\Var{\\hat{\\Theta}}}\\)\n\n\nCalled the “Normal Interval”\nOnly works if the distribution of \\(\\hat{\\Theta}\\) is approximately Normal.\nUnlikely to work well\nDon’t do this"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#types-of-intervals-2",
    "href": "schedule/slides/18-the-bootstrap.html#types-of-intervals-2",
    "title": "UBC Stat406 2024W",
    "section": "Types of intervals",
    "text": "Types of intervals\nLet \\(\\hat{\\theta}\\) be our sample statistic, \\(\\hat{\\Theta}\\) be the resamples\n\\[\n[\\theta^*_{\\alpha/2},\\ \\theta^*_{1-\\alpha/2}]\n\\]\nwhere \\(\\theta^*_q\\) is the \\(q\\) quantile of \\(\\hat{\\Theta}\\).\n\n\nCalled the “Percentile Interval”\nWorks if \\(\\exists\\) monotone \\(m\\) so that \\(m(\\hat\\Theta) \\sim N(m(\\theta), c^2)\\)\nBetter than the Normal Interval\nMore assumptions than the Pivotal Interval"
  },
  {
    "objectID": "schedule/slides/18-the-bootstrap.html#more-details",
    "href": "schedule/slides/18-the-bootstrap.html#more-details",
    "title": "UBC Stat406 2024W",
    "section": "More details",
    "text": "More details\n\nSee “All of Statistics” by Larry Wasserman, Chapter 8.3\n\nThere’s a handout with the proofs on Canvas (under Modules)"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section",
    "href": "schedule/slides/23-nnets-other.html#section",
    "title": "UBC Stat406 2024W",
    "section": "23 Neural nets - other considerations",
    "text": "23 Neural nets - other considerations\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 12 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#estimation-procedures-training",
    "href": "schedule/slides/23-nnets-other.html#estimation-procedures-training",
    "title": "UBC Stat406 2024W",
    "section": "Estimation procedures (training)",
    "text": "Estimation procedures (training)\nBack-propagation\nAdvantages:\n\nIt’s updates only depend on local information in the sense that if objects in the hierarchical model are unrelated to each other, the updates aren’t affected\n(This helps in many ways, most notably in parallel architectures)\nIt doesn’t require second-derivative information\nAs the updates are only in terms of \\(\\hat{R}_i\\), the algorithm can be run in either batch or online mode\n\nDown sides:\n\nIt can be very slow\nNeed to choose the learning rate \\(\\gamma_t\\)"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#other-algorithms",
    "href": "schedule/slides/23-nnets-other.html#other-algorithms",
    "title": "UBC Stat406 2024W",
    "section": "Other algorithms",
    "text": "Other algorithms\nThere are many variations on the fitting algorithm\nStochastic gradient descent: (SGD) discussed in the optimization lecture\nThe rest are variations that use lots of tricks\n\nRMSprop\nAdam\nAdadelta\nAdagrad\nAdamax\nNadam\nFtrl"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#regularizing-neural-networks",
    "href": "schedule/slides/23-nnets-other.html#regularizing-neural-networks",
    "title": "UBC Stat406 2024W",
    "section": "Regularizing neural networks",
    "text": "Regularizing neural networks\nNNets can almost always achieve 0 training error. Even with regularization. Because they have so many parameters.\nFlavours:\n\na complexity penalization term \\(\\longrightarrow\\) solve \\(\\min \\hat{R} + \\rho(\\alpha,\\beta)\\)\nearly stopping on the back propagation algorithm used for fitting\n\n\nWeight decay\n\nThis is like ridge regression in that we penalize the squared Euclidean norm of the weights \\(\\rho(\\mathbf{W},\\mathbf{B}) = \\sum w_i^2 + \\sum b_i^2\\)\n\nWeight elimination\n\nThis encourages more shrinking of small weights \\(\\rho(\\mathbf{W},\\mathbf{B}) =  \\sum \\frac{w_i^2}{1+w_i^2} + \\sum \\frac{b_i^2}{1 + b_i^2}\\) or Lasso-type\n\nDropout\n\nIn each epoch, randomly choose \\(z\\%\\) of the nodes and set those weights to zero."
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#other-common-pitfalls",
    "href": "schedule/slides/23-nnets-other.html#other-common-pitfalls",
    "title": "UBC Stat406 2024W",
    "section": "Other common pitfalls",
    "text": "Other common pitfalls\nThere are a few areas to watch out for\nNonconvexity:\nThe neural network optimization problem is non-convex.\nThis makes any numerical solution highly dependent on the initial values. These should be\n\nchosen carefully, typically random near 0. DON’T use all 0.\nregenerated several times to check sensitivity\n\nScaling:\nBe sure to standardize the covariates before training"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#other-common-pitfalls-1",
    "href": "schedule/slides/23-nnets-other.html#other-common-pitfalls-1",
    "title": "UBC Stat406 2024W",
    "section": "Other common pitfalls",
    "text": "Other common pitfalls\nNumber of hidden units:\nIt is generally better to have too many hidden units than too few (regularization can eliminate some).\nSifting the output:\n\nChoose the solution that minimizes training error\nChoose the solution that minimizes the penalized training error\nAverage the solutions across runs"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#tuning-parameters",
    "href": "schedule/slides/23-nnets-other.html#tuning-parameters",
    "title": "UBC Stat406 2024W",
    "section": "Tuning parameters",
    "text": "Tuning parameters\nThere are many.\n\nRegularization\nStopping criterion\nlearning rate\nArchitecture\nDropout %\nothers…\n\nThese are hard to tune.\nIn practice, people might choose “some” with a validation set, and fix the rest largely arbitrarily\n\nMore often, people set them all arbitrarily"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#thoughts-on-nnets",
    "href": "schedule/slides/23-nnets-other.html#thoughts-on-nnets",
    "title": "UBC Stat406 2024W",
    "section": "Thoughts on NNets",
    "text": "Thoughts on NNets\nOff the top of my head, without lots of justification\n\n\n🤬😡 Why don’t statisticians like them? 🤬😡\n\nThere is little theory (though this is increasing)\nStat theory applies to global minima, here, only local determined by the optimizer\nLittle understanding of when they work\nIn large part, NNets look like logistic regression + feature creation. We understand that well, and in many applications, it performs as well\nExplosion of tuning parameters without a way to decide\nRequire massive datasets to work\nLots of examples where they perform exceedingly poorly\n\n\n\n🔥🔥Why are they hot?🔥🔥\n\nPerform exceptionally well on typical CS tasks (images, translation)\nTake advantage of SOTA computing (parallel, GPUs)\nVery good for multinomial logistic regression\nAn excellent example of “transfer learning”\nThey generate pretty pictures (the nets, pseudo-responses at hidden units)"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#keras",
    "href": "schedule/slides/23-nnets-other.html#keras",
    "title": "UBC Stat406 2024W",
    "section": "Keras",
    "text": "Keras\nMost people who do deep learning use Python \\(+\\) Keras \\(+\\) Tensorflow\nIt takes some work to get all this software up and running.\nIt is possible to do in with R using an interface to Keras.\n\nI used to try to do a walk-through, but the interface is quite brittle\nIf you want to explore, see the handout:\n\nKnitted: https://ubc-stat.github.io/stat-406-lectures/handouts/keras-nnet.html\nRmd: https://ubc-stat.github.io/stat-406-lectures/handouts/keras-nnet.Rmd"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section-1",
    "href": "schedule/slides/23-nnets-other.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "The Bias-Variance Trade-Off & \"DOUBLE DESCENT\" 🧵Remember the bias-variance trade-off? It says that models  perform well for an \"intermediate level of flexibility\".  You've seen the picture of the U-shape test error curve.We try to hit the \"sweet spot\" of flexibility.1/🧵 pic.twitter.com/HPk05izkZh— Daniela Witten (@daniela_witten) August 9, 2020"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#where-does-this-u-shape-come-from",
    "href": "schedule/slides/23-nnets-other.html#where-does-this-u-shape-come-from",
    "title": "UBC Stat406 2024W",
    "section": "Where does this U shape come from?",
    "text": "Where does this U shape come from?\nMSE = Squared Bias + Variance + Irreducible Noise\nAs we increase flexibility:\n\nSquared bias goes down\nVariance goes up\nEventually, | \\(\\partial\\) Variance | \\(&gt;\\) | \\(\\partial\\) Squared Bias |.\n\nGoal: Choose amount of flexibility to balance these and minimize MSE.\n\nUse CV or something to estimate MSE and decide how much flexibility."
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section-2",
    "href": "schedule/slides/23-nnets-other.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "In the past few yrs, (and particularly in the context of deep learning) ppl have noticed \"double descent\" -- when you continue to fit increasingly flexible models that interpolate the training data, then the test error can start to DECREASE again!! Check it out: 3/ pic.twitter.com/Vo54tRVRNG— Daniela Witten (@daniela_witten) August 9, 2020"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#zero-training-error-and-model-saturation",
    "href": "schedule/slides/23-nnets-other.html#zero-training-error-and-model-saturation",
    "title": "UBC Stat406 2024W",
    "section": "Zero training error and model saturation",
    "text": "Zero training error and model saturation\n\nIn Deep Learning, the recommendation is to “fit until you get zero training error”\nThis somehow magically, leads to a continued decrease in test error.\nSo, who cares about the Bias-Variance Trade off!!\n\n\nLesson:\nBV Trade off is not wrong. 😢\nThis is a misunderstanding of black box algorithms and flexibility.\nWe don’t even need deep learning to illustrate."
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section-3",
    "href": "schedule/slides/23-nnets-other.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "library(splines)\nset.seed(20221102)\nn &lt;- 20\ndf &lt;- tibble(\n  x = seq(-1.5 * pi, 1.5 * pi, length.out = n),\n  y = sin(x) + runif(n, -0.5, 0.5)\n)\ng &lt;- ggplot(df, aes(x, y)) + geom_point() + stat_function(fun = sin) + ylim(c(-2, 2))\ng + stat_smooth(method = lm, formula = y ~ bs(x, df = 4), se = FALSE, color = green) + # too smooth\n  stat_smooth(method = lm, formula = y ~ bs(x, df = 8), se = FALSE, color = orange) # looks good"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section-4",
    "href": "schedule/slides/23-nnets-other.html#section-4",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "xn &lt;- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)\n# Spline by hand\nX &lt;- bs(df$x, df = 20, intercept = TRUE)\nXn &lt;- bs(xn, df = 20, intercept = TRUE)\nS &lt;- svd(X)\nyhat &lt;- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)\ng + geom_line(data = tibble(x=xn, y=yhat), colour = orange) +\n  ggtitle(\"20 degrees of freedom\")"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#section-5",
    "href": "schedule/slides/23-nnets-other.html#section-5",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "xn &lt;- seq(-1.5 * pi, 1.5 * pi, length.out = 1000)\n# Spline by hand\nX &lt;- bs(df$x, df = 40, intercept = TRUE)\nXn &lt;- bs(xn, df = 40, intercept = TRUE)\nS &lt;- svd(X)\nyhat &lt;- Xn %*% S$v %*% diag(1/S$d) %*% crossprod(S$u, df$y)\ng + geom_line(data = tibble(x = xn, y = yhat), colour = orange) +\n  ggtitle(\"40 degrees of freedom\")"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#what-happened",
    "href": "schedule/slides/23-nnets-other.html#what-happened",
    "title": "UBC Stat406 2024W",
    "section": "What happened?!",
    "text": "What happened?!\n\ndoffs &lt;- 4:50\nmse &lt;- function(x, y) mean((x - y)^2)\nget_errs &lt;- function(doff) {\n  X &lt;- bs(df$x, df = doff, intercept = TRUE)\n  Xn &lt;- bs(xn, df = doff, intercept = TRUE)\n  S &lt;- svd(X)\n  yh &lt;- S$u %*% crossprod(S$u, df$y)\n  bhat &lt;- S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)\n  yhat &lt;- Xn %*% S$v %*% diag(1 / S$d) %*% crossprod(S$u, df$y)\n  nb &lt;- sqrt(sum(bhat^2))\n  tibble(train = mse(df$y, yh), test = mse(yhat, sin(xn)), norm = nb)\n}\nerrs &lt;- map(doffs, get_errs) |&gt;\n  list_rbind() |&gt; \n  mutate(`degrees of freedom` = doffs) |&gt; \n  pivot_longer(train:test, values_to = \"error\")"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#what-happened-1",
    "href": "schedule/slides/23-nnets-other.html#what-happened-1",
    "title": "UBC Stat406 2024W",
    "section": "What happened?!",
    "text": "What happened?!\n\n\nCode\nggplot(errs, aes(`degrees of freedom`, error, color = name)) +\n  geom_line(linewidth = 2) + \n  coord_cartesian(ylim = c(0, .12)) +\n  scale_x_log10() + \n  scale_colour_manual(values = c(blue, orange), name = \"\") +\n  geom_vline(xintercept = 20)"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#what-happened-2",
    "href": "schedule/slides/23-nnets-other.html#what-happened-2",
    "title": "UBC Stat406 2024W",
    "section": "What happened?!",
    "text": "What happened?!\n\n\nCode\nbest_test &lt;- errs |&gt; filter(name == \"test\")\nmin_norm &lt;- best_test$norm[which.min(best_test$error)]\nggplot(best_test, aes(norm, error)) +\n  geom_line(colour = blue, size = 2) + ylab(\"test error\") +\n  geom_vline(xintercept = min_norm, colour = orange) +\n  scale_y_log10() + scale_x_log10() + geom_vline(xintercept = 20)"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#degrees-of-freedom-and-complexity",
    "href": "schedule/slides/23-nnets-other.html#degrees-of-freedom-and-complexity",
    "title": "UBC Stat406 2024W",
    "section": "Degrees of freedom and complexity",
    "text": "Degrees of freedom and complexity\n\nIn low dimensions (where \\(n \\gg p\\)), with linear smoothers, df and model complexity are roughly the same.\nBut this relationship breaks down in more complicated settings\nWe’ve already seen this:\n\n\nlibrary(glmnet)\nout &lt;- cv.glmnet(X, df$y, nfolds = n) # leave one out\n\n\n\nCode\nwith(\n  out, \n  tibble(lambda = lambda, df = nzero, cv = cvm, cvup = cvup, cvlo = cvlo )\n) |&gt; \n  filter(df &gt; 0) |&gt;\n  pivot_longer(lambda:df) |&gt; \n  ggplot(aes(x = value)) +\n  geom_errorbar(aes(ymax = cvup, ymin = cvlo)) +\n  geom_point(aes(y = cv), colour = orange) +\n  facet_wrap(~ name, strip.position = \"bottom\", scales = \"free_x\") +\n  scale_y_log10() +\n  scale_x_log10() + theme(axis.title.x = element_blank())"
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#infinite-solutions",
    "href": "schedule/slides/23-nnets-other.html#infinite-solutions",
    "title": "UBC Stat406 2024W",
    "section": "Infinite solutions",
    "text": "Infinite solutions\n\nIn Lasso, df is not really the right measure of complexity\nBetter is \\(\\lambda\\) or the norm of the coefficients (these are basically the same)\nSo what happened with the Splines?\n\n\n\nWhen df \\(= 20\\), there’s a unique solution that interpolates the data\nWhen df \\(&gt; 20\\), there are infinitely many solutions that interpolate the data.\n\nBecause we used the SVD to solve the system, we happened to pick one: the one that has the smallest \\(\\Vert\\hat\\beta\\Vert_2\\)\nRecent work in Deep Learning shows that SGD has the same property: it returns the local optima with the smallest norm.\nIf we measure complexity in terms of the norm of the weights, rather than by counting parameters, we don’t see double descent anymore."
  },
  {
    "objectID": "schedule/slides/23-nnets-other.html#the-lesson",
    "href": "schedule/slides/23-nnets-other.html#the-lesson",
    "title": "UBC Stat406 2024W",
    "section": "The lesson",
    "text": "The lesson\n\nDeep learning isn’t magic.\nZero training error with lots of parameters doesn’t mean good test error.\nWe still need the bias variance tradeoff\nIt’s intuition still applies: more flexibility eventually leads to increased MSE\nBut we need to be careful how we measure complexity.\n\n\n\nThere is very interesting recent theory that says when we can expect lower test error to the right of the interpolation threshold than to the left."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#section",
    "href": "schedule/slides/25-pca-issues.html#section",
    "title": "UBC Stat406 2024W",
    "section": "25 Principal components, the troubles",
    "text": "25 Principal components, the troubles\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 23 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#pca",
    "href": "schedule/slides/25-pca-issues.html#pca",
    "title": "UBC Stat406 2024W",
    "section": "PCA",
    "text": "PCA\nIf we knew how to rotate our data, then we could more easily retain the structure.\nPCA gives us exactly this rotation\nPCA works when the data can be represented (in a lower dimension) as lines (or planes, or hyperplanes).\nSo, in two dimensions:"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#pca-reduced",
    "href": "schedule/slides/25-pca-issues.html#pca-reduced",
    "title": "UBC Stat406 2024W",
    "section": "PCA reduced",
    "text": "PCA reduced\nHere, we can capture a lot of the variation and underlying structure with just 1 dimension,\ninstead of the original 2 (the colouring is for visualizing)."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#pca-bad",
    "href": "schedule/slides/25-pca-issues.html#pca-bad",
    "title": "UBC Stat406 2024W",
    "section": "PCA bad",
    "text": "PCA bad\nWhat about other data structures? Again in two dimensions"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#pca-bad-1",
    "href": "schedule/slides/25-pca-issues.html#pca-bad-1",
    "title": "UBC Stat406 2024W",
    "section": "PCA bad",
    "text": "PCA bad\nHere, we have failed miserably.\nThere is actually only 1 dimension to this data (imagine walking up the spiral going from purple to yellow).\nHowever, when we write it as 1 PCA dimension, all the points are all “mixed up”."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#explanation",
    "href": "schedule/slides/25-pca-issues.html#explanation",
    "title": "UBC Stat406 2024W",
    "section": "Explanation",
    "text": "Explanation\n\n\nPCA wants to minimize distances (equivalently maximize variance).\nThis means it slices through the data at the meatiest point, and then the next one, and so on.\nIf the data are curved this is going to induce artifacts.\nPCA also looks at things as being close if they are near each other in a Euclidean sense.\nOn the spiral, our intuition says that things are close only if the distance is constrained to go along the curve.\nIn other words, purple and blue are close, blue and yellow are not."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#kernel-pca",
    "href": "schedule/slides/25-pca-issues.html#kernel-pca",
    "title": "UBC Stat406 2024W",
    "section": "Kernel PCA",
    "text": "Kernel PCA\nClassical PCA comes from \\(\\X=  \\U\\D\\V^{\\top}\\), the SVD of the (centered) data\nHowever, we can just as easily get it from the outer product \\(\\mathbf{K} = \\X\\X^{\\top} =  \\U\\D^2\\U^{\\top}\\)\nThe intuition behind KPCA is that \\(\\mathbf{K}\\) is an expansion into a kernel space, where \\[\\mathbf{K}_{i,i'} = k(x_i,\\ x_{i'}) = \\langle x_i,x_{i'} \\rangle\\]\nWe saw this trick before with feature expansion."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#procedure",
    "href": "schedule/slides/25-pca-issues.html#procedure",
    "title": "UBC Stat406 2024W",
    "section": "Procedure",
    "text": "Procedure\n\nSpecify a kernel function \\(k\\)\nmany people use \\(k(x,x') = \\exp\\left( -d(x, x')/\\gamma\\right)\\) where \\(d(x,x') = \\norm{x-x'}_2^2\\)\nForm \\(\\mathbf{K}_{i,i'} = k(x_i,x_{i'})\\)\nDouble center \\(\\mathbf{K} = \\mathbf{PKP}\\) where \\(\\mathbf{P} = \\mathbf{I}_n - \\mathbf{11}^\\top / n\\)\nTake eigendecomposition \\(\\mathbf{K} = \\U\\D^2\\U^{\\top}\\)\n\nThe scores are still \\(\\mathbf{Z} = \\U_M\\D_M\\)\n\n\n\n\n\n\nNote\n\n\nWe don’t explicitly generate the feature map \\(\\longrightarrow\\) there are NO loadings"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#an-alternate-view",
    "href": "schedule/slides/25-pca-issues.html#an-alternate-view",
    "title": "UBC Stat406 2024W",
    "section": "An alternate view",
    "text": "An alternate view\nTo get the first PC in classical PCA, we solve \\[\\max_\\alpha \\Var{\\X\\alpha} \\quad \\textrm{ subject to } \\quad \\left|\\left| \\alpha \\right|\\right|_2^2 = 1\\]\nIn the kernel setting we solve \\[\\max_{g \\in \\mathcal{H}_k} \\Var{g(X)} \\quad \\textrm{ subject to } \\quad\\left|\\left| g \\right|\\right|_{\\mathcal{H}_k} = 1\\]\nHere \\(\\mathcal{H}_k\\) is a function space determined by \\(k(x,x')\\)."
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#example-kernels",
    "href": "schedule/slides/25-pca-issues.html#example-kernels",
    "title": "UBC Stat406 2024W",
    "section": "Example kernels",
    "text": "Example kernels\n\n\\(k(x,x') = x^\\top x'\\)\n\ngives back regular PCA\n\n\\(k(x,x') = (1+x^\\top x')^d\\)\n\ngives a function space which contains all \\(d^{th}\\)-order\n\n\npolynomials.\n\n\\(k(x,x') = \\exp(-\\norm{x-x'}_2^2/\\gamma)\\)\n\ngives a function space spanned by the infinite Fourier basis\n\n\n\nFor more details see [ESL 14.5]"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#kernel-pca-on-the-spiral",
    "href": "schedule/slides/25-pca-issues.html#kernel-pca-on-the-spiral",
    "title": "UBC Stat406 2024W",
    "section": "Kernel PCA on the spiral",
    "text": "Kernel PCA on the spiral\n\n\nCode\nn &lt;- nrow(df_spiral)\nI_M &lt;- (diag(n) - tcrossprod(rep(1, n)) / n)\nkp &lt;- (tcrossprod(as.matrix(df_spiral[, 1:2])) + 1)^2\nKp &lt;- I_M %*% kp %*% I_M\nEp &lt;- eigen(Kp, symmetric = TRUE)\npolydf &lt;- tibble(\n  x = Ep$vectors[, 1] * Ep$values[1],\n  y = jit,\n  z = df_spiral$z\n)\nkg &lt;- exp(-as.matrix(dist(df_spiral[, 1:2]))^2 / 1)\nKg &lt;- I_M %*% kg %*% I_M\nEg &lt;- eigen(Kg, symmetric = TRUE)\ngaussdf &lt;- tibble(\n  x = Eg$vectors[, 1] * Eg$values[1],\n  y = jit,\n  z = df_spiral$z\n)\ndfkern &lt;- bind_rows(df_spiral, df_spiral2, polydf, gaussdf)\ndfkern$method &lt;- rep(c(\"data\", \"pca\", \"kpoly (d = 2)\", \"kgauss (gamma = 1)\"), each = n)"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#kpca-summary",
    "href": "schedule/slides/25-pca-issues.html#kpca-summary",
    "title": "UBC Stat406 2024W",
    "section": "KPCA: summary",
    "text": "KPCA: summary\nKernel PCA seeks to generalize the notion of similarity using a kernel map\nThis can be interpreted as finding smooth, orthogonal directions in an RKHS\nThis can allow us to start picking up nonlinear (in the original feature space) aspects of our data\nThis new representation can be passed to a supervised method to form a semisupervised learner\nThis kernel is different than kernel smoothing!!\n\nJust like with PCA (and lots of other things) the way you measure distance is important\nThe choice of Kernel is important\nThe embedding dimension must be chosen"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#basic-semi-supervised-see-islr-6.3.1",
    "href": "schedule/slides/25-pca-issues.html#basic-semi-supervised-see-islr-6.3.1",
    "title": "UBC Stat406 2024W",
    "section": "Basic semi-supervised (see [ISLR 6.3.1])",
    "text": "Basic semi-supervised (see [ISLR 6.3.1])\n\nYou get data \\(\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}\\).\nYou do something unsupervised on \\(\\X\\) to create new features (like PCA).\nYou use the learned features to find a predictor \\(\\hat{f}\\) (say, do OLS on them)"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#quick-example",
    "href": "schedule/slides/25-pca-issues.html#quick-example",
    "title": "UBC Stat406 2024W",
    "section": "Quick example",
    "text": "Quick example\n\nmusic &lt;- Stat406::popmusic_train\nX &lt;- music |&gt; select(danceability:energy, loudness, speechiness:valence)\npca &lt;- prcomp(X, scale = TRUE)\nZ &lt;- predict(pca)[, 1:2]\nZgrid &lt;- expand.grid(\n  Z1 = seq(min(Z[,1]), max(Z[,1]), len = 100L), \n  Z2 = seq(min(Z[,2]), max(Z[,2]), len = 100L)\n)\nout &lt;- class::knn(Z, Zgrid, music$artist, k = 6)"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#why",
    "href": "schedule/slides/25-pca-issues.html#why",
    "title": "UBC Stat406 2024W",
    "section": "Why?",
    "text": "Why?\nRecall that for nonparametric regression, we must deal with the curse of dimensionality.\n\nThis did KNN in 2 dimensions instead of 8: less curse\nHowever, the dimensions were learned independently of \\(\\y\\): may not be helpful\nThere’s a bias-variance tradeoff here…"
  },
  {
    "objectID": "schedule/slides/25-pca-issues.html#what-is-pca-kpca-estimating",
    "href": "schedule/slides/25-pca-issues.html#what-is-pca-kpca-estimating",
    "title": "UBC Stat406 2024W",
    "section": "What is PCA / KPCA estimating?",
    "text": "What is PCA / KPCA estimating?\nImagine that the Population data lie near a low-dimensional linear manifold…\n\nThink of the spiral. (1D manifold in 2D space)\nOr a flat piece of paper (2D manifold in 3D space)\n\nPCA / KPCA are estimating the manifold.\n\nThis works well under some conditions\nAnd if it’s true, then we have reduced variance but added a bit of bias\nAnd the downstream method may work better, because there are fewer predictors.\n\nWe just need the data near the manifold. But if not, then we introduced a lot of bias.\nPCA estimates linear manifolds, KPCA estimates non-linear"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#section",
    "href": "schedule/slides/24-pca-intro.html#section",
    "title": "UBC Stat406 2024W",
    "section": "24 Principal components, introduction",
    "text": "24 Principal components, introduction\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 24 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#unsupervised-learning",
    "href": "schedule/slides/24-pca-intro.html#unsupervised-learning",
    "title": "UBC Stat406 2024W",
    "section": "Unsupervised learning",
    "text": "Unsupervised learning\nIn Machine Learning, rather than calling \\(\\y\\) the response, people call it the supervisor\nSo unsupervised learning means learning without \\(\\y\\)\nThe only data you get are the features \\(\\{x_1,\\ldots,x_n\\}\\).\nThis type of analysis is more often exploratory\nWe’re not necessarily using this for prediction (but we could)\nSo now, we get \\(\\X\\)\nThe two main activities are representation learning and clustering"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#representation-learning",
    "href": "schedule/slides/24-pca-intro.html#representation-learning",
    "title": "UBC Stat406 2024W",
    "section": "Representation learning",
    "text": "Representation learning\nRepresentation learning is the idea that performance of ML methods is highly dependent on the choice of representation\nFor this reason, much of ML is geared towards transforming the data into the relevant features and then using these as inputs\nThis idea is as old as statistics itself, really,\nHowever, the idea is constantly revisited in a variety of fields and contexts\nCommonly, these learned representations capture low-level information like overall shapes\nIt is possible to quantify this intuition for PCA at least\n\n\nGoal\n\nTransform \\(\\mathbf{X}\\in \\R^{n\\times p}\\) into \\(\\mathbf{Z} \\in \\R^{n \\times ?}\\)\n\n\n?-dimension can be bigger (feature creation) or smaller (dimension reduction) than \\(p\\)"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#youve-done-this-already",
    "href": "schedule/slides/24-pca-intro.html#youve-done-this-already",
    "title": "UBC Stat406 2024W",
    "section": "You’ve done this already!",
    "text": "You’ve done this already!\n\nYou added transformations as predictors in regression\nYou “expanded” \\(\\mathbf{X}\\) using a basis \\(\\Phi\\) (polynomials, splines, etc.)\nYou used Neural Nets to do a “feature map”\n\n\nThis is the same, just no \\(Y\\) around"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#pca",
    "href": "schedule/slides/24-pca-intro.html#pca",
    "title": "UBC Stat406 2024W",
    "section": "PCA",
    "text": "PCA\nPrincipal components analysis (PCA) is an (unsupervised) dimension reduction technique\nIt solves various equivalent optimization problems\n(Maximize variance, minimize \\(\\ell_2\\) distortions, find closest subspace of a given rank, \\(\\ldots\\))\nAt its core, we are finding linear combinations of the original (centered) covariates \\[z_{ij} = \\alpha_j^{\\top} x_i\\]\nThis is expressed via the SVD: \\(\\X  = \\U\\D\\V^{\\top}\\).\n\n\n\n\n\n\n\nImportant\n\n\nWe assume throughout that \\(\\X - \\mathbf{11^\\top}\\overline{x} = 0\\) (we center the columns)\n\n\n\nThen our new features are\n\\[\\mathbf{Z} = \\X \\V = \\U\\D\\]"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#short-svd-aside-reminder-from-ridge-regression",
    "href": "schedule/slides/24-pca-intro.html#short-svd-aside-reminder-from-ridge-regression",
    "title": "UBC Stat406 2024W",
    "section": "Short SVD aside (reminder from Ridge Regression)",
    "text": "Short SVD aside (reminder from Ridge Regression)\n\nAny \\(n\\times p\\) matrix can be decomposed into \\(\\mathbf{UDV}^\\top\\).\nThese have properties:\n\n\n\\(\\mathbf{U}^\\top \\mathbf{U} = \\mathbf{I}_n\\)\n\\(\\mathbf{V}^\\top \\mathbf{V} = \\mathbf{I}_p\\)\n\\(\\mathbf{D}\\) is diagonal (0 off the diagonal)\n\nAlmost all the methods for we’ll talk about for representation learning use the SVD of some matrix."
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#why",
    "href": "schedule/slides/24-pca-intro.html#why",
    "title": "UBC Stat406 2024W",
    "section": "Why?",
    "text": "Why?\n\nGiven \\(\\X\\), find a projection \\(\\mathbf{P}\\) onto \\(\\R^M\\) with \\(M \\leq p\\) that minimizes the reconstruction error \\[\n\\begin{aligned}\n\\min_{\\mathbf{P}} &\\,\\, \\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P} \\rVert^2_F \\,\\,\\, \\textrm{(sum all the elements)}\\\\\n\\textrm{subject to} &\\,\\, \\textrm{rank}(\\mathbf{P}) = M,\\, \\mathbf{P} = \\mathbf{P}^T,\\, \\mathbf{P} = \\mathbf{P}^2\n\\end{aligned}\n\\] The conditions ensure that \\(\\mathbf{P}\\) is a projection matrix onto \\(M\\) dimensions.\nMaximize the variance explained by an orthogonal transformation \\(\\mathbf{A} \\in \\R^{p\\times M}\\) \\[\n\\begin{aligned}\n\\max_{\\mathbf{A}} &\\,\\, \\textrm{trace}\\left(\\frac{1}{n}\\mathbf{A}^\\top \\X^\\top \\X \\mathbf{A}\\right)\\\\\n\\textrm{subject to} &\\,\\, \\mathbf{A}^\\top\\mathbf{A} = \\mathbf{I}_M\n\\end{aligned}\n\\]\n\n\nIn case one, the minimizer is \\(\\mathbf{P} = \\mathbf{V}_M\\mathbf{V}_M^\\top\\)\nIn case two, the maximizer is \\(\\mathbf{A} = \\mathbf{V}_M\\)."
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings",
    "href": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings",
    "title": "UBC Stat406 2024W",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nSuppose we have predictors \\(\\x_1\\) and \\(\\x_2\\)\n\nWe more faithfully preserve the structure of this data by keeping \\(\\x_1\\) and setting \\(\\x_2\\) to zero than the opposite"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings-1",
    "href": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings-1",
    "title": "UBC Stat406 2024W",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nAn important feature of the previous example is that \\(\\x_1\\) and \\(\\x_2\\) aren’t correlated\nWhat if they are?\n\nWe lose a lot of structure by setting either \\(\\x_1\\) or \\(\\x_2\\) to zero"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings-2",
    "href": "schedule/slides/24-pca-intro.html#lower-dimensional-embeddings-2",
    "title": "UBC Stat406 2024W",
    "section": "Lower dimensional embeddings",
    "text": "Lower dimensional embeddings\nThe only difference is the first is a rotation of the second"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#pca-1",
    "href": "schedule/slides/24-pca-intro.html#pca-1",
    "title": "UBC Stat406 2024W",
    "section": "PCA",
    "text": "PCA\nIf we knew how to rotate our data, then we could more easily retain the structure.\nPCA gives us exactly this rotation\n\nCenter (+scale?) the data matrix \\(\\X\\)\nCompute the SVD of \\(\\X = \\U\\D \\V^\\top\\) or \\(\\X\\X^\\top = \\U\\D^2\\U^\\top\\) or \\(\\X^\\top \\X = \\V\\D^2 \\V^\\top\\)\nReturn \\(\\U_M\\D_M\\), where \\(\\D_M\\) is the largest \\(M\\) singular values of \\(\\X\\)"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#pca-2",
    "href": "schedule/slides/24-pca-intro.html#pca-2",
    "title": "UBC Stat406 2024W",
    "section": "PCA",
    "text": "PCA\n\n\nCode\ns &lt;- svd(X)\ntib &lt;- rbind(X, s$u %*% diag(s$d), s$u %*% diag(c(s$d[1], 0)))\ntib &lt;- tibble(\n  x1 = tib[, 1], x2 = tib[, 2],\n  name = rep(1:3, each = 20)\n)\nplotter &lt;- function(set = 1, main = \"original\") {\n  tib |&gt;\n    filter(name == set) |&gt;\n    ggplot(aes(x1, x2)) +\n    geom_point(colour = blue) +\n    coord_cartesian(c(-2, 2), c(-2, 2)) +\n    theme(legend.title = element_blank(), legend.position = \"bottom\") +\n    ggtitle(main)\n}\ncowplot::plot_grid(\n  plotter() + labs(x = bquote(x[1]), y = bquote(x[2])),\n  plotter(2, \"rotated\") +\n    labs(x = bquote((UD)[1] == (XV)[1]), y = bquote((UD)[2] == (XV)[2])),\n  plotter(3, \"rotated and projected\") +\n    labs(x = bquote(U[1] ~ D[1] == (XV)[1]), y = bquote(U[2] ~ D[2] %==% 0)),\n  nrow = 1\n)"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#pca-on-some-pop-music-data",
    "href": "schedule/slides/24-pca-intro.html#pca-on-some-pop-music-data",
    "title": "UBC Stat406 2024W",
    "section": "PCA on some pop music data",
    "text": "PCA on some pop music data\n\nmusic &lt;- Stat406::popmusic_train\nstr(music)\n\ntibble [1,269 × 15] (S3: tbl_df/tbl/data.frame)\n $ artist          : Factor w/ 3 levels \"Radiohead\",\"Taylor Swift\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ danceability    : num [1:1269] 0.781 0.627 0.516 0.629 0.686 0.522 0.31 0.705 0.553 0.419 ...\n $ energy          : num [1:1269] 0.357 0.266 0.917 0.757 0.705 0.691 0.374 0.621 0.604 0.908 ...\n $ key             : int [1:1269] 0 9 11 1 9 2 6 2 1 9 ...\n $ loudness        : num [1:1269] -16.39 -15.43 -3.19 -8.37 -10.82 ...\n $ mode            : int [1:1269] 1 1 0 0 1 1 1 1 0 1 ...\n $ speechiness     : num [1:1269] 0.912 0.929 0.0827 0.0512 0.249 0.0307 0.0275 0.0334 0.0258 0.0651 ...\n $ acousticness    : num [1:1269] 0.717 0.796 0.0139 0.00384 0.832 0.00609 0.761 0.101 0.202 0.00048 ...\n $ instrumentalness: num [1:1269] 0.00 0.00 6.37e-06 7.45e-01 4.55e-06 0.00 2.46e-05 4.30e-06 0.00 0.00 ...\n $ liveness        : num [1:1269] 0.185 0.292 0.36 0.0864 0.134 0.249 0.11 0.147 0.125 0.815 ...\n $ valence         : num [1:1269] 0.645 0.646 0.635 0.623 0.919 0.651 0.16 0.395 0.186 0.472 ...\n $ tempo           : num [1:1269] 118.3 79.3 145.8 157 151.9 ...\n $ time_signature  : int [1:1269] 4 4 4 4 5 4 4 4 4 4 ...\n $ duration_ms     : int [1:1269] 107133 89648 217160 201853 180653 201106 285640 240773 302266 290520 ...\n $ explicit        : logi [1:1269] FALSE FALSE FALSE FALSE FALSE FALSE ...\n\nX &lt;- music |&gt; select(danceability:energy, loudness, speechiness:valence)\npca &lt;- prcomp(X, scale = TRUE) ## DON'T USE princomp()"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#pca-on-some-pop-music-data-1",
    "href": "schedule/slides/24-pca-intro.html#pca-on-some-pop-music-data-1",
    "title": "UBC Stat406 2024W",
    "section": "PCA on some pop music data",
    "text": "PCA on some pop music data\n\nproj_pca &lt;- predict(pca)[,1:2] |&gt;\n  as_tibble() |&gt;\n  mutate(artist = music$artist)"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#things-to-look-at",
    "href": "schedule/slides/24-pca-intro.html#things-to-look-at",
    "title": "UBC Stat406 2024W",
    "section": "Things to look at",
    "text": "Things to look at\n\npca$rotation[, 1:2]\n\n                         PC1          PC2\ndanceability     -0.02811038  0.577020027\nenergy           -0.56077454 -0.001137424\nloudness         -0.53893087  0.085912854\nspeechiness       0.30125038  0.431188730\nacousticness      0.51172138  0.082108326\ninstrumentalness  0.01374425 -0.370058813\nliveness         -0.02282669 -0.194947054\nvalence          -0.20242211  0.540418541\n\npca$sdev\n\n[1] 1.6233469 1.3466829 1.0690017 0.9510417 0.7638669 0.6737958 0.5495869\n[8] 0.4054698\n\npca$sdev^2 / sum(pca$sdev^2)\n\n[1] 0.32940690 0.22669435 0.14284558 0.11306004 0.07293658 0.05675010 0.03775572\n[8] 0.02055072"
  },
  {
    "objectID": "schedule/slides/24-pca-intro.html#plotting-the-weights",
    "href": "schedule/slides/24-pca-intro.html#plotting-the-weights",
    "title": "UBC Stat406 2024W",
    "section": "Plotting the weights",
    "text": "Plotting the weights\n\n\nCode\npca$rotation[, 1:2] |&gt;\n  as_tibble() |&gt;\n  mutate(feature = names(X)) |&gt;\n  pivot_longer(-feature) |&gt;\n  ggplot(aes(value, feature, fill = feature)) +\n  facet_wrap(~name) +\n  geom_col() +\n  theme(legend.position = \"none\", axis.title = element_blank()) +\n  scale_fill_brewer(palette = \"Dark2\") +\n  geom_vline(xintercept = 0)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#section",
    "href": "schedule/slides/15-LDA-and-QDA.html#section",
    "title": "UBC Stat406 2024W",
    "section": "15 LDA and QDA",
    "text": "15 LDA and QDA\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 09 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#last-time",
    "href": "schedule/slides/15-LDA-and-QDA.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\nWe showed that with two classes, the Bayes’ classifier is\n\\[g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} &gt; \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}\\]\nwhere \\(p_1(X) = Pr(X \\given Y=1)\\), \\(p_0(X) = Pr(X \\given Y=0)\\) and \\(\\pi = Pr(Y=1)\\)\n\nFor more than two classes.\n\\[g_*(X) =\n\\argmax_k \\frac{\\pi_k p_k(X)}{\\sum_k \\pi_k p_k(X)}\\]\nwhere \\(p_k(X) = Pr(X \\given Y=k)\\) and \\(\\pi_k = P(Y=k)\\)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#estimating-these",
    "href": "schedule/slides/15-LDA-and-QDA.html#estimating-these",
    "title": "UBC Stat406 2024W",
    "section": "Estimating these",
    "text": "Estimating these\nLet’s make some assumptions:\n\n\\(Pr(X\\given Y=k) = \\mbox{N}(\\mu_k,\\Sigma_k)\\)\n\\(\\Sigma_k = \\Sigma_{k'} = \\Sigma\\)\n\n\nThis leads to Linear Discriminant Analysis (LDA), one of the oldest classifiers"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#lda",
    "href": "schedule/slides/15-LDA-and-QDA.html#lda",
    "title": "UBC Stat406 2024W",
    "section": "LDA",
    "text": "LDA\n\nSplit your training data into \\(K\\) subsets based on \\(y_i=k\\).\nIn each subset, estimate the mean of \\(X\\): \\(\\widehat\\mu_k = \\overline{X}_k\\)\nEstimate the pooled variance: \\[\\widehat\\Sigma = \\frac{1}{n-K} \\sum_{k \\in \\mathcal{K}} \\sum_{i \\in k} (x_i - \\overline{X}_k) (x_i - \\overline{X}_k)^{\\top}\\]\nEstimate the class proportion: \\(\\widehat\\pi_k = n_k/n\\)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#lda-1",
    "href": "schedule/slides/15-LDA-and-QDA.html#lda-1",
    "title": "UBC Stat406 2024W",
    "section": "LDA",
    "text": "LDA\nAssume just \\(K = 2\\) so \\(k \\in \\{0,\\ 1\\}\\)\nWe predict \\(\\widehat{y} = 1\\) if\n\\[\\widehat{p_1}(x) / \\widehat{p_0}(x) &gt; \\widehat{\\pi_0} / \\widehat{\\pi_1}\\]\nPlug in the density estimates:\n\\[\\widehat{p_k}(x) = N(x - \\widehat{\\mu}_k,\\ \\widehat\\Sigma)\\]"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#lda-2",
    "href": "schedule/slides/15-LDA-and-QDA.html#lda-2",
    "title": "UBC Stat406 2024W",
    "section": "LDA",
    "text": "LDA\nNow we take \\(\\log\\) and simplify \\((K=2)\\):\n\\[\n\\begin{aligned}\n&\\Rightarrow \\log(\\widehat{p_1}(x)\\times\\widehat{\\pi_1}) - \\log(\\widehat{p_0}(x)\\times\\widehat{\\pi_0})\n= \\cdots = \\cdots\\\\\n&= \\underbrace{\\left(x^\\top\\widehat\\Sigma^{-1}\\overline X_1-\\frac{1}{2}\\overline X_1^\\top \\widehat\\Sigma^{-1}\\overline X_1 + \\log \\widehat\\pi_1\\right)}_{\\delta_1(x)} -  \\underbrace{\\left(x^\\top\\widehat\\Sigma^{-1}\\overline X_0-\\frac{1}{2}\\overline X_0^\\top \\widehat\\Sigma^{-1}\\overline X_0 + \\log \\widehat\\pi_0\\right)}_{\\delta_0(x)}\\\\\n&= \\delta_1(x) - \\delta_0(x)\n\\end{aligned}\n\\]\nIf \\(\\delta_1(x) &gt; \\delta_0(x)\\), we set \\(\\widehat g(x)=1\\)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#one-dimensional-intuition",
    "href": "schedule/slides/15-LDA-and-QDA.html#one-dimensional-intuition",
    "title": "UBC Stat406 2024W",
    "section": "One dimensional intuition",
    "text": "One dimensional intuition\n\nset.seed(406406406)\nn &lt;- 100\npi &lt;- .6\nmu0 &lt;- -1\nmu1 &lt;- 2\nsigma &lt;- 2\ntib &lt;- tibble(\n  y = rbinom(n, 1, pi),\n  x = rnorm(n, mu0, sigma) * (y == 0) + rnorm(n, mu1, sigma) * (y == 1)\n)\n\n\n\nCode\ngg &lt;- ggplot(tib, aes(x, y)) +\n  geom_point(colour = blue) +\n  stat_function(fun = ~ 6 * (1 - pi) * dnorm(.x, mu0, sigma), colour = orange) +\n  stat_function(fun = ~ 6 * pi * dnorm(.x, mu1, sigma), colour = orange) +\n  annotate(\"label\",\n    x = c(-3, 4.5), y = c(.5, 2 / 3),\n    label = c(\"(1-pi)*p[0](x)\", \"pi*p[1](x)\"), parse = TRUE\n  )\ngg"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#what-is-linear",
    "href": "schedule/slides/15-LDA-and-QDA.html#what-is-linear",
    "title": "UBC Stat406 2024W",
    "section": "What is linear?",
    "text": "What is linear?\nLook closely at the equation for \\(\\delta_1(x)\\):\n\\[\\delta_1(x)=x^\\top\\widehat\\Sigma^{-1}\\overline X_1-\\frac{1}{2}\\overline X_1^\\top \\widehat\\Sigma^{-1}\\overline X_1 + \\log \\widehat\\pi_1\\]\nWe can write this as \\(\\delta_1(x) = x^\\top a_1 + b_1\\) with \\(a_1 = \\widehat\\Sigma^{-1}\\overline X_1\\) and \\(b_1=-\\frac{1}{2}\\overline X_1^\\top \\widehat\\Sigma^{-1}\\overline X_1 + \\log \\widehat\\pi_1\\).\nWe can do the same for \\(\\delta_0(x)\\) (in terms of \\(a_0\\) and \\(b_0\\))\nTherefore,\n\\[\\delta_1(x)-\\delta_0(x) = x^\\top(a_1-a_0) + (b_1-b_0)\\]\nThis is how we discriminate between the classes.\nWe just calculate \\((a_1 - a_0)\\) (a vector in \\(\\R^p\\)), and \\(b_1 - b_0\\) (a scalar)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#baby-example",
    "href": "schedule/slides/15-LDA-and-QDA.html#baby-example",
    "title": "UBC Stat406 2024W",
    "section": "Baby example",
    "text": "Baby example\n\n\n\nlibrary(mvtnorm)\nlibrary(MASS)\ngenerate_lda_2d &lt;- function(\n    n, p = c(.5, .5), \n    mu = matrix(c(0, 0, 1, 1), 2),\n    Sigma = diag(2)) {\n  X &lt;- rmvnorm(n, sigma = Sigma)\n  tibble(\n    y = which(rmultinom(n, 1, p) == 1, TRUE)[,1],\n    x1 = X[, 1] + mu[1, y],\n    x2 = X[, 2] + mu[2, y]\n  )\n}\ndat1 &lt;- generate_lda_2d(100, Sigma = .5 * diag(2))\nlda_fit &lt;- lda(y ~ ., dat1)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#multiple-classes",
    "href": "schedule/slides/15-LDA-and-QDA.html#multiple-classes",
    "title": "UBC Stat406 2024W",
    "section": "Multiple classes",
    "text": "Multiple classes\n\nmoreclasses &lt;- generate_lda_2d(150, c(.2, .3, .5), matrix(c(0, 0, 1, 1, 1, 0), 2), .5 * diag(2))\nseparateclasses &lt;- generate_lda_2d(150, c(.2, .3, .5), matrix(c(-1, -1, 2, 2, 2, -1), 2), .1 * diag(2))"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#qda",
    "href": "schedule/slides/15-LDA-and-QDA.html#qda",
    "title": "UBC Stat406 2024W",
    "section": "QDA",
    "text": "QDA\nJust like LDA, but \\(\\Sigma_k\\) is separate for each class.\nProduces Quadratic decision boundary.\nEverything else is the same.\n\nqda_fit &lt;- qda(y ~ ., dat1)\nqda_3fit &lt;- qda(y ~ ., moreclasses)"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#class-comparison",
    "href": "schedule/slides/15-LDA-and-QDA.html#class-comparison",
    "title": "UBC Stat406 2024W",
    "section": "3 class comparison",
    "text": "3 class comparison"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#notes",
    "href": "schedule/slides/15-LDA-and-QDA.html#notes",
    "title": "UBC Stat406 2024W",
    "section": "Notes",
    "text": "Notes\n\nLDA is a linear classifier. It is not a linear smoother.\n\nIt is derived from Bayes rule.\nAssume each class-conditional density in Gaussian\nIt assumes the classes have different mean vectors, but the same (common) covariance matrix.\nIt estimates densities and probabilities and “plugs in”\n\nQDA is not a linear classifier. It depends on quadratic functions of the data.\n\nIt is derived from Bayes rule.\nAssume each class-conditional density in Gaussian\nIt assumes the classes have different mean vectors and different covariance matrices.\nIt estimates densities and probabilities and “plugs in”"
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#section-1",
    "href": "schedule/slides/15-LDA-and-QDA.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "It is hard (maybe impossible) to come up with reasonable classifiers that are linear smoothers. Many “look” like a linear smoother, but then apply a nonlinear transformation."
  },
  {
    "objectID": "schedule/slides/15-LDA-and-QDA.html#naïve-bayes",
    "href": "schedule/slides/15-LDA-and-QDA.html#naïve-bayes",
    "title": "UBC Stat406 2024W",
    "section": "Naïve Bayes",
    "text": "Naïve Bayes\nAssume that \\(Pr(X | Y = k) = Pr(X_1 | Y = k)\\cdots Pr(X_p | Y = k)\\).\nThat is, conditional on the class, the feature distribution is independent.\n\nIf we further assume that \\(Pr(X_j | Y = k)\\) is Gaussian,\nThis is the same as QDA but with \\(\\Sigma_k\\) Diagonal.\n\n\nDon’t have to assume Gaussian. Could do lots of stuff."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#section",
    "href": "schedule/slides/08-ridge-regression.html#section",
    "title": "UBC Stat406 2024W",
    "section": "08 Ridge regression",
    "text": "08 Ridge regression\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 27 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#recap",
    "href": "schedule/slides/08-ridge-regression.html#recap",
    "title": "UBC Stat406 2024W",
    "section": "Recap",
    "text": "Recap\nSo far, we have emphasized model selection as\nDecide which predictors we would like to use in our linear model\nOr similarly:\nDecide which of a few linear models to use\nTo do this, we used a risk estimate, and chose the “model” with the lowest estimate\n\nMoving forward, we need to generalize this to\nDecide which of possibly infinite prediction functions \\(f\\in\\mathcal{F}\\) to use\nThankfully, this isn’t really any different. We still use those same risk estimates.\nRemember: We were choosing models that balance bias and variance (and hence have low prediction risk).\n\\[\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#regularization",
    "href": "schedule/slides/08-ridge-regression.html#regularization",
    "title": "UBC Stat406 2024W",
    "section": "Regularization",
    "text": "Regularization\n\nAnother way to control bias and variance is through regularization or shrinkage.\nRather than selecting a few predictors that seem reasonable, maybe trying a few combinations, use them all.\nI mean ALL.\nBut, make your estimates of \\(\\beta\\) “smaller”"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#brief-aside-on-optimization",
    "href": "schedule/slides/08-ridge-regression.html#brief-aside-on-optimization",
    "title": "UBC Stat406 2024W",
    "section": "Brief aside on optimization",
    "text": "Brief aside on optimization\n\nAn optimization problem has 2 components:\n\nThe “Objective function”: e.g. \\(\\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\).\nThe “constraint”: e.g. “fewer than 5 non-zero entries in \\(\\beta\\)”.\n\nA constrained minimization problem is written\n\n\\[\\min_\\beta f(\\beta)\\;\\; \\mbox{ subject to }\\;\\; C(\\beta)\\]\n\n\\(f(\\beta)\\) is the objective function\n\\(C(\\beta)\\) is the constraint"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-constrained-version",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-constrained-version",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression (constrained version)",
    "text": "Ridge regression (constrained version)\nOne way to do this for regression is to solve (say): \\[\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\sum_j \\beta^2_j &lt; s\n\\] for some \\(s&gt;0\\).\n\nThis is called “ridge regression”.\nCall the minimizer of this problem \\(\\brt\\)\n\n\nCompare this to ordinary least squares:\n\\[\n\\minimize_\\beta \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\n\\quad \\st \\beta \\in \\R^p\n\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#geometry-of-ridge-regression-contours",
    "href": "schedule/slides/08-ridge-regression.html#geometry-of-ridge-regression-contours",
    "title": "UBC Stat406 2024W",
    "section": "Geometry of ridge regression (contours)",
    "text": "Geometry of ridge regression (contours)\n\n\nCode\nlibrary(mvtnorm)\nnorm_ball &lt;- function(q = 1, len = 1000) {\n  tg &lt;- seq(0, 2 * pi, length = len)\n  out &lt;- tibble(x = cos(tg), b = (1 - abs(x)^q)^(1 / q), bm = -b) |&gt;\n    pivot_longer(-x, values_to = \"y\")\n  out$lab &lt;- paste0('\"||\" * beta * \"||\"', \"[\", signif(q, 2), \"]\")\n  return(out)\n}\n\nellipse_data &lt;- function(\n  n = 75, xlim = c(-2, 3), ylim = c(-2, 3),\n  mean = c(1, 1), Sigma = matrix(c(1, 0, 0, .5), 2)) {\n  expand_grid(\n    x = seq(xlim[1], xlim[2], length.out = n),\n    y = seq(ylim[1], ylim[2], length.out = n)) |&gt;\n    rowwise() |&gt;\n    mutate(z = dmvnorm(c(x, y), mean, Sigma))\n}\n\nlballmax &lt;- function(ed, q = 1, tol = 1e-6, niter = 20) {\n  ed &lt;- filter(ed, x &gt; 0, y &gt; 0)\n  feasible &lt;- (ed$x^q + ed$y^q)^(1 / q) &lt;= 1\n  best &lt;- ed[feasible, ]\n  best[which.max(best$z), ]\n}\n\n\nnb &lt;- norm_ball(2)\ned &lt;- ellipse_data()\nbols &lt;- data.frame(x = 1, y = 1)\nbhat &lt;- lballmax(ed, 2)\nggplot(nb, aes(x, y)) +\n  xlim(-2, 2) +\n  ylim(-2, 2) +\n  geom_path(colour = red) +\n  geom_contour(mapping = aes(z = z), colour = blue, data = ed, bins = 7) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_point(data = bols) +\n  coord_equal() +\n  geom_label(\n    data = bols,\n    mapping = aes(label = bquote(\"hat(beta)[ols]\")),\n    parse = TRUE, \n    nudge_x = .3, nudge_y = .3\n  ) +\n  geom_point(data = bhat) +\n  xlab(bquote(beta[1])) +\n  ylab(bquote(beta[2])) +\n  theme_bw(base_size = 24) +\n  geom_label(\n    data = bhat,\n    mapping = aes(label = bquote(\"hat(beta)[s]^R\")),\n    parse = TRUE,\n    nudge_x = -.4, nudge_y = -.4\n  )"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#brief-aside-on-norms",
    "href": "schedule/slides/08-ridge-regression.html#brief-aside-on-norms",
    "title": "UBC Stat406 2024W",
    "section": "Brief aside on norms",
    "text": "Brief aside on norms\nRecall, for a vector \\(z \\in \\R^p\\)\n\\[\\snorm{z}_2 = \\sqrt{z_1^2 + z_2^2 + \\cdots + z^2_p} = \\sqrt{\\sum_{j=1}^p z_j^2}\\]\nSo,\n\\[\\snorm{z}^2_2 = z_1^2 + z_2^2 + \\cdots + z^2_p = \\sum_{j=1}^p z_j^2.\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#other-norms-we-should-remember",
    "href": "schedule/slides/08-ridge-regression.html#other-norms-we-should-remember",
    "title": "UBC Stat406 2024W",
    "section": "Other norms we should remember:",
    "text": "Other norms we should remember:\n\n\\(\\ell_q\\)-norm\n\n\\(\\left(\\sum_{j=1}^p |z_j|^q\\right)^{1/q}\\)\n\n\\(\\ell_1\\)-norm (special case)\n\n\\(\\sum_{j=1}^p |z_j|\\)\n\n\\(\\ell_0\\)-norm\n\n\\(\\sum_{j=1}^p I(z_j \\neq 0 ) = \\lvert \\{j : z_j \\neq 0 \\}\\rvert\\)\n\n\\(\\ell_\\infty\\)-norm\n\n\\(\\max_{1\\leq j \\leq p} |z_j|\\)\n\n\n\n\nRecall what a norm is: https://en.wikipedia.org/wiki/Norm_(mathematics)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression",
    "text": "Ridge regression\nAn equivalent way to write\n\\[\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\]\nis in the Lagrangian form\n\\[\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2.\\]\nFor every \\(\\lambda\\) there is a unique \\(s\\) (and vice versa) that makes\n\\[\\brt = \\brl\\]"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-1",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-1",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression",
    "text": "Ridge regression\n\\(\\brt = \\argmin_{ || \\beta ||_2^2 \\leq s} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2\\)\n\\(\\brl = \\argmin_{ \\beta} \\frac{1}{n}\\sum_i (y_i-x^\\top_i \\beta)^2 + \\lambda || \\beta ||_2^2\\)\nObserve:\n\n\\(\\lambda = 0\\) (or \\(s = \\infty\\)) makes \\(\\brl = \\bls\\)\nAny \\(\\lambda &gt; 0\\) (or \\(s &lt;\\infty\\)) penalizes larger values of \\(\\beta\\), effectively shrinking them.\n\n\\(\\lambda\\) and \\(s\\) are known as tuning parameters"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#visualizing-ridge-regression-2-coefficients",
    "href": "schedule/slides/08-ridge-regression.html#visualizing-ridge-regression-2-coefficients",
    "title": "UBC Stat406 2024W",
    "section": "Visualizing ridge regression (2 coefficients)",
    "text": "Visualizing ridge regression (2 coefficients)\n\n\nCode\nb &lt;- c(1, 1)\nn &lt;- 1000\nlams &lt;- c(1, 5, 10)\nols_loss &lt;- function(b1, b2) colMeans((y - X %*% rbind(b1, b2))^2) / 2\npen &lt;- function(b1, b2, lambda = 1) lambda * (b1^2 + b2^2) / 2\ngr &lt;- expand_grid(\n  b1 = seq(b[1] - 0.5, b[1] + 0.5, length.out = 100),\n  b2 = seq(b[2] - 0.5, b[2] + 0.5, length.out = 100)\n)\n\nX &lt;- mvtnorm::rmvnorm(n, c(0, 0), sigma = matrix(c(1, .3, .3, .5), nrow = 2))\ny &lt;- drop(X %*% b + rnorm(n))\n\nbols &lt;- coef(lm(y ~ X - 1))\nbridge &lt;- coef(MASS::lm.ridge(y ~ X - 1, lambda = lams * sqrt(n)))\n\npenalties &lt;- lams |&gt;\n  set_names(~ paste(\"lam =\", .)) |&gt;\n  map(~ pen(gr$b1, gr$b2, .x)) |&gt;\n  as_tibble()\ngr &lt;- gr |&gt;\n  mutate(loss = ols_loss(b1, b2)) |&gt;\n  bind_cols(penalties)\n\ng1 &lt;- ggplot(gr, aes(b1, b2)) +\n  geom_raster(aes(fill = loss)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = 0.5))\n\ng2 &lt;- gr |&gt;\n    pivot_longer(starts_with(\"lam\")) |&gt;\n    mutate(name = factor(name, levels = paste(\"lam =\", lams))) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"penalty\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ng3 &lt;- gr |&gt; \n  mutate(across(starts_with(\"lam\"), ~ loss + .x)) |&gt;\n  pivot_longer(starts_with(\"lam\")) |&gt;\n  mutate(name = factor(name, levels = paste(\"lam =\", lams))) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = value)) +\n  scale_fill_viridis_c(direction = -1, name = \"loss + pen\") +\n  facet_wrap(~name, ncol = 1) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2])) +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 10, barheight = 0.5))\n\ncowplot::plot_grid(g1, g2, g3, rel_widths = c(2, 1, 1), nrow = 1)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#the-effect-on-the-estimates",
    "href": "schedule/slides/08-ridge-regression.html#the-effect-on-the-estimates",
    "title": "UBC Stat406 2024W",
    "section": "The effect on the estimates",
    "text": "The effect on the estimates\n\n\nCode\ngr |&gt; \n  mutate(z = ols_loss(b1, b2) + max(lams) * pen(b1, b2)) |&gt;\n  ggplot(aes(b1, b2)) +\n  geom_raster(aes(fill = z)) +\n  scale_fill_viridis_c(direction = -1) +\n  geom_point(data = tibble(\n    b1 = c(bols[1], bridge[,1]),\n    b2 = c(bols[2], bridge[,2]),\n    estimate = factor(c(\"ols\", paste0(\"ridge = \", lams)), \n                      levels = c(\"ols\", paste0(\"ridge = \", lams)))\n  ),\n  aes(shape = estimate), size = 3) +\n  geom_point(data = data.frame(b1 = b[1], b2 = b[2]), colour = orange, size = 4)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#example-data",
    "href": "schedule/slides/08-ridge-regression.html#example-data",
    "title": "UBC Stat406 2024W",
    "section": "Example data",
    "text": "Example data\nprostate data from [ESL]\n\ndata(prostate, package = \"ElemStatLearn\")\nprostate |&gt; as_tibble()\n\n# A tibble: 97 × 10\n   lcavol lweight   age   lbph   svi   lcp gleason pgg45   lpsa train\n    &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;  &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;   &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;lgl&gt;\n 1 -0.580    2.77    50 -1.39      0 -1.39       6     0 -0.431 TRUE \n 2 -0.994    3.32    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 3 -0.511    2.69    74 -1.39      0 -1.39       7    20 -0.163 TRUE \n 4 -1.20     3.28    58 -1.39      0 -1.39       6     0 -0.163 TRUE \n 5  0.751    3.43    62 -1.39      0 -1.39       6     0  0.372 TRUE \n 6 -1.05     3.23    50 -1.39      0 -1.39       6     0  0.765 TRUE \n 7  0.737    3.47    64  0.615     0 -1.39       6     0  0.765 FALSE\n 8  0.693    3.54    58  1.54      0 -1.39       6     0  0.854 TRUE \n 9 -0.777    3.54    47 -1.39      0 -1.39       6     0  1.05  FALSE\n10  0.223    3.24    63 -1.39      0 -1.39       6     0  1.05  FALSE\n# ℹ 87 more rows\n\n\n\nUse lpsa as response."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-path",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-path",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression path",
    "text": "Ridge regression path\n\nY &lt;- prostate$lpsa\nX &lt;- model.matrix(~ ., data = prostate |&gt; dplyr::select(-train, -lpsa))\nlibrary(glmnet)\nridge &lt;- glmnet(x = X, y = Y, alpha = 0, lambda.min.ratio = .00001)\n\n\n\n\n\nplot(ridge, xvar = \"lambda\", lwd = 3)\n\n\n\n\n\n\n\n\n\n\nModel selection here:\n\nmeans choose some \\(\\lambda\\)\nA value of \\(\\lambda\\) is a vertical line.\nThis graphic is a “path” or “coefficient trace”\nCoefficients for varying \\(\\lambda\\)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#solving-the-minimization",
    "href": "schedule/slides/08-ridge-regression.html#solving-the-minimization",
    "title": "UBC Stat406 2024W",
    "section": "Solving the minimization",
    "text": "Solving the minimization\n\nOne nice thing about ridge regression is that it has a closed-form solution (like OLS)\n\n\\[\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nThis is easy to calculate in R for any \\(\\lambda\\).\nHowever, computations and interpretation are simplified if we examine the Singular Value Decomposition of \\(\\X = \\mathbf{UDV}^\\top\\).\nRecall: any matrix has an SVD.\nHere \\(\\mathbf{D}\\) is diagonal and \\(\\mathbf{U}\\) and \\(\\mathbf{V}\\) are orthonormal: \\(\\mathbf{U}^\\top\\mathbf{U} = \\mathbf{I}\\)."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#solving-the-minization",
    "href": "schedule/slides/08-ridge-regression.html#solving-the-minization",
    "title": "UBC Stat406 2024W",
    "section": "Solving the minization",
    "text": "Solving the minization\n\\[\\brl = (\\X^\\top\\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y\\]\n\nNote that \\(\\mathbf{X}^\\top\\mathbf{X} = \\mathbf{VDU}^\\top\\mathbf{UDV}^\\top = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^\\top\\).\nThen,\n\n\\[\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top + \\lambda \\mathbf{I})^{-1}\\mathbf{VDU}^\\top \\y\n= \\mathbf{V}(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1} \\mathbf{DU}^\\top \\y.\\]\n\nFor computations, now we only need to invert \\(\\mathbf{D}\\)."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#comparing-with-ols",
    "href": "schedule/slides/08-ridge-regression.html#comparing-with-ols",
    "title": "UBC Stat406 2024W",
    "section": "Comparing with OLS",
    "text": "Comparing with OLS\n\n\\(\\mathbf{D}\\) is a diagonal matrix\n\n\\[\\bls = (\\X^\\top\\X)^{-1}\\X^\\top \\y = (\\mathbf{VD}^2\\mathbf{V}^\\top)^{-1}\\mathbf{VDU}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-2}\\mathbf{D}}\\mathbf{U}^\\top \\y = \\mathbf{V}\\color{red}{\\mathbf{D}^{-1}}\\mathbf{U}^\\top \\y\\]\n\\[\\brl = (\\X^\\top \\X + \\lambda \\mathbf{I})^{-1}\\X^\\top \\y = \\mathbf{V}\\color{red}{(\\mathbf{D}^2+\\lambda \\mathbf{I})^{-1}} \\mathbf{DU}^\\top \\y.\\]\n\nNotice that \\(\\bls\\) depends on \\(d_j/d_j^2\\) while \\(\\brl\\) depends on \\(d_j/(d_j^2 + \\lambda)\\).\nRidge regression makes the coefficients smaller relative to OLS.\nBut if \\(\\X\\) has small singular values, ridge regression compensates with \\(\\lambda\\) in the denominator."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-and-multicollinearity",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-and-multicollinearity",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression and multicollinearity",
    "text": "Ridge regression and multicollinearity\nMulticollinearity: a linear combination of predictor variables is nearly equal to another predictor variable.\nSome comments:\n\nA better phrase: \\(\\X\\) is ill-conditioned\nAKA “(numerically) rank-deficient”.\n\\(\\X = \\mathbf{U D V}^\\top\\) ill-conditioned \\(\\Longleftrightarrow\\) some elements of \\(\\mathbf{D} \\approx 0\\)\n\\(\\bls= \\mathbf{V D}^{-1} \\mathbf{U}^\\top \\y\\), so small entries of \\(\\mathbf{D}\\) \\(\\Longleftrightarrow\\) huge elements of \\(\\mathbf{D}^{-1}\\)\nMeans huge variance: \\(\\Var{\\bls} =  \\sigma^2(\\X^\\top \\X)^{-1} = \\sigma^2 \\mathbf{V D}^{-2} \\mathbf{V}^\\top\\)"
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#ridge-regression-and-ill-posed-x",
    "href": "schedule/slides/08-ridge-regression.html#ridge-regression-and-ill-posed-x",
    "title": "UBC Stat406 2024W",
    "section": "Ridge regression and ill-posed \\(\\X\\)",
    "text": "Ridge regression and ill-posed \\(\\X\\)\nRidge Regression fixes this problem by preventing the division by a near-zero number\n\nConclusion\n\n\\((\\X^{\\top}\\X)^{-1}\\) can be really unstable, while \\((\\X^{\\top}\\X + \\lambda \\mathbf{I})^{-1}\\) is not.\n\nAside\n\nEngineering approach to solving linear systems is to always do this with small \\(\\lambda\\). The thinking is about the numerics rather than the statistics.\n\n\nWhich \\(\\lambda\\) to use?\n\nComputational\n\nUse CV and pick the \\(\\lambda\\) that makes this smallest.\n\nIntuition (bias)\n\nAs \\(\\lambda\\rightarrow\\infty\\), bias ⬆\n\nIntuition (variance)\n\nAs \\(\\lambda\\rightarrow\\infty\\), variance ⬇\n\n\nYou should think about why."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds",
    "href": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds",
    "title": "UBC Stat406 2024W",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\nTo recap:\n\nDeciding which predictors to include, adding quadratic terms, or interactions is model selection (more precisely variable selection within a linear model).\nRidge regression provides regularization, which trades off bias and variance and also stabilizes multicollinearity.\nIf the LM is true,\n\nOLS is unbiased, but Variance depends on \\(\\mathbf{D}^{-2}\\). Can be big.\nRidge is biased (can you find the bias?). But Variance is smaller than OLS.\n\nRidge regression does not perform variable selection.\nBut picking \\(\\lambda=3.7\\) and thereby deciding to predict with \\(\\widehat{\\beta}^R_{3.7}\\) is model selection."
  },
  {
    "objectID": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds-1",
    "href": "schedule/slides/08-ridge-regression.html#can-we-get-the-best-of-both-worlds-1",
    "title": "UBC Stat406 2024W",
    "section": "Can we get the best of both worlds?",
    "text": "Can we get the best of both worlds?\n\nRidge regression\n\n\\(\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_2^2 \\leq s\\)\n\nBest (in-sample) linear regression model of size \\(s\\)\n\n\\(\\minimize \\frac{1}{n}||\\y-\\X\\beta||_2^2 \\ \\st\\ ||\\beta||_0 \\leq s\\)\n\n\n\\(||\\beta||_0\\) is the number of nonzero elements in \\(\\beta\\)\nFinding the best in-sample linear model (of size \\(s\\), among these predictors) is a nonconvex optimization problem (In fact, it is NP-hard)\nRidge regression is convex (easy to solve), but doesn’t do variable selection\nCan we somehow “interpolate” to get both?\nNote: selecting \\(\\lambda\\) is still model selection, but we’ve included all the variables."
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#section",
    "href": "schedule/slides/00-quiz-0-wrap.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Quiz 0 fun",
    "text": "00 Quiz 0 fun\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 10 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#why-this-class",
    "href": "schedule/slides/00-quiz-0-wrap.html#why-this-class",
    "title": "UBC Stat406 2024W",
    "section": "Why this class?",
    "text": "Why this class?\n\n“This is a required course”\n“Heard it’s fun!”\n“I want to graduate”\n“8am!” (/s)\n“I want to learn a lot about ML.”\n“I want to dig deeper after taking 306.”\n“I want to get… more comfortable with R.”\n“At first it was because everyone I knew was taking it.”\n“Practical skills in preperation for future careers”"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#syllabus-q",
    "href": "schedule/slides/00-quiz-0-wrap.html#syllabus-q",
    "title": "UBC Stat406 2024W",
    "section": "Syllabus Q",
    "text": "Syllabus Q"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#programming-languages",
    "href": "schedule/slides/00-quiz-0-wrap.html#programming-languages",
    "title": "UBC Stat406 2024W",
    "section": "Programming languages",
    "text": "Programming languages"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#plans",
    "href": "schedule/slides/00-quiz-0-wrap.html#plans",
    "title": "UBC Stat406 2024W",
    "section": "Plans",
    "text": "Plans"
  },
  {
    "objectID": "schedule/slides/00-quiz-0-wrap.html#grade-predictions",
    "href": "schedule/slides/00-quiz-0-wrap.html#grade-predictions",
    "title": "UBC Stat406 2024W",
    "section": "Grade predictions",
    "text": "Grade predictions\n\n\n5 people say 100%\n18 say 90%\n20 say 85%\n41 say 80%\n\n\n\n\nUBC Stat 406 - 2024"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#section",
    "href": "schedule/slides/27-kmeans.html#section",
    "title": "UBC Stat406 2024W",
    "section": "27 K-means clustering",
    "text": "27 K-means clustering\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 30 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#clustering",
    "href": "schedule/slides/27-kmeans.html#clustering",
    "title": "UBC Stat406 2024W",
    "section": "Clustering",
    "text": "Clustering\nSo far, we’ve looked at ways of reducing the dimension.\nEither linearly or nonlinearly,\n\nThe goal is visualization/exploration or possibly for an input to supervised learning.\n\nNow we try to find groups or clusters in our data.\nThink of clustering as classification without the labels."
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#k-means-ideally",
    "href": "schedule/slides/27-kmeans.html#k-means-ideally",
    "title": "UBC Stat406 2024W",
    "section": "K-means (ideally)",
    "text": "K-means (ideally)\n\nSelect a number of clusters \\(K\\).\nLet \\(C_1,\\ldots,C_K\\) partition \\(\\{1,2,3,\\ldots,n\\}\\) such that\n\nAll observations belong to some set \\(C_k\\).\nNo observation belongs to more than one set.\n\nMake within-cluster variation, \\(W(C_k)\\), as small as possible. \\[\\min_{C_1,\\ldots,C_K} \\sum_{k=1}^K W(C_k).\\]\nDefine \\(W\\) as \\[W(C_k) =  \\frac{1}{2|C_k|} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2.\\] That is, the average (Euclidean) distance between all cluster members.\n\n\nTo work, K-means needs distance to a center and notion of center"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#why-this-formula",
    "href": "schedule/slides/27-kmeans.html#why-this-formula",
    "title": "UBC Stat406 2024W",
    "section": "Why this formula?",
    "text": "Why this formula?\nLet \\(\\overline{x}_k = \\frac{1}{|C_k|} \\sum_{i\\in C_k} x_i\\)\n\\[\n\\begin{aligned}\n\\sum_{k=1}^K W(C_k)\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2\n= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i\\neq i' \\in C_k} \\norm{x_i - x_{i'}}_2^2 \\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\sum_{i \\neq i' \\in C_k} \\norm{x_i -\\overline{x}_k + \\overline{x}_k - x_{i'}}_2^2\\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\left[\\sum_{i \\neq i' \\in C_k} \\left(\\norm{x_i - \\overline{x}_k}_2^2 +\n\\norm{x_{i'} - \\overline{x}_k}_2^2\\right) + \\sum_{i \\neq i' \\in C_k} 2 (x_i-\\overline{x}_k)^\\top(\\overline{x}_k - x_{i'})\\right]\\\\\n&= \\sum_{k=1}^K \\frac{1}{2|C_k|} \\left[2(|C_k|-1)\\sum_{i \\in C_k} \\norm{x_i - \\overline{x}_k}_2^2  + 2\\sum_{i \\in C_k} \\norm{x_i - \\overline{x}_k}_2^2 \\right]\\\\\n&= \\sum_{k=1}^K \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2\n\\end{aligned}\n\\]\nIf you wanted (equivalently) to minimize \\(\\sum_{k=1}^K \\frac{1}{|C_k|} \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2\\), then you’d use \\(\\sum_{k=1}^K \\frac{1}{\\binom{C_k}{2}} \\sum_{i, i' \\in C_k} \\norm{x_i - x_{i'}}_2^2\\)"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#k-means-in-reality",
    "href": "schedule/slides/27-kmeans.html#k-means-in-reality",
    "title": "UBC Stat406 2024W",
    "section": "K-means (in reality)",
    "text": "K-means (in reality)\nThis is too computationally challenging ( \\(K^n\\) partions! ) \\[\\min_{C_1,\\ldots,C_K} \\sum_{k=1}^K W(C_k).\\] So, we make a greedy approximation:\n\nRandomly assign observations to the \\(K\\) clusters\nIterate the following:\n\nFor each cluster, compute the \\(p\\)-length vector of the means in that cluster.\nAssign each observation to the cluster whose centroid is closest (in Euclidean distance).\n\n\nThis procedure is guaranteed to decrease \\(\\sum_{k=1}^K W(C_k)\\) at each step."
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#best-practices",
    "href": "schedule/slides/27-kmeans.html#best-practices",
    "title": "UBC Stat406 2024W",
    "section": "Best practices",
    "text": "Best practices\nTo fit K-means, you need to\n\nPick \\(K\\) (inherent in the method)\nConvince yourself you have found a good solution (due to the randomized / greedy algorithm).\n\nFor 2., run K-means many times with different starting points. Pick the solution that has the smallest value for \\[\\sum_{k=1}^K W(C_k)\\]\nIt turns out that 1. is difficult to do in a principled way."
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#choosing-the-number-of-clusters",
    "href": "schedule/slides/27-kmeans.html#choosing-the-number-of-clusters",
    "title": "UBC Stat406 2024W",
    "section": "Choosing the Number of Clusters",
    "text": "Choosing the Number of Clusters\nWhy is it important?\n\nIt might make a big difference (concluding there are \\(K = 2\\) cancer sub-types versus \\(K = 3\\)).\nOne of the major goals of statistical learning is automatic inference. A good way of choosing \\(K\\) is certainly a part of this."
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#withinness-and-betweenness",
    "href": "schedule/slides/27-kmeans.html#withinness-and-betweenness",
    "title": "UBC Stat406 2024W",
    "section": "Withinness and betweenness",
    "text": "Withinness and betweenness\n\\[W(K) = \\sum_{k=1}^K W(C_k) = \\sum_{k=1}^K \\sum_{x \\in C_k} \\norm{x-\\overline{x}_k}^2_2,\\]\nWithin-cluster variation measures how tightly grouped the clusters are.\n\nIt’s opposite is Between-cluster variation: How spread apart are the clusters?\n\\[B(K) = \\sum_{k=1}^K |C_k| \\norm{\\overline{x}_k - \\overline{x} }_2^2,\\]\nwhere \\(|C_k|\\) is the number of points in \\(C_k\\), and \\(\\overline{x}\\) is the grand mean\n\n\n\\(W\\)  when \\(K\\) \n\n\n\\(B\\)  when \\(K\\) \n\n\n\\(B/K\\)  when \\(K\\)"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#ch-index",
    "href": "schedule/slides/27-kmeans.html#ch-index",
    "title": "UBC Stat406 2024W",
    "section": "CH index",
    "text": "CH index\n\nWant small \\(W\\), big \\(B/K\\)\n\n\\[\\textrm{CH}(K) = \\frac{B(K)/(K-1)}{W(K)/(n-K)}\\]\nTo choose \\(K\\), pick some maximum number of clusters to be considered, \\(K_{\\max} = 20\\), for example\n\\[\\widehat K = \\argmax_{K \\in \\{ 2,\\ldots, K_{\\max} \\}} CH(K).\\]\n\n\n\n\n\n\nNote\n\n\n\nCH is undefined for \\(K = 1\\).\nThe divisors \\((K-1)\\) and \\((n-K)\\) scale \\(B\\) and \\(W\\) appropriately."
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#dumb-example",
    "href": "schedule/slides/27-kmeans.html#dumb-example",
    "title": "UBC Stat406 2024W",
    "section": "Dumb example",
    "text": "Dumb example\n\nlibrary(mvtnorm)\nset.seed(406406406)\nX1 &lt;- rmvnorm(50, c(-1, 2), sigma = matrix(c(1, .5, .5, 1), 2))\nX2 &lt;- rmvnorm(40, c(2, -1), sigma = matrix(c(1.5, .5, .5, 1.5), 2))\nX3 &lt;- rmvnorm(40, c(4, 4))"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#dumb-example-1",
    "href": "schedule/slides/27-kmeans.html#dumb-example-1",
    "title": "UBC Stat406 2024W",
    "section": "Dumb example",
    "text": "Dumb example\n\nWe would maximize CH\n\n\n\nCode\nK &lt;- 2:40\nN &lt;- nrow(clust_raw)\nall_clusters &lt;- map(K, ~ kmeans(clust_raw, .x, nstart = 20))\nall_assignments &lt;- map_dfc(all_clusters, \"cluster\")\nnames(all_assignments) &lt;- paste0(\"K = \", K)\nsummaries &lt;- map_dfr(all_clusters, `[`, c(\"tot.withinss\", \"betweenss\")) |&gt;\n  rename(W = tot.withinss, B = betweenss) |&gt;\n  mutate(\n    K = K,\n    `W / (N - K)` = W / (N - K),\n    `B / K` = B / (K - 1), \n    `CH index` = `B / K` / `W / (N - K)`\n  )\nsummaries |&gt;\n  pivot_longer(-K) |&gt;\n  ggplot(aes(K, value)) +\n  geom_line(color = blue, linewidth = 2) +\n  ylab(\"\") +\n  coord_cartesian(c(1, 20)) +\n  facet_wrap(~name, ncol = 3, scales = \"free_y\")"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#dumb-example-2",
    "href": "schedule/slides/27-kmeans.html#dumb-example-2",
    "title": "UBC Stat406 2024W",
    "section": "Dumb example",
    "text": "Dumb example"
  },
  {
    "objectID": "schedule/slides/27-kmeans.html#dumb-example-3",
    "href": "schedule/slides/27-kmeans.html#dumb-example-3",
    "title": "UBC Stat406 2024W",
    "section": "Dumb example",
    "text": "Dumb example\n\n\\(K = 3\\)\n\n\nkm &lt;- kmeans(clust_raw, 3, nstart = 20)\nnames(km)\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\ncenters &lt;- as_tibble(km$centers, .name_repair = \"unique\")"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#section",
    "href": "schedule/slides/14-classification-intro.html#section",
    "title": "UBC Stat406 2024W",
    "section": "14 Classification",
    "text": "14 Classification\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 09 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#an-overview-of-classification",
    "href": "schedule/slides/14-classification-intro.html#an-overview-of-classification",
    "title": "UBC Stat406 2024W",
    "section": "An Overview of Classification",
    "text": "An Overview of Classification\n\nA person arrives at an emergency room with a set of symptoms that could be 1 of 3 possible conditions. Which one is it?\nAn online banking service must be able to determine whether each transaction is fraudulent or not, using a customer’s location, past transaction history, etc.\nGiven a set of individuals sequenced DNA, can we determine whether various mutations are associated with different phenotypes?\n\n\nThese problems are not regression problems. They are classification problems."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#the-set-up",
    "href": "schedule/slides/14-classification-intro.html#the-set-up",
    "title": "UBC Stat406 2024W",
    "section": "The Set-up",
    "text": "The Set-up\nIt begins just like regression: suppose we have observations \\[\\{(x_1,y_1),\\ldots,(x_n,y_n)\\}\\]\nAgain, we want to estimate a function that maps \\(X\\) to \\(Y\\) to predict as yet observed data.\n(This function is known as a classifier)\nThe same constraints apply:\n\nWe want a classifier that predicts test data, not just the training data.\nOften, this comes with the introduction of some bias to get lower variance and better predictions."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\nBefore in regression, we have \\(y_i \\in \\mathbb{R}\\) and use squared error loss to measure accuracy: \\((y - \\hat{y})^2\\).\nInstead, let \\(y \\in \\mathcal{K} = \\{1,\\ldots, K\\}\\)\n(This is arbitrary, sometimes other numbers, such as \\(\\{-1,1\\}\\) will be used)\nWe can always take “factors”: \\(\\{\\textrm{cat},\\textrm{dog}\\}\\) and convert to integers, which is what we assume.\nWe again make predictions \\(\\hat{y}=k\\) based on the data\n\nWe get zero loss if we predict the right class\nWe lose \\(\\ell(k,k')\\) on \\((k\\neq k')\\) for incorrect predictions"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-1",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-1",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\nSuppose you have a fever of 39º C. You get a rapid test on campus.\n\n\n\nLoss\nTest +\nTest -\n\n\n\n\nAre +\n0\nInfect others\n\n\nAre -\nIsolation\n0"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-2",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-2",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\nSuppose you have a fever of 39º C. You get a rapid test on campus.\n\n\n\nLoss\nTest +\nTest -\n\n\n\n\nAre +\n0\n1\n\n\nAre -\n1\n0"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-3",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-3",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\n\nWe’re going to use \\(g(x)\\) to be our classifier. It takes values in \\(\\mathcal{K}\\)."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-4",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-4",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\nAgain, we appeal to risk \\[R_n(g) = E [\\ell(Y,g(X))]\\] If we use the law of total probability, this can be written \\[R_n(g) = E_X \\sum_{y=1}^K \\ell(y,\\; g(X)) Pr(Y = y \\given X)\\] We minimize this over a class of options \\(\\mathcal{G}\\), to produce \\[g_*(X) = \\argmin_{g\\in\\mathcal{G}} E_X \\sum_{y=1}^K \\ell(y,g(X)) Pr(Y = y \\given X)\\]"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-5",
    "href": "schedule/slides/14-classification-intro.html#how-do-we-measure-quality-5",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality?",
    "text": "How do we measure quality?\n\\(g_*\\) is named the Bayes’ classifier for loss \\(\\ell\\) in class \\(\\mathcal{G}\\).\n\\(R_n(g_*)\\) is the called the Bayes’ limit or Bayes’ Risk.\nIt’s the best we could hope to do in terms of \\(\\ell\\) if we knew the distribution of the data.\n\nBut we don’t, so we’ll try to do our best to estimate \\(g_*\\)."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#best-classifier-overall",
    "href": "schedule/slides/14-classification-intro.html#best-classifier-overall",
    "title": "UBC Stat406 2024W",
    "section": "Best classifier overall",
    "text": "Best classifier overall\n(for now, we limit to 2 classes)\nOnce we make a specific choice for \\(\\ell\\), we can find \\(g_*\\) exactly (pretending we know the distribution)\nBecause \\(Y\\) takes only a few values, zero-one loss is natural (but not the only option) \\[\\ell(y,\\ g(x)) = \\begin{cases}0 & y=g(x)\\\\1 & y\\neq g(x) \\end{cases} \\Longrightarrow R_n(g) = \\Expect{\\ell(Y,\\ g(X))} = Pr(g(X) \\neq Y),\\]"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#best-classifier-overall-1",
    "href": "schedule/slides/14-classification-intro.html#best-classifier-overall-1",
    "title": "UBC Stat406 2024W",
    "section": "Best classifier overall",
    "text": "Best classifier overall\n\n\n\nLoss\nTest +\nTest -\n\n\n\n\nAre +\n0\n1\n\n\nAre -\n1\n0"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#best-classifier-overall-2",
    "href": "schedule/slides/14-classification-intro.html#best-classifier-overall-2",
    "title": "UBC Stat406 2024W",
    "section": "Best classifier overall",
    "text": "Best classifier overall\nThis means we want to classify a new observation \\((x_0,y_0)\\) such that \\(g(x_0) = y_0\\) as often as possible\nUnder this loss, we have \\[\n\\begin{aligned}\ng_*(X) &= \\argmin_{g} Pr(g(X) \\neq Y) \\\\\n&= \\argmin_{g} \\left[ 1 - Pr(Y = g(x) | X=x)\\right]  \\\\\n&= \\argmax_{g} Pr(Y = g(x) | X=x )\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#estimating-g_",
    "href": "schedule/slides/14-classification-intro.html#estimating-g_",
    "title": "UBC Stat406 2024W",
    "section": "Estimating \\(g_*\\)",
    "text": "Estimating \\(g_*\\)\nClassifier approach 1 (empirical risk minimization):\n\nChoose some class of classifiers \\(\\mathcal{G}\\).\nFind \\(\\argmin_{g\\in\\mathcal{G}} \\sum_{i = 1}^n I(g(x_i) \\neq y_i)\\)"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#bayes-classifier-and-class-densities-2-classes",
    "href": "schedule/slides/14-classification-intro.html#bayes-classifier-and-class-densities-2-classes",
    "title": "UBC Stat406 2024W",
    "section": "Bayes’ Classifier and class densities (2 classes)",
    "text": "Bayes’ Classifier and class densities (2 classes)\nUsing Bayes’ theorem, and recalling that \\(f_*(X) = E[Y \\given X]\\)\n\\[\\begin{aligned}\nf_*(X) & = E[Y \\given X] = Pr(Y = 1 \\given X) \\\\\n&= \\frac{Pr(X\\given Y=1) Pr(Y=1)}{Pr(X)}\\\\\n& =\\frac{Pr(X\\given Y = 1) Pr(Y = 1)}{\\sum_{k \\in \\{0,1\\}} Pr(X\\given Y = k) Pr(Y = k)} \\\\ & = \\frac{p_1(X) \\pi}{ p_1(X)\\pi + p_0(X)(1-\\pi)}\\end{aligned}\\]\n\nWe call \\(p_k(X)\\) the class (conditional) densities\n\\(\\pi\\) is the marginal probability \\(P(Y=1)\\)"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#bayes-classifier-and-class-densities-2-classes-1",
    "href": "schedule/slides/14-classification-intro.html#bayes-classifier-and-class-densities-2-classes-1",
    "title": "UBC Stat406 2024W",
    "section": "Bayes’ Classifier and class densities (2 classes)",
    "text": "Bayes’ Classifier and class densities (2 classes)\nThe Bayes’ Classifier (best classifier for 0-1 loss) can be rewritten\n\\[g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} &gt; \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}\\]\nApproach 2: estimate everything in the expression above.\n\nWe need to estimate \\(p_1\\), \\(p_2\\), \\(\\pi\\), \\(1-\\pi\\)\nEasily extended to more than two classes"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#an-alternative-easy-classifier",
    "href": "schedule/slides/14-classification-intro.html#an-alternative-easy-classifier",
    "title": "UBC Stat406 2024W",
    "section": "An alternative easy classifier",
    "text": "An alternative easy classifier\nZero-One loss was natural, but try something else\nLet’s try using squared error loss instead: \\(\\ell(y,\\ f(x)) = (y - f(x))^2\\)\nThen, the Bayes’ Classifier (the function that minimizes the Bayes Risk) is \\[g_*(x) = f_*(x) = E[ Y \\given X = x] = Pr(Y = 1 \\given X)\\] (recall that \\(f_* \\in [0,1]\\) is still the regression function)\nIn this case, our “class” will actually just be a probability. But this isn’t a class, so it’s a bit unsatisfying.\nHow do we get a class prediction?\n\nDiscretize the probability:\n\\[g(x) = \\begin{cases}0 & f_*(x) &lt; 1/2\\\\1 & \\textrm{else}\\end{cases}\\]"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#estimating-g_-1",
    "href": "schedule/slides/14-classification-intro.html#estimating-g_-1",
    "title": "UBC Stat406 2024W",
    "section": "Estimating \\(g_*\\)",
    "text": "Estimating \\(g_*\\)\nApproach 3:\n\nEstimate \\(f_*\\) using any method we’ve learned so far.\nPredict 0 if \\(\\hat{f}(x)\\) is less than 1/2, else predict 1."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression",
    "href": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression",
    "title": "UBC Stat406 2024W",
    "section": "Claim: Classification is easier than regression",
    "text": "Claim: Classification is easier than regression\n\nLet \\(\\hat{f}\\) be any estimate of \\(f_*\\)\nLet \\(\\widehat{g} (x) = \\begin{cases}0 & \\hat f(x) &lt; 1/2\\\\1 & else\\end{cases}\\)\n\nProof by picture."
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-1",
    "href": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-1",
    "title": "UBC Stat406 2024W",
    "section": "Claim: Classification is easier than regression",
    "text": "Claim: Classification is easier than regression\n\n\nCode\nset.seed(12345)\nx &lt;- 1:99 / 100\ny &lt;- rbinom(99, 1, \n            .25 + .5 * (x &gt; .3 & x &lt; .5) + \n              .6 * (x &gt; .7))\ndmat &lt;- as.matrix(dist(x))\nksm &lt;- function(sigma) {\n  gg &lt;-  dnorm(dmat, sd = sigma) \n  sweep(gg, 1, rowSums(gg), '/') %*% y\n}\nfstar &lt;- ksm(.04)\ngg &lt;- tibble(x = x, fstar = fstar, y = y) %&gt;%\n  ggplot(aes(x)) +\n  geom_point(aes(y = y), color = blue) +\n  geom_line(aes(y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1)) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5)\ngg"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-2",
    "href": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-2",
    "title": "UBC Stat406 2024W",
    "section": "Claim: Classification is easier than regression",
    "text": "Claim: Classification is easier than regression\n\n\nCode\ngg + geom_hline(yintercept = .5, color = green)"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-3",
    "href": "schedule/slides/14-classification-intro.html#claim-classification-is-easier-than-regression-3",
    "title": "UBC Stat406 2024W",
    "section": "Claim: Classification is easier than regression",
    "text": "Claim: Classification is easier than regression\n\n\nCode\ntib &lt;- tibble(x = x, fstar = fstar, y = y)\nggplot(tib) +\n  geom_vline(data = filter(tib, fstar &gt; 0.5), aes(xintercept = x), alpha = .5, color = green) +\n  annotate(\"label\", x = .75, y = .65, label = \"f_star\", size = 5) + \n  geom_point(aes(x = x, y = y), color = blue) +\n  geom_line(aes(x = x, y = fstar), color = orange, size = 2) +\n  coord_cartesian(ylim = c(0,1), xlim = c(0,1))"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#how-to-find-a-classifier",
    "href": "schedule/slides/14-classification-intro.html#how-to-find-a-classifier",
    "title": "UBC Stat406 2024W",
    "section": "How to find a classifier",
    "text": "How to find a classifier\nWhy did we go through that math?\nEach of these approaches suggests a way to find a classifier\n\nEmpirical risk minimization: Choose a set of classifiers \\(\\mathcal{G}\\) and find \\(g \\in \\mathcal{G}\\) that minimizes some estimate of \\(R_n(g)\\)\n\n\n(This can be quite challenging as, unlike in regression, the training error is nonconvex)\n\n\nDensity estimation: Estimate \\(\\pi\\) and \\(p_k\\)\nRegression: Find an estimate \\(\\hat{f}\\) of \\(f^*\\) and compare the predicted value to 1/2"
  },
  {
    "objectID": "schedule/slides/14-classification-intro.html#section-1",
    "href": "schedule/slides/14-classification-intro.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Easiest classifier when \\(y\\in \\{0,\\ 1\\}\\):\n(stupidest version of the third case…)\n\nghat &lt;- round(predict(lm(y ~ ., data = trainingdata)))\n\nThink about why this may not be very good. (At least 2 reasons I can think of.)"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section",
    "href": "schedule/slides/04-bias-variance.html#section",
    "title": "UBC Stat406 2024W",
    "section": "04 Bias and variance",
    "text": "04 Bias and variance\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 16 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section-1",
    "href": "schedule/slides/04-bias-variance.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "We just talked about\n\nVariance of an estimator.\nIrreducible error when making predictions.\nThese are 2 of the 3 components of the “Prediction Risk” \\(R_n\\)"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#component-3-the-bias",
    "href": "schedule/slides/04-bias-variance.html#component-3-the-bias",
    "title": "UBC Stat406 2024W",
    "section": "Component 3, the Bias",
    "text": "Component 3, the Bias\nWe need to be specific about what we mean when we say bias.\nBias is neither good nor bad in and of itself.\nA very simple example: let \\(Y_1,\\ \\ldots,\\ Y_n \\sim N(\\mu, 1)\\). - We don’t know \\(\\mu\\), so we try to use the data (the \\(Y_i\\)’s) to estimate it.\n\nI propose 3 estimators:\n\n\\(\\widehat{\\mu}_1 = 12\\),\n\\(\\widehat{\\mu}_2=Y_6\\),\n\\(\\widehat{\\mu}_3=\\overline{Y}\\).\n\nThe bias (by definition) of my estimator is \\(E[\\widehat{\\mu_i}]-\\mu\\).\n\n\nCalculate the bias and variance of each estimator."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#regression-in-general",
    "href": "schedule/slides/04-bias-variance.html#regression-in-general",
    "title": "UBC Stat406 2024W",
    "section": "Regression in general",
    "text": "Regression in general\nIf I want to predict \\(Y\\) from \\(X\\), it is almost always the case that\n\\[\n\\mu(x) = \\Expect{Y\\given X=x} \\neq x^{\\top}\\beta\n\\]\nSo the bias of using a linear model is not zero.\n\nWhy? Because\n\\[\n\\Expect{Y\\given X=x}-x^\\top\\beta \\neq \\Expect{Y\\given X=x} - \\mu(x) = 0.\n\\]\nWe can include as many predictors as we like,\nbut this doesn’t change the fact that the world is non-linear."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#continuation-predicting-new-ys",
    "href": "schedule/slides/04-bias-variance.html#continuation-predicting-new-ys",
    "title": "UBC Stat406 2024W",
    "section": "(Continuation) Predicting new Y’s",
    "text": "(Continuation) Predicting new Y’s\nSuppose we want to predict \\(Y\\),\nwe know \\(E[Y]= \\mu \\in \\mathbb{R}\\) and \\(\\textrm{Var}[Y] = 1\\).\nOur data is \\(\\{y_1,\\ldots,y_n\\}\\)\nWe have considered estimating \\(\\mu\\) in various ways, and using \\(\\widehat{Y} = \\widehat{\\mu}\\)\n\n\nLet’s try one more: \\(\\widehat Y_a = a\\overline{Y}_n\\) for some \\(a \\in (0,1]\\)."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#one-can-show-wait-for-the-proof",
    "href": "schedule/slides/04-bias-variance.html#one-can-show-wait-for-the-proof",
    "title": "UBC Stat406 2024W",
    "section": "One can show… (wait for the proof)",
    "text": "One can show… (wait for the proof)\n\\(\\widehat Y_a = a\\overline{Y}_n\\) for some \\(a \\in (0,1]\\)\n\\[\nR_n(\\widehat Y_a) = \\Expect{(\\widehat Y_a-Y)^2} = (1 - a)^2\\mu^2 +\n\\frac{a^2}{n} +1\n\\]\n\nWe can minimize this to get the best possible prediction risk for an estimator of the form \\(\\widehat Y_a\\):\n\\[\n\\argmin_{a} R_n(\\widehat Y_a) = \\left(\\frac{\\mu^2}{\\mu^2 + 1/n} \\right)\\qquad\n\\min_{a} R_n(\\widehat Y_a) = 1+\\left(\\frac{\\mu^2}{n\\mu^2 + 1} \\right).\n\\]\n\n\nIs this less than or greater than the risk we saw for \\(\\bar Y\\)?\n\n\nAm I cheating here?"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section-2",
    "href": "schedule/slides/04-bias-variance.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Important\n\n\n\nWait a minute! I’m saying there is a better estimator than \\(\\overline{Y}_n\\)!"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-estimating-the-mean",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-estimating-the-mean",
    "title": "UBC Stat406 2024W",
    "section": "Bias-variance tradeoff: Estimating the mean",
    "text": "Bias-variance tradeoff: Estimating the mean\n\\[\nR_n(\\widehat Y_a) = (a - 1)^2\\mu^2 +  \\frac{a^2}{n} + \\sigma^2\n\\]\n\nmu = 1; n = 5; sig = 1"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#to-restate",
    "href": "schedule/slides/04-bias-variance.html#to-restate",
    "title": "UBC Stat406 2024W",
    "section": "To restate",
    "text": "To restate\nIf \\(\\mu=\\) 1 and \\(n=\\) 5\nthen it is better to predict with 0.83 \\(\\overline{Y}_5\\)\nthan with \\(\\overline{Y}_5\\) itself.\n\nFor this \\(a =\\) 0.83 and \\(n=5\\)\n\n\\(R_5(\\widehat{Y}_a) =\\) 1.17\n\\(R_5(\\overline{Y}_5)=\\) 1.2"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-decomposition",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-decomposition",
    "title": "UBC Stat406 2024W",
    "section": "Bias-variance decomposition",
    "text": "Bias-variance decomposition\n\\[R_n(\\widehat{Y}_a)=(a - 1)^2\\mu^2 + \\frac{a^2}{n} + 1\\]\n\nprediction risk = \\(\\textrm{bias}^2\\) + variance + irreducible error\nestimation risk = \\(\\textrm{bias}^2\\) + variance\n\nWhat is \\(R_n(\\widehat{Y}_a)\\) for our estimator \\(\\widehat{Y}_a=a\\overline{Y}_n\\)?\n\\[\\begin{aligned}\n\\textrm{bias}(\\widehat{Y}_a) &= \\Expect{a\\overline{Y}_n} - \\mu=(a-1)\\mu\\\\\n\\textrm{var}(\\widehat f(x)) &= \\Expect{ \\left(a\\overline{Y}_n - \\Expect{a\\overline{Y}_n}\\right)^2}\n=a^2\\Expect{\\left(\\overline{Y}_n-\\mu\\right)^2}=\\frac{a^2}{n} \\\\\n\\sigma^2 &= \\Expect{(Y-\\mu)^2}=1\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#this-decomposition-holds-generally",
    "href": "schedule/slides/04-bias-variance.html#this-decomposition-holds-generally",
    "title": "UBC Stat406 2024W",
    "section": "This decomposition holds generally",
    "text": "This decomposition holds generally\n\\[\\begin{aligned}\nR_n(\\hat{Y})\n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu + \\mu - \\hat{Y})^2} \\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} +\n2\\Expect{(Y-\\mu)(\\mu-\\hat{Y})}\\\\\n&= \\Expect{(Y-\\mu)^2} + \\Expect{(\\mu - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}] + E[\\hat{Y}] - \\hat{Y})^2}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 2\\Expect{(\\mu-E[\\hat{Y}])(E[\\hat{Y}] - \\hat{Y})}\\\\\n&= \\sigma^2 + \\Expect{(\\mu - E[\\hat{Y}])^2} + \\Expect{(E[\\hat{Y}] - \\hat{Y})^2} + 0\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-decomposition-1",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-decomposition-1",
    "title": "UBC Stat406 2024W",
    "section": "Bias-variance decomposition",
    "text": "Bias-variance decomposition\n\\[\\begin{aligned}\nR_n(\\hat{Y})\n&= \\Expect{(Y-\\hat{Y})^2} \\\\\n&= \\text{irr. error} + \\text{estimation risk}\\\\\n&= \\text{irr. error} + \\text{squared bias} + \\text{variance}\n\\end{aligned}\\]\n\n\n\n\n\n\nImportant\n\n\n\nImplication: prediction risk is estimation risk plus something you can’t control. However, defining estimation risk requires stronger assumptions (not always just estimating a parameter).\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nIn order to make good predictions, we want our prediction risk to be small. This means that we want to “balance” the bias and variance."
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#section-3",
    "href": "schedule/slides/04-bias-variance.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Code\ncols = c(blue, red, green, orange)\npar(mfrow = c(2, 2), bty = \"n\", ann = FALSE, xaxt = \"n\", yaxt = \"n\", \n    family = \"serif\", mar = c(0, 0, 0, 0), oma = c(0, 2, 2, 0))\nlibrary(mvtnorm)\nmv &lt;- matrix(c(0, 0, 0, 0, -.5, -.5, -.5, -.5), 4, byrow = TRUE)\nva &lt;- matrix(c(.02, .02, .1, .1, .02, .02, .1, .1), 4, byrow = TRUE)\n\nfor (i in 1:4) {\n  plot(0, 0, ylim = c(-2, 2), xlim = c(-2, 2), pch = 19, cex = 42, \n       col = blue, ann = FALSE, pty = \"s\")\n  points(0, 0, pch = 19, cex = 30, col = \"white\")\n  points(0, 0, pch = 19, cex = 18, col = green)\n  points(0, 0, pch = 19, cex = 6, col = orange)\n  points(rmvnorm(20, mean = mv[i, ], sigma = diag(va[i, ])), cex = 1, pch = 19)\n  switch(i,\n    \"1\" = {\n      mtext(\"low variance\", 3, cex = 2)\n      mtext(\"low bias\", 2, cex = 2)\n    },\n    \"2\" = mtext(\"high variance\", 3, cex = 2),\n    \"3\" = mtext(\"high bias\", 2, cex = 2)\n  )\n}"
  },
  {
    "objectID": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-overview",
    "href": "schedule/slides/04-bias-variance.html#bias-variance-tradeoff-overview",
    "title": "UBC Stat406 2024W",
    "section": "Bias-variance tradeoff: Overview",
    "text": "Bias-variance tradeoff: Overview\nbias: how well does \\(\\widehat{f}(x)\\) approximate the truth \\(\\Expect{Y\\given X=x}\\)\n\nIf we allow more complicated possible \\(\\widehat{f}\\), lower bias. Flexibility \\(\\Rightarrow\\) Expressivity\nBut, more flexibility \\(\\Rightarrow\\) larger variance\nComplicated models are hard to estimate precisely for fixed \\(n\\)\nIrreducible error\n\n\n\nSadly, that whole exercise depends on knowing the truth to evaluate \\(E\\ldots\\)"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#section",
    "href": "schedule/slides/00-cv-for-many-models.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 CV for many models",
    "text": "00 CV for many models\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 19 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#some-data-and-4-models",
    "href": "schedule/slides/00-cv-for-many-models.html#some-data-and-4-models",
    "title": "UBC Stat406 2024W",
    "section": "Some data and 4 models",
    "text": "Some data and 4 models\n\ndata(\"mobility\", package = \"Stat406\")\n\nModel 1: Lasso on all predictors, use CV min\nModel 2: Ridge on all predictors, use CV min\nModel 3: OLS on all predictors (no tuning parameters)\nModel 4: (1) Lasso on all predictors, then (2) OLS on those chosen at CV min\n\nHow do I decide between these 4 models?"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#cv-functions",
    "href": "schedule/slides/00-cv-for-many-models.html#cv-functions",
    "title": "UBC Stat406 2024W",
    "section": "CV functions",
    "text": "CV functions\n\nkfold_cv &lt;- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  fold_labels &lt;- sample(rep(seq_len(kfolds), length.out = nrow(data)))\n  errors &lt;- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows &lt;- fold_labels == fold\n    train &lt;- data[!test_rows, ]\n    test &lt;- data[test_rows, ]\n    current_model &lt;- estimator(train)\n    test$.preds &lt;- predictor(current_model, test)\n    errors[fold] &lt;- error_fun(test)\n  }\n  mean(errors)\n}\n\nloo_cv &lt;- function(dat) {\n  mdl &lt;- lm(Mobility ~ ., data = dat)\n  mean( abs(residuals(mdl)) / abs(1 - hatvalues(mdl)) ) # MAE version\n}"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#experiment-setup",
    "href": "schedule/slides/00-cv-for-many-models.html#experiment-setup",
    "title": "UBC Stat406 2024W",
    "section": "Experiment setup",
    "text": "Experiment setup\n\n# prepare our data\n# note that mob has only continuous predictors, otherwise could be trouble\nmob &lt;- mobility[complete.cases(mobility), ] |&gt; select(-ID, -State, -Name)\n# avoid doing this same operation a bunch\nxmat &lt;- function(dat) dat |&gt; select(!Mobility) |&gt; as.matrix()\n\n# set up our model functions\nlibrary(glmnet)\nmod1 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, type.measure = \"mae\", ...)\nmod2 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, alpha = 0, type.measure = \"mae\", ...)\nmod3 &lt;- function(dat, ...) glmnet(xmat(dat), dat$Mobility, lambda = 0, ...) # just does lm()\nmod4 &lt;- function(dat, ...) cv.glmnet(xmat(dat), dat$Mobility, relax = TRUE, gamma = 1, type.measure = \"mae\", ...)\n\n# this will still \"work\" on mod3, because there's only 1 s\npredictor &lt;- function(mod, dat) drop(predict(mod, newx = xmat(dat), s = \"lambda.min\"))\n\n# chose mean absolute error just 'cause\nerror_fun &lt;- function(testdata) mean(abs(testdata$Mobility - testdata$.preds))"
  },
  {
    "objectID": "schedule/slides/00-cv-for-many-models.html#run-the-experiment",
    "href": "schedule/slides/00-cv-for-many-models.html#run-the-experiment",
    "title": "UBC Stat406 2024W",
    "section": "Run the experiment",
    "text": "Run the experiment\n\nall_model_funs &lt;- lst(mod1, mod2, mod3, mod4)\nall_fits &lt;- map(all_model_funs, .f = exec, dat = mob)\n\n# unfortunately, does different splits for each method, so we use 10, \n# it would be better to use the _SAME_ splits\nten_fold_cv &lt;- map_dbl(all_model_funs, ~ kfold_cv(mob, .x, predictor, error_fun, 10)) \n\nin_sample_cv &lt;- c(\n  mod1 = min(all_fits[[1]]$cvm),\n  mod2 = min(all_fits[[2]]$cvm),\n  mod3 = loo_cv(mob),\n  mod4 = min(all_fits[[4]]$cvm)\n)\n\ntib &lt;- bind_rows(in_sample_cv, ten_fold_cv)\ntib$method = c(\"in_sample\", \"out_of_sample\")\ntib\n\n# A tibble: 2 × 5\n    mod1   mod2   mod3   mod4 method       \n   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;        \n1 0.0159 0.0161 0.0164 0.0156 in_sample    \n2 0.0158 0.0161 0.0165 0.0161 out_of_sample\n\n\n\n\n\nUBC Stat 406 - 2024"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#section",
    "href": "schedule/slides/01-lm-review.html#section",
    "title": "UBC Stat406 2024W",
    "section": "01 Linear model review",
    "text": "01 Linear model review\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 11 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#the-normal-linear-model",
    "href": "schedule/slides/01-lm-review.html#the-normal-linear-model",
    "title": "UBC Stat406 2024W",
    "section": "The normal linear model",
    "text": "The normal linear model\nAssume that\n\\[\ny_i = x_i^\\top \\beta + \\epsilon_i.\n\\]\n\n\nWhat variables are fixed, what are parameters, and what are random?\nWhat is the distribution of \\(\\epsilon_i\\)?\nWhat is the mean of \\(y_i\\)?\nWhat is the notation \\(\\mathbf{X}\\) or \\(\\mathbf{y}\\)?"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#drawing-a-sample",
    "href": "schedule/slides/01-lm-review.html#drawing-a-sample",
    "title": "UBC Stat406 2024W",
    "section": "Drawing a sample",
    "text": "Drawing a sample\n\\[\ny_i = x_i^\\top \\beta + \\epsilon_i.\n\\]\nHow would I create data from this model (draw a sample)?\n\nSet up constants\n\np &lt;- 3  # number of covariates\nn &lt;- 100  # number of data points\nsigma &lt;- 2  # stddev of \\epsilon\n\n\n\nCreate the data\n\nepsilon &lt;- rnorm(n, sd = sigma) # this is random\nX &lt;- matrix(runif(n * p), n, p) # treat this as fixed, but I need numbers\nbeta &lt;- (p + 1):1 # parameter, also fixed, but I again need numbers\nY &lt;- cbind(1, X) %*% beta + epsilon # epsilon is random, so this is\n## Equiv: Y &lt;- beta[1] + X %*% beta[-1] + epsilon"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#how-do-we-estimate-beta",
    "href": "schedule/slides/01-lm-review.html#how-do-we-estimate-beta",
    "title": "UBC Stat406 2024W",
    "section": "How do we estimate beta?",
    "text": "How do we estimate beta?\n\nGuess.\nOrdinary least squares (OLS).\nMaximum likelihood.\nDo something more creative."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ols",
    "href": "schedule/slides/01-lm-review.html#method-2.-ols",
    "title": "UBC Stat406 2024W",
    "section": "Method 2. OLS",
    "text": "Method 2. OLS\n\n\n\nI want to find an estimator \\(\\widehat\\beta\\) that makes small errors on my data.\nI measure errors with the squared difference between predictions \\(\\mathbf{X}\\widehat\\beta\\) and the responses \\(\\mathbf{y}\\).\n(Don’t care if the differences are positive or negative)\n\n\n\\[\\mathrm{Error} = \\sum_{i=1}^n ( y_i - x_i^\\top \\widehat\\beta )^2.\\]\n\n\n\n\n\n\nWhy squared errors? \\(( y_i - x_i^\\top \\widehat\\beta )^2\\)\nWhy not absolute errors \\(\\left\\lvert y_i - x_i^\\top \\widehat\\beta \\right\\rvert\\)?"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ols-solution",
    "href": "schedule/slides/01-lm-review.html#method-2.-ols-solution",
    "title": "UBC Stat406 2024W",
    "section": "Method 2. OLS solution",
    "text": "Method 2. OLS solution\nWe write this as\n\\[\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n\nFind the \\(\\beta\\) which minimizes the sum of squared errors.\n\n\nNote that this is the same as\n\\[\\widehat\\beta = \\argmin_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n\nFind the beta which minimizes the mean squared error."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-2.-ok-do-it",
    "href": "schedule/slides/01-lm-review.html#method-2.-ok-do-it",
    "title": "UBC Stat406 2024W",
    "section": "Method 2. Ok, do it",
    "text": "Method 2. Ok, do it\nWe differentiate and set to zero\n\\[\\begin{aligned}\n& \\frac{\\partial}{\\partial \\beta} \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2\\\\\n&= -\\frac{2}{n}\\sum_{i=1}^n x_i (y_i - x_i^\\top\\beta)\\\\\n&= \\frac{2}{n}\\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n0 &\\equiv \\sum_{i=1}^n x_i x_i^\\top \\beta - x_i y_i\\\\\n&\\Rightarrow \\sum_{i=1}^n x_i x_i^\\top \\beta = \\sum_{i=1}^n x_i y_i\\\\\n\\\\\n&\\Rightarrow \\beta = \\left(\\sum_{i=1}^n x_i x_i^\\top\\right)^{-1}\\sum_{i=1}^n x_i y_i\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#in-matrix-notation",
    "href": "schedule/slides/01-lm-review.html#in-matrix-notation",
    "title": "UBC Stat406 2024W",
    "section": "In matrix notation…",
    "text": "In matrix notation…\n…this is\n\\[\\hat\\beta = ( \\mathbf{X}^\\top  \\mathbf{X})^{-1} \\mathbf{X}^\\top\\mathbf{y}.\\]\nThe \\(\\beta\\) which “minimizes the sum of squared errors”\nAKA, the SSE."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood",
    "href": "schedule/slides/01-lm-review.html#method-3-maximum-likelihood",
    "title": "UBC Stat406 2024W",
    "section": "Method 3: maximum likelihood",
    "text": "Method 3: maximum likelihood\nMethod 2 didn’t use anything about the distribution of \\(\\epsilon\\).\nBut if we know that \\(\\epsilon\\) has a normal distribution, we can write down the joint distribution of \\(\\mathbf{y}=(y_1,\\ldots,y_n)^\\top = \\mathbf{X}\\beta + (\\epsilon_1, \\ldots, \\epsilon_n)^\\top\\):\n\\[\n\\epsilon_i = \\left( y_i - x_i^\\top \\beta \\right) \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\n\nSo the probability density of \\(Y = \\mathbf y\\) is…\n\\[\\begin{aligned}\nf_Y(\\mathbf{y} ; \\beta) &= \\prod_{i=1}^n f_{y_i ; \\beta}(y_i) \\\\\n  &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{1}{2\\sigma^2} (y_i-x_i^\\top \\beta)^2\\right) \\\\\n  &= \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#the-likelihood-function",
    "href": "schedule/slides/01-lm-review.html#the-likelihood-function",
    "title": "UBC Stat406 2024W",
    "section": "The likelihood function",
    "text": "The likelihood function\n\\[\nf_Y(\\mathbf{y} ; \\beta) = \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right)\n\\]\nIn probability courses, we think of \\(f_Y\\) as a function of \\(\\mathbf{y}\\) with \\(\\beta\\) fixed:\n\nIf we integrate over \\(\\mathbf{y}\\), it’s \\(1\\).\nIf we want the probability of \\((a,b)\\), we integrate from \\(a\\) to \\(b\\).\netc."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#the-likelihood-function-1",
    "href": "schedule/slides/01-lm-review.html#the-likelihood-function-1",
    "title": "UBC Stat406 2024W",
    "section": "The likelihood function",
    "text": "The likelihood function\n…instead, think of it as a function of \\(\\beta\\).\nWe call this “the likelihood” of beta: \\(\\mathcal{L}(\\beta)\\).\nGiven some data, we can evaluate the likelihood for any value of \\(\\beta\\) (assuming \\(\\sigma\\) is known).\nIt won’t integrate to 1 over \\(\\beta\\).\nBut we can maximize it with respect to \\(\\beta\\)."
  },
  {
    "objectID": "schedule/slides/01-lm-review.html#so-lets-maximize",
    "href": "schedule/slides/01-lm-review.html#so-lets-maximize",
    "title": "UBC Stat406 2024W",
    "section": "So let’s maximize",
    "text": "So let’s maximize\nThe derivative of \\(\\mathcal L(\\beta)\\) is ugly…\n\\[\n\\frac{\\partial}{\\partial \\beta} \\left[ \\left( \\frac{1}{2\\pi\\sigma^2}\\right)^{n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\right) \\right] = \\text{😔}\n\\]\nI claim we can maximize \\(\\mathcal L(\\beta)\\) over \\(\\beta\\) by instead maximizing the simpler function \\(\\ell(\\beta) = \\log \\mathcal L(\\beta)\\). (Why?)\n\n\\[\n\\hat\\beta = \\argmax_\\beta \\ell(\\beta) = \\argmax_\\beta \\left[ -\\frac{n}{2}\\log (2\\pi\\sigma^2) -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2 \\right]\n\\]\n\n\nWe can also throw out the constants. (Why?)\n\n\nSo…\n\\[\\widehat\\beta = \\argmax_\\beta -\\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2 = \\argmin_\\beta \\sum_{i=1}^n (y_i-x_i^\\top \\beta)^2\\]\nThe same as before!"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#section",
    "href": "schedule/slides/06-information-criteria.html#section",
    "title": "UBC Stat406 2024W",
    "section": "06 Information criteria",
    "text": "06 Information criteria\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 18 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#loo-cv",
    "href": "schedule/slides/06-information-criteria.html#loo-cv",
    "title": "UBC Stat406 2024W",
    "section": "LOO-CV",
    "text": "LOO-CV\n\nTrain \\(\\hat f\\) on all but one data point, estimate \\(R_n(\\hat f)\\) on the left-out point.\nRepeat this process for all \\(n\\) data points\nRequires training \\(n\\) models 🤮\n\nA magic formula for some models\nFor certain “nice” models of the form \\(\\widehat{y}_i = \\boldsymbol h_i(\\mathbf{X})^\\top \\mathbf{y}\\) (for some vector \\(h_i\\)), we get a closed form expression.\n\\[\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-[\\boldsymbol h_i(x_i)]_{i})^2}.\\]\n\nNumerator is the squared residual (loss) for training point \\(i\\).\nDenominator weights each residual by some factor (more on that in a bit…)"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#loo-cv-1",
    "href": "schedule/slides/06-information-criteria.html#loo-cv-1",
    "title": "UBC Stat406 2024W",
    "section": "LOO-CV",
    "text": "LOO-CV\n\\[\n\\text{When } \\widehat{y}_i = \\boldsymbol h_i(\\mathbf{X})^\\top \\mathbf{y},\n\\qquad\n\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-[\\boldsymbol h_i(x_i)]_{i})^2}\n\\]\nCollecting all the terms into matrices and vectors\n\n\\(\\hat{\\mathbf y} = \\begin{bmatrix} \\hat y_1 & \\cdots & \\hat y_n \\end{bmatrix}^\\top \\in \\mathbb R^{n}\\)\n\\({\\mathbf y} = \\begin{bmatrix} y_1 & \\cdots & y_n \\end{bmatrix}^\\top \\in \\mathbb R^{n}\\)\n\\(\\mathbf H = \\begin{bmatrix} \\mathbf h_1(\\boldsymbol x_1) & \\cdots & \\mathbf h_n(\\mathbf x_n) \\end{bmatrix}^\\top \\in \\mathbb R^{n \\times n}\\)\n\nWe have\n\\[\n\\hat{\\mathbf y} = \\mathbf H \\mathbf y,\n\\qquad\n\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-h_{ii})^2}\n\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#what-happens-when-we-cant-use-the-magic-formula",
    "href": "schedule/slides/06-information-criteria.html#what-happens-when-we-cant-use-the-magic-formula",
    "title": "UBC Stat406 2024W",
    "section": "What happens when we can’t use the magic formula?",
    "text": "What happens when we can’t use the magic formula?\n(And can we get a better intuition about what’s going on?)\n\\[\n\\hat{\\mathbf y} = \\mathbf H \\mathbf y,\n\\qquad\n\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-h_{ii})^2}\n\\]\nWhat do we know about \\(h_{ii}\\)?\nWriting out the \\(\\mathbf H\\) matrix for ordinary least squares (OLS) regression… \\[ \\hat Y = X \\hat \\beta, \\qquad \\beta = (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top \\mathbf y \\]\n\nThis implies that \\(\\mathbf H = \\mathbf X (\\mathbf X^\\top \\mathbf X)^{-1} \\mathbf X^\\top\\)\n\n\nThe diagonals \\(h_{ii}\\) are called hat values.\n\\(\\mathbf H\\) has lots of nice properties.\n\nThe most important (for us) is that \\(\\tr{\\mathbf H} = p\\). (Why?)"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#generalized-cv",
    "href": "schedule/slides/06-information-criteria.html#generalized-cv",
    "title": "UBC Stat406 2024W",
    "section": "Generalized CV",
    "text": "Generalized CV\n\\[\\textrm{GCV} = \\frac{\\textrm{MSE}}{(1-\\textrm{df}/n)^2}\\]\nWe can use this formula for models that aren’t of the form \\(\\widehat{y}_i = \\boldsymbol h_i(\\mathbf{X})^\\top \\mathbf{y}\\).\n(Assuming we have some model-specific formula for estimating \\(\\textrm{df}\\).)\n\nObservations\n\nGCV &gt; training error (Why?)\nWhat happens as \\(n\\) increases?\nWhat happens as \\(\\text{df}\\) (\\(p\\) in our OLS model) increases?"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#alternative-interpretation",
    "href": "schedule/slides/06-information-criteria.html#alternative-interpretation",
    "title": "UBC Stat406 2024W",
    "section": "Alternative interpretation:",
    "text": "Alternative interpretation:\nLet’s go back to our linear model \\(\\mathbf{Y} = \\mathbf{X}\\beta + \\epsilon\\), with \\(\\epsilon \\sim \\mathcal N(0, \\sigma^2)\\).\n(Define \\(\\boldsymbol \\mu = \\Expect{\\boldsymbol Y \\mid \\boldsymbol X} = \\boldsymbol X \\beta\\) )\nLet \\(\\widehat{\\mathbf{Y}}\\) be the estimator of \\(\\boldsymbol \\mu\\) we get from OLS. What is the risk of \\(\\widehat{\\mathbf{Y}}\\)?\n\n\\[\\begin{aligned}\n& R_n(\\widehat{\\mathbf{Y}}) = \\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-\\mu_i)^2} \\\\\n&= \\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-Y_i + Y_i -\\mu_i)^2}\\\\\n&= \\frac{1}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)^2} + \\frac{1}{n}\\Expect{\\sum (Y_i-\\mu_i)^2} + \\frac{2}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)(Y_i-\\mu_i)}\\\\\n&= \\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2} + \\sigma^2 + \\frac{2}{n}\\Expect{\\sum (\\widehat Y_i-Y_i)(Y_i-\\mu_i)} = \\cdots =\\\\\n&= \\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2} - \\sigma^2 + \\frac{2}{n}\\sum\\Cov{Y_i}{\\widehat Y_i}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#alternative-interpretation-1",
    "href": "schedule/slides/06-information-criteria.html#alternative-interpretation-1",
    "title": "UBC Stat406 2024W",
    "section": "Alternative interpretation:",
    "text": "Alternative interpretation:\n\\[\\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-\\mu_i)^2} =\n\\underbrace{\\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2}}_{(1)} -\n\\underbrace{\\sigma^2}_{(2)} +\n\\underbrace{\\frac{2}{n}\\sum\\Cov{Y_i}{\\widehat Y_i}}_{(3)}\n\\]\n\n\nTraining MSE\nObservational noise / irreducible error (recall \\(\\eps \\sim \\mathcal{N}(0, \\sigma^2)\\)).\n???"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#alternative-interpretation-2",
    "href": "schedule/slides/06-information-criteria.html#alternative-interpretation-2",
    "title": "UBC Stat406 2024W",
    "section": "Alternative interpretation:",
    "text": "Alternative interpretation:\n\\[\\Expect{\\frac{1}{n}\\sum (\\widehat Y_i-\\mu_i)^2} =\n\\underbrace{\\frac{1}{n}\\sum \\Expect{(\\widehat Y_i-Y_i)^2}}_{\\text{training error}} -\n\\underbrace{\\sigma^2}_{\\text{irr. error}} +\n\\underbrace{\\frac{2}{n}\\sum\\Cov{Y_i}{\\widehat Y_i}}_{\\text{???}}\n\\]\nRecall that \\(\\widehat{\\mathbf{Y}} = \\mathbf H \\mathbf{Y}\\) for some matrix \\(\\mathbf H\\),\n\\(\\sum\\Cov{Y_i}{\\widehat Y_i} = \\Expect{\\mathbf{Y}^\\top \\mathbf H \\mathbf{Y}} = \\sigma^2 \\textrm{tr}(\\mathbf H)\\)\nThis gives Mallow’s \\(C_p\\) aka Stein’s Unbiased Risk Estimator:\n\\[ C_p = \\text{MSE} + 2\\hat{\\sigma}^2 \\: \\textrm{df}/n \\]"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#mallows-c_p",
    "href": "schedule/slides/06-information-criteria.html#mallows-c_p",
    "title": "UBC Stat406 2024W",
    "section": "Mallow’s \\(C_p\\)",
    "text": "Mallow’s \\(C_p\\)\n\\[ C_p = \\text{MSE} + 2\\hat{\\sigma}^2 \\: \\textrm{df}/n \\] (We derived it for the OLS model, but again it can be generalized to other models.)\n\n\n\n\n\n\nImportant\n\n\nUnfortunately, \\(\\text{df}\\) may be difficult or impossible to calculate for complicated prediction methods. But one can often estimate it well. This idea is beyond the level of this course.\n\n\n\nObservations\n\n\\(C_p\\) &gt; training error\nWhat happens as \\(n\\) increases?\nWhat happens as \\(\\text{df}\\) (\\(p\\) in our OLS model) increases?\nWhat happens as the irreducible noise increase?"
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#aic-and-bic",
    "href": "schedule/slides/06-information-criteria.html#aic-and-bic",
    "title": "UBC Stat406 2024W",
    "section": "AIC and BIC",
    "text": "AIC and BIC\nThese have a very similar flavor to \\(C_p\\), but their genesis is different.\nWithout going into too much detail, they look like\n\\(\\textrm{AIC}/n = -2\\textrm{loglikelihood}/n + 2\\textrm{df}/n\\)\n\\(\\textrm{BIC}/n = -2\\textrm{loglikelihood}/n + 2\\log(n)\\textrm{df}/n\\)\n\nIn the case of a linear model with Gaussian errors and \\(p\\) predictors\n\\[\\begin{aligned}\n\\textrm{AIC}/n &= \\log(2\\pi) + \\log(RSS/n) + 2(p+1)/n \\\\\n&\\propto \\log(RSS) + 2(p+1)/n\n\\end{aligned}\\]\n( \\(p+1\\) because of the unknown variance, intercept included in \\(p\\) or not)\n\n\n\n\n\n\n\n\nImportant\n\n\nUnfortunately, different books/software/notes define these differently. Even different R packages. This is super annoying.\nForms above are in [ESL] eq. (7.29) and (7.35). [ISLR] gives special cases in Section 6.1.3. Remember the generic form here."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#over-fitting-vs.-under-fitting",
    "href": "schedule/slides/06-information-criteria.html#over-fitting-vs.-under-fitting",
    "title": "UBC Stat406 2024W",
    "section": "Over-fitting vs. Under-fitting",
    "text": "Over-fitting vs. Under-fitting\n\nOver-fitting means estimating a really complicated function when you don’t have enough data.\n\nThis is likely a low-bias / high-variance situation.\n\nUnder-fitting means estimating a really simple function when you have lots of data.\n\nThis is likely a high-bias / low-variance situation.\nBoth of these outcomes are bad (they have high risk \\(=\\) big \\(R_n\\) ).\nThe best way to avoid them is to use a reasonable estimate of prediction risk to choose how complicated your model should be."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#recommendations",
    "href": "schedule/slides/06-information-criteria.html#recommendations",
    "title": "UBC Stat406 2024W",
    "section": "Recommendations",
    "text": "Recommendations\n\nWhen comparing models, choose one criterion: CV / AIC / BIC / Cp / GCV.\nCV is usually easiest to make sense of and doesn’t depend on other unknown parameters.\nBut, it requires refitting the model.\nAlso, it can be strange in cases with discrete predictors, time series, repeated measurements, graph structures, etc."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#high-level-intuition-of-these",
    "href": "schedule/slides/06-information-criteria.html#high-level-intuition-of-these",
    "title": "UBC Stat406 2024W",
    "section": "High-level intuition of these:",
    "text": "High-level intuition of these:\n\nGCV tends to choose “dense” models.\nTheory says AIC chooses the “best predicting model” asymptotically.\nTheory says BIC should choose the “true model” asymptotically, tends to select fewer predictors.\nIn some special cases, AIC = Cp = SURE \\(\\approx\\) LOO-CV\nAs a technical point, CV (or validation set) is estimating error on new data, unseen \\((X_0, Y_0)\\), while AIC / CP are estimating error on new Y at the observed \\(x_1,\\ldots,x_n\\). This is subtle.\n\n\n\nFor more information: see [ESL] Chapter 7. This material is more challenging than the level of this course, and is easily and often misunderstood."
  },
  {
    "objectID": "schedule/slides/06-information-criteria.html#a-few-more-caveats",
    "href": "schedule/slides/06-information-criteria.html#a-few-more-caveats",
    "title": "UBC Stat406 2024W",
    "section": "A few more caveats",
    "text": "A few more caveats\nIt is often tempting to “just compare” risk estimates from vastly different models.\nFor example,\n\ndifferent transformations of the predictors,\ndifferent transformations of the response,\nPoisson likelihood vs. Gaussian likelihood in glm()\n\nThis is not always justified.\n\nThe “high-level intuition” is for “nested” models.\nDifferent likelihoods aren’t comparable.\nResiduals / response variables on different scales aren’t directly comparable.\n\n“Validation set” is easy, because you’re always comparing to the “right” thing. But it has lots of drawbacks."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#section",
    "href": "schedule/slides/12-why-smooth.html#section",
    "title": "UBC Stat406 2024W",
    "section": "12 To(o) smooth or not to(o) smooth?",
    "text": "12 To(o) smooth or not to(o) smooth?\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 09 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#last-time",
    "href": "schedule/slides/12-why-smooth.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time…",
    "text": "Last time…\nWe’ve been discussing smoothing methods in 1-dimension:\n\\[\\Expect{Y\\given X=x} = f(x),\\quad x\\in\\R\\]\nWe looked at basis expansions, e.g.:\n\\[f(x) \\approx \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\cdots + \\beta_k x^k\\]\nWe looked at local methods, e.g.:\n\\[f(x_i) \\approx  s_i^\\top \\y\\]\n\nWhat if \\(x \\in \\R^p\\) and \\(p&gt;1\\)?\n\n\n\nNote that \\(p\\) means the dimension of \\(x\\), not the dimension of the space of the polynomial basis or something else. That’s why I put \\(k\\) above."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#kernels-and-interactions",
    "href": "schedule/slides/12-why-smooth.html#kernels-and-interactions",
    "title": "UBC Stat406 2024W",
    "section": "Kernels and interactions",
    "text": "Kernels and interactions\nIn multivariate nonparametric regression, you estimate a surface over the input variables.\nThis is trying to find \\(\\widehat{f}(x_1,\\ldots,x_p)\\).\nTherefore, this function by construction includes interactions, handles categorical data, etc. etc.\nThis is in contrast with explicit linear models which need you to specify these things.\nThis extra complexity (automatically including interactions, as well as other things) comes with tradeoffs.\n\nMore complicated functions (smooth Kernel regressions vs. linear models) tend to have lower bias but higher variance."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1",
    "href": "schedule/slides/12-why-smooth.html#issue-1",
    "title": "UBC Stat406 2024W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\n\n\n\n\n\n\nImportant\n\n\nyou don’t need to memorize these formulas but you should know the intuition\nthe constants don’t matter for the intuition, but they matter for a particular data set. We don’t know them. So you estimate this."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-1",
    "href": "schedule/slides/12-why-smooth.html#issue-1-1",
    "title": "UBC Stat406 2024W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nRecall, this decomposition is squared bias + variance + irreducible error\n\nIt depends on the choice of \\(h\\)\n\n\\[\\textrm{MSE}(\\hat{f}) = C_1 h^4 + \\frac{C_2}{nh} + \\sigma^2\\]\n\nUsing \\(h = cn^{-1/5}\\) balances squared bias and variance, leads to the above rate. (That balance minimizes the MSE)"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-2",
    "href": "schedule/slides/12-why-smooth.html#issue-1-2",
    "title": "UBC Stat406 2024W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nIntuition:\nas you collect data, use a smaller bandwidth and the MSE (on future data) decreases"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-3",
    "href": "schedule/slides/12-why-smooth.html#issue-1-3",
    "title": "UBC Stat406 2024W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nHow does this compare to just using a linear model?\nBias\n\nThe bias of using a linear model when the truth nonlinear is a number \\(b &gt; 0\\) which doesn’t depend on \\(n\\).\nThe bias of using kernel regression is \\(C_1/n^{4/5}\\). This goes to 0 as \\(n\\rightarrow\\infty\\).\n\nVariance\n\nThe variance of using a linear model is \\(C/n\\) no matter what\nThe variance of using kernel regression is \\(C_2/n^{4/5}\\)."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-1-4",
    "href": "schedule/slides/12-why-smooth.html#issue-1-4",
    "title": "UBC Stat406 2024W",
    "section": "Issue 1",
    "text": "Issue 1\nFor \\(p=1\\), one can show that for kernels (with the correct bandwidth)\n\\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nTo conclude:\n\nbias of kernels goes to zero, bias of lines doesn’t (unless the truth is linear).\nbut variance of lines goes to zero faster than for kernels.\n\nIf the linear model is right, you win.\nBut if it’s wrong, you (eventually) lose as \\(n\\) grows.\nHow do you know if you have enough data?\nCompare of the kernel version with CV-selected tuning parameter with the estimate of the risk for the linear model."
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-2",
    "href": "schedule/slides/12-why-smooth.html#issue-2",
    "title": "UBC Stat406 2024W",
    "section": "Issue 2",
    "text": "Issue 2\nFor \\(p&gt;1\\), there is more trouble.\nFirst, lets look again at \\[\\textrm{MSE}(\\hat{f}) = \\frac{C_1}{n^{4/5}} + \\frac{C_2}{n^{4/5}} + \\sigma^2\\]\nThat is for \\(p=1\\). It’s not that much slower than \\(C/n\\), the variance for linear models.\nIf \\(p&gt;1\\) similar calculations show,\n\\[\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .\\]"
  },
  {
    "objectID": "schedule/slides/12-why-smooth.html#issue-2-1",
    "href": "schedule/slides/12-why-smooth.html#issue-2-1",
    "title": "UBC Stat406 2024W",
    "section": "Issue 2",
    "text": "Issue 2\n\\[\\textrm{MSE}(\\hat f) = \\frac{C_1+C_2}{n^{4/(4+p)}} + \\sigma^2 \\hspace{2em} \\textrm{MSE}(\\hat \\beta)  = b + \\frac{Cp}{n} + \\sigma^2 .\\]\nWhat if \\(p\\) is big (and \\(n\\) is really big)?\n\nThen \\((C_1 + C_2) / n^{4/(4+p)}\\) is still big.\nBut \\(Cp / n\\) is small.\nSo unless \\(b\\) is big, we should use the linear model.\n\nHow do you tell? Do model selection to decide.\nA very, very questionable rule of thumb: if \\(p&gt;\\log(n)\\), don’t do smoothing."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#section",
    "href": "schedule/slides/00-intro-to-class.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Intro to class",
    "text": "00 Intro to class\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 04 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#about-us",
    "href": "schedule/slides/00-intro-to-class.html#about-us",
    "title": "UBC Stat406 2024W",
    "section": "About us",
    "text": "About us\n\n\n\nGeoff Pleiss\ngeoff.pleiss@stat.ubc.ca\nhttp://geoffpleiss.com/\nAssistant Professor, Department of Statistics\n\n\n\n\n\nTrevor Campbell\ntrevor@stat.ubc.ca\nhttp://trevorcampbell.me/\nAssociate Professor, Department of Statistics"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#wait-theres-two-of-you",
    "href": "schedule/slides/00-intro-to-class.html#wait-theres-two-of-you",
    "title": "UBC Stat406 2024W",
    "section": "Wait, there’s two of you?",
    "text": "Wait, there’s two of you?\nGeoff & Trevor are co-teaching this course!\n\nThink of the two of us as interchangeable people.\n(It’s not that hard. We’re very similar.)\n\nWe will both be present at (almost) all lectures\nWe will roughly alternate who is giving the lecture\nWe are both in charge of course material / course policies / grades / etc."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#philosophy-of-the-class",
    "href": "schedule/slides/00-intro-to-class.html#philosophy-of-the-class",
    "title": "UBC Stat406 2024W",
    "section": "Philosophy of the class",
    "text": "Philosophy of the class\nWe and the TAs are here to help you learn. Ask questions.\nWe encourage engagement and curiosity\nWe favour steady work through the term (vs. sleeping until finals)\nThe assessments attempt to reflect this ethos."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#more-philosophy",
    "href": "schedule/slides/00-intro-to-class.html#more-philosophy",
    "title": "UBC Stat406 2024W",
    "section": "More philosophy",
    "text": "More philosophy\nWhen the term ends, we want\n\nYou to be better at coding.\nYou to have an understanding of the variety of methods available to do prediction and data analysis.\nYou to articulate their strengths and weaknesses.\nYou to be able to choose between different methods using your intuition and the data.\n\n\nWe do not want\n\nYou to be under undo stress\nYou to feel the need to cheat, plagiarize, or drop the course\nYou to feel treated unfairly."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#healthcovid-policies-tl-dr",
    "href": "schedule/slides/00-intro-to-class.html#healthcovid-policies-tl-dr",
    "title": "UBC Stat406 2024W",
    "section": "Health/COVID Policies (TL; DR)",
    "text": "Health/COVID Policies (TL; DR)\n\nAttend class whenever you are healthy\nWe encourage you to wear a mask if you want\nDo NOT come to class if you are possibly sick\nThe Marking scheme is flexible enough to allow some missed classes"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#what-this-course-is-not",
    "href": "schedule/slides/00-intro-to-class.html#what-this-course-is-not",
    "title": "UBC Stat406 2024W",
    "section": "What this course is not",
    "text": "What this course is not"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#what-this-course-is-not-1",
    "href": "schedule/slides/00-intro-to-class.html#what-this-course-is-not-1",
    "title": "UBC Stat406 2024W",
    "section": "What this course is not",
    "text": "What this course is not"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#what-this-course-is-not-2",
    "href": "schedule/slides/00-intro-to-class.html#what-this-course-is-not-2",
    "title": "UBC Stat406 2024W",
    "section": "What this course is not",
    "text": "What this course is not"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#what-this-course-is",
    "href": "schedule/slides/00-intro-to-class.html#what-this-course-is",
    "title": "UBC Stat406 2024W",
    "section": "What this course is",
    "text": "What this course is\n\n5 easy steps to use scikit-learn\nEverything there is to know about machine learning\nThe hypeist new machine leraning models\nThe fundamentals for developing strong intuitions/understanding about ML"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#predictive-models",
    "href": "schedule/slides/00-intro-to-class.html#predictive-models",
    "title": "UBC Stat406 2024W",
    "section": "Predictive models",
    "text": "Predictive models\n\n1. Preprocessing\ncentering / scaling / factors-to-dummies / basis expansion / missing values / dimension reduction / discretization / transformations\n2. Model fitting\nWhich box do you use?\n3. Prediction\nRepeat all the preprocessing on new data. But be careful.\n4. Postprocessing, interpretation, and evaluation\n\nWe will focus mostly on 1 and 4."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#section-1",
    "href": "schedule/slides/00-intro-to-class.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Source: https://vas3k.com/blog/machine_learning/"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#modules",
    "href": "schedule/slides/00-intro-to-class.html#modules",
    "title": "UBC Stat406 2024W",
    "section": "6 modules",
    "text": "6 modules\n\n\n\nReview (today and next week)\nModel accuracy and selection\nRegularization, smoothing, trees\nClassifiers\nModern techniques (classification and regression)\nUnsupervised learning\n\n\n\n\nEach module is approximately 2 weeks long\nEach module is based on a collection of readings and lectures\nEach module (except the review) has a homework assignment"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#assessments",
    "href": "schedule/slides/00-intro-to-class.html#assessments",
    "title": "UBC Stat406 2024W",
    "section": "Assessments",
    "text": "Assessments\nEffort-based\nTotal across three components: 65 points, any way you want\n\nLabs, up to 20 points (2 each)\nAssignments, up to 50 points (10 each)\nClickers, up to 10 points\n\neffort_grade = max(65, labs + assignments + clickers)\n\nKnowledge-based\nFinal Exam, 35 points"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#labs-assignments",
    "href": "schedule/slides/00-intro-to-class.html#labs-assignments",
    "title": "UBC Stat406 2024W",
    "section": "Labs / Assignments",
    "text": "Labs / Assignments\nThe goal is to “Do the work”\n\n\nAssignments\n\nNot easy, especially the first 2, especially if you are unfamiliar with R / Rmarkdown / ggplot\nYou may revise to raise your score to 7/10, see Syllabus. Only if you lose 3+ for content (penalties can’t be redeemed).\nDon’t leave these for the last minute\n\n\n\nLabs\n\nLabs should give you practice, allow for questions with the TAs.\nThey are due at 2300 on the day of your lab, lightly graded.\nYou may do them at home, but you must submit individually (in lab, you may share submission)\nLabs are lightly graded"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#clickers",
    "href": "schedule/slides/00-intro-to-class.html#clickers",
    "title": "UBC Stat406 2024W",
    "section": "Clickers",
    "text": "Clickers\n\nQuestions are similar to the Final\n0 points for skipping, 2 points for trying, 4 points for correct\n\nAverage of 3 = 10 points (the max)\nAverage of 2 = 5 points\nAverage of 1 = 0 points\ntotal = max(0, min(5 * points / N - 5, 10))\n\nBe sure to sync your device in Canvas.\n\n\n\n\n\n\n\nDon’t do this!\n\n\nAverage &lt; 1 drops your Final Mark 1 letter grade.\nA- becomes B-, C+ becomes D."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#final-exam",
    "href": "schedule/slides/00-intro-to-class.html#final-exam",
    "title": "UBC Stat406 2024W",
    "section": "Final Exam",
    "text": "Final Exam\n\nScheduled by the university.\nIt is hard\nThe median last year was 50% \\(\\Rightarrow\\) A-\n\nPhilosophy:\n\nIf you put in the effort, you’re guaranteed a C+.\nBut to get an A+, you should really deeply understand the material.\n\nNo penalty for skipping the final.\nIf you’re cool with C+ and hate tests, then that’s fine."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#late-policy",
    "href": "schedule/slides/00-intro-to-class.html#late-policy",
    "title": "UBC Stat406 2024W",
    "section": "Late policy",
    "text": "Late policy\nIf you have not submitted your lab/assignment by the time grading starts, you will get a 0.\n\n\n\n\n\n\n\n\n\nWhen you submit\nLikelihood that your submission gets a 0\n\n\n\n\nBefore 11pm on due date (i.e. on time)\n0%\n\n\n11:01pm on due date\n0.01%\n\n\n9am after due date\n50%\n\n\n2 weeks after due date\n99.99999999%"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#late-policy-1",
    "href": "schedule/slides/00-intro-to-class.html#late-policy-1",
    "title": "UBC Stat406 2024W",
    "section": "Late policy",
    "text": "Late policy\nWe will only make exceptions when you have grounds for academic consession. (See the UBC policy.)\n\n\n\n\n\n\nTip\n\n\nRemember: you can still get a “perfect” effort grade even if you get a 0 on one assignment."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#time-expectations-per-week",
    "href": "schedule/slides/00-intro-to-class.html#time-expectations-per-week",
    "title": "UBC Stat406 2024W",
    "section": "Time expectations per week:",
    "text": "Time expectations per week:\n\nComing to class – 3 hours\nReading the book – 1 hour\nLabs – 1 hour\nHomework – 4 hours\nStudy / thinking / playing – 1 hour\n\n\nQuestions?"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#textbooks",
    "href": "schedule/slides/00-intro-to-class.html#textbooks",
    "title": "UBC Stat406 2024W",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\nJames, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: http://statlearning.com/\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning\n\n\nHastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\nIt’s worth your time to read.\nIf you need more practice, read the Worksheets."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#computer",
    "href": "schedule/slides/00-intro-to-class.html#computer",
    "title": "UBC Stat406 2024W",
    "section": "Computer",
    "text": "Computer\n\n\n\n\n\n\nWe will use R and we assume some background knowledge.\nSuggest you use RStudio IDE\nSee https://ubc-stat.github.io/stat-406/ for what you need to install for the whole term.\nLinks to useful supplementary resources are available on the website.\n\n\n\n\n\n\nThis course is not an intro to R / python / MongoDB / SQL."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#other-resources",
    "href": "schedule/slides/00-intro-to-class.html#other-resources",
    "title": "UBC Stat406 2024W",
    "section": "Other resources",
    "text": "Other resources\n\nCanvas (minimal)\n\nQuiz 0, grades, course time/location info, links to videos from class\n\nCourse website\n\nAll the material (slides, extra worksheets) https://ubc-stat.github.io/stat-406\n\nSlack\n\nDiscussion board, questions\n\nGithub\n\nHomework / Lab submission\n\n\n\n\n\nAll lectures will be recorded and posted\nWe cannot guarantee that they will all work properly (sometimes we mess it up)"
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#some-more-words",
    "href": "schedule/slides/00-intro-to-class.html#some-more-words",
    "title": "UBC Stat406 2024W",
    "section": "Some more words",
    "text": "Some more words\n\nLectures are hard. It’s 8am, everyone’s tired.\nCoding is hard. We hope you’ll get better at it.\nWe strongly urge you to get up at the same time everyday.  It’s really hard to sleep in until 10 on MWF and make class at 8 on T/Th.\n\n\n\nWe have to give you a grade, but we want that grade to reflect your learning and effort, not other junk.\n\n\n\nIf you need help, please ask."
  },
  {
    "objectID": "schedule/slides/00-intro-to-class.html#some-things-to-see-on-the-website",
    "href": "schedule/slides/00-intro-to-class.html#some-things-to-see-on-the-website",
    "title": "UBC Stat406 2024W",
    "section": "Some things to see on the website",
    "text": "Some things to see on the website\n\nRead the syllabus (See also Lab 0)\nLinks to slides, how to download / print, browse source code\nInstall the R package, read documentation, check your LaTeX installation\nBE SURE to follow the Computer Setup instructions!\nWorksheets for extra help.\nRead the FAQ!\nView the Course GitHub (once you have access)\n\n\n\n\nUBC Stat 406 - 2024"
  },
  {
    "objectID": "schedule/handouts/lab00-git.html",
    "href": "schedule/handouts/lab00-git.html",
    "title": "Lab 00 Git",
    "section": "",
    "text": "Check your Canvas profile settings to ensure the email associated with your Canvas account is correct.\nReview your Canvas notification settings and decide what you want to be notified about.\nVisit the Course website. In particular, as you might expect, this course requires computing. We will use R and RStudio as well as Git and GitHub. See the Computing tab.\nIf you have never used GitHub before, go to https://github.com/ create an account. You should be aware that this data is stored on US servers. Please exercise caution whenever using personal information. You may wish to use a pseudonym to protect your privacy if you have concerns. k_asking_for_help/).\n\nIf you haven’t already, visit https://ubc-stat.github.io/stat-406/computing/ and follow the instructions to set up your computer.\nNow we have to clone your labs-&lt;username&gt; repo.\n\nNavigate to the Course Github using the link at the top of the Course Website or from Canvas.\nThen go to your labs-&lt;username&gt;.\nClick the Green “Code” button, and copy the url by clicking the two overlapping squares.\nThen in RStudio, choose “New project” &gt; “Version Control” &gt; “Git” and paste the address.\nChoose a location on your machine where you want all your labs to be.\nSelect “Create Project”."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "href": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "title": "Lab 00 Git",
    "section": "Scenario 1. You do work on the wrong branch.",
    "text": "Scenario 1. You do work on the wrong branch.\nMake sure that you are on main. Remember that the actual submission is on the lab00-git branch.\nIn the R code chunk below, fit a linear model to the data and print the estimated coefficients, rounded to 2 decimal places.\n\nlibrary(tibble)\nset.seed(12345)\ndat &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = 2 + 3*x1 - x2 + rnorm(100)\n)\n\nNow, stage the .Rmd. Commit with the message “on the wrong branch” and push.\nYou likely see an error like:\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThat’s because you’re on main. Ugh! But I did some work, and now I need to be on a different branch!\nSo let’s fix it. We want the stuff we just did on main to be on lab00-git. Note that everything you did is saved! Here are the steps:\nGet our changes onto the correct branch\n\nUse the dropdown to switch branches to lab00-git.\nGo to the Terminal (next to console).\nType git merge main.\n\nThat should copy all your changes in the .Rmd that you made on main into the correct place. Did it?\nIf you do this and you ever see stuff like\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the stuff that is currently on this branch.\n=======\nThis is stuff that got added on the other branch.\nWhile someone else changed stuff on this branch!\nI (git) don't know which to keep!?\nYou have to decide for me.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict\nThis means that there were conflicts between the two versions. The stuff above ====== was in your current branch. The stuff below is what you’re trying to merge in. You decide what to keep, the top, the bottom, or both (or neither). Just be sure to delete the junk lines with &lt;, &gt;, or =.\nOnce you’ve resolved conflicts (and committed the conflict changes), double check the following:\n\nYou are on the correct branch (lab00-git)\nYou have no files with uncommitted changes in the “Git” tab\nYour changes to the R chunk above exist (on this branch)\n\nAnother way you can check for your changes is by running the git log commmand. You should see something like the following:\ncommit 1efefd8473c2cc81893dd2a5ded929978d9ee2aa (HEAD -&gt; lab00-git, main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 16:41:58 2024 -0700\n\n    on the wrong branch\n\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nThere’s a lot of information here, but you should (hopefully) see at the top your latest commit with the message “on the wrong branch.” The long string at the start of the commit (1efefd8473c2cc81893dd2a5ded929978d9ee2aa) is the hash. It is a unique identifier of the commit, which can be useful if you want to reference a specific commit with other commands.\nType q to exit the log viewer.\n\nOk. So now we have our changes in the right spot. Commit and Push the .Rmd (only). Let’s clean up main so we don’t have problems later. Switch back to main.\nUndo mistakes on the wrong branch.\nIn the terminal, type the following two commands:\ngit fetch\ngit reset --hard origin/main\nThere’s a lot to unpack in these two commands, but here’s the high level idea: we want to make sure that our main branch matches what’s on Github’s remote main branch. The second command resets our local main branch so that it has exactly the same commits as Github’s remote main branch. (The first command makes sure that our local computer knows about the latest changes on Github’s remote branches.)\nIf you now type git log, you should now see\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (HEAD -&gt; main, origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nSo our local main branch matches what’s on Github, and no longer contains the “on the wrong branch” commit. You can also verify that your changes to the R code on this branch are now gone.\nTo recap, now the work we want is in the right place (on the other branch), and the mess on main is cleaned up. Boom."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "href": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "title": "Lab 00 Git",
    "section": "Scenario 2. You did something you shouldn’t have",
    "text": "Scenario 2. You did something you shouldn’t have\nSwitch your branch back to lab00-git (or whatever you named it).\nOpen the file lab01.Rmd. Select everything after # Instructions and delete it. Save. Then Knit (producing a pdf). Commit both files with a message “did the wrong lab, and built a pdf”. Push your commits with the Green up arrow.\nTake a look at the PR on Github now. There’s a bunch of crud that shouldn’t be there.\nWe’ve done 3 things here that we shouldn’t have.\n\nWe built a pdf that we don’t want at all. It needs to go away.\nWe bollixed up the lab01.Rmd file. We don’t want that or it will screw up the lab next week.\nWe pushed it all into our submission for this week.\n\nThe first instinct is to Delete both files, commit, and push. This is VERY BAD. That will further screw up everything. Basically, you’re telling git “I don’t want these files at all” when you mean “I don’t want changes to these files in this branch”. The difference is subtle but important. Because you DO want these files (without the changes) at some point, but you don’t want them here.\nLet’s fix these issues.\n\nFirst, we want to “get rid of” the pdf. In the Terminal type\ngit reset HEAD^ -- lab01.pdf\nClick the little “Refresh” arrow in the Git panel. You should now see lab01.pdf twice, once with a red D that is checked and once with two yellow question marks that is NOT checked. This is what we want.\nCommit exactly as is. Use a message like “remove the stray pdf” and Push. Now, take a look at the PR on Github. It should be gone from the list of files in the PR.\nThere’s still that annoying two-yellow-question-mark version in the Git panel. Don’t click the check box (that will just redo everything we undid). Instead, highlight the file by clicking the file name, click the Gear Icon Dropdown, and then select “Revert”. Now it’s gone, and the pdf should disappear from your filesystem.\n\nSecond, let’s “undo” the deletion in the .Rmd. This is easy, and a useful pattern to remember.\nIn the Terminal, type\ngit checkout main -- lab01.Rmd\nWhat this does is grabs the version on main that isn’t messed up and puts it here, overwriting your changes. This isn’t the only way to fix your problem (you could have done the same thing we did with the pdf), but it’s pretty easy.\nStage commit and push. Now look at the PR on Github. Even though you made two changes (one deleting everything, and one restoring everything) to the lab01.Rmd, it should be “gone” from the PR now. That’s because the version on this branch looks just like the version on main, so there are no changes to be made into the main branch. This is just what we want.\n\nNow we’ve also fixed the third error already. None of those bogus changes to lab01 are in our PR for this week anymore."
  },
  {
    "objectID": "course-setup.html",
    "href": "course-setup.html",
    "title": "Guide for setting up the course infrastructure",
    "section": "",
    "text": "Version 2024\nThis guide (hopefully) gives enough instructions for recreating new iterations of Stat 406."
  },
  {
    "objectID": "course-setup.html#create-a-github.com-organization",
    "href": "course-setup.html#create-a-github.com-organization",
    "title": "Guide for setting up the course infrastructure",
    "section": "Create a GitHub.com organization",
    "text": "Create a GitHub.com organization\n\nThis is free for faculty with instructor credentials.\n\nNote make sure you upgrade the organization to a “Github Team.” If you have registered your instructor credentials with Github, you should be able to upgrade for free from the Github Global Campus page under “Upgrade your academic organizations.”\n\nAllows more comprehensive GitHub actions, PR templates and CODEOWNER behaviour than the UBC Enterprise version (last I checked)\nDownside is getting students added (though we include R scripts for this)\n\nOnce done, go to https://github.com/watching. Click the Red Down arrow “Unwatch all”. Then select this Org. The TAs should do the same.\n\nPermissions and structure\nSettings &gt; Member Privileges\nWe list only the important ones.\n\nBase Permissions: No Permission\nRepository creation: None\nRepo forking: None\nPages creation: None\nTeam creation rules: No\n\nBe sure to click save in each area after making changes.\nSettings &gt; Actions &gt; General\nAll repositories: Allow all actions and reusable workflows.\nWorkflow permissions: Read and write permissions.\n\n\nTeams\n\n2 teams, one for the TAs and one for the students\nYou must then manually add the teams to any repos they should access\n\nI generally give the TAs “Write” permission, and the students “Read” permission with some exceptions. See the Repos section below."
  },
  {
    "objectID": "course-setup.html#repos",
    "href": "course-setup.html#repos",
    "title": "Guide for setting up the course infrastructure",
    "section": "Repos",
    "text": "Repos\nThere are typically about 10 repositories. Homeworks and Labs each have 3 with very similar behaviours.\nBe careful copying directories. All of them have hidden files and folders, e.g. .git. Of particular importance are the .github directories which contain PR templates and GitHub Actions. Also relevant are the .Rprofile files which try to override Student Language settings and avoid unprintible markdown characters.\n\nHomeworks\n\nhomework-solutions\nThis is where most of the work happens. My practice is to create the homework solutions first. I edit these (before school starts) until I’m happy. I then duplicate the file and remove the answers. The result is hwxx-instructions.Rmd. The .gitignore file should ignore all of the solutions and commit only the instructions. Then, about 1 week after the deadline, I adjust the .gitignore and push the solution files.\n\nStudents have Read permission.\nTAs have Write permission.\nThe preamble.tex file is common to HWs and Labs. It creates a lavender box where the solution will go. This makes life easy for the TAs.\n\n\n\nhomework-solutions-private\nExactly the same as homework-solutions except that all solutions are available from the beginning for TA access. To create this, after I’m satisfied with homework-solutions I copy all files (not the directory) into a new directory, git init then upload to the org. The students never have permission here.\n\n\nhomework-template\nThis is a “template repo” used for creating student specific homework-studentgh repos (using the setup scripts).\nVery Important: copy the hwxx-instructions files over to a new directory. Do NOT copy the directory or you’ll end up with the solutions visible to the students.\nThen rename hwxx-instructions.Rmd to hwxx.Rmd. Now the students have a .pdf with instructions, and a template .Rmd to work on.\nOther important tasks:\n\nThe .gitignore is more elaborate in an attempt to avoid students pushing junk into these repos.\nThe .github directory contains 3 files:\n\nCODEOWNERS begins as an empty doc which will be populated with the assigned grader later;\npull_request_template.md is used for all HW submission PRs;\nworkflows contains a GH-action to comment on the PR with the date+time when the PR is opened.\n\nUnder Settings &gt; General, select “Template repository”. This makes it easier to duplicate to the student repos.\nSetup branch protection rules for the main branch. Create a new ruleset for default branches, and select the following:\n\nRequire a pull request before merging\nRequire review from Code Owners\nBlock force pushes\nI recommend adding the @TAs team to the bypass list.\n\n\n\n\n\nLabs\nThe three Labs repos operate exactly as the analogous homework repos.\n\nlabs-solutions\nDo any edits here before class begins.\n\n\nlabs-solutions-private\nSame as with the homeworks\n\n\nlabs-template\nSame as with the homeworks\n\n\n\nclicker-solutions\nThis contains the complete set of clicker questions.\nAnswers are hidden in comments on the presentation.\nI release them incrementally after each module (copying over from my clicker deck).\n\n\npractice-final\nContains a lengthy practice exam as well as solutions. I usually only post this during the last week of class.\n\n\nopen-pr-log\nThis contains a some GitHub actions to automatically keep track of open PRs for the TAs.\nIt’s still in testing phase, but should work properly. It will create two markdown docs, 1 for labs and 1 for homework. Each shows the assigned TA, the date the PR was opened, and a link to the PR. If everything is configured properly, it should run automatically at 3am every night.\n\nOnly the TAs should have access.\nUnder Settings &gt; Secrets and Variables &gt; Actions you must add a “Repository Secret”. This should be a GitHub Personal Access Token created in your account (Settings &gt; Developer settings &gt; Tokens (classic)). It needs Repo, Workflow, and Admin:Org permissions. I set it to expire at the end of the course. I use it only for this purpose (rather than my other tokens for typical logins).\n\n\n\n.github / .github-private\nThese contain README files that give some basic information about the available repos and the course.\nIt’s visible Publically, and appears on the Org homepage for all to see. The .github-private has the same function, but applies only to Org members.\n\n\nbakeoff-bakeoff\nThis is for the bonus for HW4. Both TAs and Students have access. I put the TA team as CODEOWNERS and protect the main branch (Settings &gt; Branches &gt; Branch Protection Rules). Here, we “Require approvals” and “Require Review from Code Owners”."
  },
  {
    "objectID": "computing/windows.html",
    "href": "computing/windows.html",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#installation-notes",
    "href": "computing/windows.html#installation-notes",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#terminal",
    "href": "computing/windows.html#terminal",
    "title": " Windows",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works."
  },
  {
    "objectID": "computing/windows.html#github",
    "href": "computing/windows.html#github",
    "title": " Windows",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/windows.html#git-bash-and-windows-terminal",
    "href": "computing/windows.html#git-bash-and-windows-terminal",
    "title": " Windows",
    "section": "Git, Bash, and Windows Terminal",
    "text": "Git, Bash, and Windows Terminal\nAlthough these three are separate programs, we are including them in the same section here since they are packaged together in the same installer on Windows. Briefly, we will be using the Bash shell to interact with our computers via a command line interface, Git to keep a version history of our files and upload to/download from to GitHub, and Windows Terminal to run the both Bash and Git.\nGo to https://git-scm.com/download/win and download the windows version of git. After the download has finished, run the installer and accept the default configuration for all pages except for the following:\n\nOn the Select Components page, add a Git Bash profile to Windows Terminal.\n\n\nTo install windows terminal visit this link and click Get to open it in Windows Store. Inside the Store, click Get again and then click Install. After installation, click Launch to start Windows Terminal. In the top of the window, you will see the tab bar with one open tab, a plus sign, and a down arrow. Click the down arrow and select Settings (or type the shortcut Ctrl + ,). In the Startup section, click the dropdown menu under Default profile and select Git Bash.\n\nYou can now launch the Windows terminal from the start menu or pin it to the taskbar like any other program (you can read the rest of the article linked above for additional tips if you wish). To make sure everything worked, close down Windows Terminal, and open it again. Git Bash should open by default, the text should be green and purple, and the tab should read MINGW64:/c/Users/$USERNAME (you should also see /c/Users/$USERNAME if you type pwd into the terminal). This screenshot shows what it should look like:\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we refer to “the terminal” in these installation instructions, we want you to use the Windows Terminal that you just installed with the Git Bash profile. Do not use Windows PowerShell, CMD, or anything else unless explicitly instructed to do so.\n\n\nTo open a new tab you can click the plus sign or use Ctrl + Shift + t (you can close a tab with Ctrl + Shift + w). To copy text from the terminal, you can highlight it with the mouse and then click Ctrl + Shift + c. To paste text you use Ctrl + Shift + v, try it by pasting the following into the terminal to check which version of Bash you just installed:\nbash --version\nThe output should look similar to this:\nGNU bash, version 4.4.23(1)-release (x86_64-pc-sys)\nCopyright (C) 2019 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nNote\n\n\n\nIf there is a newline (the enter character) in the clipboard when you are pasting into the terminal, you will be asked if you are sure you want to paste since this newline will act as if you pressed enter and run the command. As a guideline you can press Paste anyway unless you are sure you don’t want this to happen.\n\n\nLet’s also check which version of git was installed:\ngit --version\ngit version 2.32.0.windows.2\n\n\n\n\n\n\nNote\n\n\n\nSome of the Git commands we will use are only available since Git 2.23, so make sure your if your Git is at least this version.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/windows.html#latex",
    "href": "computing/windows.html#latex",
    "title": " Windows",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nNote that you might see two error messages regarding lua during the installation, you can safely ignore these, the installation will complete successfully after clicking “OK”.\nIf it still fails, proceed with the instructions\n\nIn RStudio, run the following commands to install the tinytex package and setup tinytex:\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nIn order for Git Bash to be able to find the location of TinyTex, you will need to sign out of Windows and back in again. After doing that, you can check that the installation worked by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2021/W32TeX)\nkpathsea version 6.3.3\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/windows.html#github-pat",
    "href": "computing/windows.html#github-pat",
    "title": " Windows",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/windows.html#post-installation-notes",
    "href": "computing/windows.html#post-installation-notes",
    "title": " Windows",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/windows.html#attributions",
    "href": "computing/windows.html#attributions",
    "title": " Windows",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/mac_x86.html",
    "href": "computing/mac_x86.html",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#installation-notes",
    "href": "computing/mac_x86.html#installation-notes",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#terminal",
    "href": "computing/mac_x86.html#terminal",
    "title": " MacOS x86",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_x86.html#github",
    "href": "computing/mac_x86.html#github",
    "title": " MacOS x86",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_x86.html#git",
    "href": "computing/mac_x86.html#git",
    "title": " MacOS x86",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_x86.html#latex",
    "href": "computing/mac_x86.html#latex",
    "title": " MacOS x86",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_x86.html#github-pat",
    "href": "computing/mac_x86.html#github-pat",
    "title": " MacOS x86",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_x86.html#post-installation-notes",
    "href": "computing/mac_x86.html#post-installation-notes",
    "title": " MacOS x86",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_x86.html#attributions",
    "href": "computing/mac_x86.html#attributions",
    "title": " MacOS x86",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/index.html",
    "href": "computing/index.html",
    "title": " Computing",
    "section": "",
    "text": "In order to participate in this class, we will require the use of R, and encourage the use of RStudio. Both are free, and you likely already have both.\nYou also need Git, Github and Slack.\nBelow are instructions for installation. These are edited and simplified from the DSCI 310 Setup Instructions. If you took DSCI 310 last year, you may be good to go, with the exception of the R package."
  },
  {
    "objectID": "computing/index.html#laptop-requirements",
    "href": "computing/index.html#laptop-requirements",
    "title": " Computing",
    "section": "Laptop requirements",
    "text": "Laptop requirements\n\nRuns one of the following operating systems: Ubuntu 20.04, macOS (version 11.4.x or higher), Windows 10 (version 2004, 20H2, 21H1 or higher).\n\nWhen installing Ubuntu, checking the box “Install third party…” will (among other things) install proprietary drivers, which can be helpful for wifi and graphics cards.\n\nCan connect to networks via a wireless connection for on campus work\nHas at least 30 GB disk space available\nHas at least 4 GB of RAM\nUses a 64-bit CPU\nIs at most 6 years old (4 years old or newer is recommended)\nUses English as the default language. Using other languages is possible, but we have found that it often causes problems in the homework. We’ve done our best to fix them, but we may ask you to change it if you are having trouble.\nStudent user has full administrative access to the computer."
  },
  {
    "objectID": "computing/index.html#software-installation-instructions",
    "href": "computing/index.html#software-installation-instructions",
    "title": " Computing",
    "section": "Software installation instructions",
    "text": "Software installation instructions\nPlease click the appropriate link below to view the installation instructions for your operating system:\n\nmacOS x86 or macOS arm\nUbuntu\nWindows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 406",
    "section": "",
    "text": "Jump to Schedule Syllabus\n\n\nAt the end of the course, you will be able to:\n\nAssess the prediction properties of the supervised learning methods covered in class;\nCorrectly use regularization to improve predictions from linear models, and also to identify important explanatory variables;\nExplain the practical difference between predictions obtained with parametric and non-parametric methods, and decide in specific applications which approach should be used;\nSelect and construct appropriate ensembles to obtain improved predictions in different contexts;\nUse and interpret principal components and other dimension reduction techniques;\nEmploy reasonable coding practices and understand basic R syntax and function.\nWrite reports and use proper version control; engage with standard software."
  },
  {
    "objectID": "computing/mac_arm.html",
    "href": "computing/mac_arm.html",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#installation-notes",
    "href": "computing/mac_arm.html#installation-notes",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#terminal",
    "href": "computing/mac_arm.html#terminal",
    "title": " MacOS ARM",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_arm.html#github",
    "href": "computing/mac_arm.html#github",
    "title": " MacOS ARM",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_arm.html#git",
    "href": "computing/mac_arm.html#git",
    "title": " MacOS ARM",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_arm.html#latex",
    "href": "computing/mac_arm.html#latex",
    "title": " MacOS ARM",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_arm.html#github-pat",
    "href": "computing/mac_arm.html#github-pat",
    "title": " MacOS ARM",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_arm.html#post-installation-notes",
    "href": "computing/mac_arm.html#post-installation-notes",
    "title": " MacOS ARM",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_arm.html#attributions",
    "href": "computing/mac_arm.html#attributions",
    "title": " MacOS ARM",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/ubuntu.html",
    "href": "computing/ubuntu.html",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#installation-notes",
    "href": "computing/ubuntu.html#installation-notes",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#ubuntu-software-settings",
    "href": "computing/ubuntu.html#ubuntu-software-settings",
    "title": " Ubuntu",
    "section": "Ubuntu software settings",
    "text": "Ubuntu software settings\nTo ensure that you are installing the right version of the software in this guide, open “Software & Updates” and make sure that the boxes in the screenshot are checked (this is the default configuration)."
  },
  {
    "objectID": "computing/ubuntu.html#github",
    "href": "computing/ubuntu.html#github",
    "title": " Ubuntu",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/ubuntu.html#git",
    "href": "computing/ubuntu.html#git",
    "title": " Ubuntu",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, so if your Git is older than this version, we ask you to update it using the following commands:\nsudo apt update\nsudo apt install git\nYou can check your git version with the following command:\ngit --version\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Linux section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/ubuntu.html#latex",
    "href": "computing/ubuntu.html#latex",
    "title": " Ubuntu",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it still fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/ubuntu.html#github-pat",
    "href": "computing/ubuntu.html#github-pat",
    "title": " Ubuntu",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/ubuntu.html#post-installation-notes",
    "href": "computing/ubuntu.html#post-installation-notes",
    "title": " Ubuntu",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/ubuntu.html#attributions",
    "href": "computing/ubuntu.html#attributions",
    "title": " Ubuntu",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": " Syllabus",
    "section": "",
    "text": "Term 2024 Winter 1: 03 Sep - 05 Dec 2024"
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": " Syllabus",
    "section": "Course info",
    "text": "Course info\nInstructors:\nGeoff Pleiss\nWebsite: https://geoffpleiss.com/\nEmail: geoff.pleiss@stat.ubc.ca\nSlack: @geoff\nTrevor Campbell\nWebsite: https://trevorcampbell.me/\nEmail: trevor@stat.ubc.ca\nSlack: @trevor\nOffice hours:\nSee Canvas for times and locations.\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGithub: https://github.com/stat-406-2024/\nCanvas: https://canvas.ubc.ca/courses/147492/\nLectures/Labs:\nSee Canvas for times and locations.\nTextbooks:\n[ISLR]\n[ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": " Syllabus",
    "section": "Course objectives",
    "text": "Course objectives\nThis is a course in statistical learning methods. Based on the theory of linear models covered in Stat 306, this course will focus on applying many techniques of data analysis to interesting datasets.\nThe course combines analysis with methodology and computational aspects. It treats both the “art” of understanding unfamiliar data and the “science” of analyzing that data in terms of statistical properties. The focus will be on practical aspects of methodology and intuition to help students develop tools for selecting appropriate methods and approaches to problems in their own lives.\nThis is not a “how to program” course, nor a “tour of machine learning methods”. Rather, this course is about how to understand some ML methods. STAT 306 tends to give background in many of the tools of understanding as well as working with already-written R packages. On the other hand, CPSC 340 introduces many methods with a focus on “from-scratch” implementation (in Julia or Python). This course will try to bridge the gap between these approaches. Depending on which course you took, you may be more or less skilled in some aspects than in others. That’s OK and expected.\n\nLearning outcomes\n\nAssess the prediction properties of the supervised learning methods covered in class;\nCorrectly use regularization to improve predictions from linear models, and also to identify important explanatory variables;\nExplain the practical difference between predictions obtained with parametric and non-parametric methods, and decide in specific applications which approach should be used;\nSelect and construct appropriate ensembles to obtain improved predictions in different contexts;\nUse and interpret principal components and other dimension reduction techniques;\nEmploy reasonable coding practices and understand basic R syntax and function.\nWrite reports and use proper version control; engage with standard software."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": " Syllabus",
    "section": "Textbooks",
    "text": "Textbooks\n\nRequired:\nAn Introduction to Statistical Learning, James, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: https://www.statlearning.com\n\n\nOptional (but excellent):\nThe Elements of Statistical Learning, Hastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\nThis second book is a more advanced treatment of a superset of the topics we will cover. If you want to learn more and understand the material more deeply, this is the book for you. All readings from [ESL] are optional."
  },
  {
    "objectID": "syllabus.html#course-assessment-opportunities",
    "href": "syllabus.html#course-assessment-opportunities",
    "title": " Syllabus",
    "section": "Course assessment opportunities",
    "text": "Course assessment opportunities\n\nEffort-based component\nLabs: [0, 20]\nHomework assignments: [0, 50]\nClickers: [0, 10]\nTotal: min(65, Labs + Homework + Clickers)\n\n\nLabs\nThese are intended to keep you on track. They are to be submitted via pull requests in your personal labs-&lt;username&gt; repo (see the computing tab for descriptions on how to do this).\nLabs typically have a few questions for you to answer or code to implement. These are to be done during lab periods. But you can do them on your own as well. These are worth 2 points each up to a maximum of 20 points. They are due at 2300 on the day of your assigned lab section.\nIf you attend lab, you may share a submission with another student (with acknowledgement on the PR). If you do not attend lab, you must work on your own (subject to the collaboration instructions for Assignments below).\n\nRules.\nYou must submit via PR by the deadline. Your PR must include at least 3 commits. After lab 2, failure to include at least 3 commits will result in a maximum score of 1.\n\n\n\n\n\n\nTip\n\n\n\nIf you attend your lab section, you may work in pairs, submitting a single document to one of your Repos. Be sure to put both names on the document, and mention the collaboration on your PR. You still have until 11pm to submit.\n\n\n\n\nMarking.\nThe overriding theme here is “if you put in the effort, you’ll get all the points.” Grading scheme:\n\n2 if basically all correct\n\n1 if complete but with some major errors, or mostly complete and mostly correct\n\n0 otherwise\n\nYou may submit as many labs as you wish up to 20 total points.\nThere are no appeals on grades.\nIt’s important here to recognize just how important active participation in these activities is. You learn by doing, and this is your opportunity to learn in a low-stakes environment. One thing you’ll learn, for example, is that all animals urinate in 21 seconds.1\n\n\n\nAssignments\nThere will be 5 assignments. These are submitted via pull request similar to the labs but to the homework-&lt;username&gt; repo. Each assignment is worth up to 10 points. They are due by 2300 on the deadline. You must make at least 5 commits. Failure to have at least 5 commits will result in a 25% deduction on HW1 and a 50% deduction thereafter. No exceptions.\nAssignments are typically lightly marked. The median last year was 8/10. But they are not easy. Nor are they short. They often involve a combination of coding, writing, description, and production of statistical graphics.\nAfter receiving a mark and feedback, if you score less than 7, you may make corrections to bring your total to 7. This means, if you fix everything that you did wrong, you get 7. Not 10. The revision must be submitted within 1 week of getting your mark. Only 1 revision per assignment. The TA decision is final. Note that the TAs will only regrade parts you missed, but if you somehow make it worse, they can deduct more points.\nThe revision allowance applies only if you got 3 or more points of “content” deductions. If you missed 3 points for content and 2 more for “penalties” (like insufficient commits, code that runs off the side of the page, etc), then you are ineligible.\n\nPolicy on collaboration on assignments\nDiscussing assignments with your classmates is allowed and encouraged, but it is important that every student get practice working on these problems. This means that all the work you turn in must be your own. The general policy on homework collaboration is:\n\nYou must first make a serious effort to solve the problem.\nIf you are stuck after doing so, you may ask for help from another student. You may discuss strategies to solve the problem, but you may not look at their code, nor may they spell out the solution to you step-by-step.\nOnce you have gotten help, you must write your own solution individually. You must disclose, in your GitHub pull request, the names of anyone from whom you got help.\nThis also applies in reverse: if someone approaches you for help, you must not provide it unless they have already attempted to solve the problem, and you may not share your code or spell out the solution step-by-step.\n\n\n\n\n\n\n\nWarning\n\n\n\nAdherence to the above policy means that identical answers, or nearly identical answers, cannot occur. Thus, such occurrences are violations of the Course’s Academic honesty policy.\n\n\nThese rules also apply to getting help from other people such as friends not in the course (try the problem first, discuss strategies, not step-by-step solutions, acknowledge those from whom you received help).\nYou may not use homework help websites under any circumstances. You are also very strongly discouraged from using ChatGPT, Stack Overflow, and so on. The purpose here is to learn. Good faith efforts toward learning are rewarded.\nYou can always, of course, ask me for help on Slack. And public Slack questions are allowed and encouraged.\nYou may also use external sources (books, websites, papers, …) to\n\nLook up programming language documentation, find useful packages, find explanations for error messages, or remind yourself about the syntax for some feature. I do this all the time in the real world. Wikipedia is your friend.\nRead about general approaches to solving specific problems (e.g. a guide to dynamic programming or a tutorial on unit testing in your programming language), or\nClarify material from the course notes or assignments.\n\nBut external sources must be used to support your solution, not to obtain your solution. You may not use them to\n\nFind solutions to the specific problems assigned as homework (in words or in code)—you must independently solve the problem assigned, not translate a solution presented online or elsewhere.\nFind course materials or solutions from this or similar courses from previous years, or\nCopy text or code to use in your submissions without attribution.\n\nIf you use code from online or other sources (including generative AI), you must include code comments identifying the source. It must be clear what code you wrote and what code is from other sources. This rule also applies to text, images, and any other material you submit.\nPlease talk to me if you have any questions about this policy. Any form of plagiarism or cheating will result in sanctions to be determined by me, including grade penalties (such as negative points for the assignment or reductions in letter grade) or course failure. I am obliged to report violations to the appropriate University authorities. See also the text below.\n\n\n\nClickers\nThese are short multiple choice and True / False questions. They happen in class. For each question, correct answers are worth 4, incorrect answers are worth 2. You get 0 points for not answering.\nSuppose there are N total clicker questions, and you have x points. Your final score for this component is\nmax(0, min(5 * x / N - 5, 10)).\nNote that if your average is less than 1, you get 0 points in this component.\n\n\n\n\n\n\nImportant\n\n\n\nIn addition, your final grade in this course will be reduced by 1 full letter grade.\n\n\nThis means that if you did everything else and get a perfect score on the final exam, you will get a 79. Two people did this last year. They were sad.\n\n\n\n\n\n\nWarning\n\n\n\nDON’T DO THIS!!\n\n\nThis may sound harsh, but think about what is required for such a penalty. You’d have to skip more than 50% of class meetings and get every question wrong when you are in class. This is an in-person course. It is not possible to get an A without attending class on a regular basis.\nTo compensate, I will do my best to post recordings of lectures. Past experience has shown 2 things:\n\nYou learn better by attending class than by skipping and “watching”.\nSometimes the technology messes up. So there’s no guarantee that these will be available.\n\nThe purpose is to let you occasionally miss class for any reason with minimal consequences. See also below. If for some reason you need to miss longer streches of time, please contact me or discuss your situation with your Academic Advisor as soon as possible. Don’t wait until December.\n\n\n\nYour score on HW, Labs, and Clickers\nThe total you can accumulate across these 3 components is 65 points. But you can get there however you want. The total available is 80 points. The rest is up to you. But with choice, comes responsibility.\nRules:\n\nNothing dropped.\nNo extensions.\nIf you miss a lab or a HW deadline, then you miss it.\nMake up for missed work somewhere else.\nIf you isolate due to Covid, fine. You miss a few clickers and maybe a lab (though you can do it remotely).\nIf you have a job interview and can’t complete an assignment on time, then skip it.\n\nWe’re not going to police this stuff. You don’t need to let me know. There is no reason that every single person enrolled in this course shouldn’t get &gt; 65 in this class.\nIllustrative scenarios:\n\nDoing 80% on 5 homeworks, coming to class and getting 50% correct, get 2 points on 8 labs gets you 65 points.\nDoing 90% on 5 homeworks, getting 50% correct on all the clickers, averaging 1/2 on all the labs gets you 65 points.\nGoing to all the labs and getting 100%, 100% on 4 homeworks, plus being wrong on every clicker gets you 65 points\n\nChoose your own adventure. Note that the biggest barrier to getting to 65 is skipping the assignments.\n\n\nLate policy\nLate lab/homework submissions will not be accepted.\nMore specifically, any submission that we receive after grading has commenced will receive a 0. We likely won’t start grading at 11:01pm on the due date, so don’t worry if you’re a few minutes late. On the other hand, don’t even bother submitting if you’ve missed the deadline by a few days.\nThis policy may seem harsh, but remember that there are many paths to a full 65 on the effort based grade. If you miss one assignment, focus on doing well on the other labs/assignments and you might still end up with an A in the course.\n\n\n\n\nFinal exam\n35 points\n\n\nAll multiple choice, T/F, matching.\nThe clickers are the best preparation.\nQuestions may ask you to understand or find mistakes in code.\nNo writing code.\n\nThe Final is very hard. By definition, it cannot be effort-based.\nIt is intended to separate those who really understand the material from those who don’t. Last year, the median was 50%. But if you put in the work (do all the effort points) and get 50%, you get an 83 (an A-). If you put in the work (do all the effort points) and skip the final, you get a 65. You do not have to pass the final to pass the course. You don’t even have to take the final.\nThe point of this scheme is for those who work hard to do well. But only those who really understand the material will get 90+."
  },
  {
    "objectID": "syllabus.html#health-issues-and-considerations",
    "href": "syllabus.html#health-issues-and-considerations",
    "title": " Syllabus",
    "section": "Health issues and considerations",
    "text": "Health issues and considerations\n\nCovid Safety in the Classroom\n\n\n\n\n\n\nImportant\n\n\n\nIf you think you’re sick, stay home no matter what.\n\n\nMasks. For our in-person meetings in this class, it is important that all of us feel as comfortable as possible engaging in class activities while sharing an indoor space. Masks are a primary tool to make it harder for Covid-19 to find a new host. Please feel free to wear one or not given your own personal circumstances. Note that there are some people who cannot wear a mask. These individuals are equally welcome in our class.\nVaccination. If you have not yet had a chance to get vaccinated against Covid-19, vaccines are available to you, free. See http://www.vch.ca/covid-19/covid-19-vaccine for help finding an appointment. Boosters will be available later this term. The higher the rate of vaccination in our community overall, the lower the chance of spreading this virus. You are an important part of the UBC community. Please arrange to get vaccinated if you have not already done so. The same goes for Flu.\n\n\nYour personal health\n\n\n\n\n\n\nWarning\n\n\n\nIf you are sick, it’s important that you stay home – no matter what you think you may be sick with (e.g., cold, flu, other).\n\n\n\nDo not come to class if you have Covid symptoms, have recently tested positive for Covid, or are required to quarantine. You can check this website to find out if you should self-isolate or self-monitor: http://www.bccdc.ca/health-info/diseases-conditions/covid-19/self-isolation#Who.\nYour precautions will help reduce risk and keep everyone safer. In this class, the marking scheme is intended to provide flexibility so that you can prioritize your health and still be able to succeed. All work can be completed outside of class with reasonable time allowances.\nIf you do miss class because of illness:\n\nMake a connection early in the term to another student or a group of students in the class. You can help each other by sharing notes. If you don’t yet know anyone in the class, post on the discussion forum to connect with other students.\nConsult the class resources on here and on Canvas. We will post all the slides, readings, and recordings for each class day.\nUse Slack for help.\nCome to virtual office hours.\nSee the marking scheme for reassurance about what flexibility you have. No part of your final grade will be directly impacted by missing class.\n\nIf you are sick on final exam day, do not attend the exam. You must follow up with your home faculty’s advising office to apply for deferred standing. Students who are granted deferred standing write the final exam at a later date. If you’re a Science student, you must apply for deferred standing (an academic concession) through Science Advising no later than 48 hours after the missed final exam/assignment. Learn more and find the application online. For additional information about academic concessions, see the UBC policy here.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease talk with me if you have any concerns or ask me if you are worried about falling behind."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": " Syllabus",
    "section": "University policies",
    "text": "University policies\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here.\n\nAcademic honesty and standards\nUBC Vancouver Statement\nAcademic honesty is essential to the continued functioning of the University of British Columbia as an institution of higher learning and research. All UBC students are expected to behave as honest and responsible members of an academic community. Breach of those expectations or failure to follow the appropriate policies, principles, rules, and guidelines of the University with respect to academic honesty may result in disciplinary action.\nFor the full statement, please see the 2022/23 Vancouver Academic Calendar\nCourse specific\nSeveral commercial services have approached students regarding selling class notes/study guides to their classmates. Please be advised that selling a faculty member’s notes/study guides individually or on behalf of one of these services using UBC email or Canvas, violates both UBC information technology and UBC intellectual property policy. Selling the faculty member’s notes/study guides to fellow students in this course is not permitted. Violations of this policy will be considered violations of UBC Academic Honesty and Standards and will be reported to the Dean of Science as a violation of course rules. Sanctions for academic misconduct may include a failing grade on the assignment for which the notes/study guides are being sold, a reduction in your final course grade, a failing grade in the course, among other possibilities. Similarly, contracting with any service that results in an individual other than the enrolled student providing assistance on quizzes or exams or posing as an enrolled student is considered a violation of UBC’s academic honesty standards.\nSome of the problems that are assigned are similar or identical to those assigned in previous years by me or other instructors for this or other courses. Using proofs or code from anywhere other than the textbooks, this year’s course notes, or the course website is not only considered cheating (as described above), it is easily detectable cheating. Such behavior is strictly forbidden.\nIn previous years, I have caught students cheating on the exams or assignments. I did not enforce any penalty because the action did not help. Cheating, in my experience, occurs because students don’t understand the material, so the result is usually a failing grade even before I impose any penalty and report the incident to the Dean’s office. I carefully structure exams and assignments to make it so that I can catch these issues. I will catch you, and it does not help. Do your own work, and use the TAs and me as resources. If you are struggling, we are here to help.\n\n\n\n\n\n\nCaution\n\n\n\nIf I suspect cheating, your case will be forwarded to the Dean’s office. No questions asked.\n\n\nGenerative AI\nTools to help you code more quickly are rapidly becoming more prevalent. I use them regularly myself. The point of this course is not to “complete assignments” but to learn coding (and other things). With that goal in mind, I recommend you avoid the use of Generative AI. It is unlikely to contribute directly to your understanding of the material. Furthermore, I have experimented with certain tools on the assignments for this course and have found the results underwhelming.\nThe material in this course is best learned through trial and error. Avoiding this mechanism (with generative AI or by copying your friend) is a short-term solution at best. I have tried to structure this course to discourage these types of short cuts, and minimize the pressure you may feel to take them.\n\n\nAcademic Concessions\nThese are handled according to UBC policy. Please see\n\nUBC student services\nUBC Vancouver Academic Calendar\nFaculty of Science Concessions\n\n\n\nMissed final exam\nStudents who miss the final exam must report to their Faculty advising office within 72 hours of the missed exam, and must supply supporting documentation. Only your Faculty Advising office can grant deferred standing in a course. You must also notify your instructor prior to (if possible) or immediately after the exam. Your instructor will let you know when you are expected to write your deferred exam. Deferred exams will ONLY be provided to students who have applied for and received deferred standing from their Faculty.\n\n\nTake care of yourself\nCourse work at this level can be intense, and I encourage you to take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. I struggle with these issues too, and I try hard to set aside time for things that make me happy (cooking, playing/listening to music, exercise, going for walks).\nAll of us benefit from support during times of struggle. If you are having any problems or concerns, do not hesitate to speak with me. There are also many resources available on campus that can provide help and support. Asking for support sooner rather than later is almost always a good idea.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support. UBC Counseling Services is here to help: call 604 822 3811 or visit their website. Consider also reaching out to a friend, faculty member, or family member you trust to help get you the support you need.\n\nA dated PDF is available at this link."
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": " Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA careful reading of this paper with the provocative title “Law of Urination: all mammals empty their bladders over the same duration” reveals that the authors actually mean something far less precise. In fact, their claim is more accurately stated as “mammals over 3kg in body weight urinate in 21 seconds with a standard deviation of 13 seconds”. But the accurate characterization is far less publicity-worthy.↩︎"
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": " Schedule",
    "section": "",
    "text": "Required readings slides are listed below for each module. Readings from [ISLR] are always required while those from [ESL] are optional and supplemental."
  },
  {
    "objectID": "schedule/index.html#introduction-and-review",
    "href": "schedule/index.html#introduction-and-review",
    "title": " Schedule",
    "section": "0 Introduction and Review",
    "text": "0 Introduction and Review\nRequired reading below is meant to reengage brain cells which have no doubt forgotten all the material that was covered in STAT 306 or CPSC 340. We don’t presume that you remember all these details, but that, upon rereading, they at least sound familiar. If this all strikes you as completely foreign, this class may not be for you.\n\nRequired reading\n\n[ISLR] 2.1, 2.2, and Chapter 3 (this material is review)\n\nOptional reading\n\n[ESL] 2.4 and 2.6\n\nHandouts\n\nProgramming in R .Rmd, .pdf\n\n\nUsing in RMarkdown .Rmd, .pdf\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n03 Sep 24\n(no class, Imagine UBC)\n\n\n\n05 Sep 24\nIntro to class, Git\n(Quiz 0 due tomorrow)\n\n\n10 Sep 24\nUnderstanding R / Rmd\nLab 00, (Labs begin)\n\n\n12 Sep 24\nLM review, LM Example"
  },
  {
    "objectID": "schedule/index.html#model-accuracy",
    "href": "schedule/index.html#model-accuracy",
    "title": " Schedule",
    "section": "1 Model Accuracy",
    "text": "1 Model Accuracy\n\nTopics\n\nModel selection; cross validation; information criteria; stepwise regression\n\nRequired reading\n\n[ISLR] Ch 2.2 (not 2.2.3), 5.1 (not 5.1.5), 6.1, 6.4\n\nOptional reading\n\n[ESL] 7.1-7.5, 7.10\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n17 Sep 24\nRegression function, Bias and Variance\n\n\n\n19 Sep 24\nRisk estimation, Info Criteria\n\n\n\n24 Sep 24\nGreedy selection\n\n\n\n26 Sep 24\n\nHW 1 due"
  },
  {
    "objectID": "schedule/index.html#regularization-smoothing-and-trees",
    "href": "schedule/index.html#regularization-smoothing-and-trees",
    "title": " Schedule",
    "section": "2 Regularization, smoothing, and trees",
    "text": "2 Regularization, smoothing, and trees\n\nTopics\n\nRidge regression, lasso, and related; linear smoothers (splines, kernels); kNN\n\nRequired reading\n\n[ISLR] Ch 6.2, 7.1-7.7.1, 8.1, 8.1.1, 8.1.3, 8.1.4\n\nOptional reading\n\n[ESL] 3.4, 3.8, 5.4, 6.3\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n1 Oct 24\nRidge, Lasso\n\n\n\n3 Oct 24\nCV for comparison, NP 1\n\n\n\n8 Oct 24\nNP 2, Why smoothing?\n\n\n\n10 Oct 23\nOther"
  },
  {
    "objectID": "schedule/index.html#classification",
    "href": "schedule/index.html#classification",
    "title": " Schedule",
    "section": "3 Classification",
    "text": "3 Classification\n\nTopics\n\nlogistic regression; LDA/QDA; naive bayes; trees\n\nRequired reading\n\n[ISLR] Ch 2.2.3, 5.1.5, 4-4.5, 8.1.2\n\nOptional reading\n\n[ESL] 4-4.4, 9.2, 13.3\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n15 Oct 24\nClassification, LDA and QDA\n\n\n\n17 Oct 24\nLogistic regression, Gradient descent\nHW 2 due\n\n\n22 Oct 24\nNonlinear, Other losses\n\n\n\n24 Oct 24\nThe bootstrap"
  },
  {
    "objectID": "schedule/index.html#modern-techniques",
    "href": "schedule/index.html#modern-techniques",
    "title": " Schedule",
    "section": "4 Modern techniques",
    "text": "4 Modern techniques\n\nTopics\n\nbagging; boosting; random forests; neural networks\n\nRequired reading\n\n[ISLR] 5.2, 8.2, 10.1, 10.2, 10.6, 10.7\n\nOptional reading\n\n[ESL] 10.1-10.10 (skip 10.7), 11.1, 11.3, 11.4, 11.7\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n29 Oct 24\nBagging and random forests, Boosting\n\n\n\n31 Oct 24\nIntro to neural nets\nHW 3 due\n\n\n5 Nov 24\nEstimating neural nets\n\n\n\n7 Nov 24\nMore neural nets (TBD), Optional NNet handout\n\n\n\n12 Nov 24\nNo class. (Midterm break)\n\n\n\n14 Nov 24\nNeural nets wrapup"
  },
  {
    "objectID": "schedule/index.html#unsupervised-learning",
    "href": "schedule/index.html#unsupervised-learning",
    "title": " Schedule",
    "section": "5 Unsupervised learning",
    "text": "5 Unsupervised learning\n\nTopics\n\ndimension reduction and clustering\n\nRequired reading\n\n[ISLR] 12\n\nOptional reading\n\n[ESL] 8.5, 13.2, 14.3, 14.5.1, 14.8, 14.9\n\n\n\n\n\nDate\nSlides\nDeadlines\n\n\n\n\n19 Nov 24\nIntro to PCA, Issues with PCA\nHW 4 due\n\n\n21 Nov 24\nPCA v KPCA\n\n\n\n26 Nov 24\nK means clustering\n\n\n\n28 Dec 24\nHierarchical clustering\n\n\n\n3 Dec 24\nTopic TBD and/or Review\n\n\n\n5 Dec 24\nReview\nHW 5 due"
  },
  {
    "objectID": "schedule/index.html#f-final-exam",
    "href": "schedule/index.html#f-final-exam",
    "title": " Schedule",
    "section": "F Final exam",
    "text": "F Final exam\n\n\n\n\n\n\nImportant\n\n\n\nDo not make any plans to leave Vancouver before the final exam date is announced.\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou must bring your computer as the exam will be given through Canvas\nPlease arrange to borrow one from the library if you do not have your own. Let me know ASAP if this may pose a problem.\nYou may bring 2 sheets of front/back 8.5x11 paper with handwritten notes you want to use. No other materials will be allowed.\nThere will be no required coding, but I may show code or output and ask questions about it.\nIt will be entirely multiple choice / True-False / matching, etc. Delivered on Canvas."
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section",
    "href": "schedule/slides/00-course-review.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Review and bonus clickers",
    "text": "00 Review and bonus clickers\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 30 July 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#big-picture",
    "href": "schedule/slides/00-course-review.html#big-picture",
    "title": "UBC Stat406 2024W",
    "section": "Big picture",
    "text": "Big picture\n\nWhat is a model?\nHow do we evaluate models?\nHow do we decide which models to use?\nHow do we improve models?"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#general-stuff",
    "href": "schedule/slides/00-course-review.html#general-stuff",
    "title": "UBC Stat406 2024W",
    "section": "General stuff",
    "text": "General stuff\n\nLinear algebra (SVD, matrix multiplication, matrix properties, etc.)\nOptimization (derivitive + set to 0, gradient descent, Newton’s method, etc.)\nProbability (conditional probability, Bayes rule, etc.)\nStatistics (likelihood, MLE, confidence intervals, etc.)"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#model-selection",
    "href": "schedule/slides/00-course-review.html#model-selection",
    "title": "UBC Stat406 2024W",
    "section": "1. Model selection",
    "text": "1. Model selection\n\nWhat is a statistical model?\nWhat is the goal of model selection?\nWhat is the difference between training and test error?\nWhat is overfitting?\nWhat is the bias-variance tradeoff?\nWhat is the difference between AIC / BIC / CV / Held-out validation?"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#regression",
    "href": "schedule/slides/00-course-review.html#regression",
    "title": "UBC Stat406 2024W",
    "section": "2. Regression",
    "text": "2. Regression\n\nWhat do we mean by regression?\nWhat is the difference between linear and non-linear regression?\nWhat are linear smoothers and why do we care?\nWhat is feature creation?\nWhat is regularization?\nWhat is the difference between L1 and L2 regularization?"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#classification",
    "href": "schedule/slides/00-course-review.html#classification",
    "title": "UBC Stat406 2024W",
    "section": "3. Classification",
    "text": "3. Classification\n\nWhat is classification? Bayes Rule?\nWhat are linear decision boundaries?\nCompare logistic regression to discriminant analysis.\nWhat are the positives and negatives of trees?\nWhat about loss functions? How do we measure performance?"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#modern-methods",
    "href": "schedule/slides/00-course-review.html#modern-methods",
    "title": "UBC Stat406 2024W",
    "section": "4. Modern methods",
    "text": "4. Modern methods\n\nWhat is the difference between bagging and boosting?\nWhat is the point of the bootstrap?\nWhat is the difference between random forests and bagging?\nHow do we understand Neural Networks?"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#unsupervised-learning",
    "href": "schedule/slides/00-course-review.html#unsupervised-learning",
    "title": "UBC Stat406 2024W",
    "section": "5. Unsupervised learning",
    "text": "5. Unsupervised learning\n\nWhat is unsupervised learning?\nCan be used for feature creation / EDA.\nUnderstanding linear vs. non-linear methods.\nWhat does PCA / KPCA estimate?\nPositives and negatives of clustering procedures."
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-2",
    "href": "schedule/slides/00-course-review.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "The singular value decomposition applies to any matrix.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-3",
    "href": "schedule/slides/00-course-review.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Which of the following produces the ridge regression estimate of \\(\\beta\\) with \\(\\lambda = 1\\)?\n\n\nlm(y ~ x, lambda = 1)\n(crossprod(x)) + diag(ncol(x))) %*% crossprod(x, y)\nsolve(crossprod(x) + diag(ncol(x))) %*% crossprod(x, y)\nglmnet(x, y, lambda = 1, alpha = 0)"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-4",
    "href": "schedule/slides/00-course-review.html#section-4",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "If Classifier A has higher AUC than Classifier B, then Classifier A is preferred.\n\n\nTrue\nFalse"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-5",
    "href": "schedule/slides/00-course-review.html#section-5",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Which of the following is true about the bootstrap?\n\n\nIt is a method for estimating the sampling distribution of a statistic.\nIt is a method for estimating expected prediction error.\nIt is a method for improving the performance of a classifier.\nIt is a method for estimating the variance of a statistic."
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-6",
    "href": "schedule/slides/00-course-review.html#section-6",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Which campus eatery is the best place to celebrate the end of the Term?\n\n\nKoerner’s\nSports Illustrated Clubhouse (formerly Biercraft)\nBrown’s Crafthouse\nRain or Shine"
  },
  {
    "objectID": "schedule/slides/00-course-review.html#section-7",
    "href": "schedule/slides/00-course-review.html#section-7",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "UBC Stat 406 - 2024"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#section",
    "href": "schedule/slides/00-version-control.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Git, Github, and Slack",
    "text": "00 Git, Github, and Slack\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 04 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#course-communication",
    "href": "schedule/slides/00-version-control.html#course-communication",
    "title": "UBC Stat406 2024W",
    "section": "Course communication",
    "text": "Course communication\nSlack:\n\nLink to join on Canvas. This is our discussion board.\nNote that this data is hosted on servers outside of Canada. You may wish to use a pseudonym to protect your privacy.\nAnything super important will be posted to Slack and Canvas.\nBe sure you get Canvas email."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#course-communication-1",
    "href": "schedule/slides/00-version-control.html#course-communication-1",
    "title": "UBC Stat406 2024W",
    "section": "Course communication",
    "text": "Course communication\nGitHub organization\n\nLinked from the website.\nThis is where you complete / submit assignments / projects / in-class-work\nThis is also hosted on Servers outside Canada https://github.com/stat-406-2024/"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-these",
    "href": "schedule/slides/00-version-control.html#why-these",
    "title": "UBC Stat406 2024W",
    "section": "Why these?",
    "text": "Why these?\n\nYes, some data is hosted on servers in the US.\nBut in the real world, no one uses Canvas / Piazza, so why not learn things they do use?\nMuch easier to communicate, “mark” or comment on your work\nMuch more DS friendly\nNote that MDS uses both of these, the Stat and CS departments use both, many faculty use them, Google / Amazon / Meta use things like these, etc.\n\n\nSlack help from MDS features and rules"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control",
    "href": "schedule/slides/00-version-control.html#why-version-control",
    "title": "UBC Stat406 2024W",
    "section": "Why version control?",
    "text": "Why version control?\n\nMuch of this lecture is based on material from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control-1",
    "href": "schedule/slides/00-version-control.html#why-version-control-1",
    "title": "UBC Stat406 2024W",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWhen you get really good\n\n\nVersion control can act as a living lab notebook"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#overview",
    "href": "schedule/slides/00-version-control.html#overview",
    "title": "UBC Stat406 2024W",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "href": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "title": "UBC Stat406 2024W",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#typical-workflow",
    "href": "schedule/slides/00-version-control.html#typical-workflow",
    "title": "UBC Stat406 2024W",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\n```{bash}\ngit clone https://github.com/stat550-2021/lecture-slides.git\n```\n\nCreate a branch\n\n```{bash}\ngit branch &lt;branchname&gt;\n```\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\n```{bash}\ngit add &lt;name/of/tracked/file&gt;\n```\n\nCommit your changes\n\n```{bash}\ngit commit -m \"Some explanatory message\"\n```\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to GitHub\n\n```{bash}\ngit push\ngit push -u origin &lt;branchname&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "href": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "title": "UBC Stat406 2024W",
    "section": "Commit messages should be meaningful",
    "text": "Commit messages should be meaningful\n\n\n\n\n\nInstead, try “Update linear model in Question 1.2”"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "title": "UBC Stat406 2024W",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nDefinitely\n\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\n\n\n\nPossibly\n\njupyter notebooks, images (that won’t change), …\n\n\n\n\nQuestionable\n\nprocessed data, static pdfs, …\n\n\n\n\nDefinitely not\n\nfull data, continually updated pdfs, other things compiled from source code, logs…"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "title": "UBC Stat406 2024W",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nTLDR\nAny file that YOU edit should be tracked\nAny file that’s computer generated should PROBABLY NOT be tracked\nHowever, in this course you will track rendered PDFs of your homeworks/labs. This makes it easier for the graders."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "title": "UBC Stat406 2024W",
    "section": "What should be tracked?",
    "text": "What should be tracked?\nA file called .gitignore tells git files or types to never track\n```{bash}\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n```\nShortcut to track everything (use carefully):\n```{bash}\ngit add .\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#rules",
    "href": "schedule/slides/00-version-control.html#rules",
    "title": "UBC Stat406 2024W",
    "section": "Rules",
    "text": "Rules\nHomework and Labs\n\nYou each have your own repo\nYou make a branch\nDO NOT rename files\nMake enough commits (3 for labs, 5 for HW).\nPush your changes (at anytime) and make a PR against main when done.\nTAs review your work.\nOn HW, if you want to revise, make changes in response to feedback and push to the same branch. Then “re-request review”."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr",
    "href": "schedule/slides/00-version-control.html#whats-a-pr",
    "title": "UBC Stat406 2024W",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "href": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "title": "UBC Stat406 2024W",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "href": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "title": "UBC Stat406 2024W",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThere are guardrails in place. But those won’t stop a bulldozer.\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 9 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#protection",
    "href": "schedule/slides/00-version-control.html#protection",
    "title": "UBC Stat406 2024W",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#guardrails",
    "href": "schedule/slides/00-version-control.html#guardrails",
    "title": "UBC Stat406 2024W",
    "section": "Guardrails",
    "text": "Guardrails\n\nIn this course, we protect main so that you can’t push there\n\n\n\n\n\n\nWarning\n\n\nIf you try to push to main, it will give an error like\n```{bash}\nremote: error: GH006: Protected branch update failed for refs/heads/main.\n```\nThe fix is: make a new branch, then push that."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "href": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "title": "UBC Stat406 2024W",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\nCovers:\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\nCommand line versions (of the same)\n```{bash}\ngit add &lt;name/of/file&gt;\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b &lt;name/of/branch&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "href": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "title": "UBC Stat406 2024W",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\n```{bash}\ngit config user.name --global \"Geoff Pleiss\"\ngit config user.email --global \"geoff.pleiss@stat.ubc.ca\"\ngit config core.editor --global nano \n# or emacs or ... (Geoff loves vim and you should too!)\n```\nStaging\n```{bash}\ngit add name/of/file # stage 1 file\ngit add . # stage all\n```\nCommitting\n```{bash}\n# stage/commit simultaneously\ngit commit -am \"message\" \n\n# open editor to write long commit message\ngit commit \n```\nPushing\n```{bash}\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n```\n\n\nBranching\n```{bash}\n# switch to branchname, error if uncommitted changes\ngit checkout branchname \n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\n```\nCheck the status\n```{bash}\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#conflicts",
    "href": "schedule/slides/00-version-control.html#conflicts",
    "title": "UBC Stat406 2024W",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\n\nThey look like this:\n\nHere are lines that are either unchanged from\nthe common ancestor, or cleanly resolved \nbecause only one side changed.\n\nBut below we have some troubles\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs:sample.txt\n\nAnd here is another line that is cleanly \nresolved or unmodified.\n\n\nYou get to decide, do you want to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nBut always delete the &lt;&lt;&lt;&lt;&lt;, ======, and &gt;&gt;&gt;&gt;&gt; lines.\nOnce you’re satisfied, committing resolves the conflict."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-other-pointers",
    "href": "schedule/slides/00-version-control.html#some-other-pointers",
    "title": "UBC Stat406 2024W",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "href": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "title": "UBC Stat406 2024W",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\n\nIn RStudio, select the file and click   then select  Revert…\n\n```{bash}\n# grab the previously committed version\ngit checkout -- README.md \n```\n2. Staged but not committed\n\nIn RStudio, uncheck the box by the file, then use the method above.\n\n```{bash}\n# unstage\ngit reset HEAD README.md\ngit checkout -- README.md\n```\n\n\n3. Committed\n\nNot easy to do in RStudio…\n\n```{bash}\n# check the log to see where you made the chg, \ngit log\n# go one step before that (eg to 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n```\n\n```{bash}\n# alternatively\n# if it happens to also be on another branch\ngit checkout otherbranch -- README.md\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#recovering-from-things",
    "href": "schedule/slides/00-version-control.html#recovering-from-things",
    "title": "UBC Stat406 2024W",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main, Tried to Push but got refused\n\n```{bash}\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# undo everything that hasn't been pushed to main\ngit fetch && git reset --hard origin/main\ngit checkout newbranch\n```\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\n```{bash}\ngit checkout main\ngit branch -d badbranch\n```\n\nAnything more complicated, either post to Slack or LMGTFY\nIn the Lab next week, you’ll practice\n\nDoing it right.\nRecovering from some mistakes."
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Complete readings before the material is covered in class, and then review again afterwards.\nParticipate actively in class. If you don’t understand something, I can guarantee no one else does either. I have a Ph.D., and I’ve been doing this for more than 10 years. It’s hard for me to remember what it’s like to be you and what you don’t know. Say something! I want you to learn this stuff, and I love to explain more carefully.\nCome to office hours. Again, I like explaining things.\nTry the Labs again without the help of your classmates.\nRead the examples at the end of the [ISLR] chapters. Try the exercises.\nDo not procrastinate — don’t let a module go by with unanswered questions as it will just make the following module’s material even more difficult to follow.\nDo the Worksheets."
  },
  {
    "objectID": "faq.html#how-do-i-succeed-in-this-class",
    "href": "faq.html#how-do-i-succeed-in-this-class",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Complete readings before the material is covered in class, and then review again afterwards.\nParticipate actively in class. If you don’t understand something, I can guarantee no one else does either. I have a Ph.D., and I’ve been doing this for more than 10 years. It’s hard for me to remember what it’s like to be you and what you don’t know. Say something! I want you to learn this stuff, and I love to explain more carefully.\nCome to office hours. Again, I like explaining things.\nTry the Labs again without the help of your classmates.\nRead the examples at the end of the [ISLR] chapters. Try the exercises.\nDo not procrastinate — don’t let a module go by with unanswered questions as it will just make the following module’s material even more difficult to follow.\nDo the Worksheets."
  },
  {
    "objectID": "faq.html#git-and-github",
    "href": "faq.html#git-and-github",
    "title": " Frequently asked questions",
    "section": "Git and Github",
    "text": "Git and Github\n\nHomework/Labs workflow\nRstudio version (uses the Git tab. Usually near Environment/History in the upper right)\n\nMake sure you are on main. Pull in remote changes. Click .\nCreate a new branch by clicking the think that looks kinda like .\nWork on your documents and save frequently.\nStage your changes by clicking the check boxes.\nCommit your changes by clicking Commit.\nRepeat 3-5 as necessary.\nPush to Github \nWhen done, go to Github and open a PR.\nUse the dropdown menu to go back to main and avoid future headaches.\n\nCommand line version\n\n(Optional, but useful. Pull in any remote changes.) git pull\nCreate a new branch git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes git add &lt;name-of-document1&gt; repeat for each changed document. git add . stages all changed documents.\nCommit your changes git commit -m \"some message that is meaningful\"\nRepeat 3-5 as necessary.\nPush to Github git push. It may suggest a longer form of this command, obey.\nWhen done, go to Github and open a PR.\nSwitch back to main to avoid future headaches. git checkout main.\n\n\n\nAsking for a HW regrade.\n\n\n\n\n\n\nTo be eligible\n\n\n\n\nYou must have received &gt;3 points of deductions to be eligible.\nAnd they must have been for “content”, not penalties.\nIf you fix the errors, you can raise your grade to 7/10.\nYou must make revisions and re-request review within 1 week of your initial review.\n\n\n\n\nGo to the your local branch for this HW. If you don’t remember the right name, you can check the PRs in your repo on GitHub by clicking “Pull Requests” tab. It might be closed.\nMake any changes you need to make to the files, commit and push. Make sure to rerender the .pdf if needed.\nGo to GitHub.com and find the original PR for this assignment. There should now be additional commits since the previous Review.\nAdd a comment to the TA describing the changes you’ve made. Be concise and clear.\nUnder “Reviewers” on the upper right of the screen, you should see a 🔁 button. Once you click that, the TA will be notified to review your changes.\n\n\n\nFixing common problems\n\nmaster/main\n“master” has some pretty painful connotations. So as part of an effort to remove racist names from code, the default branch is now “main” on new versions of GitHub. But old versions (like the UBC version) still have “master”. Below, I’ll use “main”, but if you see “master” on what you’re doing, that’s the one to use.\n\n\nStart from main\nBranches should be created from the main branch, not the one you used for the last assignment.\ngit checkout main\nThis switches to main. Then pull and start the new assignment following the workflow above. (In Rstudio, use the dropdown menu.)\n\n\nYou forgot to work on a new branch\nUgh, you did some labs before realizing you forgot to create a new branch. Don’t stress. There are some things below to try. But if you’re confused ASK. We’ve had practice with this, and soon you will too!\n(1) If you started from main and haven’t made any commits (but you SAVED!!):\ngit branch -b &lt;new-branch-name&gt;\nThis keeps everything you have and puts you on a new branch. No problem. Commit and proceed as usual.\n(2) If you are on main and made some commits:\ngit branch &lt;new-branch-name&gt;\ngit log\nThe first line makes a new branch with all the stuff you’ve done. Then we look at the log. Locate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit reset ac2a8 --hard\nThis rolls main back to that commit. You don’t need the whole string, just the first few characters. Finally\ngit checkout &lt;new-branch-name&gt;\nand continue working.\n(3) If you started work on &lt;some-old-branch&gt; for work you already submitted:\nThis one is harder, and I would suggest getting in touch with the TAs. Here’s the procedure.\ngit commit -am \"uhoh, I need to be on a different branch\"\ngit branch &lt;new-branch-name&gt;\nCommit your work with a dumb message, then create a new branch. It’s got all your stuff.\ngit log\nLocate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit rebase --onto main ac2a8 &lt;new-branch-name&gt;\ngit checkout &lt;new-branch-name&gt;\nThis makes the new branch look like main but without the differences from main that are on ac2a8 and WITH all the work you did after ac2a8. It’s pretty cool. And should work. Finally, we switch to our new branch.\n\n\n\nHow can I get better at R?\nI get this question a lot. The answer is almost never “go read the book How to learn R fast” or “watch the video on FreeRadvice.com”. To learn programming, the only thing to do is to program. Do your tutorialls. Redo your tutorials. Run through the code in the textbook. Ask yourself why we used one function instead of another. Ask questions. Play little coding games. If you find yourself wondering how some bit of code works, run through it step by step. Print out the results and see what it’s doing. If you take on these kinds of tasks regularly, you will improve rapidly.\nCoding is an active activity just like learning Spanish. You have to practice constantly. For the same reasons that it is difficult/impossible to learn Spanish just from reading a textbook, it is difficult/impossible to learn R just from reading/watching.\nWhen I took German in 7th grade, I remember my teacher saying “to learn a language, you have to constantly tell lies”. What he meant was, you don’t just say “yesterday I went to the gym”. You say “yesterday I went to the market”, “yesterday I went to the movies”, “today she’s going to the gym”, etc. The point is to internalize conjugation, vocabulary, and the inner workings of the language. The same is true when coding. Do things different ways. Try automating regular tasks.\nRecommended resources\n\nData Science: A first introduction This is the course textbook for UBC’s DSCI 100\nR4DS written by Hadley Wickham and Garrett Grolemund\nDSCI 310 Coursenotes by Tiffany A. Timbers, Joel Ostblom, Florencia D’Andrea, and Rodolfo Lourenzutti\nHappy Git with R by Jenny Bryan\nModern Dive: Statistical Inference via Data Science\nStat545\nGoogle\n\n\n\nMy code doesn’t run. What do I do?\nThis is a constant issue with code, and it happens to everyone. The following is a general workflow for debugging stuck code.\n\nIf the code is running, but not doing what you want, see below.\nRead the Error message. It will give you some important hints. Sometimes these are hard to parse, but that’s ok.\n\n\nset.seed(12345)\ny &lt;- rnorm(10)\nx &lt;- matrix(rnorm(20), 2)\nlinmod &lt;- lm(y ~ x)\n## Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE): variable lengths differ (found for 'x')\n\nThis one is a little difficult. The first stuff before the colon is telling me where the error happened, but I didn’t use a function called model.frame.default. Nonetheless, after the colon it says variable lengths differ. Well y is length 10 and x has 10 rows right? Oh wait, how many rows does x have?\n\nRead the documentation for the function in the error message. For the above, I should try ?matrix.\nGoogle!! If the first few steps didn’t help, copy the error message into Google. This almost always helps. Best to remove any overly specific information first.\nAsk your classmates Slack. In order to ask most effectively, you should probably provide them some idea of how the error happened. See the section on MWEs for how to do this.\nSee me or the TA. Note that it is highly likely that I will ask if you did the above steps first. And I will want to see your minimal working example (MWE).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you meet with me, be prepared to show me your code! Or message me your MWE. Or both. But not neither.\n\n\nIf the error cannot be reproduced in my presence, it is very unlikely that I can fix it.\n\n\nMinimal working examples\nAn MWE is a small bit of code which will work on anyone’s machine and reproduce the error that you are getting. This is a key component of getting help debugging. When you do your homework, there’s lots of stuff going on that will differ from most other students. To allow them (or me, or the TA) to help you, you need to be able to get their machine to reproduce your error (and only your error) without much hassle.\nI find that, in the process of preparing an MWE, I can often answer my own question. So it is a useful exercise even if you aren’t ready to call in the experts yet. The process of stripping your problem down to its bare essence often reveals where the root issue lies. My above code is an MWE: I set a seed, so we both can use exactly the same data, and it’s only a few lines long without calling any custom code that you don’t have.\nFor a good discussion of how to do this, see the R Lecture or stackexchange.\n\n\nHow to write good code\nThis is covered in much greater detail in the lectures, so see there. Here is my basic advice.\n\nWrite script files (which you save) and source them. Don’t do everything in the console. R (and python and Matlab and SAS) is much better as a scripting language than as a calculator.\nDon’t write anything more than once. This has three corollaries:\n\nIf you are tempted to copy/paste, don’t.\nDon’t use magic numbers. Define all constants at the top of the script.\nWrite functions.\n\nThe third is very important. Functions are easy to test. You give different inputs and check whether the output is as expected. This helps catch mistakes.\nThere are two kinds of errors: syntax and function.\n\nThe first R can find (missing close parenthesis, wrong arguments, etc.)\n\nThe second you can only catch by thorough testing\n\nDon’t use magic numbers.\nUse meaningful names. Don’t do this:\n\ndata(\"ChickWeight\")\nout &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\n\nComment things that aren’t clear from the (meaningful) names.\nComment long formulas that don’t immediately make sense:\n\ngarbage &lt;- with(\n  ChickWeight, \n  by(weight, Chick, function(x) (x^2 + 23) / length(x))\n) ## WTF???"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#section",
    "href": "schedule/slides/21-nnets-intro.html#section",
    "title": "UBC Stat406 2024W",
    "section": "21 Neural nets",
    "text": "21 Neural nets\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 02 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#overview",
    "href": "schedule/slides/21-nnets-intro.html#overview",
    "title": "UBC Stat406 2024W",
    "section": "Overview",
    "text": "Overview\nNeural networks are models for supervised learning\nLinear combinations of features are passed through a non-linear transformation in successive layers\nAt the top layer, the resulting latent factors are fed into an algorithm for predictions\n(Most commonly via least squares or logistic loss)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#background",
    "href": "schedule/slides/21-nnets-intro.html#background",
    "title": "UBC Stat406 2024W",
    "section": "Background",
    "text": "Background\n\n\nNeural networks have come about in 3 “waves”\nThe first was an attempt in the 1950s to model the mechanics of the human brain\nIt appeared the brain worked by\n\ntaking atomic units known as neurons, which can be “on” or “off”\nputting them in networks\n\nA neuron itself interprets the status of other neurons\nThere weren’t really computers, so we couldn’t estimate these things"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#background-1",
    "href": "schedule/slides/21-nnets-intro.html#background-1",
    "title": "UBC Stat406 2024W",
    "section": "Background",
    "text": "Background\nAfter the development of parallel, distributed computation in the 1980s, this “artificial intelligence” view was diminished\nAnd neural networks gained popularity\nBut, the growing popularity of SVMs and boosting/bagging in the late 1990s, neural networks again fell out of favor\nThis was due to many of the problems we’ll discuss (non-convexity being the main one)\n\nState-of-the-art performance on various classification tasks has been accomplished via neural networks\nToday, Neural Networks/Deep Learning are the hottest…"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#high-level-overview",
    "href": "schedule/slides/21-nnets-intro.html#high-level-overview",
    "title": "UBC Stat406 2024W",
    "section": "High level overview",
    "text": "High level overview"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#recall-nonparametric-regression",
    "href": "schedule/slides/21-nnets-intro.html#recall-nonparametric-regression",
    "title": "UBC Stat406 2024W",
    "section": "Recall nonparametric regression",
    "text": "Recall nonparametric regression\nSuppose \\(Y \\in \\mathbb{R}\\) and we are trying estimate the regression function \\[\\Expect{Y\\given X} = f_*(X)\\]\nIn Module 2, we discussed basis expansion,\n\nWe know \\(f_*(x) =\\sum_{k=1}^\\infty \\beta_k \\phi_k(x)\\) some basis \\(\\phi_1,\\phi_2,\\ldots\\)\nTruncate this expansion at \\(K\\): \\(f_*^K(x) \\approx \\sum_{k=1}^K \\beta_k \\phi_k(x)\\)\nEstimate \\(\\beta_k\\) with least squares"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#recall-nonparametric-regression-1",
    "href": "schedule/slides/21-nnets-intro.html#recall-nonparametric-regression-1",
    "title": "UBC Stat406 2024W",
    "section": "Recall nonparametric regression",
    "text": "Recall nonparametric regression\nThe weaknesses of this approach are:\n\nThe basis is fixed and independent of the data\nIf \\(p\\) is large, then nonparametrics doesn’t work well at all (recall the Curse of Dimensionality)\nIf the basis doesn’t “agree” with \\(f_*\\), then \\(K\\) will have to be large to capture the structure\nWhat if parts of \\(f_*\\) have substantially different structure? Say \\(f_*(x)\\) really wiggly for \\(x \\in [-1,3]\\) but smooth elsewhere\n\nAn alternative would be to have the data tell us what kind of basis to use (Module 5)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#layer-for-regression",
    "href": "schedule/slides/21-nnets-intro.html#layer-for-regression",
    "title": "UBC Stat406 2024W",
    "section": "1-layer for Regression",
    "text": "1-layer for Regression\n\n\nA single layer neural network model is \\[\n\\begin{aligned}\n&f(x) = \\sum_{k=1}^K \\beta_k h_k(x) \\\\\n&= \\sum_{k=1}^K \\beta_k \\ g(w_k^{\\top}x)\\\\\n&= \\sum_{k=1}^K \\beta_k \\ A_k\\\\\n\\end{aligned}\n\\]\nCompare: A nonparametric regression \\[f(x) = \\sum_{k=1}^K \\beta_k {\\phi_k(x)}\\]"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#terminology",
    "href": "schedule/slides/21-nnets-intro.html#terminology",
    "title": "UBC Stat406 2024W",
    "section": "Terminology",
    "text": "Terminology\n\\[f(x) = \\sum_{k=1}^{{K}} {\\beta_k} {g( w_k^{\\top}x)}\\] The main components are\n\nThe derived features \\({A_k = g(w_k^{\\top}x)}\\) and are called the hidden units or activations\nThe function \\(g\\) is called the activation function (more on this later)\nThe parameters \\({\\beta_k},{w_k}\\) are estimated from the data for all \\(k = 1,\\ldots, K\\).\nThe number of hidden units \\({K}\\) is a tuning parameter\n\n\\[f(x) = \\sum_{k=1}^{{K}} \\beta_0 + {\\beta_k} {g(w_{k0} + w_k^{\\top}x)}\\]\n\nCould add \\(\\beta_0\\) and \\(w_{k0}\\). Called biases (I’m going to ignore them. It’s just an intercept)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#terminology-1",
    "href": "schedule/slides/21-nnets-intro.html#terminology-1",
    "title": "UBC Stat406 2024W",
    "section": "Terminology",
    "text": "Terminology\n\\[f(x) = \\sum_{k=1}^{{K}} {\\beta_k} {g(w_k^{\\top}x)}\\]\nNotes (no biases):\n\n\\(\\beta \\in \\R^k\\)\n\\(w_k \\in \\R^p,\\ k = 1,\\ldots,K\\)\n\\(\\mathbf{W} \\in \\R^{K\\times p}\\)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#what-about-classification-10-classes-2-layers",
    "href": "schedule/slides/21-nnets-intro.html#what-about-classification-10-classes-2-layers",
    "title": "UBC Stat406 2024W",
    "section": "What about classification (10 classes, 2 layers)",
    "text": "What about classification (10 classes, 2 layers)\n\n\n\\[\n\\begin{aligned}\nA_k^{(1)} &= g\\left(\\sum_{j=1}^p w^{(1)}_{k,j} x_j\\right)\\\\\nA_\\ell^{(2)} &= g\\left(\\sum_{k=1}^{K_1} w^{(2)}_{\\ell,k} A_k^{(1)} \\right)\\\\\nz_m &= \\sum_{\\ell=1}^{K_2} \\beta_{m,\\ell} A_\\ell^{(2)}\\\\\nf_m(x) &= \\frac{1}{1 + \\exp(-z_m)}\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\n\n\nPredict class with largest probability \\(\\longrightarrow\\ \\widehat{Y} = \\argmax_{m} f_m(x)\\)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#what-about-classification-10-classes-2-layers-1",
    "href": "schedule/slides/21-nnets-intro.html#what-about-classification-10-classes-2-layers-1",
    "title": "UBC Stat406 2024W",
    "section": "What about classification (10 classes, 2 layers)",
    "text": "What about classification (10 classes, 2 layers)\n\n\nNotes:\n\\(B \\in \\R^{M\\times K_2}\\) (here \\(M=10\\)).\n\\(\\mathbf{W}_2 \\in \\R^{K_2\\times K_1}\\)\n\\(\\mathbf{W}_1 \\in \\R^{K_1\\times p}\\)"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#two-observations",
    "href": "schedule/slides/21-nnets-intro.html#two-observations",
    "title": "UBC Stat406 2024W",
    "section": "Two observations",
    "text": "Two observations\n\nThe \\(g\\) function generates a feature map\n\nWe start with \\(p\\) covariates and we generate \\(K\\) features (1-layer)\n\n\nLogistic / Least-squares with a polynomial transformation\n\\[\n\\begin{aligned}\n&\\Phi(x) \\\\\n& =\n(1, x_1, \\ldots, x_p, x_1^2,\\ldots,x_p^2,\\ldots\\\\\n& \\quad \\ldots x_1x_2, \\ldots, x_{p-1}x_p) \\\\\n& =\n(\\phi_1(x),\\ldots,\\phi_{K_2}(x))\\\\\nf(x) &=  \\sum_{k=1}^{K_2} \\beta_k \\phi_k(x) = \\beta^\\top \\Phi(x)\n\\end{aligned}\n\\]\n\n\nNeural network\n\\[\\begin{aligned}\nA_k &= g\\left( \\sum_{j=1}^p w_{kj}x_j\\right) = g\\left( w_{k}^{\\top}x\\right)\\\\\n\\Phi(x) &= (A_1,\\ldots, A_K)^\\top \\in \\mathbb{R}^{K}\\\\\nf(x) &=\\beta^{\\top} \\Phi(x)=\\beta^\\top A\\\\\n&=  \\sum_{k=1}^K \\beta_k g\\left( \\sum_{j=1}^p w_{kj}x_j\\right)\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/21-nnets-intro.html#two-observations-1",
    "href": "schedule/slides/21-nnets-intro.html#two-observations-1",
    "title": "UBC Stat406 2024W",
    "section": "Two observations",
    "text": "Two observations\n\nIf \\(g(u) = u\\), (or \\(=3u\\)) then neural networks reduce to (massively underdetermined) ordinary least squares (try to show this)\n\n\nReLU is the current fashion (used to be tanh or logistic)"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#section",
    "href": "schedule/slides/03-regression-function.html#section",
    "title": "UBC Stat406 2024W",
    "section": "03 The regression function",
    "text": "03 The regression function\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 16 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#mean-squared-error-mse",
    "href": "schedule/slides/03-regression-function.html#mean-squared-error-mse",
    "title": "UBC Stat406 2024W",
    "section": "Mean squared error (MSE)",
    "text": "Mean squared error (MSE)\nLast time… Ordinary Least Squares\n\\[\\widehat\\beta = \\argmin_\\beta \\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n“Find the \\(\\beta\\) which minimizes the sum of squared errors.”\n\\[\\widehat\\beta = \\arg\\min_\\beta \\frac{1}{n}\\sum_{i=1}^n ( y_i - x_i^\\top \\beta )^2.\\]\n“Find the beta which minimizes the mean squared error.”"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#forget-all-that",
    "href": "schedule/slides/03-regression-function.html#forget-all-that",
    "title": "UBC Stat406 2024W",
    "section": "Forget all that…",
    "text": "Forget all that…\nThat’s “stuff that seems like a good idea”\nAnd it is for many reasons\nThis class is about those reasons, and the “statistics” behind it\n\n\n\nMethods for “Statistical” Learning\nStarts with “what is a model?”"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#what-is-a-model",
    "href": "schedule/slides/03-regression-function.html#what-is-a-model",
    "title": "UBC Stat406 2024W",
    "section": "What is a model?",
    "text": "What is a model?\nIn statistics, “model” has a mathematical meaning.\nDistinct from “algorithm” or “procedure”.\nDefining a model often leads to a procedure/algorithm with good properties.\nSometimes procedure/algorithm \\(\\Rightarrow\\) a specific model.\n\nStatistics (the field) tells me how to understand when different procedures are desirable and the mathematical guarantees that they satisfy.\n\nWhen are certain models appropriate?\n\nOne definition of “Statistical Learning” is the “statistics behind the procedure”."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-101",
    "href": "schedule/slides/03-regression-function.html#statistical-models-101",
    "title": "UBC Stat406 2024W",
    "section": "Statistical models 101",
    "text": "Statistical models 101\nWe observe data \\(Z_1,\\ Z_2,\\ \\ldots,\\ Z_n\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\n\nA statistical model is a set of distributions \\(\\mathcal{P}\\).\n\nSome examples:\n\n\\(\\P = \\{P : P(Z=1)=p,\\ P(Z=0)=1-p, 0 \\leq p \\leq 1\\}\\).\n\\(\\P = \\{P : Y | X \\sim N(X^\\top\\beta,\\sigma^2),\\  \\beta \\in \\R^p,\\ \\sigma&gt;0\\}\\) (here \\(Z = (Y,X)\\))\n\\(\\P = \\{P \\mbox{ given by any CDF }F\\}\\).\n\\(\\P = \\{P : E[Y | X] = f(X) \\mbox{ for some smooth } f: \\R^p \\rightarrow \\R\\}\\) (here \\(Z = (Y,X)\\))"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models",
    "href": "schedule/slides/03-regression-function.html#statistical-models",
    "title": "UBC Stat406 2024W",
    "section": "Statistical models",
    "text": "Statistical models\nWe want to use the data to select a distribution \\(P\\) that probably generated the data.\n\nMy model:\n\\[\n\\P = \\{P: P(z=1)=p,\\ P(z=0)=1-p,\\ 0 &lt; p &lt; 1 \\}\n\\]\n\nTo completely characterize \\(P\\), I just need to estimate \\(p\\).\nNeed to assume that \\(P \\in \\P\\).\nThis assumption is mostly empty: need independent, can’t see \\(z=12\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-1",
    "href": "schedule/slides/03-regression-function.html#statistical-models-1",
    "title": "UBC Stat406 2024W",
    "section": "Statistical models",
    "text": "Statistical models\nWe observe data \\((Y, X)\\) generated by some probability distribution \\(P\\) on \\(\\R \\times \\R^p\\). We want to use the data to learn about \\(P\\).\n\nMy model\n\\[\n\\P = \\{P : Y | X \\sim N(X^\\top\\beta,\\ \\sigma^2), \\beta \\in \\R^p, \\sigma&gt;0\\}.\n\\]\n\nTo completely characterize the \\(Y|X\\)-conditional of \\(P\\), I just need to estimate \\(\\beta\\) and \\(\\sigma\\).\n\nI’m not interested in learning the \\(X\\)-marginal of \\(P\\)\n\nNeed to assume that \\(P\\in\\P\\).\nThis time, I have to assume a lot more: (conditional) linearity, independence, conditional Gaussian noise, no ignored variables, no collinearity, etc."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#statistical-models-unfamiliar-example",
    "href": "schedule/slides/03-regression-function.html#statistical-models-unfamiliar-example",
    "title": "UBC Stat406 2024W",
    "section": "Statistical models, unfamiliar example",
    "text": "Statistical models, unfamiliar example\nWe observe data \\(Z \\in \\R\\) generated by some probability distribution \\(P\\). We want to use the data to learn about \\(P\\).\nMy model\n\\[\n\\P = \\{P : Z \\textrm{ has a density function } f \\}.\n\\]\n\nTo completely characterize \\(P\\), I need to estimate \\(f\\).\nIn fact, we can’t hope to do this.\n\nRevised Model 1 - \\(\\P=\\{ Z \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; M \\}\\)\nRevised Model 2 - \\(\\P=\\{ Z \\textrm{ has a density function } f : \\int (f'')^2 dx &lt; K &lt; M \\}\\)\nRevised Model 3 - \\(\\P=\\{ Z \\textrm{ has a density function } f : \\int |f'| dx &lt;  M \\}\\)\n\nEach of these suggests different ways of estimating \\(f\\)"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#assumption-lean-regression",
    "href": "schedule/slides/03-regression-function.html#assumption-lean-regression",
    "title": "UBC Stat406 2024W",
    "section": "Assumption Lean Regression",
    "text": "Assumption Lean Regression\nImagine \\(Z = (Y, \\mathbf{X}) \\sim P\\) with \\(Y \\in \\R\\) and \\(\\mathbf{X} = (1, X_1, \\ldots, X_p)^\\top\\).\nWe are interested in the conditional distribution \\(P_{Y|\\mathbf{X}}\\)\nSuppose we think that there is some function of interest which relates \\(Y\\) and \\(X\\).\nLet’s call this function \\(\\mu(\\mathbf{X})\\) for the moment. How do we estimate \\(\\mu\\)? What is \\(\\mu\\)?\n\n\nTo make this precise, we\n\nHave a model \\(\\P\\).\nNeed to define a “good” functional \\(\\mu\\).\nLet’s loosely define “good” as\n\n\nGiven a new (random) \\(Z\\), \\(\\mu(\\mathbf{X})\\) is “close” to \\(Y\\).\n\n\n\nSee Berk et al. Assumption Lean Regression."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#evaluating-close",
    "href": "schedule/slides/03-regression-function.html#evaluating-close",
    "title": "UBC Stat406 2024W",
    "section": "Evaluating “close”",
    "text": "Evaluating “close”\nWe need more functions.\nChoose some loss function \\(\\ell\\) that measures how close \\(\\mu\\) and \\(Y\\) are.\n\n\n\nSquared-error:\n\\(\\ell(y,\\ \\mu) = (y-\\mu)^2\\)\nAbsolute-error:\n\\(\\ell(y,\\ \\mu) = |y-\\mu|\\)\nZero-One:\n\\(\\ell(y,\\ \\mu) = I(y\\neq\\mu)=\\begin{cases} 0 & y=\\mu\\\\1 & \\mbox{else}\\end{cases}\\)\nCauchy:\n\\(\\ell(y,\\ \\mu) = \\log(1 + (y - \\mu)^2)\\)\n\n\n\n\n\nCode\nggplot() +\n  xlim(-2, 2) +\n  geom_function(fun = ~log(1+.x^2), colour = 'purple', linewidth = 2) +\n  geom_function(fun = ~.x^2, colour = tertiary, linewidth = 2) +\n  geom_function(fun = ~abs(.x), colour = primary, linewidth = 2) +\n  geom_line(\n    data = tibble(x = seq(-2, 2, length.out = 100), y = as.numeric(x != 0)), \n    aes(x, y), colour = orange, linewidth = 2) +\n  geom_point(data = tibble(x = 0, y = 0), aes(x, y), \n             colour = orange, pch = 16, size = 3) +\n  ylab(bquote(\"\\u2113\" * (y - mu))) + xlab(bquote(y - mu))"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#start-with-expected-squared-error",
    "href": "schedule/slides/03-regression-function.html#start-with-expected-squared-error",
    "title": "UBC Stat406 2024W",
    "section": "Start with (Expected) Squared Error",
    "text": "Start with (Expected) Squared Error\nLet’s try to minimize the expected squared error (MSE).\nClaim: \\(\\mu(X) = \\Expect{Y\\ \\vert\\ X}\\) minimizes MSE.\nThat is, for any \\(r(X)\\), \\(\\Expect{(Y - \\mu(X))^2} \\leq \\Expect{(Y-r(X))^2}\\).\n\nProof of Claim:\n\\[\\begin{aligned}\n\\Expect{(Y-r(X))^2}\n&= \\Expect{(Y- \\mu(X) + \\mu(X) - r(X))^2}\\\\\n&= \\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2\\Expect{(Y- \\mu(X))(\\mu(X) - r(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} \\\\\n&\\quad +2(\\mu(X) - r(X))\\Expect{(Y- \\mu(X))}\\\\\n&=\\Expect{(Y- \\mu(X))^2} + \\Expect{(\\mu(X) - r(X))^2} + 0\\\\\n&\\geq \\Expect{(Y- \\mu(X))^2}\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#the-regression-function",
    "href": "schedule/slides/03-regression-function.html#the-regression-function",
    "title": "UBC Stat406 2024W",
    "section": "The regression function",
    "text": "The regression function\nSometimes people call this solution:\n\\[\\mu(X) = \\Expect{Y \\ \\vert\\  X}\\]\nthe regression function. (But don’t forget that it depended on \\(\\ell\\).)\nIf we assume that \\(\\mu(x) = \\Expect{Y \\ \\vert\\  X=x} = x^\\top \\beta\\), then we get back exactly OLS.\n\nBut why should we assume \\(\\mu(x) = x^\\top \\beta\\)?"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#brief-aside",
    "href": "schedule/slides/03-regression-function.html#brief-aside",
    "title": "UBC Stat406 2024W",
    "section": "Brief aside",
    "text": "Brief aside\nSome notation / terminology\n\n“Hats” on things mean “estimates”, so \\(\\widehat{\\mu}\\) is an estimate of \\(\\mu\\)\nParameters are “properties of the model”, so \\(f_X(x)\\) or \\(\\mu\\) or \\(\\Var{Y}\\)\nRandom variables like \\(X\\), \\(Y\\), \\(Z\\) may eventually become data, \\(x\\), \\(y\\), \\(z\\), once observed.\n“Estimating” means “using observations to estimate parameters”\n“Predicting” means “using observations to predict future data”\nOften, there is a parameter whose estimate will provide a prediction.\n\n\n\nThis last point can lead to confusion."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#the-regression-function-1",
    "href": "schedule/slides/03-regression-function.html#the-regression-function-1",
    "title": "UBC Stat406 2024W",
    "section": "The regression function",
    "text": "The regression function\nIn mathematics: \\(\\mu(x) = \\Expect{Y \\ \\vert\\  X=x}\\).\nIn words:\nRegression with squared-error loss is really about estimating the (conditional) mean.\n\nIf \\(Y\\sim \\textrm{N}(\\mu,\\ 1)\\), our best guess for a new \\(Y\\) is \\(\\mu\\).\nFor regression, we let the mean \\((\\mu)\\) depend on \\(X\\).\n\nThink of \\(Y\\sim \\textrm{N}(\\mu(X),\\ 1)\\), then conditional on \\(X=x\\), our best guess for a new \\(Y\\) is \\(\\mu(x)\\)\n\n[whatever this function \\(\\mu\\) is]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#anything-strange",
    "href": "schedule/slides/03-regression-function.html#anything-strange",
    "title": "UBC Stat406 2024W",
    "section": "Anything strange?",
    "text": "Anything strange?\nFor any two variables \\(Y\\) and \\(X\\), we can always write\n\\[Y = E[Y\\given X] + (Y - E[Y\\given X]) = \\mu(X) + \\eta(X)\\]\nsuch that \\(\\Expect{\\eta(X)}=0\\).\n\n\nSuppose, \\(\\mu(X)=\\mu_0\\) (constant in \\(X\\)), are \\(Y\\) and \\(X\\) independent?\n\n\n\n\nSuppose \\(Y\\) and \\(X\\) are independent, is \\(\\mu(X)=\\mu_0\\)?\n\n\n\n\nFor more practice on this see the Fun Worksheet on Theory and solutions\nIn this course, I do not expect you to be able to create this math, but understanding and explaining it is important."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#what-do-we-mean-by-good-predictions",
    "href": "schedule/slides/03-regression-function.html#what-do-we-mean-by-good-predictions",
    "title": "UBC Stat406 2024W",
    "section": "What do we mean by good predictions?",
    "text": "What do we mean by good predictions?\nWe make observations and then attempt to “predict” new, unobserved data.\nSometimes this is the same as estimating the (conditional) mean.\nMostly, we observe \\((y_1,x_1),\\ \\ldots,\\ (y_n,x_n)\\), and we want some way to predict \\(Y\\) from \\(X\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#expected-test-mse",
    "href": "schedule/slides/03-regression-function.html#expected-test-mse",
    "title": "UBC Stat406 2024W",
    "section": "Expected test MSE",
    "text": "Expected test MSE\nFor regression applications, we will use squared-error loss:\n\\(R_n(\\widehat{\\mu}) = \\Expect{(Y-\\widehat{\\mu}(X))^2}\\)\n\nI’m giving this a name, \\(R_n\\) for ease.\nDifferent than text.\nThis is expected test MSE."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#example-estimatingpredicting-the-conditional-mean",
    "href": "schedule/slides/03-regression-function.html#example-estimatingpredicting-the-conditional-mean",
    "title": "UBC Stat406 2024W",
    "section": "Example: Estimating/Predicting the (conditional) mean",
    "text": "Example: Estimating/Predicting the (conditional) mean\nSuppose we know that we want to predict a quantity \\(Y\\),\nwhere \\(\\Expect{Y}= \\mu \\in \\mathbb{R}\\) and \\(\\Var{Y} = 1\\).\nOur data is \\(\\{y_1,\\ldots,y_n\\}\\)\nWe will use the sample mean \\(\\overline{Y}_n\\) to estimate both \\(\\mu\\) and \\(Y\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#estimating-the-mean",
    "href": "schedule/slides/03-regression-function.html#estimating-the-mean",
    "title": "UBC Stat406 2024W",
    "section": "Estimating the mean",
    "text": "Estimating the mean\nWe evaluate the estimation risk (since we’re estimating \\(\\mu\\)) via:\n\n\n\\[\\begin{aligned}\n    E[(\\overline{Y}_n-\\mu)^2]\n    &= E[\\overline{Y}_n^2]\n    -2\\mu E[\\overline{Y}_n] + \\mu^2 \\\\\n    &= \\mu^2 + \\frac{1}{n} - 2\\mu^2 +\n    \\mu^2\\\\ &= \\frac{1}{n}\n\\end{aligned}\\]\n\n\nUseful trick\nFor any \\(Z\\),\n\\(\\Var{Z} = \\Expect{Z^2} - \\Expect{Z}^2\\).\nTherefore:\n\\(\\Expect{Z^2} = \\Var{Z} + \\Expect{Z}^2\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys",
    "title": "UBC Stat406 2024W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\nWe evaluate the prediction risk of \\(\\overline{Y}_n\\) (since we’re predicting \\(Y\\)) via:\n\n\n\\[\\begin{aligned}\n  R_n(\\overline{Y}_n)\n  &= \\E[(\\overline{Y}_n-Y)^2]\\\\\n  &= \\E[(\\overline{Y}_n - \\mu)^2] + \\E[(\\mu-Y)^2]\\\\\n  &= \\frac{1}{n} + 1\n\\end{aligned}\\]\n\n\\(1/n\\) for estimation risk\n\\(1\\) for remaining noise in \\(Y\\)\n\n\n\nTricks:\nAdd and subtract \\(\\mu\\) inside the square.\n\\(\\overline{Y}_n\\) and \\(Y\\) are independent and mean \\(\\mu\\)."
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys-1",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys-1",
    "title": "UBC Stat406 2024W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\n\nWhat is the prediction risk of guessing \\(Y=0\\)?\nYou can probably guess that this is a stupid idea.\nLet’s show why it’s stupid.\n\n\\[\\begin{aligned}\n        R_n(0) &= \\E[(0-Y)^2] = 1 + \\mu^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#predicting-new-ys-2",
    "href": "schedule/slides/03-regression-function.html#predicting-new-ys-2",
    "title": "UBC Stat406 2024W",
    "section": "Predicting new Y’s",
    "text": "Predicting new Y’s\n\nWhat is the prediction risk of guessing \\(Y=\\mu\\)?\nThis is a great idea, but we don’t know \\(\\mu\\).\nLet’s see what happens anyway.\n\n\\[\\begin{aligned}\n        R_n(\\mu) &= \\E[(Y-\\mu)^2]= 1\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/03-regression-function.html#risk-relations",
    "href": "schedule/slides/03-regression-function.html#risk-relations",
    "title": "UBC Stat406 2024W",
    "section": "Risk relations",
    "text": "Risk relations\nPrediction risk: \\(R_n(\\overline{Y}_n) = 1 + \\frac{1}{n}\\)\nEstimation risk: \\(E[(\\overline{Y}_n - \\mu)^2] =  \\frac{1}{n}\\)\nThere is actually a nice interpretation here:\n\nThe common \\(1/n\\) term is \\(\\Var{\\overline{Y}_n}\\)\n\nThe extra factor of \\(1\\) in the prediction risk is irreducible error\n\n\\(Y\\) is a random variable, and hence noisy.\nWe can never eliminate it’s intrinsic variance.\n\nIn other words, even if we knew \\(\\mu\\), we could never get closer than \\(1\\), on average.\n\n\nIntuitively, \\(\\overline{Y}_n\\) is the obvious thing to do.\nBut what about unintuitive things…"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#section",
    "href": "schedule/slides/19-bagging-and-rf.html#section",
    "title": "UBC Stat406 2024W",
    "section": "19 Bagging and random forests",
    "text": "19 Bagging and random forests\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 11 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging",
    "title": "UBC Stat406 2024W",
    "section": "Bagging",
    "text": "Bagging\nMany methods (trees, nonparametric smoothers) tend to have low bias but high variance.\nEspecially fully grown trees (that’s why we prune them)\n\nHigh-variance\n\nif we split the training data into two parts at random and fit a decision tree to each part, the results will be quite different.\n\nIn contrast, a low variance estimator\n\nwould yield similar results if applied to the two parts (consider \\(\\widehat{f} = 0\\)).\n\n\nBagging, short for bootstrap aggregation, is a general purpose procedure for reducing variance.\nWe’ll use it specifically in the context of trees, but it can be applied much more broadly."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging-the-heuristic-motivation",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging-the-heuristic-motivation",
    "title": "UBC Stat406 2024W",
    "section": "Bagging: The heuristic motivation",
    "text": "Bagging: The heuristic motivation\nSuppose we have \\(n\\) uncorrelated observations \\(Z_1, \\ldots, Z_n\\), each with variance \\(\\sigma^2\\).\nWhat is the variance of\n\\[\\overline{Z} = \\frac{1}{n} \\sum_{i=1}^n Z_i\\ \\ \\ ?\\]\n\nSuppose we had \\(B\\) separate (uncorrelated) training sets, \\(1, \\ldots, B\\),\nWe can form \\(B\\) separate model fits, \\(\\widehat{f}^1(x), \\ldots, \\widehat{f}^B(x)\\), and then average them:\n\\[\\widehat{f}_{B}(x) = \\frac{1}{B} \\sum_{b=1}^B \\widehat{f}^b(x)\\]"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging-the-bootstrap-part",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging-the-bootstrap-part",
    "title": "UBC Stat406 2024W",
    "section": "Bagging: The bootstrap part",
    "text": "Bagging: The bootstrap part\n\nThis isn’t practical\n\nwe don’t have many training sets.\n\n\nWe therefore turn to the bootstrap to simulate having many training sets.\nSuppose we have data \\(Z_1, \\ldots, Z_n\\)\n\nChoose some large number of samples, \\(B\\).\nFor each \\(b = 1,\\ldots,B\\), resample from \\(Z_1, \\ldots, Z_n\\), call it \\(\\widetilde{Z}_1, \\ldots, \\widetilde{Z}_n\\).\nCompute \\(\\widehat{f}^b = \\widehat{f}(\\widetilde{Z}_1, \\ldots, \\widetilde{Z}_n)\\).\n\n\\[\\widehat{f}_{\\textrm{bag}}(x) = \\frac{1}{B} \\sum_{b=1}^B \\widehat{f}^b(x)\\]\nThis process is known as Bagging"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging-trees",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging-trees",
    "title": "UBC Stat406 2024W",
    "section": "Bagging trees",
    "text": "Bagging trees\n\n\n\n\n\nThe procedure for trees is the following\n\nChoose a large number \\(B\\).\nFor each \\(b = 1,\\ldots, B\\), grow an unpruned tree on the \\(b^{th}\\) bootstrap draw from the data.\nAverage all these trees together."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging-trees-1",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging-trees-1",
    "title": "UBC Stat406 2024W",
    "section": "Bagging trees",
    "text": "Bagging trees\n\n\n\n\n\nEach tree, since it is unpruned, will have\n\nlow / high variance\nlow / high bias\n\nTherefore averaging many trees results in an estimator that has\n\nlower / higher variance and\nlow / high bias."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#bagging-trees-variable-importance-measures",
    "href": "schedule/slides/19-bagging-and-rf.html#bagging-trees-variable-importance-measures",
    "title": "UBC Stat406 2024W",
    "section": "Bagging trees: Variable importance measures",
    "text": "Bagging trees: Variable importance measures\nBagging can dramatically improve predictive performance of trees\nBut we sacrificed some interpretability.\nWe no longer have that nice diagram that shows the segmentation of the predictor space\n(more accurately, we have \\(B\\) of them).\nTo recover some information, we can do the following:\n\nFor each of the \\(b\\) trees and each of the \\(p\\) variables, we record the amount that the Gini index is reduced by the addition of that variable\nReport the average reduction over all \\(B\\) trees."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#random-forest",
    "href": "schedule/slides/19-bagging-and-rf.html#random-forest",
    "title": "UBC Stat406 2024W",
    "section": "Random Forest",
    "text": "Random Forest\nRandom Forest is an extension of Bagging, in which the bootstrap trees are decorrelated.\nRemember: \\(\\Var{\\overline{Z}} = \\frac{1}{n}\\Var{Z_1}\\) unless the \\(Z_i\\)’s are correlated\nSo Bagging may not reduce the variance that much because the training sets are correlated across trees.\n\nHow do we decorrelate?\nDraw a bootstrap sample and start to build a tree.\n\nBut\n\nBefore we split, we randomly pick\n\n\n\\(m\\) of the possible \\(p\\) predictors as candidates for the split."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#decorrelating",
    "href": "schedule/slides/19-bagging-and-rf.html#decorrelating",
    "title": "UBC Stat406 2024W",
    "section": "Decorrelating",
    "text": "Decorrelating\nA new sample of size \\(m\\) of the predictors is taken at each split.\nUsually, we use about \\(m = \\sqrt{p}\\)\nIn other words, at each split, we aren’t even allowed to consider the majority of possible predictors!"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#what-is-going-on-here",
    "href": "schedule/slides/19-bagging-and-rf.html#what-is-going-on-here",
    "title": "UBC Stat406 2024W",
    "section": "What is going on here?",
    "text": "What is going on here?\nSuppose there is 1 really strong predictor and many mediocre ones.\n\nThen each tree will have this one predictor in it,\nTherefore, each tree will look very similar (i.e. highly correlated).\nAveraging highly correlated things leads to much less variance reduction than if they were uncorrelated.\n\nIf we don’t allow some trees/splits to use this important variable, each of the trees will be much less similar and hence much less correlated.\nBagging Trees is Random Forest when \\(m = p\\), that is, when we can consider all the variables at each split."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#example-with-mobility-data",
    "href": "schedule/slides/19-bagging-and-rf.html#example-with-mobility-data",
    "title": "UBC Stat406 2024W",
    "section": "Example with Mobility data",
    "text": "Example with Mobility data\n\nlibrary(randomForest)\nlibrary(kableExtra)\nset.seed(406406)\nmob &lt;- Stat406::mobility |&gt;\n  mutate(mobile = as.factor(Mobility &gt; .1)) |&gt;\n  select(-ID, -Name, -Mobility, -State) |&gt;\n  drop_na()\nn &lt;- nrow(mob)\ntrainidx &lt;- sample.int(n, floor(n * .75))\ntestidx &lt;- setdiff(1:n, trainidx)\ntrain &lt;- mob[trainidx, ]\ntest &lt;- mob[testidx, ]\nrf &lt;- randomForest(mobile ~ ., data = train)\nbag &lt;- randomForest(mobile ~ ., data = train, mtry = ncol(mob) - 1)\npreds &lt;-  tibble(truth = test$mobile, rf = predict(rf, test), bag = predict(bag, test))\n\nkbl(cbind(table(preds$truth, preds$rf), table(preds$truth, preds$bag))) |&gt;\n  add_header_above(c(\"Truth\" = 1, \"RF\" = 2, \"Bagging\" = 2))\n\n\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\nRF\n\n\nBagging\n\n\n\n\nFALSE\nTRUE\nFALSE\nTRUE\n\n\n\n\nFALSE\n61\n10\n60\n11\n\n\nTRUE\n12\n22\n10\n24"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#example-with-mobility-data-1",
    "href": "schedule/slides/19-bagging-and-rf.html#example-with-mobility-data-1",
    "title": "UBC Stat406 2024W",
    "section": "Example with Mobility data",
    "text": "Example with Mobility data\n\nvarImpPlot(rf, pch = 16, col = orange)"
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#one-last-thing",
    "href": "schedule/slides/19-bagging-and-rf.html#one-last-thing",
    "title": "UBC Stat406 2024W",
    "section": "One last thing…",
    "text": "One last thing…\n\nOn average\n\ndrawing \\(n\\) samples from \\(n\\) observations with replacement (bootstrapping) results in ~ 2/3 of the observations being selected. (Can you show this?)\n\n\nThe remaining ~ 1/3 of the observations are not used on that tree.\nThese are referred to as out-of-bag (OOB).\nWe can think of it as a for-free cross-validation.\nEach time a tree is grown, we get its prediction error on the unused observations.\nWe average this over all bootstrap samples."
  },
  {
    "objectID": "schedule/slides/19-bagging-and-rf.html#out-of-bag-error-estimation-for-bagging-rf",
    "href": "schedule/slides/19-bagging-and-rf.html#out-of-bag-error-estimation-for-bagging-rf",
    "title": "UBC Stat406 2024W",
    "section": "Out-of-bag error estimation for bagging / RF",
    "text": "Out-of-bag error estimation for bagging / RF\nFor randomForest(), predict() without passing newdata = gives the OOB prediction\nnot like lm() where it gives the fitted values\n\ntab &lt;- table(predict(bag), train$mobile) \nkbl(tab) |&gt; add_header_above(c(\"Truth\" = 1, \"Bagging\" = 2))\n\n\n\n\n\n\n\n\n\n\nTruth\n\n\nBagging\n\n\n\n\nFALSE\nTRUE\n\n\n\n\nFALSE\n182\n28\n\n\nTRUE\n21\n82\n\n\n\n\n\n\n1 - sum(diag(tab)) / sum(tab) ## OOB misclassification error, no need for CV\n\n[1] 0.1565495"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#section",
    "href": "schedule/slides/22-nnets-estimation.html#section",
    "title": "UBC Stat406 2024W",
    "section": "22 Neural nets - estimation",
    "text": "22 Neural nets - estimation\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 16 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#neural-network-terms-again-t-hidden-layers-regression",
    "href": "schedule/slides/22-nnets-estimation.html#neural-network-terms-again-t-hidden-layers-regression",
    "title": "UBC Stat406 2024W",
    "section": "Neural Network terms again (T hidden layers, regression)",
    "text": "Neural Network terms again (T hidden layers, regression)\n\n\n\\[\n\\begin{aligned}\nA_{k}^{(1)} &= g\\left(\\sum_{j=1}^p w^{(1)}_{k,j} x_j\\right)\\\\\nA_{\\ell}^{(t)} &= g\\left(\\sum_{k=1}^{K_{t-1}} w^{(t)}_{\\ell,k} A_{k}^{(t-1)} \\right)\\\\\n\\hat{Y} &= z_m = \\sum_{\\ell=1}^{K_T} \\beta_{m,\\ell} A_{\\ell}^{(T)}\\ \\ (M = 1)\n\\end{aligned}\n\\]\n\n\\(B \\in \\R^{M\\times K_T}\\).\n\\(M=1\\) for regression\n\n\\(\\mathbf{W}_t \\in \\R^{K_2\\times K_1}\\) \\(t=1,\\ldots,T\\)"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#training-neural-networks.-first-choices",
    "href": "schedule/slides/22-nnets-estimation.html#training-neural-networks.-first-choices",
    "title": "UBC Stat406 2024W",
    "section": "Training neural networks. First, choices",
    "text": "Training neural networks. First, choices\n\nChoose the architecture: how many layers, units per layer, what connections?\nChoose the loss: common choices (for each data point \\(i\\))\n\n\nRegression\n\n\\(\\hat{R}_i = \\frac{1}{2}(y_i - \\hat{y}_i)^2\\) (the 1/2 just makes the derivative nice)\n\nClassification\n\n\\(\\hat{R}_i = I(y_i = m)\\log( 1 + \\exp(-z_{im}))\\)\n\n\n\nChoose the activation function \\(g\\)"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#training-neural-networks-intuition",
    "href": "schedule/slides/22-nnets-estimation.html#training-neural-networks-intuition",
    "title": "UBC Stat406 2024W",
    "section": "Training neural networks (intuition)",
    "text": "Training neural networks (intuition)\n\nWe need to estimate \\(B\\), \\(\\mathbf{W}_t\\), \\(t=1,\\ldots,T\\)\nWe want to minimize \\(\\hat{R} = \\sum_{i=1}^n \\hat{R}_i\\) as a function of all this.\nWe use gradient descent, but in this dialect, we call it back propagation\n\n\n\nDerivatives via the chain rule: computed by a forward and backward sweep\nAll the \\(g(u)\\)’s that get used have \\(g'(u)\\) “nice”.\nIf \\(g\\) is ReLu:\n\n\\(g(u) = xI(x&gt;0)\\)\n\\(g'(u) = I(x&gt;0)\\)\n\n\n\nOnce we have derivatives from backprop,\n\\[\n\\begin{align}\n\\widetilde{B} &\\leftarrow B - \\gamma \\frac{\\partial \\widehat{R}}{\\partial B}\\\\\n\\widetilde{\\mathbf{W}_t} &\\leftarrow \\mathbf{W}_t - \\gamma \\frac{\\partial \\widehat{R}}{\\partial \\mathbf{W}_t}\n\\end{align}\n\\]"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#chain-rule",
    "href": "schedule/slides/22-nnets-estimation.html#chain-rule",
    "title": "UBC Stat406 2024W",
    "section": "Chain rule",
    "text": "Chain rule\nWe want \\(\\frac{\\partial}{\\partial B} \\hat{R}_i\\) and \\(\\frac{\\partial}{\\partial W_{t}}\\hat{R}_i\\) for all \\(t\\).\nRegression: \\(\\hat{R}_i = \\frac{1}{2}(y_i - \\hat{y}_i)^2\\)\n\\[\\begin{aligned}\n\\frac{\\partial\\hat{R}_i}{\\partial B} &= -(y_i - \\hat{y}_i)\\frac{\\partial \\hat{y_i}}{\\partial B} =\\underbrace{-(y_i - \\hat{y}_i)}_{-r_i}  \\mathbf{A}^{(T)}\\\\\n\\frac{\\partial}{\\partial \\mathbf{W}_T} \\hat{R}_i &= -(y_i - \\hat{y}_i)\\frac{\\partial\\hat{y_i}}{\\partial \\mathbf{W}_T} = -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\mathbf{A}^{(T)}} \\frac{\\partial \\mathbf{A}^{(T)}}{\\partial \\mathbf{W}_T}\\\\\n&= -\\left(r_i  B \\odot g'(\\mathbf{W}_T \\mathbf{A}^{(T)}) \\right) \\left(\\mathbf{A}^{(T-1)}\\right)^\\top\\\\\n\\frac{\\partial}{\\partial \\mathbf{W}_{T-1}} \\hat{R}_i &= -(y_i - \\hat{y}_i)\\frac{\\partial\\hat{y_i}}{\\partial \\mathbf{W}_{T-1}} = -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\mathbf{A}^{(T)}} \\frac{\\partial \\mathbf{A}^{(T)}}{\\partial \\mathbf{W}_{T-1}}\\\\\n&= -r_i \\frac{\\partial \\hat{y}_i}{\\partial \\mathbf{A}^{(T)}} \\frac{\\partial \\mathbf{A}^{(T)}}{\\partial \\mathbf{W}_{T}}\\frac{\\partial \\mathbf{W}_{T}}{\\partial \\mathbf{A}^{(T-1)}}\\frac{\\partial \\mathbf{A}^{(T-1)}}{\\partial \\mathbf{W}_{T-1}}\\\\\n\\cdots &= \\cdots\n\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#mapping-it-out",
    "href": "schedule/slides/22-nnets-estimation.html#mapping-it-out",
    "title": "UBC Stat406 2024W",
    "section": "Mapping it out",
    "text": "Mapping it out\nGiven current \\(\\mathbf{W}_t, B\\), we want to get new, \\(\\widetilde{\\mathbf{W}}_t,\\ \\widetilde B\\) for \\(t=1,\\ldots,T\\)\n\nSquared error for regression, cross-entropy for classification\n\n\n\nFeed forward \n\\[\\mathbf{A}^{(0)} = \\mathbf{X}  \\in \\R^{n\\times p}\\]\nRepeat, \\(t= 1,\\ldots, T\\)\n\n\\(\\mathbf{Z}_{t} = \\mathbf{A}^{(t-1)}\\mathbf{W}_t \\in \\R^{n\\times K_t}\\)\n\\(\\mathbf{A}^{(t)} = g(\\mathbf{Z}_{t})\\) (component wise)\n\\(\\dot{\\mathbf{A}}^{(t)} = g'(\\mathbf{Z}_t)\\)\n\n\\[\\begin{cases}\n\\hat{\\mathbf{y}} =\\mathbf{A}^{(T)} B \\in \\R^n \\\\\n\\hat{\\Pi} = \\left(1 + \\exp\\left(-\\mathbf{A}^{(T)}\\mathbf{B}\\right)\\right)^{-1} \\in \\R^{n \\times M}\\end{cases}\\]\n\n\nBack propogate \n\\[-r = \\begin{cases}\n-\\left(\\mathbf{y} - \\widehat{\\mathbf{y}}\\right) \\\\\n-\\left(1 - \\widehat{\\Pi}\\right)[y]\\end{cases}\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\mathbf{B}} \\widehat{R} &= \\left(\\mathbf{A}^{(T)}\\right)^\\top \\mathbf{r}\\\\\n-\\boldsymbol{\\Gamma} &\\leftarrow -\\mathbf{r}\\\\\n\\mathbf{W}_{T+1} &\\leftarrow \\mathbf{B}\n\\end{aligned}\n\\]\nRepeat, \\(t = T,...,1\\),\n\n\\(-\\boldsymbol{\\Gamma} \\leftarrow -\\left(\\boldsymbol{\\Gamma} \\mathbf{W}_{t+1}\\right) \\odot\\dot{\\mathbf{A}}^{(t)}\\)\n\\(\\frac{\\partial R}{\\partial \\mathbf{W}_t} = -\\left(\\mathbf{A}^{(t)}\\right)^\\top \\Gamma\\)"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#deep-nets",
    "href": "schedule/slides/22-nnets-estimation.html#deep-nets",
    "title": "UBC Stat406 2024W",
    "section": "Deep nets",
    "text": "Deep nets\nSome comments on adding layers:\n\nIt has been shown that one hidden layer is sufficient to approximate any bounded piecewise continuous function\nHowever, this may take a huge number of hidden units (i.e. \\(K_1 \\gg 1\\)).\nThis is what people mean when they say that NNets are “universal approximators”\nBy including multiple layers, we can have fewer hidden units per layer.\nAlso, we can encode (in)dependencies that can speed computations\nWe don’t have to connect everything the way we have been"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#simple-example",
    "href": "schedule/slides/22-nnets-estimation.html#simple-example",
    "title": "UBC Stat406 2024W",
    "section": "Simple example",
    "text": "Simple example\n\nn &lt;- 200\ndf &lt;- tibble(\n  x = seq(.05, 1, length = n),\n  y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function\n)\ntestdata &lt;- matrix(seq(.05, 1, length.out = 1e3), ncol = 1)\nlibrary(neuralnet)\nnn_out &lt;- neuralnet(y ~ x, data = df, hidden = c(10, 5, 15), threshold = 0.01, rep = 3)\nnn_preds &lt;- map(1:3, ~ compute(nn_out, testdata, .x)$net.result)\nyhat &lt;- nn_preds |&gt; bind_cols() |&gt; rowMeans() # average over the runs\n\n\n\nCode\n# This code will reproduce the analysis, takes some time\nset.seed(406406406)\nn &lt;- 200\ndf &lt;- tibble(\n  x = seq(.05, 1, length = n),\n  y = sin(1 / x) + rnorm(n, 0, .1) # Doppler function\n)\ntestx &lt;- matrix(seq(.05, 1, length.out = 1e3), ncol = 1)\nlibrary(neuralnet)\nlibrary(splines)\nfstar &lt;- sin(1 / testx)\nspline_test_err &lt;- function(k) {\n  fit &lt;- lm(y ~ bs(x, df = k), data = df)\n  yhat &lt;- predict(fit, newdata = tibble(x = testx))\n  mean((yhat - fstar)^2)\n}\nKs &lt;- 1:15 * 10\nSplineErr &lt;- map_dbl(Ks, ~ spline_test_err(.x))\n\nJgrid &lt;- c(5, 10, 15)\nNNerr &lt;- double(length(Jgrid)^3)\nNNplot &lt;- character(length(Jgrid)^3)\nsweep &lt;- 0\nfor (J1 in Jgrid) {\n  for (J2 in Jgrid) {\n    for (J3 in Jgrid) {\n      sweep &lt;- sweep + 1\n      NNplot[sweep] &lt;- paste(J1, J2, J3, sep = \" \")\n      nn_out &lt;- neuralnet(y ~ x, df,\n        hidden = c(J1, J2, J3),\n        threshold = 0.01, rep = 3\n      )\n      nn_results &lt;- sapply(1:3, function(x) {\n        compute(nn_out, testx, x)$net.result\n      })\n      # Run them through the neural network\n      Yhat &lt;- rowMeans(nn_results)\n      NNerr[sweep] &lt;- mean((Yhat - fstar)^2)\n    }\n  }\n}\n\nbestK &lt;- Ks[which.min(SplineErr)]\nbestspline &lt;- predict(lm(y ~ bs(x, bestK), data = df), newdata = tibble(x = testx))\nbesthidden &lt;- as.numeric(unlist(strsplit(NNplot[which.min(NNerr)], \" \")))\nnn_out &lt;- neuralnet(y ~ x, df, hidden = besthidden, threshold = 0.01, rep = 3)\nnn_results &lt;- sapply(1:3, function(x) compute(nn_out, testdata, x)$net.result)\n# Run them through the neural network\nbestnn &lt;- rowMeans(nn_results)\nplotd &lt;- data.frame(\n  x = testdata, spline = bestspline, nnet = bestnn, truth = fstar\n)\nsave.image(file = \"data/nnet-example.Rdata\")"
  },
  {
    "objectID": "schedule/slides/22-nnets-estimation.html#different-architectures",
    "href": "schedule/slides/22-nnets-estimation.html#different-architectures",
    "title": "UBC Stat406 2024W",
    "section": "Different architectures",
    "text": "Different architectures"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#section",
    "href": "schedule/slides/16-logistic-regression.html#section",
    "title": "UBC Stat406 2024W",
    "section": "16 Logistic regression",
    "text": "16 Logistic regression\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 25 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#last-time",
    "href": "schedule/slides/16-logistic-regression.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\n\nWe showed that with two classes, the Bayes’ classifier is\n\n\\[g_*(X) = \\begin{cases}\n1 & \\textrm{ if } \\frac{p_1(X)}{p_0(X)} &gt; \\frac{1-\\pi}{\\pi} \\\\\n0  &  \\textrm{ otherwise}\n\\end{cases}\\]\nwhere \\(p_1(X) = Pr(X \\given Y=1)\\) and \\(p_0(X) = Pr(X \\given Y=0)\\)\n\nWe then looked at what happens if we assume \\(Pr(X \\given Y=y)\\) is Normally distributed.\n\nWe then used this distribution and the class prior \\(\\pi\\) to find the posterior \\(Pr(Y=1 \\given X=x)\\)."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#direct-model",
    "href": "schedule/slides/16-logistic-regression.html#direct-model",
    "title": "UBC Stat406 2024W",
    "section": "Direct model",
    "text": "Direct model\nInstead, let’s directly model the posterior\n\\[\n\\begin{aligned}\nPr(Y = 1 \\given X=x)  & = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}} \\\\\nPr(Y = 0 | X=x) & = \\frac{1}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}=1-\\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\n\\end{aligned}\n\\]\nThis is logistic regression."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#why-this",
    "href": "schedule/slides/16-logistic-regression.html#why-this",
    "title": "UBC Stat406 2024W",
    "section": "Why this?",
    "text": "Why this?\n\\[Pr(Y = 1 \\given X=x) = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\\]\n\nThere are lots of ways to map \\(\\R \\mapsto [0,1]\\).\nThe “logistic” function \\(z\\mapsto (1 + \\exp(-z))^{-1} =  \\exp(z) / (1+\\exp(z)) =:h(z)\\) is nice.\nIt’s symmetric: \\(1 - h(z) = h(-z)\\)\nHas a nice derivative: \\(h'(z) = \\frac{\\exp(z)}{(1 + \\exp(z))^2} = h(z)(1-h(z))\\).\nIt’s the inverse of the “log-odds” (logit): \\(\\log(p / (1-p))\\)."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#another-linear-classifier",
    "href": "schedule/slides/16-logistic-regression.html#another-linear-classifier",
    "title": "UBC Stat406 2024W",
    "section": "Another linear classifier",
    "text": "Another linear classifier\nLike LDA, logistic regression is a linear classifier\nThe logit (i.e.: log odds) transformation gives a linear decision boundary \\[\\log\\left( \\frac{\\P(Y = 1 \\given X=x)}{\\P(Y = 0 \\given X=x) } \\right) = \\beta_0 + \\beta^{\\top} x\\] The decision boundary is the hyperplane \\(\\{x : \\beta_0 + \\beta^{\\top} x = 0\\}\\)\nIf the log-odds are below 0, classify as 0, above 0 classify as a 1."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#logistic-regression-is-also-easy-in-r",
    "href": "schedule/slides/16-logistic-regression.html#logistic-regression-is-also-easy-in-r",
    "title": "UBC Stat406 2024W",
    "section": "Logistic regression is also easy in R",
    "text": "Logistic regression is also easy in R\n\nlogistic &lt;- glm(y ~ ., dat, family = \"binomial\")\n\nOr we can use lasso or ridge regression or a GAM as before\n\nlasso_logit &lt;- cv.glmnet(x, y, family = \"binomial\")\nridge_logit &lt;- cv.glmnet(x, y, alpha = 0, family = \"binomial\")\ngam_logit &lt;- gam(y ~ s(x), data = dat, family = \"binomial\")\n\n\n\nglm means generalized linear model"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#baby-example-continued-from-last-time",
    "href": "schedule/slides/16-logistic-regression.html#baby-example-continued-from-last-time",
    "title": "UBC Stat406 2024W",
    "section": "Baby example (continued from last time)",
    "text": "Baby example (continued from last time)\n\ndat1 &lt;- generate_lda_2d(100, Sigma = .5 * diag(2))\nlogit &lt;- glm(y ~ ., dat1 |&gt; mutate(y = y - 1), family = \"binomial\")\nsummary(logit)\n\n\nCall:\nglm(formula = y ~ ., family = \"binomial\", data = mutate(dat1, \n    y = y - 1))\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.6649     0.6281  -4.243 2.21e-05 ***\nx1            2.5305     0.5995   4.221 2.43e-05 ***\nx2            1.6610     0.4365   3.805 0.000142 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.469  on 99  degrees of freedom\nResidual deviance:  68.681  on 97  degrees of freedom\nAIC: 74.681\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#visualizing-the-classification-boundary",
    "href": "schedule/slides/16-logistic-regression.html#visualizing-the-classification-boundary",
    "title": "UBC Stat406 2024W",
    "section": "Visualizing the classification boundary",
    "text": "Visualizing the classification boundary\n\n\nCode\ngr &lt;- expand_grid(x1 = seq(-2.5, 3, length.out = 100), \n                  x2 = seq(-2.5, 3, length.out = 100))\npts &lt;- predict(logit, gr)\ng0 &lt;- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_steps2(n.breaks = 6, name = \"log odds\") \ng0"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#calculation",
    "href": "schedule/slides/16-logistic-regression.html#calculation",
    "title": "UBC Stat406 2024W",
    "section": "Calculation",
    "text": "Calculation\nWhile the R formula for logistic regression is straightforward, it’s not as easy to compute as OLS or LDA or QDA.\nLogistic regression for two classes simplifies to a likelihood:\nWrite \\(p_i(\\beta) = \\P(Y_i = 1 | X = x_i,\\beta)\\)\n\n\\(P(Y_i = y_i \\given X = x_i, \\beta) = p_i^{y_i}(1-p_i)^{1-y_i}\\) (…Bernoulli distribution)\n\\(P(\\mathbf{Y} \\given \\mathbf{X}, \\beta) = \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}\\)."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#calculation-1",
    "href": "schedule/slides/16-logistic-regression.html#calculation-1",
    "title": "UBC Stat406 2024W",
    "section": "Calculation",
    "text": "Calculation\nWrite \\(p_i(\\beta) = \\P(Y_i = 1 | X = x_i,\\beta)\\)\n\\[\n\\begin{aligned}\n\\ell(\\beta)\n& = \\log \\left( \\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i} \\right)\\\\\n&=\\sum_{i=1}^n \\left( y_i\\log(p_i(\\beta)) + (1-y_i)\\log(1-p_i(\\beta))\\right) \\\\\n& =\n\\sum_{i=1}^n \\left( y_i\\log(e^{\\beta^{\\top}x_i}/(1+e^{\\beta^{\\top}x_i})) - (1-y_i)\\log(1+e^{\\beta^{\\top}x_i})\\right) \\\\\n& =\n\\sum_{i=1}^n \\left( y_i\\beta^{\\top}x_i -\\log(1 + e^{\\beta^{\\top} x_i})\\right)\n\\end{aligned}\n\\]\nThis gets optimized via Newton-Raphson updates and iteratively reweighed least squares."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#irwls-for-logistic-regression-skip-for-now",
    "href": "schedule/slides/16-logistic-regression.html#irwls-for-logistic-regression-skip-for-now",
    "title": "UBC Stat406 2024W",
    "section": "IRWLS for logistic regression (skip for now)",
    "text": "IRWLS for logistic regression (skip for now)\n(This is preparation for Neural Networks.)\n\nlogit_irwls &lt;- function(y, x, maxit = 100, tol = 1e-6) {\n  p &lt;- ncol(x)\n  beta &lt;- double(p) # initialize coefficients\n  beta0 &lt;- 0\n  conv &lt;- FALSE # hasn't converged\n  iter &lt;- 1 # first iteration\n  while (!conv && (iter &lt; maxit)) { # check loops\n    iter &lt;- iter + 1 # update first thing (so as not to forget)\n    eta &lt;- beta0 + x %*% beta\n    mu &lt;- 1 / (1 + exp(-eta))\n    gp &lt;- 1 / (mu * (1 - mu)) # inverse of derivative of logistic\n    z &lt;- eta + (y - mu) * gp # effective transformed response\n    beta_new &lt;- coef(lm(z ~ x, weights = 1 / gp)) # do Weighted Least Squares\n    conv &lt;- mean(abs(c(beta0, beta) - betaNew)) &lt; tol # check if the betas are \"moving\"\n    beta0 &lt;- betaNew[1] # update betas\n    beta &lt;- betaNew[-1]\n  }\n  return(c(beta0, beta))\n}"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#comparing-lda-and-logistic-regression",
    "href": "schedule/slides/16-logistic-regression.html#comparing-lda-and-logistic-regression",
    "title": "UBC Stat406 2024W",
    "section": "Comparing LDA and Logistic regression",
    "text": "Comparing LDA and Logistic regression\nBoth decision boundaries are linear in \\(x\\):\n\nLDA \\(\\longrightarrow \\alpha_0 + \\alpha_1^\\top x\\)\nLogit \\(\\longrightarrow \\beta_0 + \\beta_1^\\top x\\).\n\nBut the parameters are estimated differently."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#comparing-lda-and-logistic-regression-1",
    "href": "schedule/slides/16-logistic-regression.html#comparing-lda-and-logistic-regression-1",
    "title": "UBC Stat406 2024W",
    "section": "Comparing LDA and Logistic regression",
    "text": "Comparing LDA and Logistic regression\nExamine the joint distribution of \\((X,\\ Y)\\) (not the posterior):\n\nLDA: \\(f(X_i,\\ Y_i) = \\underbrace{ f(X_i \\given Y_i)}_{\\textrm{Gaussian}}\\underbrace{ f(Y_i)}_{\\textrm{Bernoulli}}\\)\nLogistic Regression: \\(f(X_i,Y_i) = \\underbrace{ f(Y_i\\given X_i)}_{\\textrm{Logistic}}\\underbrace{ f(X_i)}_{\\textrm{Ignored}}\\)\nLDA estimates the joint, but Logistic estimates only the conditional (posterior) distribution. But this is really all we need.\nSo logistic requires fewer assumptions.\nBut if the two classes are perfectly separable, logistic crashes (and the MLE is undefined, too many solutions)\nLDA “works” even if the conditional isn’t normal, but works very poorly if any X is qualitative"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#comparing-with-qda-2-classes",
    "href": "schedule/slides/16-logistic-regression.html#comparing-with-qda-2-classes",
    "title": "UBC Stat406 2024W",
    "section": "Comparing with QDA (2 classes)",
    "text": "Comparing with QDA (2 classes)\n\nRecall: this gives a “quadratic” decision boundary (it’s a curve).\nIf we have \\(p\\) columns in \\(X\\)\n\nLogistic estimates \\(p+1\\) parameters\nLDA estimates \\(2p + p(p+1)/2 + 1\\)\nQDA estimates \\(2p + p(p+1) + 1\\)\n\nIf \\(p=50\\),\n\nLogistic: 51\nLDA: 1376\nQDA: 2651\n\nQDA doesn’t get used much: there are better nonlinear versions with way fewer parameters"
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#bad-parameter-counting",
    "href": "schedule/slides/16-logistic-regression.html#bad-parameter-counting",
    "title": "UBC Stat406 2024W",
    "section": "Bad parameter counting",
    "text": "Bad parameter counting\nI’ve motivated LDA as needing \\(\\Sigma\\), \\(\\pi\\) and \\(\\mu_0\\), \\(\\mu_1\\)\nIn fact, we don’t need all of this to get the decision boundary.\nSo the “degrees of freedom” is much lower if we only want the classes and not the probabilities.\nThe decision boundary only really depends on\n\n\\(\\Sigma^{-1}(\\mu_1-\\mu_0)\\)\n\\((\\mu_1+\\mu_0)\\),\nso appropriate algorithms estimate \\(&lt;2p\\) parameters."
  },
  {
    "objectID": "schedule/slides/16-logistic-regression.html#note-again",
    "href": "schedule/slides/16-logistic-regression.html#note-again",
    "title": "UBC Stat406 2024W",
    "section": "Note again:",
    "text": "Note again:\nwhile logistic regression and LDA produce linear decision boundaries, they are not linear smoothers\nAIC/BIC/Cp work if you use the likelihood correctly and count degrees-of-freedom correctly\nMust people use either test set or CV"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#section",
    "href": "schedule/slides/20-boosting.html#section",
    "title": "UBC Stat406 2024W",
    "section": "20 Boosting",
    "text": "20 Boosting\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 02 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#last-time",
    "href": "schedule/slides/20-boosting.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\nWe learned about bagging, for averaging low-bias / high-variance estimators.\nToday, we examine it’s opposite: Boosting.\nBoosting also combines estimators, but it combines high-bias / low-variance estimators.\nBoosting has a number of flavours. And if you Google descriptions, most are wrong.\nFor a deep (and accurate) treatment, see [ESL] Chapter 10\n\nWe’ll discuss 2 flavours: AdaBoost and Gradient Boosting\nNeither requires a tree, but that’s the typical usage.\nBoosting needs a “weak learner”, so small trees (stumps) are natural."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#adaboost-intuition-for-classification",
    "href": "schedule/slides/20-boosting.html#adaboost-intuition-for-classification",
    "title": "UBC Stat406 2024W",
    "section": "AdaBoost intuition (for classification)",
    "text": "AdaBoost intuition (for classification)\nAt each iteration, we weight the observations.\nObservations that are currently misclassified, get higher weights.\nSo on the next iteration, we’ll try harder to correctly classify our mistakes.\nThe number of iterations must be chosen."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#adaboost-freund-and-schapire-generic",
    "href": "schedule/slides/20-boosting.html#adaboost-freund-and-schapire-generic",
    "title": "UBC Stat406 2024W",
    "section": "AdaBoost (Freund and Schapire, generic)",
    "text": "AdaBoost (Freund and Schapire, generic)\nLet \\(G(x, \\theta)\\) be any weak learner\n⛭ imagine a tree with one split: then \\(\\theta=\\) (feature, split point)\nAlgorithm (AdaBoost) 🛠️\n\nSet observation weights \\(w_i=1/n\\).\nUntil we quit ( \\(m&lt;M\\) iterations )\n\nEstimate the classifier \\(G(x,\\theta_m)\\) using weights \\(w_i\\)\nCalculate it’s weighted error \\(\\textrm{err}_m = \\sum_{i=1}^n w_i I(y_i \\neq G(x_i, \\theta_m)) / \\sum w_i\\)\nSet \\(\\alpha_m = \\log((1-\\textrm{err}_m)/\\text{err}_m)\\)\nUpdate \\(w_i \\leftarrow w_i \\exp(\\alpha_m I(y_i \\neq G(x_i,\\theta_m)))\\)\n\nFinal classifier is \\(G(x) = \\textrm{sign}\\left( \\sum_{m=1}^M \\alpha_m G(x, \\theta_m)\\right)\\)"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#using-mobility-data-again",
    "href": "schedule/slides/20-boosting.html#using-mobility-data-again",
    "title": "UBC Stat406 2024W",
    "section": "Using mobility data again",
    "text": "Using mobility data again\n\n\nCode\nlibrary(kableExtra)\nlibrary(randomForest)\nmob &lt;- Stat406::mobility |&gt;\n  mutate(mobile = as.factor(Mobility &gt; .1)) |&gt;\n  select(-ID, -Name, -Mobility, -State) |&gt;\n  drop_na()\nn &lt;- nrow(mob)\ntrainidx &lt;- sample.int(n, floor(n * .75))\ntestidx &lt;- setdiff(1:n, trainidx)\ntrain &lt;- mob[trainidx, ]\ntest &lt;- mob[testidx, ]\nrf &lt;- randomForest(mobile ~ ., data = train)\nbag &lt;- randomForest(mobile ~ ., data = train, mtry = ncol(mob) - 1)\npreds &lt;- tibble(truth = test$mobile, rf = predict(rf, test), bag = predict(bag, test))\n\n\n\n\nlibrary(gbm)\ntrain_boost &lt;- train |&gt;\n  mutate(mobile = as.integer(mobile) - 1)\n# needs {0, 1} responses\ntest_boost &lt;- test |&gt;\n  mutate(mobile = as.integer(mobile) - 1)\nadab &lt;- gbm(\n  mobile ~ .,\n  data = train_boost,\n  n.trees = 500,\n  distribution = \"adaboost\"\n)\npreds$adab &lt;- as.numeric(\n  predict(adab, test_boost) &gt; 0\n)\npar(mar = c(5, 11, 0, 1))\ns &lt;- summary(adab, las = 1)"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#forward-stagewise-additive-modeling-fsam-completely-generic",
    "href": "schedule/slides/20-boosting.html#forward-stagewise-additive-modeling-fsam-completely-generic",
    "title": "UBC Stat406 2024W",
    "section": "Forward stagewise additive modeling (FSAM, completely generic)",
    "text": "Forward stagewise additive modeling (FSAM, completely generic)\nAlgorithm 🛠️\n\nSet initial predictor \\(f_0(x)=0\\)\nUntil we quit ( \\(m&lt;M\\) iterations )\n\nCompute \\((\\beta_m, \\theta_m) = \\argmin_{\\beta, \\theta} \\sum_{i=1}^n L\\left(y_i,\\ f_{m-1}(x_i) + \\beta G(x_i,\\ \\theta)\\right)\\)\nSet \\(f_m(x) = f_{m-1}(x) + \\beta_m G(x,\\ \\theta_m)\\)\n\nFinal classifier is \\(G(x, \\theta_M) = \\textrm{sign}\\left( f_M(x) \\right)\\)\n\nHere, \\(L\\) is a loss function that measures prediction accuracy\n\n\nIf (1) \\(L(y,\\ f(x))= \\exp(-y f(x))\\), (2) \\(G\\) is a classifier, and WLOG \\(y \\in \\{-1, 1\\}\\)\n\nFSAM is equivalent to AdaBoost. Proven 5 years later (Friedman, Hastie, and Tibshirani 2000)."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#so-what",
    "href": "schedule/slides/20-boosting.html#so-what",
    "title": "UBC Stat406 2024W",
    "section": "So what?",
    "text": "So what?\nIt turns out that “exponential loss” \\(L(y,\\ f(x))= \\exp(-y f(x))\\) is not very robust.\nHere are some other loss functions for 2-class classification\n\n\nWant losses which penalize negative margin, but not positive margins.\nRobust means don’t over-penalize large negatives"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#gradient-boosting",
    "href": "schedule/slides/20-boosting.html#gradient-boosting",
    "title": "UBC Stat406 2024W",
    "section": "Gradient boosting",
    "text": "Gradient boosting\nIn the forward stagewise algorithm, we solved a minimization and then made an update:\n\\[f_m(x) = f_{m-1}(x) + \\beta_m G(x, \\theta_m)\\]\nFor most loss functions \\(L\\) / procedures \\(G\\) this optimization is difficult: \\[\\argmin_{\\beta, \\theta} \\sum_{i=1}^n L\\left(y_i,\\ f_{m-1}(x_i) + \\beta G(x_i, \\theta)\\right)\\]\n💡 Just take one gradient step toward the minimum 💡\n\\[f_m(x) = f_{m-1}(x) -\\gamma_m \\nabla L(y,f_{m-1}(x)) = f_{m-1}(x) +\\gamma_m \\left(-\\nabla L(y,f_{m-1}(x))\\right)\\]\nThis is called Gradient boosting\nNotice how similar the update steps look."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#gradient-boosting-1",
    "href": "schedule/slides/20-boosting.html#gradient-boosting-1",
    "title": "UBC Stat406 2024W",
    "section": "Gradient boosting",
    "text": "Gradient boosting\n\\[f_m(x) = f_{m-1}(x) -\\gamma_m \\nabla L(y,f_{m-1}(x)) = f_{m-1}(x) +\\gamma_m \\left(-\\nabla L(y,f_{m-1}(x))\\right)\\]\nGradient boosting goes only part of the way toward the minimum at each \\(m\\).\nThis has two advantages:\n\nSince we’re not fitting \\(\\beta, \\theta\\) to the data as “hard”, the learner is weaker.\nThis procedure is computationally much simpler.\n\nSimpler because we only require the gradient at one value, don’t have to fully optimize."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#gradient-boosting-algorithm",
    "href": "schedule/slides/20-boosting.html#gradient-boosting-algorithm",
    "title": "UBC Stat406 2024W",
    "section": "Gradient boosting – Algorithm 🛠️",
    "text": "Gradient boosting – Algorithm 🛠️\n\nSet initial predictor \\(f_0(x)=\\overline{\\y}\\)\nUntil we quit ( \\(m&lt;M\\) iterations )\n\nCompute pseudo-residuals (what is the gradient of \\(L(y,f)=(y-f(x))^2\\)?) \\[r_i = -\\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)}\\bigg|_{f(x_i)=f_{m-1}(x_i)}\\]\nEstimate weak learner, \\(G(x, \\theta_m)\\), with the training set \\(\\{r_i, x_i\\}\\).\nFind the step size \\(\\gamma_m = \\argmin_\\gamma \\sum_{i=1}^n L(y_i, f_{m-1}(x_i) + \\gamma G(x_i, \\theta_m))\\)\nSet \\(f_m(x) = f_{m-1}(x) + \\gamma_m G(x, \\theta_m)\\)\n\nFinal predictor is \\(f_M(x)\\)."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#gradient-boosting-modifications",
    "href": "schedule/slides/20-boosting.html#gradient-boosting-modifications",
    "title": "UBC Stat406 2024W",
    "section": "Gradient boosting modifications",
    "text": "Gradient boosting modifications\n\ngrad_boost &lt;- gbm(mobile ~ ., data = train_boost, n.trees = 500, distribution = \"bernoulli\")\n\n\nTypically done with “small” trees, not stumps because of the gradient. You can specify the size. Usually 4-8 terminal nodes is recommended (more gives more interactions between predictors)\nUsually modify the gradient step to \\(f_m(x) = f_{m-1}(x) + \\gamma_m \\alpha G(x,\\theta_m)\\) with \\(0&lt;\\alpha&lt;1\\). Helps to keep from fitting too hard.\nOften combined with Bagging so that each step is fit using a bootstrap resample of the data. Gives us out-of-bag options.\nThere are many other extensions, notably XGBoost."
  },
  {
    "objectID": "schedule/slides/20-boosting.html#results-for-mobility",
    "href": "schedule/slides/20-boosting.html#results-for-mobility",
    "title": "UBC Stat406 2024W",
    "section": "Results for mobility",
    "text": "Results for mobility\n\n\nCode\nlibrary(cowplot)\nboost_preds &lt;- tibble(\n  adaboost = predict(adab, test_boost),\n  gbm = predict(grad_boost, test_boost),\n  truth = test$mobile\n)\ng1 &lt;- ggplot(boost_preds, aes(adaboost, gbm, color = as.factor(truth))) +\n  geom_text(aes(label = as.integer(truth) - 1)) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  xlab(\"adaboost margin\") +\n  ylab(\"gbm margin\") +\n  theme(legend.position = \"none\") +\n  scale_color_manual(values = c(\"orange\", \"blue\")) +\n  annotate(\"text\",\n    x = -4, y = 5, color = red,\n    label = paste(\n      \"gbm error\\n\",\n      round(with(boost_preds, mean((gbm &gt; 0) != truth)), 2)\n    )\n  ) +\n  annotate(\"text\",\n    x = 4, y = -5, color = red,\n    label = paste(\"adaboost error\\n\", round(with(boost_preds, mean((adaboost &gt; 0) != truth)), 2))\n  )\nboost_oob &lt;- tibble(\n  adaboost = adab$oobag.improve, gbm = grad_boost$oobag.improve,\n  ntrees = 1:500\n)\ng2 &lt;- boost_oob %&gt;%\n  pivot_longer(-ntrees, values_to = \"OOB_Error\") %&gt;%\n  ggplot(aes(x = ntrees, y = OOB_Error, color = name)) +\n  geom_line() +\n  scale_color_manual(values = c(orange, blue)) +\n  theme(legend.title = element_blank())\nplot_grid(g1, g2, rel_widths = c(.4, .6))"
  },
  {
    "objectID": "schedule/slides/20-boosting.html#major-takeaways",
    "href": "schedule/slides/20-boosting.html#major-takeaways",
    "title": "UBC Stat406 2024W",
    "section": "Major takeaways",
    "text": "Major takeaways\n\nTwo flavours of Boosting\n\nAdaBoost (the original) and\ngradient boosting (easier and more computationally friendly)\n\nThe connection is “Forward stagewise additive modelling” (AdaBoost is a special case)\nThe connection reveals that AdaBoost “isn’t robust because it uses exponential loss” (squared error is even worse)\nGradient boosting is a computationally easier version of FSAM\nAll use weak learners (compare to Bagging)\nThink about the Bias-Variance implications\nYou can use these for regression or classification\nYou can do this with other weak learners besides trees."
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#section",
    "href": "schedule/slides/00-classification-losses.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Evaluating classifiers",
    "text": "00 Evaluating classifiers\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 16 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#how-do-we-measure-accuracy",
    "href": "schedule/slides/00-classification-losses.html#how-do-we-measure-accuracy",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure accuracy?",
    "text": "How do we measure accuracy?\nSo far — 0-1 loss. If correct class, lose 0 else lose 1.\nAsymmetric classification loss — If correct class, lose 0 else lose something.\nFor example, consider facial recognition. Goal is “person OK”, “person has expired passport”, “person is a known terrorist”\n\nIf classify OK, but was terrorist, lose 1,000,000\nIf classify OK, but expired passport, lose 2\nIf classify terrorist, but was OK, lose 100\nIf classify terrorist, but was expired passport, lose 10\netc.\n\n\nResults in a 3x3 matrix of losses with 0 on the diagonal.\n\n\n        [,1]  [,2] [,3]\n[1,]       0     2   30\n[2,]      10     0  100\n[3,] 1000000 50000    0"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#deviance-loss",
    "href": "schedule/slides/00-classification-losses.html#deviance-loss",
    "title": "UBC Stat406 2024W",
    "section": "Deviance loss",
    "text": "Deviance loss\nSometimes we output probabilities as well as class labels.\nFor example, logistic regression returns the probability that an observation is in class 1. \\(P(Y_i = 1 \\given x_i) = 1 / (1 + \\exp\\{-x'_i \\hat\\beta\\})\\)\nLDA and QDA produce probabilities as well. So do Neural Networks (typically)\n(Trees “don’t”, neither does KNN, though you could fake it)\n\n\n\nDeviance loss for 2-class classification is \\(-2\\textrm{loglikelihood}(y, \\hat{p}) = -2 (y_i x'_i\\hat{\\beta} - \\log (1-\\hat{p}))\\)\n\n(Technically, it’s the difference between this and the loss of the null model, but people play fast and loose)\n\nCould also use cross entropy or Gini index."
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#calibration",
    "href": "schedule/slides/00-classification-losses.html#calibration",
    "title": "UBC Stat406 2024W",
    "section": "Calibration",
    "text": "Calibration\nSuppose we predict some probabilities for our data, how often do those events happen?\nIn principle, if we predict \\(\\hat{p}(x_i)=0.2\\) for a bunch of events observations \\(i\\), we’d like to see about 20% 1 and 80% 0. (In training set and test set)\nThe same goes for the other probabilities. If we say “20% chance of rain” it should rain 20% of such days.\nOf course, we didn’t predict exactly \\(\\hat{p}(x_i)=0.2\\) ever, so lets look at \\([.15, .25]\\).\n\nn &lt;- 250\ndat &lt;- tibble(\n  x = seq(-5, 5, length.out = n),\n  p = 1 / (1 + exp(-x)),\n  y = rbinom(n, 1, p)\n)\nfit &lt;- glm(y ~ x, family = binomial, data = dat)\ndat$phat &lt;- predict(fit, type = \"response\") # predicted probabilities\ndat |&gt;\n  filter(phat &gt; .15, phat &lt; .25) |&gt;\n  summarize(target = .2, obs = mean(y))\n\n\n\n# A tibble: 1 × 2\n  target   obs\n   &lt;dbl&gt; &lt;dbl&gt;\n1    0.2 0.222"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#calibration-plot",
    "href": "schedule/slides/00-classification-losses.html#calibration-plot",
    "title": "UBC Stat406 2024W",
    "section": "Calibration plot",
    "text": "Calibration plot\n\nbinary_calibration_plot &lt;- function(y, phat, nbreaks = 10) {\n  dat &lt;- tibble(y = y, phat = phat) |&gt;\n    mutate(bins = cut_number(phat, n = nbreaks))\n  midpts &lt;- quantile(dat$phat, seq(0, 1, length.out = nbreaks + 1), na.rm = TRUE)\n  midpts &lt;- midpts[-length(midpts)] + diff(midpts) / 2\n  sum_dat &lt;- dat |&gt;\n    group_by(bins) |&gt;\n    summarise(\n      p = mean(y, na.rm = TRUE),\n      se = sqrt(p * (1 - p) / n())\n    ) |&gt;\n    arrange(p)\n  sum_dat$x &lt;- midpts\n\n  ggplot(sum_dat, aes(x = x)) +\n    geom_errorbar(aes(ymin = pmax(p - 1.96 * se, 0), ymax = pmin(p + 1.96 * se, 1))) +\n    geom_point(aes(y = p), colour = blue) +\n    geom_abline(slope = 1, intercept = 0, colour = orange) +\n    ylab(\"observed frequency\") +\n    xlab(\"average predicted probability\") +\n    coord_cartesian(xlim = c(0, 1), ylim = c(0, 1)) +\n    geom_rug(data = dat, aes(x = phat), sides = \"b\")\n}"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#amazingly-well-calibrated",
    "href": "schedule/slides/00-classification-losses.html#amazingly-well-calibrated",
    "title": "UBC Stat406 2024W",
    "section": "Amazingly well-calibrated",
    "text": "Amazingly well-calibrated\n\nbinary_calibration_plot(dat$y, dat$phat, 20L)"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#less-well-calibrated",
    "href": "schedule/slides/00-classification-losses.html#less-well-calibrated",
    "title": "UBC Stat406 2024W",
    "section": "Less well-calibrated",
    "text": "Less well-calibrated"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#true-positive-false-negative-sensitivity-specificity",
    "href": "schedule/slides/00-classification-losses.html#true-positive-false-negative-sensitivity-specificity",
    "title": "UBC Stat406 2024W",
    "section": "True positive, false negative, sensitivity, specificity",
    "text": "True positive, false negative, sensitivity, specificity\n\nTrue positive rate\n\n# correct predict positive / # actual positive (1 - FNR)\n\nFalse negative rate\n\n# incorrect predict negative / # actual positive (1 - TPR), Type II Error\n\nTrue negative rate\n\n# correct predict negative / # actual negative\n\nFalse positive rate\n\n# incorrect predict positive / # actual negative (1 - TNR), Type I Error\n\nSensitivity\n\nTPR, 1 - Type II error\n\nSpecificity\n\nTNR, 1 - Type I error"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#roc-and-thresholds",
    "href": "schedule/slides/00-classification-losses.html#roc-and-thresholds",
    "title": "UBC Stat406 2024W",
    "section": "ROC and thresholds",
    "text": "ROC and thresholds\n\nROC (Receiver Operating Characteristic) Curve\n\nTPR (sensitivity) vs. FPR (1 - specificity)\n\nAUC (Area under the curve)\n\nIntegral of ROC. Closer to 1 is better.\n\n\nSo far, we’ve been thresholding at 0.5, though you shouldn’t always do that.\nWith unbalanced data (say 10% 0 and 90% 1), if you care equally about predicting both classes, you might want to choose a different cutoff (like in LDA).\nTo make the ROC we look at our errors as we vary the cutoff"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#roc-curve",
    "href": "schedule/slides/00-classification-losses.html#roc-curve",
    "title": "UBC Stat406 2024W",
    "section": "ROC curve",
    "text": "ROC curve\n\n\nroc &lt;- function(prediction, y) {\n  op &lt;- order(prediction, decreasing = TRUE)\n  preds &lt;- prediction[op]\n  y &lt;- y[op]\n  noty &lt;- 1 - y\n  if (any(duplicated(preds))) {\n    y &lt;- rev(tapply(y, preds, sum))\n    noty &lt;- rev(tapply(noty, preds, sum))\n  }\n  tibble(\n    FPR = cumsum(noty) / sum(noty),\n    TPR = cumsum(y) / sum(y)\n  )\n}\n\nggplot(roc(dat$phat, dat$y), aes(FPR, TPR)) +\n  geom_step(colour = blue, size = 2) +\n  geom_abline(slope = 1, intercept = 0)"
  },
  {
    "objectID": "schedule/slides/00-classification-losses.html#other-stuff",
    "href": "schedule/slides/00-classification-losses.html#other-stuff",
    "title": "UBC Stat406 2024W",
    "section": "Other stuff",
    "text": "Other stuff\n\n\nSource: worth exploring Wikipedia\n\n\n\n\nUBC Stat 406 - 2024"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section",
    "href": "schedule/slides/10-basis-expansions.html#section",
    "title": "UBC Stat406 2024W",
    "section": "10 Basis expansions",
    "text": "10 Basis expansions\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 27 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#what-about-nonlinear-things",
    "href": "schedule/slides/10-basis-expansions.html#what-about-nonlinear-things",
    "title": "UBC Stat406 2024W",
    "section": "What about nonlinear things",
    "text": "What about nonlinear things\n\\[\\Expect{Y \\given X=x} = \\sum_{j=1}^p x_j\\beta_j\\]\nNow we relax this assumption of linearity:\n\\[\\Expect{Y \\given X=x} = f(x)\\]\nHow do we estimate \\(f\\)?\n\nFor this lecture, we use \\(x \\in \\R\\) (1 dimensional)\nHigher dimensions are possible, but complexity grows exponentially.\nWe’ll see some special techniques for \\(x\\in\\R^p\\) later this Module."
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#start-simple",
    "href": "schedule/slides/10-basis-expansions.html#start-simple",
    "title": "UBC Stat406 2024W",
    "section": "Start simple",
    "text": "Start simple\nFor any \\(f : \\R \\rightarrow [0,1]\\)\n\\[f(x) = f(x_0) + f'(x_0)(x-x_0) + \\frac{1}{2}f''(x_0)(x-x_0)^2 + \\frac{1}{3!}f'''(x_0)(x-x_0)^3 + R_3(x-x_0)\\]\nSo we can linearly regress \\(y_i = f(x_i)\\) on the polynomials.\nThe more terms we use, the smaller \\(R\\).\n\n\nCode\nset.seed(406406)\ndata(arcuate, package = \"Stat406\") \narcuate &lt;- arcuate |&gt; slice_sample(n = 220)\narcuate %&gt;% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) +\n  geom_smooth(color = orange, formula = y ~ poly(x, 3), method = \"lm\", se = FALSE)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#same-thing-different-orders",
    "href": "schedule/slides/10-basis-expansions.html#same-thing-different-orders",
    "title": "UBC Stat406 2024W",
    "section": "Same thing, different orders",
    "text": "Same thing, different orders\n\n\nCode\narcuate %&gt;% \n  ggplot(aes(position, fa)) + \n  geom_point(color = blue) + \n  geom_smooth(aes(color = \"a\"), formula = y ~ poly(x, 4), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"b\"), formula = y ~ poly(x, 7), method = \"lm\", se = FALSE) +\n  geom_smooth(aes(color = \"c\"), formula = y ~ poly(x, 25), method = \"lm\", se = FALSE) +\n  scale_color_manual(name = \"Taylor order\",\n    values = c(green, red, orange), labels = c(\"4 terms\", \"7 terms\", \"25 terms\"))"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#still-a-linear-smoother",
    "href": "schedule/slides/10-basis-expansions.html#still-a-linear-smoother",
    "title": "UBC Stat406 2024W",
    "section": "Still a “linear smoother”",
    "text": "Still a “linear smoother”\nReally, this is still linear regression, just in a transformed space.\nIt’s not linear in \\(x\\), but it is linear in \\((x,x^2,x^3)\\) (for the 3rd-order case)\nSo, we’re still doing OLS with\n\\[\\X=\\begin{bmatrix}1& x_1 & x_1^2 & x_1^3 \\\\ \\vdots&&&\\vdots\\\\1& x_n & x_n^2 & x_n^3\\end{bmatrix}\\]\nSo we can still use our nice formulas for LOO-CV, GCV, Cp, AIC, etc.\n\nmax_deg &lt;- 20\ncv_nice &lt;- function(mdl) mean( residuals(mdl)^2 / (1 - hatvalues(mdl))^2 ) \ncvscores &lt;- map_dbl(seq_len(max_deg), ~ cv_nice(lm(fa ~ poly(position, .), data = arcuate)))"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section-1",
    "href": "schedule/slides/10-basis-expansions.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Code\nlibrary(cowplot)\ng1 &lt;- ggplot(tibble(cvscores, degrees = seq(max_deg)), aes(degrees, cvscores)) +\n  geom_point(colour = blue) +\n  geom_line(colour = blue) + \n  labs(ylab = 'LOO-CV', xlab = 'polynomial degree') +\n  geom_vline(xintercept = which.min(cvscores), linetype = \"dotted\") \ng2 &lt;- ggplot(arcuate, aes(position, fa)) + \n  geom_point(colour = blue) + \n  geom_smooth(\n    colour = orange, \n    formula = y ~ poly(x, which.min(cvscores)), \n    method = \"lm\", \n    se = FALSE\n  )\nplot_grid(g1, g2, ncol = 2)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#other-bases",
    "href": "schedule/slides/10-basis-expansions.html#other-bases",
    "title": "UBC Stat406 2024W",
    "section": "Other bases",
    "text": "Other bases\n\nPolynomials\n\n\\(x \\mapsto \\left(1,\\ x,\\ x^2, \\ldots, x^p\\right)\\) (technically, not quite this, they are orthogonalized)\n\nLinear splines\n\n\\(x \\mapsto \\bigg(1,\\ x,\\ (x-k_1)_+,\\ (x-k_2)_+,\\ldots, (x-k_p)_+\\bigg)\\) for some choices \\(\\{k_1,\\ldots,k_p\\}\\)\n\nCubic splines\n\n\\(x \\mapsto \\bigg(1,\\ x,\\ x^2,\\ x^3,\\ (x-k_1)^3_+,\\ (x-k_2)^3_+,\\ldots, (x-k_p)^3_+\\bigg)\\) for some choices \\(\\{k_1,\\ldots,k_p\\}\\)\n\nFourier series\n\n\\(x \\mapsto \\bigg(1,\\ \\cos(2\\pi x),\\ \\sin(2\\pi x),\\ \\cos(2\\pi 2 x),\\ \\sin(2\\pi 2 x), \\ldots, \\cos(2\\pi p x),\\ \\sin(2\\pi p x)\\bigg)\\)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#how-do-you-choose",
    "href": "schedule/slides/10-basis-expansions.html#how-do-you-choose",
    "title": "UBC Stat406 2024W",
    "section": "How do you choose?",
    "text": "How do you choose?\nProcedure 1:\n\nPick your favorite basis. This is not as easy as it sounds. For instance, if \\(f\\) is a step function, linear splines will do well with good knots, but polynomials will be terrible unless you have lots of terms.\nPerform OLS on different orders.\nUse model selection criterion to choose the order.\n\nProcedure 2:\n\nUse a bunch of high-order bases, say Linear splines and Fourier series and whatever else you like.\nUse Lasso or Ridge regression or elastic net. (combining bases can lead to multicollinearity, but we may not care)\nUse model selection criteria to choose the tuning parameter."
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#try-both-procedures",
    "href": "schedule/slides/10-basis-expansions.html#try-both-procedures",
    "title": "UBC Stat406 2024W",
    "section": "Try both procedures",
    "text": "Try both procedures\n\nSplit arcuate into 75% training data and 25% testing data.\nEstimate polynomials up to 20 as before and choose best order.\nDo ridge, lasso and elastic net \\(\\alpha=.5\\) on 20th order polynomials, B splines with 20 knots, and Fourier series with \\(p=20\\). Choose tuning parameter (using lambda.1se).\nRepeat 1-3 10 times (different splits)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section-2",
    "href": "schedule/slides/10-basis-expansions.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "library(glmnet)\nmapto01 &lt;- function(x, pad = .005) (x - min(x) + pad) / (max(x) - min(x) + 2 * pad)\nx &lt;- mapto01(arcuate$position)\nXmat &lt;- cbind(\n  poly(x, 20), \n  splines::bs(x, df = 20), \n  cos(2 * pi * outer(x, 1:20)), sin(2 * pi * outer(x, 1:20))\n)\ny &lt;- arcuate$fa\nrmse &lt;- function(z, s) sqrt(mean( (z - s)^2 ))\nnzero &lt;- function(x) with(x, nzero[match(lambda.1se, lambda)])\nsim &lt;- function(maxdeg = 20, train_frac = 0.75) {\n  n &lt;- nrow(arcuate)\n  train &lt;- as.logical(rbinom(n, 1, train_frac))\n  test &lt;- !train # not precisely 25%, but on average\n  polycv &lt;- map_dbl(seq(maxdeg), ~ cv_nice(lm(y ~ Xmat[,seq(.)], subset = train))) # figure out which order to use\n  bpoly &lt;- lm(y[train] ~ Xmat[train, seq(which.min(polycv))]) # now use it\n  lasso &lt;- cv.glmnet(Xmat[train, ], y[train])\n  ridge &lt;- cv.glmnet(Xmat[train, ], y[train], alpha = 0)\n  elnet &lt;- cv.glmnet(Xmat[train, ], y[train], alpha = .5)\n  tibble(\n    methods = c(\"poly\", \"lasso\", \"ridge\", \"elnet\"),\n    rmses = c(\n      rmse(y[test], cbind(1, Xmat[test, 1:which.min(polycv)]) %*% coef(bpoly)),\n      rmse(y[test], predict(lasso, Xmat[test,])),\n      rmse(y[test], predict(ridge, Xmat[test,])),\n      rmse(y[test], predict(elnet, Xmat[test,]))\n    ),\n    nvars = c(which.min(polycv), nzero(lasso), nzero(ridge), nzero(elnet))\n  )\n}\nset.seed(12345)\nsim_results &lt;- map(seq(20), sim) |&gt; list_rbind() # repeat it 20 times"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#section-3",
    "href": "schedule/slides/10-basis-expansions.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "Code\nsim_results |&gt;  \n  pivot_longer(-methods) |&gt; \n  ggplot(aes(methods, value, fill = methods)) + \n  geom_boxplot() +\n  facet_wrap(~ name, scales = \"free_y\") + \n  ylab(\"\") +\n  theme(legend.position = \"none\") + \n  xlab(\"\") +\n  scale_fill_viridis_d(begin = .2, end = 1)"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#common-elements",
    "href": "schedule/slides/10-basis-expansions.html#common-elements",
    "title": "UBC Stat406 2024W",
    "section": "Common elements",
    "text": "Common elements\nIn all these cases, we transformed \\(x\\) to a higher-dimensional space\nUsed \\(p+1\\) dimensions with polynomials\nUsed \\(p+4\\) dimensions with cubic splines\nUsed \\(2p+1\\) dimensions with Fourier basis"
  },
  {
    "objectID": "schedule/slides/10-basis-expansions.html#featurization",
    "href": "schedule/slides/10-basis-expansions.html#featurization",
    "title": "UBC Stat406 2024W",
    "section": "Featurization",
    "text": "Featurization\nEach case applied a feature map to \\(x\\), call it \\(\\Phi\\)\nWe used new “features” \\(\\Phi(x) = \\bigg(\\phi_1(x),\\ \\phi_2(x),\\ldots,\\phi_k(x)\\bigg)\\)\nNeural networks (coming in module 4) use this idea\nYou’ve also probably seen it in earlier courses when you added interaction terms or other transformations.\n\nSome methods (notably Support Vector Machines and Ridge regression) allow \\(k=\\infty\\)\nSee [ISLR] 9.3.2 for baby overview or [ESL] 5.8 (note 😱)"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#section",
    "href": "schedule/slides/26-pca-v-kpca.html#section",
    "title": "UBC Stat406 2024W",
    "section": "26 PCA v KPCA",
    "text": "26 PCA v KPCA\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 24 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#pca-v-kpca",
    "href": "schedule/slides/26-pca-v-kpca.html#pca-v-kpca",
    "title": "UBC Stat406 2024W",
    "section": "PCA v KPCA",
    "text": "PCA v KPCA\n(We assume \\(\\X\\) is already centered/scaled, \\(n\\) rows, \\(p\\) columns)\n\n\nPCA:\n\nDecompose \\(\\X=\\U\\D\\V^\\top\\) (SVD).\nEmbed into \\(M\\leq p\\) dimensions: \\[\\U_M \\D_M = \\X\\V_M\\]\n\nThe “embedding” is \\(\\U_M \\D_M\\).\n(called the “Principal Components” or the “scores” or occasionally the “factors”)\nThe “loadings” or “weights” are \\(\\V_M\\)\n\n\nKPCA:\n\nChoose \\(k(x_i, x_{i'})\\). Create \\(\\mathbf{K}\\).\nDouble center \\(\\mathbf{K} = \\mathbf{PKP}\\).\nDecompose \\(\\mathbf{K} = \\U \\D^2 \\U^\\top\\) (eigendecomposition).\nEmbed into \\(M\\leq p\\) dimensions: \\[\\U_M \\D_M\\]\n\nThe “embedding” is \\(\\U_M \\D_M\\).\nThere are no “loadings”\n(\\(\\not\\exists\\ \\mathbf{B}\\) such that \\(\\X\\mathbf{B} = \\U_M \\D_M\\))"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#why-is-this-the-solution",
    "href": "schedule/slides/26-pca-v-kpca.html#why-is-this-the-solution",
    "title": "UBC Stat406 2024W",
    "section": "Why is this the solution?",
    "text": "Why is this the solution?\nThe “maximize variance” version of PCA:\n\\[\\max_\\alpha \\Var{\\X\\alpha} \\quad \\textrm{ subject to } \\quad \\left|\\left| \\alpha \\right|\\right|_2^2 = 1\\]\n( \\(\\Var{\\X\\alpha} = \\alpha^\\top\\X^\\top\\X\\alpha\\) )\nThis is equivalent to solving (Lagrangian):\n\\[\\max_\\alpha \\alpha^\\top\\X^\\top\\X\\alpha - \\lambda\\left|\\left| \\alpha \\right|\\right|_2^2\\]\nTake derivative wrt \\(\\alpha\\) and set to 0:\n\\[0 = 2\\X^\\top\\X\\alpha - 2\\lambda\\alpha\\]\nThis is the equation for an eigenproblem. The solution is \\(\\alpha=\\V_1\\) and the maximum is \\(\\D_1^2\\)."
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#example-not-real-unless-theres-code",
    "href": "schedule/slides/26-pca-v-kpca.html#example-not-real-unless-theres-code",
    "title": "UBC Stat406 2024W",
    "section": "Example (not real unless there’s code)",
    "text": "Example (not real unless there’s code)\n\nX &lt;- Stat406::mobility |&gt;\n  select(Black:Married) |&gt;\n  as.matrix()\nnot_missing &lt;- complete.cases(X)\nX &lt;- scale(X[not_missing, ], center = TRUE, scale = TRUE)\ncolors &lt;- Stat406::mobility$Mobility[not_missing]\nM &lt;- 2 # embedding dimension\nP &lt;- diag(nrow(X)) - 1 / nrow(X)\n\n\n\nPCA: (all 3 are equivalent)\n\ns &lt;- svd(X) # use svd\npca_loadings &lt;- s$v[, 1:M]\npca_scores &lt;- X %*% pca_loadings\n\n\ns &lt;- eigen(t(X) %*% X) # V D^2 V'\npca_loadings &lt;- s$vectors[, 1:M]\npca_scores &lt;- X %*% pca_loadings\n\n\ns &lt;- eigen(X %*% t(X)) # U D^2 U'\nD &lt;- sqrt(diag(s$values[1:M]))\nU &lt;- s$vectors[, 1:M]\npca_scores &lt;- U %*% D\npca_loadings &lt;- (1 / D) %*% t(U) %*% X\n\n\n\nKPCA:\n\nd &lt;- 2\nK &lt;- P %*% (1 + X %*% t(X))^d %*% P # polynomial\ne &lt;- eigen(K) # U D^2 U'\n# (different from the PCA one, K /= XX')\nU &lt;- e$vectors[, 1:M]\nD &lt;- diag(sqrt(e$values[1:M]))\nkpca_poly &lt;- U %*% D\n\n\nK &lt;- P %*% tanh(1 + X %*% t(X)) %*% P # sigmoid kernel\ne &lt;- eigen(K) # U D^2 U'\n# (different from the PCA one, K /= XX')\nU &lt;- e$vectors[, 1:M]\nD &lt;- diag(sqrt(e$values[1:M]))\nkpca_sigmoid &lt;- U %*% D"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#plotting",
    "href": "schedule/slides/26-pca-v-kpca.html#plotting",
    "title": "UBC Stat406 2024W",
    "section": "Plotting",
    "text": "Plotting"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#pca-loadings",
    "href": "schedule/slides/26-pca-v-kpca.html#pca-loadings",
    "title": "UBC Stat406 2024W",
    "section": "PCA loadings",
    "text": "PCA loadings\nShowing the first 10 PCA loadings:\n\nFirst column are the weights on the first score\neach number corresponds to a variable in the original data\nHow much does that variable contribute to that score?\n\n\nhead(round(pca_loadings, 2), 10)\n\n       [,1]  [,2]\n [1,]  0.25  0.07\n [2,]  0.13 -0.14\n [3,]  0.17 -0.34\n [4,]  0.18 -0.33\n [5,]  0.16 -0.34\n [6,] -0.24  0.11\n [7,] -0.04 -0.35\n [8,]  0.28  0.00\n [9,]  0.13 -0.14\n[10,]  0.29  0.10"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#kpca-feature-map-version",
    "href": "schedule/slides/26-pca-v-kpca.html#kpca-feature-map-version",
    "title": "UBC Stat406 2024W",
    "section": "KPCA, feature map version",
    "text": "KPCA, feature map version\n\np &lt;- ncol(X)\nwidth &lt;- p * (p - 1) / 2 + p # = 630\nZ &lt;- matrix(NA, nrow(X), width)\nk &lt;- 0\nfor (i in 1:p) {\n  for (j in i:p) {\n    k &lt;- k + 1\n    Z[, k] &lt;- X[, i] * X[, j]\n  }\n}\nwideX &lt;- scale(cbind(X, Z))\ns &lt;- RSpectra::svds(wideX, 2) # the whole svd would be super slow\nfkpca_scores &lt;- s$u %*% diag(s$d)\n\n\nUnfortunately, can’t easily compare to check whether the result is the same\nAlso can cause numerical issues\nBut should be the “same” (assuming I didn’t screw up…)\nWould also allow me to get the loadings, though they’d depend on polynomials"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#other-manifold-learning-methods",
    "href": "schedule/slides/26-pca-v-kpca.html#other-manifold-learning-methods",
    "title": "UBC Stat406 2024W",
    "section": "Other manifold learning methods",
    "text": "Other manifold learning methods\nTo name a few\n\nHessian maps\nLaplacian eigenmaps\nClassical Multidimensional Scaling\ntSNE\nUMAP\nLocally linear embeddings\nDiffusion maps\nLocal tangent space alignment\nIsomap"
  },
  {
    "objectID": "schedule/slides/26-pca-v-kpca.html#issues-with-nonlinear-techniques",
    "href": "schedule/slides/26-pca-v-kpca.html#issues-with-nonlinear-techniques",
    "title": "UBC Stat406 2024W",
    "section": "Issues with nonlinear techniques",
    "text": "Issues with nonlinear techniques\n\n\n\nNeed to choose \\(M\\) (also with linear)\nAlso other tuning parameters.\nThese others can have huge effects\nThe difference between the data lying on the manifold and the data lying near the manifold is important\n\n\n\n\n\nCode\nelephant &lt;- function(eye = TRUE) {\n  tib &lt;- tibble(\n    tt = -100:500 / 100,\n    y = -(12 * cos(3 * tt) - 14 * cos(5 * tt) + 50 * sin(tt) + 18 * sin(2 * tt)),\n    x = -30 * sin(tt) + 8 * sin(2 * tt) - 10 * sin(3 * tt) - 60 * cos(tt)\n  )\n  if (eye) tib &lt;- add_row(tib, y = 20, x = 20)\n  tib\n}\nele &lt;- elephant(FALSE)\nnoisy_ele &lt;- ele |&gt;\n  mutate(y = y + rnorm(n(), 0, 5), x = x + rnorm(n(), 0, 5))\nggplot(noisy_ele, aes(x, y, colour = tt)) +\n  geom_point() +\n  scale_color_viridis_c() +\n  theme(legend.position = \"none\") +\n  geom_path(data = ele, colour = \"black\", linewidth = 2)"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#section",
    "href": "schedule/slides/13-gams-trees.html#section",
    "title": "UBC Stat406 2024W",
    "section": "13 GAMs and Trees",
    "text": "13 GAMs and Trees\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 09 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#gams",
    "href": "schedule/slides/13-gams-trees.html#gams",
    "title": "UBC Stat406 2024W",
    "section": "GAMs",
    "text": "GAMs\nLast time we discussed smoothing in multiple dimensions.\nHere we introduce the concept of GAMs (Generalized Additive Models)\nThe basic idea is to imagine that the response is the sum of some functions of the predictors:\n\\[\\Expect{Y \\given X=x} = \\beta_0 + f_1(x_{1})+\\cdots+f_p(x_{p}).\\]\nNote that OLS is a GAM (take \\(f_j(x_{j})=\\beta_j x_{j}\\)):\n\\[\\Expect{Y \\given X=x} = \\beta_0 + \\beta_1 x_{1}+\\cdots+\\beta_p x_{p}.\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#gams-1",
    "href": "schedule/slides/13-gams-trees.html#gams-1",
    "title": "UBC Stat406 2024W",
    "section": "Gams",
    "text": "Gams\nThese work by estimating each \\(f_i\\) using basis expansions in predictor \\(i\\)\nThe algorithm for fitting these things is called “backfitting” (very similar to the CD intuition for lasso):\n\nCenter \\(\\y\\) and \\(\\X\\).\nHold \\(f_k\\) for all \\(k\\neq j\\) fixed, and regress \\(\\X_j\\) on \\((\\y - \\widehat{\\y}_{-j})\\) using your favorite smoother.\nRepeat for \\(1\\leq j\\leq p\\).\nRepeat steps 2 and 3 until the estimated functions “stop moving” (iterate)\nReturn the results."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example",
    "href": "schedule/slides/13-gams-trees.html#very-small-example",
    "title": "UBC Stat406 2024W",
    "section": "Very small example",
    "text": "Very small example\n\nlibrary(mgcv)\nset.seed(12345)\nn &lt;- 500\nsimple &lt;- tibble(\n  x1 = runif(n, 0, 2*pi),\n  x2 = runif(n),\n  y = 5 + 2 * sin(x1) + 8 * sqrt(x2) + rnorm(n, sd = .25)\n)\n\npivot_longer(simple, -y, names_to = \"predictor\", values_to = \"x\") |&gt;\n  ggplot(aes(x, y)) +\n  geom_point(col = blue) +\n  facet_wrap(~predictor, scales = \"free_x\")"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example-1",
    "href": "schedule/slides/13-gams-trees.html#very-small-example-1",
    "title": "UBC Stat406 2024W",
    "section": "Very small example",
    "text": "Very small example\nSmooth each coordinate independently\n\nex_smooth &lt;- gam(y ~ s(x1) + s(x2), data = simple)\n# s(z) means \"smooth\" z, uses spline basis for each with ridge penalty, GCV\nplot(ex_smooth, pages = 1, scale = 0, shade = TRUE, \n     resid = TRUE, se = 2, las = 1)\n\nhead(coef(ex_smooth))\n\n(Intercept)     s(x1).1     s(x1).2     s(x1).3     s(x1).4     s(x1).5 \n 10.2070490  -4.5764100   0.7117161   0.4548928   0.5535001  -0.2092996 \n\nex_smooth$gcv.ubre\n\n    GCV.Cp \n0.06619721"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#wherefore-gams",
    "href": "schedule/slides/13-gams-trees.html#wherefore-gams",
    "title": "UBC Stat406 2024W",
    "section": "Wherefore GAMs?",
    "text": "Wherefore GAMs?\nIf\n\\(\\Expect{Y \\given X=x} = \\beta_0 + f_1(x_{1})+\\cdots+f_p(x_{p}),\\)\nthen\n\\(\\textrm{MSE}(\\hat f) = \\frac{Cp}{n^{4/5}} + \\sigma^2.\\)\n\nExponent no longer depends on \\(p\\). Converges faster. (If the truth is additive.)\nYou could also use the same methods to include “some” interactions like\n\n\\[\\begin{aligned}&\\Expect{Y \\given X=x}\\\\ &= \\beta_0 + f_{12}(x_{1},\\ x_{2})+f_3(x_3)+\\cdots+f_p(x_{p}),\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#very-small-example-2",
    "href": "schedule/slides/13-gams-trees.html#very-small-example-2",
    "title": "UBC Stat406 2024W",
    "section": "Very small example",
    "text": "Very small example\nSmooth two coordinates together\n\nex_smooth2 &lt;- gam(y ~ s(x1, x2), data = simple)\nplot(ex_smooth2,\n  scheme = 2, scale = 0, shade = TRUE,\n  resid = TRUE, se = 2, las = 1\n)"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#regression-trees",
    "href": "schedule/slides/13-gams-trees.html#regression-trees",
    "title": "UBC Stat406 2024W",
    "section": "Regression trees",
    "text": "Regression trees\nTrees involve stratifying or segmenting the predictor space into a number of simple regions.\nTrees are simple and useful for interpretation.\nBasic trees are not great at prediction.\nModern methods that use trees are much better (Module 4)"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#regression-trees-1",
    "href": "schedule/slides/13-gams-trees.html#regression-trees-1",
    "title": "UBC Stat406 2024W",
    "section": "Regression trees",
    "text": "Regression trees\nRegression trees estimate piece-wise constant functions\nThe slabs are axis-parallel rectangles \\(R_1,\\ldots,R_K\\) based on \\(\\X\\)\nIn each region, we average the \\(y_i\\)’s: \\(\\hat\\mu_1,\\ldots,\\hat\\mu_k\\)\nMinimize \\(\\sum_{k=1}^K \\sum_{i=1}^n (y_i-\\mu_k)^2\\) over \\(R_k,\\mu_k\\) for \\(k\\in \\{1,\\ldots,K\\}\\)\n\nThis sounds more complicated than it is.\nThe minimization is performed greedily (like forward stepwise regression)."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#mobility-data",
    "href": "schedule/slides/13-gams-trees.html#mobility-data",
    "title": "UBC Stat406 2024W",
    "section": "Mobility data",
    "text": "Mobility data\n\nbigtree &lt;- tree(Mobility ~ ., data = mob)\nsmalltree &lt;- prune.tree(bigtree, k = .09)\ndraw.tree(smalltree, digits = 2)\n\n\nThis is called the dendrogram"
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#partition-view",
    "href": "schedule/slides/13-gams-trees.html#partition-view",
    "title": "UBC Stat406 2024W",
    "section": "Partition view",
    "text": "Partition view\n\nmob$preds &lt;- predict(smalltree)\npar(mfrow = c(1, 2), mar = c(5, 3, 0, 0))\ndraw.tree(smalltree, digits = 2)\ncols &lt;- viridisLite::viridis(20, direction = -1)[cut(log(mob$Mobility), 20)]\nplot(mob$Black, mob$Commute,\n  pch = 19, cex = .4, bty = \"n\", las = 1, col = cols,\n  ylab = \"Commute time\", xlab = \"% Black\"\n)\npartition.tree(smalltree, add = TRUE, ordvars = c(\"Black\", \"Commute\"))\n\n\nWe predict all observations in a region with the same value.\n\\(\\bullet\\) The three regions correspond to the leaves of the tree."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#section-2",
    "href": "schedule/slides/13-gams-trees.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "draw.tree(bigtree, digits = 2)\n\n\nTerminology\nWe call each split or end point a node. Each terminal node is referred to as a leaf.\nThe interior nodes lead to branches."
  },
  {
    "objectID": "schedule/slides/13-gams-trees.html#advantages-and-disadvantages-of-trees",
    "href": "schedule/slides/13-gams-trees.html#advantages-and-disadvantages-of-trees",
    "title": "UBC Stat406 2024W",
    "section": "Advantages and disadvantages of trees",
    "text": "Advantages and disadvantages of trees\n🎉 Trees are very easy to explain (much easier than even linear regression).\n🎉 Some people believe that decision trees mirror human decision.\n🎉 Trees can easily be displayed graphically no matter the dimension of the data.\n🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.\n💩 Trees aren’t very good at prediction.\n💩 Full trees badly overfit, so we “prune” them using CV\n\nWe’ll talk more about trees next module for Classification."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#section",
    "href": "schedule/slides/05-estimating-test-mse.html#section",
    "title": "UBC Stat406 2024W",
    "section": "05 Estimating (Test) MSE",
    "text": "05 Estimating (Test) MSE\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 19 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#last-time",
    "href": "schedule/slides/05-estimating-test-mse.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\n\nWhat is a model (formal definition)?\nEvaluating models (risk/loss functions)\nDecomposing risk (bias, variance, irreducible error)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#what-is-a-model",
    "href": "schedule/slides/05-estimating-test-mse.html#what-is-a-model",
    "title": "UBC Stat406 2024W",
    "section": "What is a model?",
    "text": "What is a model?\nA model is a set of distributions that explain data \\(\\{ Z = (X, Y) \\}\\), i.e.\n\\[\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' f} \\}\\]\n(Why do we have to specify that \\(f\\) is smooth? Why can’t it be any function?)\n\nGoal of learning\nChoose the \\(P \\in \\mathcal P\\) that makes the “best” predictions on new \\(X, Y\\) pairs.\n(Next slide: how do we formalize “best”?)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#how-do-we-evaluate-models",
    "href": "schedule/slides/05-estimating-test-mse.html#how-do-we-evaluate-models",
    "title": "UBC Stat406 2024W",
    "section": "How do we evaluate models?",
    "text": "How do we evaluate models?\n\\[\\mathcal{P} = \\{ P: \\quad Y \\mid X \\sim \\mathcal N( f(X), \\sigma^2) \\quad \\text{for some ``smooth'' f} \\}\\]\n\nSpecify how a \\(P \\in \\mathcal P\\) makes predictions on new inputs \\(\\hat Y\\).\n(E.g.: \\(\\hat Y = f(X)\\) for \\(P = \\mathcal N(f(X), \\sigma^2)\\).)\nIntroduce a loss function \\(\\ell(Y, \\hat{Y})\\) (a datapoint-level function).\n(E.g.: \\(\\ell(Y, \\hat Y) = (Y - \\hat Y)^2\\))\nDefine the test error of \\(P \\in \\mathcal P\\) as the expected loss (a population-level function):\n(P) = E[(Y, Y)] = E[(Y - f(X))^2]\nThe best model is the one that minimizes the test error\n(\\(P^* = \\argmin_{P \\in \\mathcal P} R_n(P)\\))"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#risk-expected-test-error-and-its-decomposition",
    "href": "schedule/slides/05-estimating-test-mse.html#risk-expected-test-error-and-its-decomposition",
    "title": "UBC Stat406 2024W",
    "section": "Risk (Expected Test Error) and its Decomposition",
    "text": "Risk (Expected Test Error) and its Decomposition\nOur estimator \\(\\hat f\\) is a random variable (it depends on training sample).\nSo let’s consider the risk (the expected test error):\n\\[\nR_n = E_{\\hat f} \\left[ \\text{Err.}(\\hat f) \\right] = E_{\\hat f, X, Y} \\left[ \\ell(Y, \\hat f(X)) \\right]\n\\]\n\n\n\n\n\n\n\nNote\n\n\nTest error is a metric for a fixed \\(\\hat f\\). It averages over all possible test points, but assumes a fixed training set.\nRisk averages over everything that is random: (1) the test data point sampled from our population, and (2) the training data that produces \\(\\hat f\\)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#risk-expected-test-error-and-its-decomposition-1",
    "href": "schedule/slides/05-estimating-test-mse.html#risk-expected-test-error-and-its-decomposition-1",
    "title": "UBC Stat406 2024W",
    "section": "Risk (Expected Test Error) and its Decomposition",
    "text": "Risk (Expected Test Error) and its Decomposition\nWhen \\(\\ell(Y, \\hat Y) = (Y - \\hat Y)^2\\), the prediction risk of \\(\\hat f(X)\\) decomposes into two factors:\n\\[\nR_n \\quad = \\quad \\underbrace{E_{\\hat f, X, Y} \\left[ \\: \\left( E[Y\\mid X] - \\hat f(X) \\right)^2 \\right]}_{(1)} \\quad + \\quad \\underbrace{E_{X, Y} \\left[ \\: \\left( Y - E[Y\\mid X] \\right)^2 \\right]}_{(2)}\n\\]\n\n\nEstimation error (or “reducible error”)\nIrreducible error (or “noise”)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#sources-of-bias-and-variance",
    "href": "schedule/slides/05-estimating-test-mse.html#sources-of-bias-and-variance",
    "title": "UBC Stat406 2024W",
    "section": "Sources of bias and variance",
    "text": "Sources of bias and variance\nWhat conditions give rise to a high bias estimator?\n\n\nNot enough covariates (small \\(p\\))\nModel is too simple\nModel is misspecified (doesn’t accurately represent the data generating process)\nBad training algorithm\n\n\nWhat conditions give rise to a high variance estimator?\n\n\nNot enough training samples (small \\(n\\))\nModel is too complicated\nLots of irreducible noise in training data (if my model has power to fit noise, it will)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error",
    "href": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error",
    "title": "UBC Stat406 2024W",
    "section": "Don’t use training error",
    "text": "Don’t use training error\nThe training error in regression is\n\\[\\widehat{R}_n(\\widehat{f}) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{f}(x_i))^2\\]\nHere, the \\(n\\) is doubly used (annoying, but simple): \\(n\\) observations to create \\(\\widehat{f}\\) and \\(n\\) terms in the sum.\n\n\n\n\n\n\nTip\n\n\nWe also call \\(\\hat R_n(\\hat f)\\) the empirical risk.\n\n\n\n\n\\(\\hat R_n(\\hat f)\\) is a bad estimator for \\(R_n\\).\nSo we should never use it."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#why-is-hat-r_n-a-bad-estimator-of-r_n",
    "href": "schedule/slides/05-estimating-test-mse.html#why-is-hat-r_n-a-bad-estimator-of-r_n",
    "title": "UBC Stat406 2024W",
    "section": "Why is \\(\\hat R_n\\) a bad estimator of \\(R_n\\)?",
    "text": "Why is \\(\\hat R_n\\) a bad estimator of \\(R_n\\)?\n\n\nIt doesn’t say anything about predictions on new data.\n(It’s a measure of how well the model fits a fixed set of training data.)\nIt can be made arbitrarily small by making your model more complex."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#it-doesnt-say-anything-about-predictions-on-new-data.",
    "href": "schedule/slides/05-estimating-test-mse.html#it-doesnt-say-anything-about-predictions-on-new-data.",
    "title": "UBC Stat406 2024W",
    "section": "1. It doesn’t say anything about predictions on new data.",
    "text": "1. It doesn’t say anything about predictions on new data.\nThese all have the same \\(R^2\\) and Training Error\n\n\n\n\nCode\nans &lt;- anscombe |&gt;\n  pivot_longer(everything(), names_to = c(\".value\", \"set\"), \n               names_pattern = \"(.)(.)\")\nggplot(ans, aes(x, y)) + \n  geom_point(colour = orange, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = blue, linewidth = 2) +\n  facet_wrap(~set, labeller = label_both)\n\n\n\n\n\n\n\n\n\n\n\n\nans %&gt;% \n  group_by(set) |&gt; \n  summarise(\n    R2 = summary(lm(y ~ x))$r.sq, \n    train_error = mean((y - predict(lm(y ~ x)))^2)\n  ) |&gt;\n  kableExtra::kable(digits = 2)\n\n\n\n\nset\nR2\ntrain_error\n\n\n\n\n1\n0.67\n1.25\n\n\n2\n0.67\n1.25\n\n\n3\n0.67\n1.25\n\n\n4\n0.67\n1.25"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#it-can-be-made-arbitrarily-small-by-making-your-model-more-complex.",
    "href": "schedule/slides/05-estimating-test-mse.html#it-can-be-made-arbitrarily-small-by-making-your-model-more-complex.",
    "title": "UBC Stat406 2024W",
    "section": "2. It can be made arbitrarily small by making your model more complex.",
    "text": "2. It can be made arbitrarily small by making your model more complex.\nAdding “junk” predictors increases \\(R^2\\) and decreases Training Error\n\nn &lt;- 100\np &lt;- 10\nq &lt;- 0:30\nx &lt;- matrix(rnorm(n * (p + max(q))), nrow = n)\ny &lt;- x[, 1:p] %*% c(5:1, 1:5) + rnorm(n, 0, 10)\n\nregress_on_junk &lt;- function(q) {\n  x &lt;- x[, 1:(p + q)]\n  mod &lt;- lm(y ~ x)\n  tibble(R2 = summary(mod)$r.sq,  train_error = mean((y - predict(mod))^2))\n}\n\n\n\nCode\nmap(q, regress_on_junk) |&gt; \n  list_rbind() |&gt;\n  mutate(q = q) |&gt;\n  pivot_longer(-q) |&gt;\n  ggplot(aes(q, value, colour = name)) +\n  geom_line(linewidth = 2) + xlab(\"train_error\") +\n  scale_colour_manual(values = c(blue, orange), guide = \"none\") +\n  facet_wrap(~ name, scales = \"free_y\")"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#other-things-you-cant-use",
    "href": "schedule/slides/05-estimating-test-mse.html#other-things-you-cant-use",
    "title": "UBC Stat406 2024W",
    "section": "Other things you can’t use",
    "text": "Other things you can’t use\nYou should not use anova\nor the \\(p\\)-values from the lm output for this purpose.\n\n\nThese things are to determine whether those parameters are different from zero if you were to repeat the experiment many times, if the model were true, etc. etc.\n\nIn other words, they are useful for inference problems.\nThis is not the same as being useful for prediction problems (i.e. how to get small \\(R_n\\))."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error-the-formal-argument",
    "href": "schedule/slides/05-estimating-test-mse.html#dont-use-training-error-the-formal-argument",
    "title": "UBC Stat406 2024W",
    "section": "Don’t use training error: the formal argument",
    "text": "Don’t use training error: the formal argument\nOur training error \\(\\hat R_n(\\hat f)\\) is an estimator of \\(R_n\\).\nSo we can ask “is \\(\\widehat{R}_n(\\hat{f})\\) a good estimator for \\(R_n\\)?”"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#the-error-of-our-risk-estimator",
    "href": "schedule/slides/05-estimating-test-mse.html#the-error-of-our-risk-estimator",
    "title": "UBC Stat406 2024W",
    "section": "The error of our risk estimator",
    "text": "The error of our risk estimator\nLet’s measure the error of our empirical risk estimator:\n\n\\[E[(R_n - \\hat R_n(\\hat f))^2]\\] (What is the expectation with respect to?)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#the-error-of-our-risk-estimator-1",
    "href": "schedule/slides/05-estimating-test-mse.html#the-error-of-our-risk-estimator-1",
    "title": "UBC Stat406 2024W",
    "section": "The error of our risk estimator",
    "text": "The error of our risk estimator\n\\[E[(R_n - \\hat R_n(\\hat f))^2]\\]\n\n\\(R_n\\) is deterministic (we average over test data and training data)\n\\(\\hat R_n(\\hat f)\\) also only depends on training data\nSo the expectation is with respect to our training dataset\n\n\nAs before, we can decompose the error of our risk estimator into bias and variance\n\\[\nE[(R_n - \\hat R_n(\\hat f))^2] = \\underbrace{( R_n - E[\\hat R_n(\\hat f)])^2}_{\\text{bias}} + \\underbrace{E[( \\hat R_n(\\hat f) - E[\\hat R_n(\\hat f)])^2]}_{\\text{variance}}\n\\]\nIs the bias of \\(\\hat R_n(\\hat f)\\) small or large? Why?"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#is-the-bias-of-hat-r_nhat-f-small-or-large-why-1",
    "href": "schedule/slides/05-estimating-test-mse.html#is-the-bias-of-hat-r_nhat-f-small-or-large-why-1",
    "title": "UBC Stat406 2024W",
    "section": "Is the bias of \\(\\hat R_n(\\hat f)\\) small or large? Why?",
    "text": "Is the bias of \\(\\hat R_n(\\hat f)\\) small or large? Why?\n\nAssume we have a very complex model capable of (nearly) fitting our training data\n\nI.e. \\(\\hat R_n(\\hat f) \\approx 0\\)\n\n\\(\\text{Bias} = ( R_n - E[\\hat R_n(\\hat f)])^2 \\approx ( R_n - 0 ) = R_n\\)\n(That’s the worst bias we could get! 😔)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#formalizing-why-hat-r_nhat-f-is-a-bad-estimator-of-r_n",
    "href": "schedule/slides/05-estimating-test-mse.html#formalizing-why-hat-r_nhat-f-is-a-bad-estimator-of-r_n",
    "title": "UBC Stat406 2024W",
    "section": "Formalizing why \\(\\hat R_n(\\hat f)\\) is a bad estimator of \\(R_n\\)",
    "text": "Formalizing why \\(\\hat R_n(\\hat f)\\) is a bad estimator of \\(R_n\\)\nConsider an alternative estimator built from \\(\\{ (X_j, Y_j) \\}_{j=1}^m\\) that was not part of the training set. \\[\\tilde R_m(\\hat f) = {\\textstyle \\frac{1}{m} \\sum_{j=1}^m} \\ell(Y_j, \\hat f(X_j)),\n\\] The error of this estimator can also be decompsed into bias and variance \\[\nE[(R_n - \\tilde R_m(\\hat f))^2] = \\underbrace{( R_n - E_{\\hat f,X_j,Y_j}[\\tilde R_m(\\hat f)])^2}_{\\text{bias}} + \\underbrace{E_{\\hat f,X_j,Y_j}[( \\tilde R_m(\\hat f) - E_{\\hat f,X_j,Y_j}[\\tilde R_m(\\hat f)])^2]}_{\\text{variance}}\n\\]\nIs the bias of \\(\\tilde R_m(\\hat f)\\) small or large? Why?"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#is-the-bias-of-tilde-r_mhat-f-small-or-large-why-1",
    "href": "schedule/slides/05-estimating-test-mse.html#is-the-bias-of-tilde-r_mhat-f-small-or-large-why-1",
    "title": "UBC Stat406 2024W",
    "section": "Is the bias of \\(\\tilde R_m(\\hat f)\\) small or large? Why?",
    "text": "Is the bias of \\(\\tilde R_m(\\hat f)\\) small or large? Why?\n\\(\\tilde R_m(\\hat f)\\) has zero bias!\n\\[\n\\begin{aligned}\nE_{\\hat f,X_j,Y_j} \\left[ \\tilde R_m(\\hat f) \\right]\n&= E_{\\hat f,X_j,Y_j} \\left[ \\frac{1}{m} \\sum_{j=1}^m \\ell(Y_j, \\hat f(X_j)) \\right] \\\\\n&= \\frac{1}{m} \\sum_{j=1}^m E_{\\hat f,X_j,Y_j} \\left[ \\ell(Y_j, \\hat f(X_j)) \\right]\n= R_n\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#holdout-sets",
    "href": "schedule/slides/05-estimating-test-mse.html#holdout-sets",
    "title": "UBC Stat406 2024W",
    "section": "Holdout sets",
    "text": "Holdout sets\nOne option is to have a separate “holdout” or “validation” dataset.\n\n\n\n\n\n\nTip\n\n\nThis option follows the logic on the previous slide.\nIf we randomly “hold out” \\(\\{ (X_j, Y_j) \\}_{j=1}^m\\) from the training set, we can use this data to get an (nearly) unbiased estimator of \\(R_n\\). \\[\nR_n \\approx \\tilde R_m(\\hat f) \\triangleq {\\textstyle{\\frac 1 m \\sum_{j=1}^m \\ell ( Y_j - \\hat Y_j(X_j))}}\n\\]\n\n\n\n\n👍 Estimates the test error\n👍 Fast computationally\n🤮 Estimate is random\n🤮 Estimate has high variance (depends on 1 choice of split)\n🤮 Estimate has a little bias (because we aren’t estimating \\(\\hat f\\) from all of the training data)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#aside",
    "href": "schedule/slides/05-estimating-test-mse.html#aside",
    "title": "UBC Stat406 2024W",
    "section": "Aside",
    "text": "Aside\nIn my experience, CS has particular definitions of “training”, “validation”, and “test” data.\nI think these are not quite the same as in Statistics.\n\nTest data - Hypothetical data you don’t get to see, ever. Infinite amounts drawn from the population.\n\nExpected test error or Risk is an expected value over this distribution. It’s not a sum over some data kept aside.\n\nSometimes I’ll give you “test data”. You pretend that this is a good representation of the expectation and use it to see how well you did on the training data.\nTraining data - This is “holdout” data that you get to touch.\nValidation set - Often, we need to choose models. One way to do this is to split off some of your training data and pretend that it’s like a “Test Set”.\n\nWhen and how you split your training data can be very important."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#cross-validation",
    "href": "schedule/slides/05-estimating-test-mse.html#cross-validation",
    "title": "UBC Stat406 2024W",
    "section": "Cross Validation",
    "text": "Cross Validation\n\nOur validation error \\(\\tilde{R}_m(\\widehat{f})\\) is a random estimator.\n(The split we use to divide our data into training versus validation is random.)\n\n\nA random estimator has variance. We can reduce this variance by averageing over multiple splits."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#cross-validation-example",
    "href": "schedule/slides/05-estimating-test-mse.html#cross-validation-example",
    "title": "UBC Stat406 2024W",
    "section": "Cross Validation Example",
    "text": "Cross Validation Example\nWhat if we set aside one observation, say the first one \\((y_1, x_1)\\):\n\nWe estimate \\(\\widehat{f}^{(1)}\\) without using the first observation.\nWe estimate \\(\\widetilde{R}_1(\\widehat{f}^{(1)})\\) using the held-out first observation.\n\n\\[\\widetilde{R}_1(\\widehat{f}^{(1)}) = (y_1 -\\widehat{f}^{(1)}(x_1))^2.\\] (Why the notation \\(\\widetilde{R}_1\\)? Because we’re estimating the risk with 1 observation. )"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#problems-with-loo-cv",
    "href": "schedule/slides/05-estimating-test-mse.html#problems-with-loo-cv",
    "title": "UBC Stat406 2024W",
    "section": "Problems with LOO-CV",
    "text": "Problems with LOO-CV\n\n🤮 The training sets overlap. This is bad.\n\nUsually, averaging reduces variance: \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\sum_{i=1}^n \\Var{X_i} = \\frac{1}{n}\\Var{X_1}.\\)\nBut only if the variables are independent. If not, then \\(\\Var{\\overline{X}} = \\frac{1}{n^2}\\Var{ \\sum_{i=1}^n X_i} = \\frac{1}{n}\\Var{X_1} + \\frac{1}{n^2}\\sum_{i\\neq j} \\Cov{X_i}{X_j}.\\)\nSince the training sets overlap a lot, that covariance can be pretty big.\n\n🤮 We have to estimate this model \\(n\\) times.\n🎉 Bias is low because we used almost all the data to fit the model: \\(E[\\mbox{LOO-CV}] = R_{n-1}\\)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#k-fold-cv",
    "href": "schedule/slides/05-estimating-test-mse.html#k-fold-cv",
    "title": "UBC Stat406 2024W",
    "section": "K-fold CV",
    "text": "K-fold CV\n\n\nTo alleviate some of these problems, people usually use \\(K\\)-fold cross validation.\nThe idea of \\(K\\)-fold is\n\nDivide the data into \\(K\\) groups.\nLeave a group out and estimate with the rest.\nTest on the held-out group. Calculate an average risk over these \\(\\sim n/K\\) data.\nRepeat for all \\(K\\) groups.\nAverage the average risks.\n\n\n\n🎉 Less overlap, smaller covariance.\n🎉 Larger hold-out sets, smaller variance.\n🎉 Less computations (only need to estimate \\(K\\) times)\n🤮 LOO-CV is (nearly) unbiased for \\(R_n\\)\n🤮 K-fold CV is unbiased for \\(R_{n(1-1/K)}\\)\nThe risk depends on how much data you use to estimate the model. \\(R_n\\) depends on \\(n\\)."
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#a-picture",
    "href": "schedule/slides/05-estimating-test-mse.html#a-picture",
    "title": "UBC Stat406 2024W",
    "section": "A picture",
    "text": "A picture\n\n\nCode\npar(mar = c(0, 0, 0, 0))\nplot(NA, NA, ylim = c(0, 5), xlim = c(0, 10), bty = \"n\", yaxt = \"n\", xaxt = \"n\")\nrect(0, .1 + c(0, 2, 3, 4), 10, .9 + c(0, 2, 3, 4), col = blue, density = 10)\nrect(c(0, 1, 2, 9), rev(.1 + c(0, 2, 3, 4)), c(1, 2, 3, 10), \n     rev(.9 + c(0, 2, 3, 4)), col = red, density = 10)\npoints(c(5, 5, 5), 1 + 1:3 / 4, pch = 19)\ntext(.5 + c(0, 1, 2, 9), .5 + c(4, 3, 2, 0), c(\"1\", \"2\", \"3\", \"K\"), cex = 3, \n     col = red)\ntext(6, 4.5, \"Training data\", cex = 3, col = blue)\ntext(2, 1.5, \"Validation data\", cex = 3, col = red)"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#code",
    "href": "schedule/slides/05-estimating-test-mse.html#code",
    "title": "UBC Stat406 2024W",
    "section": "Code",
    "text": "Code\n\n#' @param data The full data set\n#' @param estimator Function. Has 1 argument (some data) and fits a model. \n#' @param predictor Function. Has 2 args (the fitted model, the_newdata) and produces predictions\n#' @param error_fun Function. Has one arg: the test data, with fits added.\n#' @param kfolds Integer. The number of folds.\nkfold_cv &lt;- function(data, estimator, predictor, error_fun, kfolds = 5) {\n  n &lt;- nrow(data)\n  fold_labels &lt;- sample(rep(1:kfolds, length.out = n))\n  errors &lt;- double(kfolds)\n  for (fold in seq_len(kfolds)) {\n    test_rows &lt;- fold_labels == fold\n    train &lt;- data[!test_rows, ]\n    test &lt;- data[test_rows, ]\n    current_model &lt;- estimator(train)\n    test$.preds &lt;- predictor(current_model, test)\n    errors[fold] &lt;- error_fun(test)\n  }\n  mean(errors)\n}\n\n\n\nsomedata &lt;- data.frame(z = rnorm(100), x1 = rnorm(100), x2 = rnorm(100))\nest &lt;- function(dataset) lm(z ~ ., data = dataset)\npred &lt;- function(mod, dataset) predict(mod, newdata = dataset)\nerror_fun &lt;- function(testdata) mutate(testdata, errs = (z - .preds)^2) |&gt; pull(errs) |&gt; mean()\nkfold_cv(somedata, est, pred, error_fun, 5)\n\n[1] 0.8928686"
  },
  {
    "objectID": "schedule/slides/05-estimating-test-mse.html#trick",
    "href": "schedule/slides/05-estimating-test-mse.html#trick",
    "title": "UBC Stat406 2024W",
    "section": "Trick",
    "text": "Trick\nFor certain “nice” models of the form \\[\\widehat{y}_i = h_i(\\mathbf{X})^\\top \\mathbf{y}\\] for some vector \\(h_i\\), one can show\n\\[\\mbox{LOO-CV} = \\frac{1}{n} \\sum_{i=1}^n \\frac{(y_i -\\widehat{y}_i)^2}{(1-[\\boldsymbol h_i(x_i)]_{i})^2}.\\] (Proof: tedious algebra which I wouldn’t wish on my worst enemy, but might - in a fit of rage - assign as homework to belligerent students.)\n\n\nThis trick means that you only have to fit the model once rather than \\(n\\) times!\n\n\ncv_nice &lt;- function(mdl) mean( (residuals(mdl) / (1 - hatvalues(mdl)))^2 )"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#section",
    "href": "schedule/slides/28-hclust.html#section",
    "title": "UBC Stat406 2024W",
    "section": "28 Hierarchical clustering",
    "text": "28 Hierarchical clustering\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 30 November 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#from-k-means-to-hierarchical-clustering",
    "href": "schedule/slides/28-hclust.html#from-k-means-to-hierarchical-clustering",
    "title": "UBC Stat406 2024W",
    "section": "From \\(K\\)-means to hierarchical clustering",
    "text": "From \\(K\\)-means to hierarchical clustering\n\n\nK-means\n\nIt fits exactly \\(K\\) clusters.\nFinal clustering assignments depend on the chosen initial cluster centers.\n\nHierarchical clustering\n\nNo need to choose the number of clusters before hand.\nThere is no random component (nor choice of starting point).\n\nThere is a catch: we need to choose a way to measure the distance between clusters, called the linkage.\n\n\nSame data as the K-means example:\n\n\nCode\n# same data as K-means \"Dumb example\"\nheatmaply::ggheatmap(\n  as.matrix(dist(rbind(X1, X2, X3))),\n  showticklabels = c(FALSE, FALSE), hide_colorbar = TRUE\n)"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#hierarchical-clustering",
    "href": "schedule/slides/28-hclust.html#hierarchical-clustering",
    "title": "UBC Stat406 2024W",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGiven the linkage, hierarchical clustering produces a sequence of clustering assignments.\nAt one end, all points are in their own cluster.\nAt the other, all points are in one cluster.\nIn the middle, there are nontrivial solutions."
  },
  {
    "objectID": "schedule/slides/28-hclust.html#agglomeration",
    "href": "schedule/slides/28-hclust.html#agglomeration",
    "title": "UBC Stat406 2024W",
    "section": "Agglomeration",
    "text": "Agglomeration\n\n\n\n\n\n\n\n\n\n\n\n\nGiven these data points, an agglomerative algorithm chooses a cluster sequence by combining the points into groups.\nWe can also represent the sequence of clustering assignments as a dendrogram\nCutting the dendrogram horizontally partitions the data points into clusters\n\n\n\n\nNotation: Define \\(x_1,\\ldots, x_n\\) to be the data\nLet the dissimiliarities be \\(d_{ij}\\) between each pair \\(x_i, x_j\\)\nAt any level, clustering assignments can be expressed by sets \\(G = \\{ i_1, i_2, \\ldots, i_r\\}\\) giving the indicies of points in this group. Define \\(|G|\\) to be the size of \\(G\\).\n\n\nLinkage\n\nThe function \\(d(G,H)\\) that takes two groups \\(G,\\ H\\) and returns the linkage distance between them."
  },
  {
    "objectID": "schedule/slides/28-hclust.html#agglomerative-clustering-given-the-linkage",
    "href": "schedule/slides/28-hclust.html#agglomerative-clustering-given-the-linkage",
    "title": "UBC Stat406 2024W",
    "section": "Agglomerative clustering, given the linkage",
    "text": "Agglomerative clustering, given the linkage\n\nStart with each point in its own group\nUntil there is only one cluster, repeatedly merge the two groups \\(G,H\\) that minimize \\(d(G,H)\\).\n\n\n\n\n\n\n\nImportant\n\n\n\\(d\\) measures the distance between GROUPS."
  },
  {
    "objectID": "schedule/slides/28-hclust.html#single-linkage",
    "href": "schedule/slides/28-hclust.html#single-linkage",
    "title": "UBC Stat406 2024W",
    "section": "Single linkage",
    "text": "Single linkage\nIn single linkage (a.k.a nearest-neighbor linkage), the linkage distance between \\(G,\\ H\\) is the smallest dissimilarity between two points in different groups: \\[d_{\\textrm{single}}(G,H) = \\min_{i \\in G, \\, j \\in H} d_{ij}\\]"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#complete-linkage",
    "href": "schedule/slides/28-hclust.html#complete-linkage",
    "title": "UBC Stat406 2024W",
    "section": "Complete linkage",
    "text": "Complete linkage\nIn complete linkage (i.e. farthest-neighbor linkage), linkage distance between \\(G,H\\) is the largest dissimilarity between two points in different clusters: \\[d_{\\textrm{complete}}(G,H) = \\max_{i \\in G,\\, j \\in H} d_{ij}.\\]"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#average-linkage",
    "href": "schedule/slides/28-hclust.html#average-linkage",
    "title": "UBC Stat406 2024W",
    "section": "Average linkage",
    "text": "Average linkage\nIn average linkage, the linkage distance between \\(G,H\\) is the average dissimilarity over all points in different clusters: \\[d_{\\textrm{average}}(G,H) = \\frac{1}{|G| \\cdot |H| }\\sum_{i \\in G, \\,j \\in H} d_{ij}.\\]"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#common-properties",
    "href": "schedule/slides/28-hclust.html#common-properties",
    "title": "UBC Stat406 2024W",
    "section": "Common properties",
    "text": "Common properties\nSingle, complete, and average linkage share the following:\n\nThey all operate on the dissimilarities \\(d_{ij}\\).\nThis means that the points we are clustering can be quite general (number of mutations on a genome, polygons, faces, whatever).\nRunning agglomerative clustering with any of these linkages produces a dendrogram with no inversions\n“No inversions” means that the linkage distance between merged clusters only increases as we run the algorithm.\n\nIn other words, we can draw a proper dendrogram, where the height of a parent is always higher than the height of either daughter.\n(We’ll return to this again shortly)"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#centroid-linkage",
    "href": "schedule/slides/28-hclust.html#centroid-linkage",
    "title": "UBC Stat406 2024W",
    "section": "Centroid linkage",
    "text": "Centroid linkage\nCentroid linkage is relatively new. We need \\(x_i \\in \\mathbb{R}^p\\).\n\\(\\overline{x}_G\\) and \\(\\overline{x}_H\\) are group averages\n\\(d_{\\textrm{centroid}} = ||\\overline{x}_G - \\overline{x}_H||_2^2\\)"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#centroid-linkage-1",
    "href": "schedule/slides/28-hclust.html#centroid-linkage-1",
    "title": "UBC Stat406 2024W",
    "section": "Centroid linkage",
    "text": "Centroid linkage\n\n\nCentroid linkage is\n\n… quite intuitive\n… nicely analogous to \\(K\\)-means.\n… very related to average linkage (and much, much faster)\n\nHowever, it may introduce inversions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ntt &lt;- seq(0, 2 * pi, len = 50)\ntt2 &lt;- seq(0, 2 * pi, len = 75)\nc1 &lt;- tibble(x = cos(tt), y = sin(tt))\nc2 &lt;- tibble(x = 1.5 * cos(tt2), y = 1.5 * sin(tt2))\ncircles &lt;- bind_rows(c1, c2)\ndi &lt;- dist(circles[, 1:2])\nhc &lt;- hclust(di, method = \"centroid\")\npar(mar = c(.1, 5, 3, .1))\nplot(hc, xlab = \"\")"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#shortcomings-of-some-linkages",
    "href": "schedule/slides/28-hclust.html#shortcomings-of-some-linkages",
    "title": "UBC Stat406 2024W",
    "section": "Shortcomings of some linkages",
    "text": "Shortcomings of some linkages\n\nSingle\n\n👎 chaining — a single pair of close points merges two clusters. \\(\\Rightarrow\\) clusters can be too spread out, not compact\n\nComplete linkage\n\n👎 crowding — a point can be closer to points in other clusters than to points in its own cluster.\\(\\Rightarrow\\) clusters are compact, not far enough apart.\n\nAverage linkage\n\ntries to strike a balance these\n\n\n👎 Unclear what properties the resulting clusters have when we cut an average linkage tree.\n\n\n👎 Results change with a monotone increasing transformation of the dissimilarities\n\nCentroid linkage\n\n👎 same monotonicity problem\n\n\n👎 and inversions\n\nAll linkages\n\n⁇ where do we cut?"
  },
  {
    "objectID": "schedule/slides/28-hclust.html#distances",
    "href": "schedule/slides/28-hclust.html#distances",
    "title": "UBC Stat406 2024W",
    "section": "Distances",
    "text": "Distances\nNote how all the methods depend on the distance function\nCan do lots of things besides Euclidean\nThis is very important"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#section",
    "href": "schedule/slides/07-greedy-selection.html#section",
    "title": "UBC Stat406 2024W",
    "section": "07 Greedy selection",
    "text": "07 Greedy selection\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 18 September 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\]"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#recap",
    "href": "schedule/slides/07-greedy-selection.html#recap",
    "title": "UBC Stat406 2024W",
    "section": "Recap",
    "text": "Recap\nModel Selection means select a family of distributions for your data.\nIdeally, we’d do this by comparing the \\(R_n\\) for one family with that for another.\nWe’d use whichever has smaller \\(R_n\\).\nBut \\(R_n\\) depends on the truth, so we estimate it with \\(\\widehat{R}\\).\nThen we use whichever has smaller \\(\\widehat{R}\\)."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#example",
    "href": "schedule/slides/07-greedy-selection.html#example",
    "title": "UBC Stat406 2024W",
    "section": "Example",
    "text": "Example\nThe truth:\n\ndat &lt;- tibble(\n  x1 = rnorm(100), \n  x2 = rnorm(100),\n  y = 3 + x1 - 5 * x2 + sin(x1 * x2 / (2 * pi)) + rnorm(100, sd = 5)\n)\n\nModel 1: y ~ x1 + x2\nModel 2: y ~ x1 + x2 + x1*x2\nModel 3: y ~ x2 + sin(x1 * x2)\n\n(What are the families for each of these?)"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#fit-each-model-and-estimate-r_n",
    "href": "schedule/slides/07-greedy-selection.html#fit-each-model-and-estimate-r_n",
    "title": "UBC Stat406 2024W",
    "section": "Fit each model and estimate \\(R_n\\)",
    "text": "Fit each model and estimate \\(R_n\\)\n\nforms &lt;- list(\"y ~ x1 + x2\", \"y ~ x1 * x2\", \"y ~ x2 + sin(x1*x2)\") |&gt; \n  map(as.formula)\nfits &lt;- map(forms, ~ lm(.x, data = dat))\nmap(fits, ~ tibble(\n  R2 = summary(.x)$r.sq,\n  training_error = mean(residuals(.x)^2),\n  loocv = mean( (residuals(.x) / (1 - hatvalues(.x)))^2 ),\n  AIC = AIC(.x),\n  BIC = BIC(.x)\n)) |&gt; list_rbind()\n\n# A tibble: 3 × 5\n     R2 training_error loocv   AIC   BIC\n  &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.589           21.3  22.9  598.  608.\n2 0.595           21.0  23.4  598.  611.\n3 0.586           21.4  23.0  598.  609."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#model-selection-vs.-variable-selection",
    "href": "schedule/slides/07-greedy-selection.html#model-selection-vs.-variable-selection",
    "title": "UBC Stat406 2024W",
    "section": "Model Selection vs. Variable Selection",
    "text": "Model Selection vs. Variable Selection\nModel selection is very comprehensive\nYou choose a full statistical model (probability distribution) that will be hypothesized to have generated the data.\nVariable selection is a subset of this. It means\n\nchoosing which predictors to include in a predictive model\n\nEliminating a predictor, means removing it from the model.\nSome procedures automatically search predictors, and eliminate some.\nWe call this variable selection. But the procedure is implicitly selecting a model as well.\n\nMaking this all the more complicated, with lots of effort, we can map procedures/algorithms to larger classes of probability models, and analyze them."
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#selecting-variables-predictors-with-linear-methods",
    "href": "schedule/slides/07-greedy-selection.html#selecting-variables-predictors-with-linear-methods",
    "title": "UBC Stat406 2024W",
    "section": "Selecting variables / predictors with linear methods",
    "text": "Selecting variables / predictors with linear methods\n\n\nSuppose we have a pile of predictors.\nWe estimate models with different subsets of predictors and use CV / Cp / AIC / BIC to decide which is preferred.\nSometimes you might have a few plausible subsets. Easy enough to choose with our criterion.\nSometimes you might just have a bunch of predictors, then what do you do?\n\n\n\nAll subsets\n\nestimate model based on every possible subset of size \\(|\\mathcal{S}| \\leq \\min\\{n, p\\}\\), use one with lowest risk estimate\n\nForward selection\n\nstart with \\(\\mathcal{S}=\\varnothing\\), add predictors greedily\n\nBackward selection\n\nstart with \\(\\mathcal{S}=\\{1,\\ldots,p\\}\\), remove greedily\n\nHybrid\n\ncombine forward and backward smartly"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#costs-and-benefits",
    "href": "schedule/slides/07-greedy-selection.html#costs-and-benefits",
    "title": "UBC Stat406 2024W",
    "section": "Costs and benefits",
    "text": "Costs and benefits\n\nAll subsets\n\n👍 estimates each subset\n💣 takes \\(2^p\\) model fits when \\(p&lt;n\\). If \\(p=50\\), this is about \\(10^{15}\\) models.\n\nForward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n\nBackward selection\n\n👍 computationally feasible\n💣 ignores some models, correlated predictors means bad performance\n💣 doesn’t work if \\(p&gt;n\\)\n\nHybrid\n\n👍 visits more models than forward/backward\n💣 slower"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#synthetic-example",
    "href": "schedule/slides/07-greedy-selection.html#synthetic-example",
    "title": "UBC Stat406 2024W",
    "section": "Synthetic example",
    "text": "Synthetic example\n\nset.seed(123)\nn &lt;- 406\ndf &lt;- tibble( # like data.frame, but columns can be functions of preceding\n  x1 = rnorm(n),\n  x2 = rnorm(n, mean = 2, sd = 1),\n  x3 = rexp(n, rate = 1),\n  x4 = x2 + rnorm(n, sd = .1), # correlated with x2\n  x5 = x1 + rnorm(n, sd = .1), # correlated with x1\n  x6 = x1 - x2 + rnorm(n, sd = .1), # correlated with x2 and x1 (and others)\n  x7 = x1 + x3 + rnorm(n, sd = .1), # correlated with x1 and x3 (and others)\n  y = x1 * 3 + x2 / 3 + rnorm(n, sd = 2.2) # function of x1 and x2 only\n)\n\n\n\\(\\mathbf{x}_1\\) and \\(\\mathbf{x}_2\\) are the true predictors\nBut the rest are correlated with them"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#full-model",
    "href": "schedule/slides/07-greedy-selection.html#full-model",
    "title": "UBC Stat406 2024W",
    "section": "Full model",
    "text": "Full model\n\nfull &lt;- lm(y ~ ., data = df)\nsummary(full)\n\n\nCall:\nlm(formula = y ~ ., data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7739 -1.4283 -0.0929  1.4257  7.5869 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  0.03383    0.27700   0.122  0.90287   \nx1           6.70481    2.06743   3.243  0.00128 **\nx2          -0.43945    1.71650  -0.256  0.79807   \nx3           1.37293    1.11524   1.231  0.21903   \nx4          -1.19911    1.17850  -1.017  0.30954   \nx5          -0.53918    1.07089  -0.503  0.61490   \nx6          -1.88547    1.21652  -1.550  0.12196   \nx7          -1.25245    1.10743  -1.131  0.25876   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.231 on 398 degrees of freedom\nMultiple R-squared:  0.6411,    Adjusted R-squared:  0.6347 \nF-statistic: 101.5 on 7 and 398 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#true-model",
    "href": "schedule/slides/07-greedy-selection.html#true-model",
    "title": "UBC Stat406 2024W",
    "section": "True model",
    "text": "True model\n\ntruth &lt;- lm(y ~ x1 + x2, data = df)\nsummary(truth)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4519 -1.3873 -0.1941  1.3498  7.5533 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.1676     0.2492   0.673   0.5015    \nx1            3.0316     0.1146  26.447   &lt;2e-16 ***\nx2            0.2447     0.1109   2.207   0.0279 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.233 on 403 degrees of freedom\nMultiple R-squared:  0.6357,    Adjusted R-squared:  0.6339 \nF-statistic: 351.6 on 2 and 403 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#all-subsets",
    "href": "schedule/slides/07-greedy-selection.html#all-subsets",
    "title": "UBC Stat406 2024W",
    "section": "All subsets",
    "text": "All subsets\n\nlibrary(leaps)\ntrythemall &lt;- regsubsets(y ~ ., data = df)\nsummary(trythemall)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df)\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: exhaustive\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp",
    "title": "UBC Stat406 2024W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(trythemall)$bic, \n  Cp = summary(trythemall)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) + \n  geom_point() + \n  geom_line() + \n  facet_wrap(~name, scales = \"free_y\") + \n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#forward-stepwise",
    "href": "schedule/slides/07-greedy-selection.html#forward-stepwise",
    "title": "UBC Stat406 2024W",
    "section": "Forward stepwise",
    "text": "Forward stepwise\n\nstepup &lt;- regsubsets(y ~ ., data = df, method = \"forward\")\nsummary(stepup)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"forward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: forward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp-1",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp-1",
    "title": "UBC Stat406 2024W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(stepup)$bic,\n  Cp = summary(stepup)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange),\n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#backward-selection",
    "href": "schedule/slides/07-greedy-selection.html#backward-selection",
    "title": "UBC Stat406 2024W",
    "section": "Backward selection",
    "text": "Backward selection\n\nstepdown &lt;- regsubsets(y ~ ., data = df, method = \"backward\")\nsummary(stepdown)\n\nSubset selection object\nCall: regsubsets.formula(y ~ ., data = df, method = \"backward\")\n7 Variables  (and intercept)\n   Forced in Forced out\nx1     FALSE      FALSE\nx2     FALSE      FALSE\nx3     FALSE      FALSE\nx4     FALSE      FALSE\nx5     FALSE      FALSE\nx6     FALSE      FALSE\nx7     FALSE      FALSE\n1 subsets of each size up to 7\nSelection Algorithm: backward\n         x1  x2  x3  x4  x5  x6  x7 \n1  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \" \" \" \"\n2  ( 1 ) \"*\" \" \" \" \" \" \" \" \" \"*\" \" \"\n3  ( 1 ) \"*\" \" \" \" \" \"*\" \" \" \"*\" \" \"\n4  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \" \"\n5  ( 1 ) \"*\" \" \" \"*\" \"*\" \" \" \"*\" \"*\"\n6  ( 1 ) \"*\" \" \" \"*\" \"*\" \"*\" \"*\" \"*\"\n7  ( 1 ) \"*\" \"*\" \"*\" \"*\" \"*\" \"*\" \"*\""
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#bic-and-cp-2",
    "href": "schedule/slides/07-greedy-selection.html#bic-and-cp-2",
    "title": "UBC Stat406 2024W",
    "section": "BIC and Cp",
    "text": "BIC and Cp\n\n\ntibble(\n  BIC = summary(stepdown)$bic,\n  Cp = summary(stepdown)$cp,\n  size = 1:7\n) |&gt;\n  pivot_longer(-size) |&gt;\n  ggplot(aes(size, value, colour = name)) +\n  geom_point() +\n  geom_line() +\n  facet_wrap(~name, scales = \"free_y\") +\n  ylab(\"\") +\n  scale_colour_manual(\n    values = c(blue, orange), \n    guide = \"none\"\n  )"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#section-1",
    "href": "schedule/slides/07-greedy-selection.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "somehow, for this seed, everything is the same"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#randomness-and-prediction-error",
    "href": "schedule/slides/07-greedy-selection.html#randomness-and-prediction-error",
    "title": "UBC Stat406 2024W",
    "section": "Randomness and prediction error",
    "text": "Randomness and prediction error\nAll of that was for one data set.\nDoesn’t say which procedure is better generally.\nIf we want to know how they compare generally, we should repeat many times\n\nGenerate training data\nEstimate with different algorithms\nPredict held-out set data\nExamine prediction MSE (on held-out set)\n\n\nI’m not going to do all subsets, just the truth, forward selection, backward, and the full model\nFor forward/backward selection, I’ll use Cp to choose the final size"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#code-for-simulation",
    "href": "schedule/slides/07-greedy-selection.html#code-for-simulation",
    "title": "UBC Stat406 2024W",
    "section": "Code for simulation",
    "text": "Code for simulation\n… Annoyingly, no predict method for regsubsets, so we make one.\n\npredict.regsubsets &lt;- function(object, newdata, risk_estimate = c(\"cp\", \"bic\"), ...) {\n  risk_estimate &lt;- match.arg(risk_estimate)\n  chosen &lt;- coef(object, which.min(summary(object)[[risk_estimate]]))\n  predictors &lt;- names(chosen)\n  if (object$intercept) predictors &lt;- predictors[-1]\n  X &lt;- newdata[, predictors]\n  if (object$intercept) X &lt;- cbind2(1, X)\n  drop(as.matrix(X) %*% chosen)\n}"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#section-2",
    "href": "schedule/slides/07-greedy-selection.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "simulate_and_estimate_them_all &lt;- function(n = 406) {\n  N &lt;- 2 * n # generate 2x the amount of data (half train, half test)\n  df &lt;- tibble( # generate data\n    x1 = rnorm(N), \n    x2 = rnorm(N, mean = 2), \n    x3 = rexp(N),\n    x4 = x2 + rnorm(N, sd = .1), \n    x5 = x1 + rnorm(N, sd = .1),\n    x6 = x1 - x2 + rnorm(N, sd = .1), \n    x7 = x1 + x3 + rnorm(N, sd = .1),\n    y = x1 * 3 + x2 / 3 + rnorm(N, sd = 2.2)\n  )\n  train &lt;- df[1:n, ] # half the data for training\n  test &lt;- df[(n + 1):N, ] # half the data for evaluation\n  \n  oracle &lt;- lm(y ~ x1 + x2 - 1, data = train) # knowing the right model, not the coefs\n  full &lt;- lm(y ~ ., data = train)\n  stepup &lt;- regsubsets(y ~ ., data = train, method = \"forward\")\n  stepdown &lt;- regsubsets(y ~ ., data = train, method = \"backward\")\n  \n  tibble(\n    y = test$y,\n    oracle = predict(oracle, newdata = test),\n    full = predict(full, newdata = test),\n    stepup = predict(stepup, newdata = test),\n    stepdown = predict(stepdown, newdata = test),\n    truth = drop(as.matrix(test[, c(\"x1\", \"x2\")]) %*% c(3, 1/3))\n  )\n}\n\nset.seed(12345)\nour_sim &lt;- map(1:50, ~ simulate_and_estimate_them_all(406)) |&gt;\n  list_rbind(names_to = \"sim\")"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#what-is-oracle",
    "href": "schedule/slides/07-greedy-selection.html#what-is-oracle",
    "title": "UBC Stat406 2024W",
    "section": "What is “Oracle”",
    "text": "What is “Oracle”"
  },
  {
    "objectID": "schedule/slides/07-greedy-selection.html#results",
    "href": "schedule/slides/07-greedy-selection.html#results",
    "title": "UBC Stat406 2024W",
    "section": "Results",
    "text": "Results\n\n\nour_sim |&gt; \n  group_by(sim) %&gt;%\n  summarise(\n    across(oracle:truth, ~ mean((y - .)^2)), \n    .groups = \"drop\"\n  ) %&gt;%\n  transmute(across(oracle:stepdown, ~ . / truth - 1)) |&gt; \n  pivot_longer(\n    everything(), \n    names_to = \"method\", \n    values_to = \"mse\"\n  ) |&gt; \n  ggplot(aes(method, mse, fill = method)) +\n  geom_boxplot(notch = TRUE) +\n  geom_hline(yintercept = 0, linewidth = 2) +\n  scale_fill_viridis_d() +\n  theme(legend.position = \"none\") +\n  scale_y_continuous(\n    labels = scales::label_percent()\n  ) +\n  ylab(\"% increase in mse relative\\n to the truth\")"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#section",
    "href": "schedule/slides/02-lm-example.html#section",
    "title": "UBC Stat406 2024W",
    "section": "02 Linear model example",
    "text": "02 Linear model example\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 16 September 2024\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#economic-mobility",
    "href": "schedule/slides/02-lm-example.html#economic-mobility",
    "title": "UBC Stat406 2024W",
    "section": "Economic mobility",
    "text": "Economic mobility\n\ndata(\"mobility\", package = \"Stat406\")\nmobility\n\n# A tibble: 741 × 43\n      ID Name        Mobility State Population Urban Black Seg_racial Seg_income\n   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1   100 Johnson Ci…   0.0622 TN        576081     1 0.021      0.09       0.035\n 2   200 Morristown    0.0537 TN        227816     1 0.02       0.093      0.026\n 3   301 Middlesbor…   0.0726 TN         66708     0 0.015      0.064      0.024\n 4   302 Knoxville     0.0563 TN        727600     1 0.056      0.21       0.092\n 5   401 Winston-Sa…   0.0448 NC        493180     1 0.174      0.262      0.072\n 6   402 Martinsvil…   0.0518 VA         92753     0 0.224      0.137      0.024\n 7   500 Greensboro    0.0474 NC       1055133     1 0.218      0.22       0.068\n 8   601 North Wilk…   0.0517 NC         90016     0 0.032      0.114      0.012\n 9   602 Galax         0.0796 VA         64676     0 0.029      0.131      0.005\n10   700 Spartanburg   0.0431 SC        354533     1 0.207      0.139      0.045\n# ℹ 731 more rows\n# ℹ 34 more variables: Seg_poverty &lt;dbl&gt;, Seg_affluence &lt;dbl&gt;, Commute &lt;dbl&gt;,\n#   Income &lt;dbl&gt;, Gini &lt;dbl&gt;, Share01 &lt;dbl&gt;, Gini_99 &lt;dbl&gt;, Middle_class &lt;dbl&gt;,\n#   Local_tax_rate &lt;dbl&gt;, Local_gov_spending &lt;dbl&gt;, Progressivity &lt;dbl&gt;,\n#   EITC &lt;dbl&gt;, School_spending &lt;dbl&gt;, Student_teacher_ratio &lt;dbl&gt;,\n#   Test_scores &lt;dbl&gt;, HS_dropout &lt;dbl&gt;, Colleges &lt;dbl&gt;, Tuition &lt;dbl&gt;,\n#   Graduation &lt;dbl&gt;, Labor_force_participation &lt;dbl&gt;, Manufacturing &lt;dbl&gt;, …\n\n\n\nNote how many observations and predictors it has.\nWe’ll use Mobility as the response"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#a-linear-model",
    "href": "schedule/slides/02-lm-example.html#a-linear-model",
    "title": "UBC Stat406 2024W",
    "section": "A linear model",
    "text": "A linear model\n\\[\\mbox{Mobility}_i = \\beta_0 + \\beta_1 \\, \\mbox{State}_i + \\beta_2 \\, \\mbox{Urban}_i + \\cdots + \\epsilon_i\\]\nor equivalently\n\\[E \\left[ \\biggl. \\mbox{mobility} \\, \\biggr| \\, \\mbox{State}, \\mbox{Urban},\n    \\ldots \\right]  = \\beta_0 + \\beta_1 \\, \\mbox{State} +\n    \\beta_2 \\, \\mbox{Urban} + \\cdots\\]"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#analysis",
    "href": "schedule/slides/02-lm-example.html#analysis",
    "title": "UBC Stat406 2024W",
    "section": "Analysis",
    "text": "Analysis\n\nRandomly split into a training (say 3/4) and a test set (1/4)\nUse training set to fit a model\nFit the “full” model\n“Look” at the fit\n\n\n\nset.seed(20220914)\nmob &lt;- mobility[complete.cases(mobility), ] |&gt;\n    select(-Name, -ID, -State)\nn &lt;- nrow(mob)\nset &lt;- sample.int(n, floor(n * .75), FALSE)\ntrain &lt;- mob[set, ]\ntest &lt;- mob[setdiff(1:n, set), ]\nfull &lt;- lm(Mobility ~ ., data = train)\n\n\nWhy don’t we include Name or ID?"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#results",
    "href": "schedule/slides/02-lm-example.html#results",
    "title": "UBC Stat406 2024W",
    "section": "Results",
    "text": "Results\n(dispatch happening here!)\n\nsummary(full)\n\n\nCall:\nlm(formula = Mobility ~ ., data = train)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.072092 -0.010256 -0.001452  0.009170  0.090428 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                1.849e-01  8.083e-02   2.288 0.022920 *  \nPopulation                 3.378e-09  2.478e-09   1.363 0.173916    \nUrban                      2.853e-03  3.892e-03   0.733 0.464202    \nBlack                      7.807e-02  2.859e-02   2.731 0.006735 ** \nSeg_racial                -5.626e-02  1.780e-02  -3.160 0.001754 ** \nSeg_income                 8.677e-01  9.355e-01   0.928 0.354453    \nSeg_poverty               -7.416e-01  5.014e-01  -1.479 0.140316    \nSeg_affluence             -2.224e-01  4.763e-01  -0.467 0.640874    \nCommute                    6.313e-02  2.838e-02   2.225 0.026915 *  \nIncome                     4.207e-07  6.997e-07   0.601 0.548112    \nGini                       3.592e+00  3.357e+00   1.070 0.285578    \nShare01                   -3.635e-02  3.357e-02  -1.083 0.279925    \nGini_99                   -3.657e+00  3.356e+00  -1.090 0.276704    \nMiddle_class               1.031e-01  4.835e-02   2.133 0.033828 *  \nLocal_tax_rate             2.268e-01  2.620e-01   0.866 0.387487    \nLocal_gov_spending         1.273e-07  3.016e-06   0.042 0.966374    \nProgressivity              4.983e-03  1.324e-03   3.764 0.000205 ***\nEITC                      -3.324e-04  4.528e-04  -0.734 0.463549    \nSchool_spending           -9.019e-04  2.272e-03  -0.397 0.691658    \nStudent_teacher_ratio     -1.639e-03  1.123e-03  -1.459 0.145748    \nTest_scores                2.487e-04  3.137e-04   0.793 0.428519    \nHS_dropout                -1.698e-01  9.352e-02  -1.816 0.070529 .  \nColleges                  -2.811e-02  7.661e-02  -0.367 0.713942    \nTuition                    3.459e-07  4.362e-07   0.793 0.428417    \nGraduation                -1.702e-02  1.425e-02  -1.194 0.233650    \nLabor_force_participation -7.850e-02  5.405e-02  -1.452 0.147564    \nManufacturing             -1.605e-01  2.816e-02  -5.700  3.1e-08 ***\nChinese_imports           -5.165e-04  1.004e-03  -0.514 0.607378    \nTeenage_labor             -1.019e+00  2.111e+00  -0.483 0.629639    \nMigration_in               4.490e-02  3.480e-01   0.129 0.897436    \nMigration_out             -4.475e-01  4.093e-01  -1.093 0.275224    \nForeign_born               9.137e-02  5.494e-02   1.663 0.097454 .  \nSocial_capital            -1.114e-03  2.728e-03  -0.408 0.683245    \nReligious                  4.570e-02  1.298e-02   3.520 0.000506 ***\nViolent_crime             -3.393e+00  1.622e+00  -2.092 0.037373 *  \nSingle_mothers            -3.590e-01  9.442e-02  -3.802 0.000177 ***\nDivorced                   1.707e-02  1.603e-01   0.107 0.915250    \nMarried                   -5.894e-02  7.246e-02  -0.813 0.416720    \nLongitude                 -4.239e-05  2.239e-04  -0.189 0.850001    \nLatitude                   6.725e-04  5.687e-04   1.182 0.238037    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02128 on 273 degrees of freedom\nMultiple R-squared:  0.7808,    Adjusted R-squared:  0.7494 \nF-statistic: 24.93 on 39 and 273 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#diagnostic-plots",
    "href": "schedule/slides/02-lm-example.html#diagnostic-plots",
    "title": "UBC Stat406 2024W",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\nNB: the line in the QQ plot isn’t right for either geom_qq_line or plot.lm…\n\n\nstuff &lt;- tibble(\n  residuals = residuals(full),\n  fitted = fitted(full),\n  stdresiduals = rstandard(full)\n)\nggplot(stuff, aes(fitted, residuals)) +\n  geom_point() +\n  geom_smooth(\n    se = FALSE,\n    colour = \"steelblue\",\n    linewidth = 2\n  ) +\n  ggtitle(\"Residuals vs Fitted\")\n\n\n\n\n\n\n\n\n\n\n\nggplot(stuff, aes(sample = stdresiduals)) +\n  geom_qq(size = 2) +\n  geom_qq_line(linewidth = 2, color = \"steelblue\") +\n  labs(\n    x = \"Theoretical quantiles\",\n    y = \"Standardized residuals\",\n    title = \"Normal Q-Q\"\n  )"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#can-also-just-use-dispatched-plot",
    "href": "schedule/slides/02-lm-example.html#can-also-just-use-dispatched-plot",
    "title": "UBC Stat406 2024W",
    "section": "Can also just use dispatched plot",
    "text": "Can also just use dispatched plot\n\n\nplot(full, 1)\n\n\n\n\n\n\n\n\n\n\n\nplot(full, 2)"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#fit-a-reduced-model",
    "href": "schedule/slides/02-lm-example.html#fit-a-reduced-model",
    "title": "UBC Stat406 2024W",
    "section": "Fit a reduced model",
    "text": "Fit a reduced model\n\nreduced &lt;- lm(\n  Mobility ~ Commute + Gini_99 + Test_scores + HS_dropout +\n    Manufacturing + Migration_in + Religious + Single_mothers,\n  data = train\n)\n\nsummary(reduced)$coefficients \n\n                    Estimate  Std. Error   t value     Pr(&gt;|t|)\n(Intercept)     0.1663344179 0.017769995  9.360409 1.829270e-18\nCommute         0.0637329409 0.014926382  4.269819 2.618234e-05\nGini_99        -0.1086058241 0.038958986 -2.787696 5.642726e-03\nTest_scores     0.0004997645 0.000256038  1.951915 5.186618e-02\nHS_dropout     -0.2162067301 0.082003195 -2.636565 8.805228e-03\nManufacturing  -0.1594229237 0.020215791 -7.886059 5.647668e-14\nMigration_in   -0.3891567027 0.171839168 -2.264657 2.423771e-02\nReligious       0.0435673365 0.010463920  4.163577 4.084854e-05\nSingle_mothers -0.2864269552 0.046578928 -6.149282 2.444903e-09\n\nreduced |&gt;\n  broom::glance() |&gt;\n  print(width = 120)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared  sigma statistic  p.value    df logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.718         0.711 0.0229      96.9 5.46e-79     8   743. -1466. -1429.\n  deviance df.residual  nobs\n     &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1    0.159         304   313"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#diagnostic-plots-for-reduced-model",
    "href": "schedule/slides/02-lm-example.html#diagnostic-plots-for-reduced-model",
    "title": "UBC Stat406 2024W",
    "section": "Diagnostic plots for reduced model",
    "text": "Diagnostic plots for reduced model\n\n\nplot(reduced, 1)\n\n\n\n\n\n\n\n\n\n\n\nplot(reduced, 2)"
  },
  {
    "objectID": "schedule/slides/02-lm-example.html#how-do-we-decide-which-model-is-better",
    "href": "schedule/slides/02-lm-example.html#how-do-we-decide-which-model-is-better",
    "title": "UBC Stat406 2024W",
    "section": "How do we decide which model is better?",
    "text": "How do we decide which model is better?\n\n\n\nGoodness of fit versus prediction power\n\n\nmap( # smaller AIC is better\n  list(full = full, reduced = reduced),\n  ~ c(aic = AIC(.x), rsq = summary(.x)$r.sq)\n)\n\n$full\n          aic           rsq \n-1482.5981023     0.7807509 \n\n$reduced\n         aic          rsq \n-1466.088492     0.718245 \n\n\n\nUse both models to predict Mobility\nCompare both sets of predictions\n\n\n\n\nmses &lt;- function(preds, obs) round(mean((obs - preds)^2), 5)\nc(\n  full = mses(\n    predict(full, newdata = test),\n    test$Mobility\n  ),\n  reduced = mses(\n    predict(reduced, newdata = test),\n    test$Mobility\n  )\n)\n\n   full reduced \n0.00072 0.00084 \n\n\n\n\nCode\ntest$full &lt;- predict(full, newdata = test)\ntest$reduced &lt;- predict(reduced, newdata = test)\ntest |&gt;\n  select(Mobility, full, reduced) |&gt;\n  pivot_longer(-Mobility) |&gt;\n  ggplot(aes(Mobility, value)) +\n  geom_point(color = \"orange\") +\n  facet_wrap(~name, 2) +\n  xlab(\"observed mobility\") +\n  ylab(\"predicted mobility\") +\n  geom_abline(slope = 1, intercept = 0, colour = \"darkblue\")"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#section",
    "href": "schedule/slides/17-nonlinear-classifiers.html#section",
    "title": "UBC Stat406 2024W",
    "section": "17 Nonlinear classifiers",
    "text": "17 Nonlinear classifiers\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 30 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#last-time",
    "href": "schedule/slides/17-nonlinear-classifiers.html#last-time",
    "title": "UBC Stat406 2024W",
    "section": "Last time",
    "text": "Last time\nWe reviewed logistic regression\n\\[\\begin{aligned}\nPr(Y = 1 \\given X=x)  & = \\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}} \\\\\nPr(Y = 0 \\given X=x) & = \\frac{1}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}=1-\\frac{\\exp\\{\\beta_0 + \\beta^{\\top}x\\}}{1 + \\exp\\{\\beta_0 + \\beta^{\\top}x\\}}\\end{aligned}\\]"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#make-it-nonlinear",
    "href": "schedule/slides/17-nonlinear-classifiers.html#make-it-nonlinear",
    "title": "UBC Stat406 2024W",
    "section": "Make it nonlinear",
    "text": "Make it nonlinear\nWe can make LDA or logistic regression have non-linear decision boundaries by mapping the features to a higher dimension (just like with regular regression)\nSay:\nPolynomials\n\\((x_1, x_2) \\mapsto \\left(1,\\ x_1,\\ x_1^2,\\ x_2,\\ x_2^2,\\ x_1 x_2\\right)\\)\n\ndat1 &lt;- generate_lda_2d(100, Sigma = .5 * diag(2)) |&gt; mutate(y = as.factor(y))\nlogit_poly &lt;- glm(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1, family = \"binomial\")\nlda_poly &lt;- lda(y ~ x1 * x2 + I(x1^2) + I(x2^2), dat1)"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#visualizing-the-classification-boundary",
    "href": "schedule/slides/17-nonlinear-classifiers.html#visualizing-the-classification-boundary",
    "title": "UBC Stat406 2024W",
    "section": "Visualizing the classification boundary",
    "text": "Visualizing the classification boundary\n\n\nCode\nlibrary(cowplot)\ngr &lt;- expand_grid(x1 = seq(-2.5, 3, length.out = 100), x2 = seq(-2.5, 3, length.out = 100))\npts_logit &lt;- predict(logit_poly, gr)\npts_lda &lt;- predict(lda_poly, gr)\ng0 &lt;- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts_logit), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_viridis_b(n.breaks = 6, alpha = .5, name = \"log odds\") +\n  ggtitle(\"Polynomial logit\") +\n  theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\"))\ng1 &lt;- ggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = pts_lda$x), aes(x1, x2, fill = disc)) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_viridis_b(n.breaks = 6, alpha = .5, name = bquote(delta[1] - delta[0])) +\n  ggtitle(\"Polynomial lda\") +\n  theme(legend.position = \"bottom\", legend.key.width = unit(1.5, \"cm\"))\nplot_grid(g0, g1)\n\n\n\nA linear decision boundary in the higher-dimensional space corresponds to a non-linear decision boundary in low dimensions."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#trees-reforestation",
    "href": "schedule/slides/17-nonlinear-classifiers.html#trees-reforestation",
    "title": "UBC Stat406 2024W",
    "section": "Trees (reforestation)",
    "text": "Trees (reforestation)\n\n\nWe saw regression trees last module\nClassification trees are\n\nMore natural\nSlightly different computationally\n\nEverything else is pretty much the same"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#axis-parallel-splits",
    "href": "schedule/slides/17-nonlinear-classifiers.html#axis-parallel-splits",
    "title": "UBC Stat406 2024W",
    "section": "Axis-parallel splits",
    "text": "Axis-parallel splits\nLike with regression trees, classification trees operate by greedily splitting the predictor space\n\n\n\nnames(bakeoff)\n\n [1] \"winners\"                  \n [2] \"series\"                   \n [3] \"age\"                      \n [4] \"occupation\"               \n [5] \"hometown\"                 \n [6] \"percent_star\"             \n [7] \"percent_technical_wins\"   \n [8] \"percent_technical_bottom3\"\n [9] \"percent_technical_top3\"   \n[10] \"technical_highest\"        \n[11] \"technical_lowest\"         \n[12] \"technical_median\"         \n[13] \"judge1\"                   \n[14] \"judge2\"                   \n[15] \"viewers_7day\"             \n[16] \"viewers_28day\"            \n\n\n\nsmalltree &lt;- tree(\n  winners ~ technical_median + percent_star,\n  data = bakeoff\n)\n\n\n\n\n\nCode\npar(mar = c(5, 5, 0, 0) + .1)\nplot(bakeoff$technical_median, bakeoff$percent_star,\n  pch = c(\"-\", \"+\")[bakeoff$winners + 1], cex = 2, bty = \"n\", las = 1,\n  ylab = \"% star baker\", xlab = \"times above median in technical\",\n  col = orange, cex.axis = 2, cex.lab = 2\n)\npartition.tree(smalltree,\n  add = TRUE, col = blue,\n  ordvars = c(\"technical_median\", \"percent_star\")\n)"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#when-do-trees-do-well",
    "href": "schedule/slides/17-nonlinear-classifiers.html#when-do-trees-do-well",
    "title": "UBC Stat406 2024W",
    "section": "When do trees do well?",
    "text": "When do trees do well?\n\n\n\n\n\n2D example\nTop Row:\ntrue decision boundary is linear\n🍎 linear classifier\n👎 tree with axis-parallel splits\nBottom Row:\ntrue decision boundary is non-linear\n🤮 A linear classifier can’t capture the true decision boundary\n🍎 decision tree is successful."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#how-do-we-build-a-tree",
    "href": "schedule/slides/17-nonlinear-classifiers.html#how-do-we-build-a-tree",
    "title": "UBC Stat406 2024W",
    "section": "How do we build a tree?",
    "text": "How do we build a tree?\n\nDivide the predictor space into \\(J\\) non-overlapping regions \\(R_1, \\ldots, R_J\\)\n\n\nthis is done via greedy, recursive binary splitting\n\n\nEvery observation that falls into a given region \\(R_j\\) is given the same prediction\n\n\ndetermined by majority (or plurality) vote in that region.\n\nImportant:\n\nTrees can only make rectangular regions that are aligned with the coordinate axis.\nThe fit is greedy, which means that after a split is made, all further decisions are conditional on that split."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#how-do-we-measure-quality-of-fit",
    "href": "schedule/slides/17-nonlinear-classifiers.html#how-do-we-measure-quality-of-fit",
    "title": "UBC Stat406 2024W",
    "section": "How do we measure quality of fit?",
    "text": "How do we measure quality of fit?\nLet \\(p_{mk}\\) be the proportion of training observations in the \\(m^{th}\\) region that are from the \\(k^{th}\\) class.\n\n\n\n\n\n\n\nclassification error rate:\n\\(E = 1 - \\max_k (\\widehat{p}_{mk})\\)\n\n\nGini index:\n\\(G = \\sum_k \\widehat{p}_{mk}(1-\\widehat{p}_{mk})\\)\n\n\ncross-entropy:\n\\(D = -\\sum_k \\widehat{p}_{mk}\\log(\\widehat{p}_{mk})\\)\n\n\n\nBoth Gini and cross-entropy measure the purity of the classifier (small if all \\(p_{mk}\\) are near zero or 1).\nThese are preferred over the classification error rate.\nClassification error is hard to optimize.\nWe build a classifier by growing a tree that minimizes \\(G\\) or \\(D\\)."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#pruning-the-tree",
    "href": "schedule/slides/17-nonlinear-classifiers.html#pruning-the-tree",
    "title": "UBC Stat406 2024W",
    "section": "Pruning the tree",
    "text": "Pruning the tree\n\nCross-validation can be used to directly prune the tree,\nBut it is computationally expensive (combinatorial complexity).\nInstead, we use weakest link pruning, (Gini version)\n\n\\[\\sum_{m=1}^{|T|} \\sum_{k \\in R_m} \\widehat{p}_{mk}(1-\\widehat{p}_{mk}) + \\alpha |T|\\]\n\n\\(|T|\\) is the number of terminal nodes.\nEssentially, we are trading training fit (first term) with model complexity (second) term (compare to lasso).\nNow, cross-validation can be used to pick \\(\\alpha\\)."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#advantages-and-disadvantages-of-trees-again",
    "href": "schedule/slides/17-nonlinear-classifiers.html#advantages-and-disadvantages-of-trees-again",
    "title": "UBC Stat406 2024W",
    "section": "Advantages and disadvantages of trees (again)",
    "text": "Advantages and disadvantages of trees (again)\n🎉 Trees are very easy to explain (much easier than even linear regression).\n🎉 Some people believe that decision trees mirror human decision.\n🎉 Trees can easily be displayed graphically no matter the dimension of the data.\n🎉 Trees can easily handle qualitative predictors without the need to create dummy variables.\n💩 Trees aren’t very good at prediction.\n💩 Trees are highly variable. Small changes in training data \\(\\Longrightarrow\\) big changes in the tree.\nTo fix these last two, we can try to grow many trees and average their performance.\n\nWe do this next module"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#knn-classifiers",
    "href": "schedule/slides/17-nonlinear-classifiers.html#knn-classifiers",
    "title": "UBC Stat406 2024W",
    "section": "KNN classifiers",
    "text": "KNN classifiers\n\nWe saw \\(k\\)-nearest neighbors in the last module.\n\n\nlibrary(class)\nknn3 &lt;- knn(dat1[, -1], gr, dat1$y, k = 3)\n\n\n\nCode\ngr$nn03 &lt;- knn3\nggplot(dat1, aes(x1, x2)) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = tibble(gr, disc = knn3), aes(x1, x2, fill = disc), alpha = .5) +\n  geom_point(aes(shape = as.factor(y)), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, blue), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#choosing-k-is-very-important",
    "href": "schedule/slides/17-nonlinear-classifiers.html#choosing-k-is-very-important",
    "title": "UBC Stat406 2024W",
    "section": "Choosing \\(k\\) is very important",
    "text": "Choosing \\(k\\) is very important\n\n\nCode\nset.seed(406406406)\nks &lt;- c(1, 2, 5, 10, 20)\nnn &lt;- map(ks, ~ as_tibble(knn(dat1[, -1], gr[, 1:2], dat1$y, .x)) |&gt; \n  set_names(sprintf(\"k = %02s\", .x))) |&gt;\n  list_cbind() |&gt;\n  bind_cols(gr)\npg &lt;- pivot_longer(nn, starts_with(\"k =\"), names_to = \"k\", values_to = \"knn\")\n\nggplot(pg, aes(x1, x2)) +\n  geom_raster(aes(fill = knn), alpha = .6) +\n  facet_wrap(~ k) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  geom_point(data = dat1, mapping = aes(x1, x2, shape = as.factor(y)), size = 4) +\n  theme_bw(base_size = 18) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  theme(\n    legend.title = element_blank(),\n    legend.key.height = unit(3, \"cm\")\n  )\n\n\n\n\nHow should we choose \\(k\\)?\nScaling is also very important. “Nearness” is determined by distance, so better to standardize your data first.\nIf there are ties, break randomly. So even \\(k\\) is strange."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#knn.cv-leave-one-out",
    "href": "schedule/slides/17-nonlinear-classifiers.html#knn.cv-leave-one-out",
    "title": "UBC Stat406 2024W",
    "section": "knn.cv() (leave one out)",
    "text": "knn.cv() (leave one out)\n\nkmax &lt;- 20\nerr &lt;- map_dbl(1:kmax, ~ mean(knn.cv(dat1[, -1], dat1$y, k = .x) != dat1$y))\n\n\nI would use the largest (odd) k that is close to the minimum.\nThis produces simpler, smoother, decision boundaries."
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#alternative-using-deviance-loss-i-think-this-is-right",
    "href": "schedule/slides/17-nonlinear-classifiers.html#alternative-using-deviance-loss-i-think-this-is-right",
    "title": "UBC Stat406 2024W",
    "section": "Alternative (using deviance loss, I think this is right)",
    "text": "Alternative (using deviance loss, I think this is right)\n\n\nCode\ndev &lt;- function(y, prob, prob_min = 1e-5) {\n  y &lt;- as.numeric(as.factor(y)) - 1 # 0/1 valued\n  m &lt;- mean(y)\n  prob_max &lt;- 1 - prob_min\n  prob &lt;- pmin(pmax(prob, prob_min), prob_max)\n  lp &lt;- (1 - y) * log(1 - prob) + y * log(prob)\n  ly &lt;- (1 - y) * log(1 - m) + y * log(m)\n  2 * (ly - lp)\n}\nknn.cv_probs &lt;- function(train, cl, k = 1) {\n  o &lt;- knn.cv(train, cl, k = k, prob = TRUE)\n  p &lt;- attr(o, \"prob\")\n  o &lt;- as.numeric(as.factor(o)) - 1\n  p[o == 0] &lt;- 1 - p[o == 0]\n  p\n}\ndev_err &lt;- map_dbl(1:kmax, ~ mean(dev(dat1$y, knn.cv_probs(dat1[, -1], dat1$y, k = .x))))"
  },
  {
    "objectID": "schedule/slides/17-nonlinear-classifiers.html#final-version",
    "href": "schedule/slides/17-nonlinear-classifiers.html#final-version",
    "title": "UBC Stat406 2024W",
    "section": "Final version",
    "text": "Final version\n\n\n\n\nCode\nkopt &lt;- max(which(err == min(err)))\nkopt &lt;- kopt + 1 * (kopt %% 2 == 0)\ngr$opt &lt;- knn(dat1[, -1], gr[, 1:2], dat1$y, k = kopt)\ntt &lt;- table(knn(dat1[, -1], dat1[, -1], dat1$y, k = kopt), dat1$y, dnn = c(\"predicted\", \"truth\"))\nggplot(dat1, aes(x1, x2)) +\n  theme_bw(base_size = 24) +\n  scale_shape_manual(values = c(\"0\", \"1\"), guide = \"none\") +\n  geom_raster(data = gr, aes(x1, x2, fill = opt), alpha = .6) +\n  geom_point(aes(shape = y), size = 4) +\n  coord_cartesian(c(-2.5, 3), c(-2.5, 3)) +\n  scale_fill_manual(values = c(orange, green), labels = c(\"0\", \"1\")) +\n  theme(\n    legend.position = \"bottom\", legend.title = element_blank(),\n    legend.key.width = unit(2, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nBest \\(k\\): 19\nMisclassification error: 0.17\nConfusion matrix:\n\n\n\n         truth\npredicted  1  2\n        1 41  6\n        2 11 42"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section",
    "href": "schedule/slides/00-gradient-descent.html#section",
    "title": "UBC Stat406 2024W",
    "section": "00 Gradient descent",
    "text": "00 Gradient descent\nStat 406\nGeoff Pleiss, Trevor Campbell\nLast modified – 25 October 2023\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#simple-optimization-techniques",
    "href": "schedule/slides/00-gradient-descent.html#simple-optimization-techniques",
    "title": "UBC Stat406 2024W",
    "section": "Simple optimization techniques",
    "text": "Simple optimization techniques\nWe’ll see “gradient descent” a few times:\n\nsolves logistic regression (simple version of IRWLS)\ngradient boosting\nNeural networks\n\nThis seems like a good time to explain it.\nSo what is it and how does it work?"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#very-basic-example",
    "href": "schedule/slides/00-gradient-descent.html#very-basic-example",
    "title": "UBC Stat406 2024W",
    "section": "Very basic example",
    "text": "Very basic example\n\n\nSuppose I want to minimize \\(f(x)=(x-6)^2\\) numerically.\nI start at a point (say \\(x_1=23\\))\nI want to “go” in the negative direction of the gradient.\nThe gradient (at \\(x_1=23\\)) is \\(f'(23)=2(23-6)=34\\).\nMove current value toward current value - 34.\n\\(x_2 = x_1 - \\gamma 34\\), for \\(\\gamma\\) small.\nIn general, \\(x_{n+1} = x_n -\\gamma f'(x_n)\\).\n\nniter &lt;- 10\ngam &lt;- 0.1\nx &lt;- double(niter)\nx[1] &lt;- 23\ngrad &lt;- function(x) 2 * (x - 6)\nfor (i in 2:niter) x[i] &lt;- x[i - 1] - gam * grad(x[i - 1])"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#why-does-this-work",
    "href": "schedule/slides/00-gradient-descent.html#why-does-this-work",
    "title": "UBC Stat406 2024W",
    "section": "Why does this work?",
    "text": "Why does this work?\nHeuristic interpretation:\n\nGradient tells me the slope.\nnegative gradient points toward the minimum\ngo that way, but not too far (or we’ll miss it)"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#why-does-this-work-1",
    "href": "schedule/slides/00-gradient-descent.html#why-does-this-work-1",
    "title": "UBC Stat406 2024W",
    "section": "Why does this work?",
    "text": "Why does this work?\nMore rigorous interpretation:\n\nTaylor expansion \\[\nf(x) \\approx f(x_0) + \\nabla f(x_0)^{\\top}(x-x_0) + \\frac{1}{2}(x-x_0)^\\top H(x_0) (x-x_0)\n\\]\nreplace \\(H\\) with \\(\\gamma^{-1} I\\)\nminimize this quadratic approximation in \\(x\\): \\[\n0\\overset{\\textrm{set}}{=}\\nabla f(x_0) + \\frac{1}{\\gamma}(x-x_0) \\Longrightarrow x = x_0 - \\gamma \\nabla f(x_0)\n\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#visually",
    "href": "schedule/slides/00-gradient-descent.html#visually",
    "title": "UBC Stat406 2024W",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#visually-1",
    "href": "schedule/slides/00-gradient-descent.html#visually-1",
    "title": "UBC Stat406 2024W",
    "section": "Visually",
    "text": "Visually"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#what-gamma-more-details-than-we-have-time-for",
    "href": "schedule/slides/00-gradient-descent.html#what-gamma-more-details-than-we-have-time-for",
    "title": "UBC Stat406 2024W",
    "section": "What \\(\\gamma\\)? (more details than we have time for)",
    "text": "What \\(\\gamma\\)? (more details than we have time for)\nWhat to use for \\(\\gamma_k\\)?\nFixed\n\nOnly works if \\(\\gamma\\) is exactly right\nUsually does not work\n\nDecay on a schedule\n\\(\\gamma_{n+1} = \\frac{\\gamma_n}{1+cn}\\) or \\(\\gamma_{n} = \\gamma_0 b^n\\)\nExact line search\n\nTells you exactly how far to go.\nAt each iteration \\(n\\), solve \\(\\gamma_n = \\arg\\min_{s \\geq 0} f( x^{(n)} - s f(x^{(n-1)}))\\)\nUsually can’t solve this."
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section-1",
    "href": "schedule/slides/00-gradient-descent.html#section-1",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "\\[ f(x_1,x_2) = x_1^2 + 0.5x_2^2\\]\n\nx &lt;- matrix(0, 40, 2); x[1, ] &lt;- c(1, 1)\ngrad &lt;- function(x) c(2, 1) * x"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section-2",
    "href": "schedule/slides/00-gradient-descent.html#section-2",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "\\[ f(x_1,x_2) = x_1^2 + 0.5x_2^2\\]\n\ngamma &lt;- .1\nfor (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ])"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section-3",
    "href": "schedule/slides/00-gradient-descent.html#section-3",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "\\[ f(x_1,x_2) = x_1^2 + 0.5x_2^2\\]\n\ngamma &lt;- .9 # bigger gamma\nfor (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ])"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section-4",
    "href": "schedule/slides/00-gradient-descent.html#section-4",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "\\[ f(x_1,x_2) = x_1^2 + 0.5x_2^2\\]\n\ngamma &lt;- .9 # big, but decrease it on schedule\nfor (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * .9^k * grad(x[k - 1, ])"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#section-5",
    "href": "schedule/slides/00-gradient-descent.html#section-5",
    "title": "UBC Stat406 2024W",
    "section": "",
    "text": "\\[ f(x_1,x_2) = x_1^2 + 0.5x_2^2\\]\n\ngamma &lt;- .5 # theoretically optimal\nfor (k in 2:40) x[k, ] &lt;- x[k - 1, ] - gamma * grad(x[k - 1, ])"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#when-do-we-stop",
    "href": "schedule/slides/00-gradient-descent.html#when-do-we-stop",
    "title": "UBC Stat406 2024W",
    "section": "When do we stop?",
    "text": "When do we stop?\nFor \\(\\epsilon&gt;0\\), small\nCheck any / all of\n\n\\(|f'(x)| &lt; \\epsilon\\)\n\\(|x^{(k)} - x^{(k-1)}| &lt; \\epsilon\\)\n\\(|f(x^{(k)}) - f(x^{(k-1)})| &lt; \\epsilon\\)"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#stochastic-gradient-descent",
    "href": "schedule/slides/00-gradient-descent.html#stochastic-gradient-descent",
    "title": "UBC Stat406 2024W",
    "section": "Stochastic gradient descent",
    "text": "Stochastic gradient descent\nSuppose \\(f(x) = \\frac{1}{n}\\sum_{i=1}^n f_i(x)\\)\nLike if \\(f(\\beta) = \\frac{1}{n}\\sum_{i=1}^n (y_i - x^\\top_i\\beta)^2\\).\nThen \\(f'(\\beta) = \\frac{1}{n}\\sum_{i=1}^n f'_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^n -2x_i^\\top(y_i - x^\\top_i\\beta)\\)\nIf \\(n\\) is really big, it may take a long time to compute \\(f'\\)\nSo, just sample some partition our data into mini-batches \\(\\mathcal{M}_j\\)\nAnd approximate (imagine the Law of Large Numbers, use a sample to approximate the population) \\[f'(x) = \\frac{1}{n}\\sum_{i=1}^n f'_i(x) \\approx \\frac{1}{m}\\sum_{i\\in\\mathcal{M}_j}f'_{i}(x)\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#sgd",
    "href": "schedule/slides/00-gradient-descent.html#sgd",
    "title": "UBC Stat406 2024W",
    "section": "SGD",
    "text": "SGD\n\\[\n\\begin{aligned}\nf'(\\beta) &= \\frac{1}{n}\\sum_{i=1}^n f'_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^n -2x_i^\\top(y_i - x^\\top_i\\beta)\\\\\nf'(x) &= \\frac{1}{n}\\sum_{i=1}^n f'_i(x) \\approx \\frac{1}{m}\\sum_{i\\in\\mathcal{M}_j}f'_{i}(x)\n\\end{aligned}\n\\]\nUsually cycle through “mini-batches”:\n\nUse a different mini-batch at each iteration of GD\nCycle through until we see all the data\n\nThis is the workhorse for neural network optimization"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#gradient-descent-for-logistic-regression",
    "href": "schedule/slides/00-gradient-descent.html#gradient-descent-for-logistic-regression",
    "title": "UBC Stat406 2024W",
    "section": "Gradient descent for Logistic regression",
    "text": "Gradient descent for Logistic regression\nSuppose \\(Y=1\\) with probability \\(p(x)\\) and \\(Y=0\\) with probability \\(1-p(x)\\), \\(x \\in \\R\\).\nI want to model \\(P(Y=1| X=x)\\).\nI’ll assume that \\(\\log\\left(\\frac{p(x)}{1-p(x)}\\right) = ax\\) for some scalar \\(a\\). This means that \\(p(x) = \\frac{\\exp(ax)}{1+\\exp(ax)} = \\frac{1}{1+\\exp(-ax)}\\)\n\n\n\nn &lt;- 100\na &lt;- 2\nx &lt;- runif(n, -5, 5)\nlogit &lt;- function(x) 1 / (1 + exp(-x))\np &lt;- logit(a * x)\ny &lt;- rbinom(n, 1, p)\ndf &lt;- tibble(x, y)\nggplot(df, aes(x, y)) +\n  geom_point(colour = \"cornflowerblue\") +\n  stat_function(fun = ~ logit(a * .x))"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood",
    "href": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood",
    "title": "UBC Stat406 2024W",
    "section": "Reminder: the likelihood",
    "text": "Reminder: the likelihood\n\\[\nL(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n\\]\n\\[\n\\begin{aligned}\n\\ell(y | a, x) &= \\log \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\n= \\sum_{i=1}^n y_i\\log p(x_i) + (1-y_i)\\log(1-p(x_i))\\\\\n&= \\sum_{i=1}^n\\log(1-p(x_i)) + y_i\\log\\left(\\frac{p(x_i)}{1-p(x_i)}\\right)\\\\\n&=\\sum_{i=1}^n ax_i y_i + \\log\\left(1-p(x_i)\\right)\\\\\n&=\\sum_{i=1}^n ax_i y_i + \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-1",
    "href": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-1",
    "title": "UBC Stat406 2024W",
    "section": "Reminder: the likelihood",
    "text": "Reminder: the likelihood\n\\[\nL(y | a, x) = \\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n\\]\nNow, we want the negative of this. Why?\nWe would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood (and scale by \\(1/n\\))\n\\[-\\ell(y | a, x) = \\frac{1}{n}\\sum_{i=1}^n -ax_i y_i - \\log\\left(\\frac{1}{1+\\exp(ax_i)}\\right)\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-2",
    "href": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-2",
    "title": "UBC Stat406 2024W",
    "section": "Reminder: the likelihood",
    "text": "Reminder: the likelihood\n\\[\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n\\]\nThis is, in the notation of our slides \\(f(a)\\).\nWe want to minimize it in \\(a\\) by gradient descent.\nSo we need the derivative with respect to \\(a\\): \\(f'(a)\\).\nNow, conveniently, this simplifies a lot.\n\\[\n\\begin{aligned}\n\\frac{d}{d a} f(a) &= \\frac{1}{n}\\sum_{i=1}^n -x_i y_i - \\left(-\\frac{x_i \\exp(ax_i)}{1+\\exp(ax_i)}\\right)\\\\\n&=\\frac{1}{n}\\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \\frac{1}{n}\\sum_{i=1}^n -x_i(y_i-p(x_i)).\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-3",
    "href": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-3",
    "title": "UBC Stat406 2024W",
    "section": "Reminder: the likelihood",
    "text": "Reminder: the likelihood\n\\[\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n\\]\n(Simple) gradient descent to minimize \\(-\\ell(a)\\) or maximize \\(L(y|a,x)\\) is:\n\nInput \\(a_1,\\ \\gamma&gt;0,\\ j_\\max,\\ \\epsilon&gt;0,\\ \\frac{d}{da} -\\ell(a)\\).\nFor \\(j=1,\\ 2,\\ \\ldots,\\ j_\\max\\), \\[a_j = a_{j-1} - \\gamma \\frac{d}{da} (-\\ell(a_{j-1}))\\]\nStop if \\(\\epsilon &gt; |a_j - a_{j-1}|\\) or \\(|d / da\\  \\ell(a)| &lt; \\epsilon\\)."
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-4",
    "href": "schedule/slides/00-gradient-descent.html#reminder-the-likelihood-4",
    "title": "UBC Stat406 2024W",
    "section": "Reminder: the likelihood",
    "text": "Reminder: the likelihood\n\\[\n\\frac{1}{n}L(y | a, x) = \\frac{1}{n}\\prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}\\textrm{ and }\np(x) = \\frac{1}{1+\\exp(-ax)}\n\\]\n\namle &lt;- function(x, y, a0, gam = 0.5, jmax = 50, eps = 1e-6) {\n  a &lt;- double(jmax) # place to hold stuff (always preallocate space)\n  a[1] &lt;- a0 # starting value\n  for (j in 2:jmax) { # avoid possibly infinite while loops\n    px &lt;- logit(a[j - 1] * x)\n    grad &lt;- mean(-x * (y - px))\n    a[j] &lt;- a[j - 1] - gam * grad\n    if (abs(grad) &lt; eps || abs(a[j] - a[j - 1]) &lt; eps) break\n  }\n  a[1:j]\n}"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#try-it",
    "href": "schedule/slides/00-gradient-descent.html#try-it",
    "title": "UBC Stat406 2024W",
    "section": "Try it:",
    "text": "Try it:\n\nround(too_big &lt;- amle(x, y, 5, 50), 3)\n\n [1] 5.000 3.360 2.019 1.815 2.059 1.782 2.113 1.746 2.180 1.711 2.250 1.684\n[13] 2.309 1.669 2.344 1.663 2.359 1.661 2.364 1.660 2.365 1.660 2.366 1.660\n[25] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660\n[37] 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660 2.366 1.660\n[49] 2.366 1.660\n\nround(too_small &lt;- amle(x, y, 5, 1), 3)\n\n [1] 5.000 4.967 4.934 4.902 4.869 4.837 4.804 4.772 4.739 4.707 4.675 4.643\n[13] 4.611 4.579 4.547 4.515 4.483 4.451 4.420 4.388 4.357 4.326 4.294 4.263\n[25] 4.232 4.201 4.170 4.140 4.109 4.078 4.048 4.018 3.988 3.957 3.927 3.898\n[37] 3.868 3.838 3.809 3.779 3.750 3.721 3.692 3.663 3.635 3.606 3.578 3.550\n[49] 3.522 3.494\n\nround(just_right &lt;- amle(x, y, 5, 10), 3)\n\n [1] 5.000 4.672 4.351 4.038 3.735 3.445 3.171 2.917 2.688 2.488 2.322 2.191\n[13] 2.094 2.027 1.983 1.956 1.940 1.930 1.925 1.922 1.920 1.919 1.918 1.918\n[25] 1.918 1.918 1.918 1.917 1.917 1.917 1.917"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#visual",
    "href": "schedule/slides/00-gradient-descent.html#visual",
    "title": "UBC Stat406 2024W",
    "section": "Visual",
    "text": "Visual\n\n\nnegll &lt;- function(a) {\n  -a * mean(x * y) -\n    rowMeans(log(1 / (1 + exp(outer(a, x)))))\n}\nblah &lt;- list_rbind(\n  map(\n    rlang::dots_list(\n      too_big, too_small, just_right, .named = TRUE\n    ), \n    as_tibble),\n  names_to = \"gamma\"\n) |&gt; mutate(negll = negll(value))\nggplot(blah, aes(value, negll)) +\n  geom_point(aes(colour = gamma)) +\n  facet_wrap(~gamma, ncol = 1) +\n  stat_function(fun = negll, xlim = c(-2.5, 5)) +\n  scale_y_log10() + \n  xlab(\"a\") + \n  ylab(\"negative log likelihood\") +\n  geom_vline(xintercept = tail(just_right, 1)) +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "schedule/slides/00-gradient-descent.html#check-vs.-glm",
    "href": "schedule/slides/00-gradient-descent.html#check-vs.-glm",
    "title": "UBC Stat406 2024W",
    "section": "Check vs. glm()",
    "text": "Check vs. glm()\n\nsummary(glm(y ~ x - 1, family = \"binomial\"))\n\n\nCall:\nglm(formula = y ~ x - 1, family = \"binomial\")\n\nCoefficients:\n  Estimate Std. Error z value Pr(&gt;|z|)    \nx   1.9174     0.4785   4.008 6.13e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 100  degrees of freedom\nResidual deviance:  32.335  on  99  degrees of freedom\nAIC: 34.335\n\nNumber of Fisher Scoring iterations: 7\n\n\n\n\n\nUBC Stat 406 - 2024"
  }
]