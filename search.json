[
  {
    "objectID": "schedule/handouts/lab00-git.html",
    "href": "schedule/handouts/lab00-git.html",
    "title": "Lab 00 Git",
    "section": "",
    "text": "Check your Canvas profile settings to ensure the email associated with your Canvas account is correct.\nReview your Canvas notification settings and decide what you want to be notified about.\nVisit the Course website. In particular, as you might expect, this course requires computing. We will use R and RStudio as well as Git and GitHub. See the Computing tab.\nIf you have never used GitHub before, go to https://github.com/ create an account. You should be aware that this data is stored on US servers. Please exercise caution whenever using personal information. You may wish to use a pseudonym to protect your privacy if you have concerns. k_asking_for_help/).\n\nIf you haven’t already, visit https://ubc-stat.github.io/stat-406/computing/ and follow the instructions to set up your computer.\nNow we have to clone your labs-&lt;username&gt; repo.\n\nNavigate to the Course Github using the link at the top of the Course Website or from Canvas.\nThen go to your labs-&lt;username&gt;.\nClick the Green “Code” button, and copy the url by clicking the two overlapping squares.\nThen in RStudio, choose “New project” &gt; “Version Control” &gt; “Git” and paste the address.\nChoose a location on your machine where you want all your labs to be.\nSelect “Create Project”."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "href": "schedule/handouts/lab00-git.html#scenario-1.-you-do-work-on-the-wrong-branch.",
    "title": "Lab 00 Git",
    "section": "Scenario 1. You do work on the wrong branch.",
    "text": "Scenario 1. You do work on the wrong branch.\nMake sure that you are on main. Remember that the actual submission is on the lab00-git branch.\nIn the R code chunk below, fit a linear model to the data and print the estimated coefficients, rounded to 2 decimal places.\n\nlibrary(tibble)\nset.seed(12345)\ndat &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = 2 + 3*x1 - x2 + rnorm(100)\n)\n\nNow, stage the .Rmd. Commit with the message “on the wrong branch” and push.\nYou likely see an error like:\nremote: error: GH006: Protected branch update failed for refs/heads/main.\nThat’s because you’re on main. Ugh! But I did some work, and now I need to be on a different branch!\nSo let’s fix it. We want the stuff we just did on main to be on lab00-git. Note that everything you did is saved! Here are the steps:\nGet our changes onto the correct branch\n\nUse the dropdown to switch branches to lab00-git.\nGo to the Terminal (next to console).\nType git merge main.\n\nThat should copy all your changes in the .Rmd that you made on main into the correct place. Did it?\nIf you do this and you ever see stuff like\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nThis is the stuff that is currently on this branch.\n=======\nThis is stuff that got added on the other branch.\nWhile someone else changed stuff on this branch!\nI (git) don't know which to keep!?\nYou have to decide for me.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; new_branch_for_merge_conflict\nThis means that there were conflicts between the two versions. The stuff above ====== was in your current branch. The stuff below is what you’re trying to merge in. You decide what to keep, the top, the bottom, or both (or neither). Just be sure to delete the junk lines with &lt;, &gt;, or =.\nOnce you’ve resolved conflicts (and committed the conflict changes), double check the following:\n\nYou are on the correct branch (lab00-git)\nYou have no files with uncommitted changes in the “Git” tab\nYour changes to the R chunk above exist (on this branch)\n\nAnother way you can check for your changes is by running the git log commmand. You should see something like the following:\ncommit 1efefd8473c2cc81893dd2a5ded929978d9ee2aa (HEAD -&gt; lab00-git, main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 16:41:58 2024 -0700\n\n    on the wrong branch\n\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nThere’s a lot of information here, but you should (hopefully) see at the top your latest commit with the message “on the wrong branch.” The long string at the start of the commit (1efefd8473c2cc81893dd2a5ded929978d9ee2aa) is the hash. It is a unique identifier of the commit, which can be useful if you want to reference a specific commit with other commands.\nType q to exit the log viewer.\n\nOk. So now we have our changes in the right spot. Commit and Push the .Rmd (only). Let’s clean up main so we don’t have problems later. Switch back to main.\nUndo mistakes on the wrong branch.\nIn the terminal, type the following two commands:\ngit fetch\ngit reset --hard origin/main\nThere’s a lot to unpack in these two commands, but here’s the high level idea: we want to make sure that our main branch matches what’s on Github’s remote main branch. The second command resets our local main branch so that it has exactly the same commits as Github’s remote main branch. (The first command makes sure that our local computer knows about the latest changes on Github’s remote branches.)\nIf you now type git log, you should now see\ncommit 328436d60d8153db7f5b8caef56919b69a5448a2 (HEAD -&gt; main, origin/main)\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 4:44:09 2024 -0700\n\n    Update git instructions\n\ncommit bb21d0cc444e65be9d801c6b672ba7491509f030\nAuthor: Geoff Pleiss &lt;824157+gpleiss@users.noreply.github.com&gt;\nDate:   Fri Aug 30 10:59:12 2024 -0700\n\n    Init\nSo our local main branch matches what’s on Github, and no longer contains the “on the wrong branch” commit. You can also verify that your changes to the R code on this branch are now gone.\nTo recap, now the work we want is in the right place (on the other branch), and the mess on main is cleaned up. Boom."
  },
  {
    "objectID": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "href": "schedule/handouts/lab00-git.html#scenario-2.-you-did-something-you-shouldnt-have",
    "title": "Lab 00 Git",
    "section": "Scenario 2. You did something you shouldn’t have",
    "text": "Scenario 2. You did something you shouldn’t have\nSwitch your branch back to lab00-git (or whatever you named it).\nOpen the file lab01.Rmd. Select everything after # Instructions and delete it. Save. Then Knit (producing a pdf). Commit both files with a message “did the wrong lab, and built a pdf”. Push your commits with the Green up arrow.\nTake a look at the PR on Github now. There’s a bunch of crud that shouldn’t be there.\nWe’ve done 3 things here that we shouldn’t have.\n\nWe built a pdf that we don’t want at all. It needs to go away.\nWe bollixed up the lab01.Rmd file. We don’t want that or it will screw up the lab next week.\nWe pushed it all into our submission for this week.\n\nThe first instinct is to Delete both files, commit, and push. This is VERY BAD. That will further screw up everything. Basically, you’re telling git “I don’t want these files at all” when you mean “I don’t want changes to these files in this branch”. The difference is subtle but important. Because you DO want these files (without the changes) at some point, but you don’t want them here.\nLet’s fix these issues.\n\nFirst, we want to “get rid of” the pdf. In the Terminal type\ngit reset HEAD^ -- lab01.pdf\nClick the little “Refresh” arrow in the Git panel. You should now see lab01.pdf twice, once with a red D that is checked and once with two yellow question marks that is NOT checked. This is what we want.\nCommit exactly as is. Use a message like “remove the stray pdf” and Push. Now, take a look at the PR on Github. It should be gone from the list of files in the PR.\nThere’s still that annoying two-yellow-question-mark version in the Git panel. Don’t click the check box (that will just redo everything we undid). Instead, highlight the file by clicking the file name, click the Gear Icon Dropdown, and then select “Revert”. Now it’s gone, and the pdf should disappear from your filesystem.\n\nSecond, let’s “undo” the deletion in the .Rmd. This is easy, and a useful pattern to remember.\nIn the Terminal, type\ngit checkout main -- lab01.Rmd\nWhat this does is grabs the version on main that isn’t messed up and puts it here, overwriting your changes. This isn’t the only way to fix your problem (you could have done the same thing we did with the pdf), but it’s pretty easy.\nStage commit and push. Now look at the PR on Github. Even though you made two changes (one deleting everything, and one restoring everything) to the lab01.Rmd, it should be “gone” from the PR now. That’s because the version on this branch looks just like the version on main, so there are no changes to be made into the main branch. This is just what we want.\n\nNow we’ve also fixed the third error already. None of those bogus changes to lab01 are in our PR for this week anymore."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDistinguish between “learning,” “supervised learning,” “prediction,” and “inference.”\nTranslate between algorithmic and statistical perspectives on learning\n\nDefine a statistical model\nDefine an estimator\nDefine a prediction rule\n\nDefine the standard linear regression model\nDerive ordinary least squares from either the maximum likelihood estimation (MLE) or the empirical risk minimization (ERM) framework"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#learning-objectives",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#learning-objectives",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDistinguish between “learning,” “supervised learning,” “prediction,” and “inference.”\nTranslate between algorithmic and statistical perspectives on learning\n\nDefine a statistical model\nDefine an estimator\nDefine a prediction rule\n\nDefine the standard linear regression model\nDerive ordinary least squares from either the maximum likelihood estimation (MLE) or the empirical risk minimization (ERM) framework"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#what-is-supervised-learning",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#what-is-supervised-learning",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "What is (Supervised) Learning?",
    "text": "What is (Supervised) Learning?\nThere are many formulations of “learning” in statistics and machine learning. The main focus of this course is supervised learning.\n\nGoal of supervised learning: predict a response variable \\(Y\\) given a set of covariates \\(X\\)\n\nFor example, predicting house prices based on features like size, location, and number of bedrooms.\n\n\nLearning from data:\nWe are given a training dataset: \\[ \\mathcal{D} = \\{(X_1, Y_1), (X_2, Y_2), \\ldots, (X_n, Y_n)\\} \\in \\mathbb R^p \\times \\mathcal Y\\]\nwhere:\n\n\\(X_i \\in \\mathbb R^p\\) are the \\(d\\)-dimensional covariates (e.g. size, location, number of bedrooms, etc. for each house)\n\\(Y_i \\in \\mathcal Y\\) is the response variable (e.g. house price)\n\\(\\mathcal Y\\) is the space of possible responses.\n\nFor our problem and other regression problems, \\(\\mathcal Y = \\mathbb R\\).\nFor classification problems (i.e. predicting whether the house is for sale or not) \\(\\mathcal Y\\) is a finite set of classes (e.g. “for sale” vs “not for sale”).\n\n\nFrom this training set, we want to learn a function \\(\\hat f : \\mathbb R^p \\to \\mathcal Y\\) that accurately predicts the response for new observations.\n\nGiven a set of new covariates \\(X_\\mathrm{new}\\) (e.g. a new house that we don’t know the price of)\nWe predict \\(\\hat Y_\\mathrm{new} = \\hat f(X_\\mathrm{new})\\) (e.g. our predicted price for the new house)\nOur goal is for \\(\\hat Y_\\mathrm{new} \\approx Y_\\mathrm{new}\\): the true (but unknown) price of the new house.\n\n\nPrediction versus Inference\nIn learning we are primarily concerned with prediction over inference.\n\nInference: The goal is making a probabilistic statement about the relationship between \\(X\\) and \\(Y\\).\n\n(For example, we might want to know how much the price of a house increases for each additional bedroom.)\n\nPrediction: The goal is producing accurate \\(\\hat Y_\\mathrm{new}\\).\n\nWe don’t really care about the underlying relationship, or even about making probabilistic statements about it.\nWe just want some procedure to give us good predictions."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#the-supervised-learning-procedure-two-perspectives",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#the-supervised-learning-procedure-two-perspectives",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "The Supervised Learning Procedure: Two Perspectives",
    "text": "The Supervised Learning Procedure: Two Perspectives\n\n\nCS/Algorithmic Perspective\nLearning is often presented as an algorithm for producing a prediction rule \\(\\hat f\\) from data \\(\\mathcal{D}\\). This algorithm consist of 6 steps, which we will examine in the context of linear regression:\n\nTrain/val/test split: Divide data into training and testing sets\n\nGiven our dataset \\(\\mathcal{D}\\), we split it into a training set \\(\\mathcal{D}_\\mathrm{train}\\), a validation set \\(\\mathcal{D}_\\mathrm{val}\\), and a test set \\(\\mathcal{D}_\\mathrm{test}\\).\n\nHypothesis class: Select a set of candidate \\(\\hat f\\) which might make a good prediction rule.\n\nFor linear regression, the hypothesis class is linear functions of the form \\(\\hat f(x) = x^\\top \\hat\\beta\\) for some \\(\\hat \\beta\\).\n\nTraining: Define a training algorithm (a procedure to choose a \\(\\hat \\beta\\) from the training data) and apply it to \\(\\mathcal{D}_\\mathrm{train}\\).\n\nMost training algorithms involve minimizing a loss function over training data.\nFor linear models, we often choose \\(\\hat f\\) to minimize the squared error loss over training data. \\[L(Y, \\hat Y) = (Y - \\hat Y)^2, \\qquad \\hat \\beta = \\mathrm{argmin}_{\\beta} \\frac{1}{n} \\sum_{i=1}^{n} L(Y_i, X_i^\\top \\beta). \\]\n\nValidation: Test performance on withheld validation data.\n\nWe compute the average loss on the validation set to evaluate how well our model generalizes to unseen data.\n\nIteration: Refine model based on evaluation results\n\nWe may choose to change our hypothesis class, the set of features, the loss function, or the training algorithm to try to reduce validation error.\n\nTesting (confusingly referred to by some as “Inference”): Once satisfied with the model, evaluate on the test set to estimate performance on new data.\n\n\n\nStatistical Perspective\nOver the next few lectures, we will derive the CS/algorithmic perspective from first statistical principles. For this lecture, we will focus on a statistical perspective on Steps 2, 3, and 6:\n\n\n\nStep\nCS Perspective\nStatistical Perspective\n\n\n\n\n2\nHypothesis Class\nStatistical Model\n\n\n3\nTraining\nEstimation\n\n\n6\nTesting (Inference)\nPrediction"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#statistical-models",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#statistical-models",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Statistical Models",
    "text": "Statistical Models\nA statistical model defines the possible relationships between the covariates \\(X\\) and the response variable \\(Y\\).\n\nFormally, it is a set of probability distributions that could possibly generate the data we observe.\nFor the learning problem, where we care about predicting \\(Y\\) given \\(X\\), we will consider statistical models that are sets of conditional distributions \\(P(Y \\mid X)\\).\n\n\nThe Linear Regression Model\n\nFrom STAT 306, you may recall that we typically assume that the relationship between \\(X\\) and \\(Y\\) can be described as: \\[Y = X^\\top \\beta + \\varepsilon\\]\n\n\\(\\beta\\) is the vector of parameters that we are trying to estimate.\n\\(\\varepsilon\\) is the random error term, which is typicall i.i.d. \\(\\varepsilon \\sim N(0, \\sigma^2)\\) for some \\(\\sigma^2 &gt; 0\\).\n(You may notice that we don’t have an intercept term \\(\\beta_0\\) in this model. We assume that \\(X\\) has an “all-ones” covariate, i.e. \\(X = [1, X_1, X_2, \\ldots, X_p]\\), so that the intercept term \\(\\beta_0\\) is included with the other \\(\\beta\\) terms, i.e. \\(X^\\top \\beta = \\beta_0 + X_1^\\top \\beta_1 + \\ldots + X_p^\\top \\beta_p\\).)\n\nFor any given \\(\\beta\\), if we are given \\(X = x\\), then we have that \\(Y \\sim \\mathcal{N}(x^\\top\\beta, \\sigma^2)\\). Thus, the corresponding statistical model/set of possible conditional distributions is:\n\n\\[\\left\\{ P \\: : \\: P(Y \\mid X = x) \\: = \\: \\mathcal{N}(x^\\top\\beta, \\sigma^2), \\:\\: \\beta \\in \\mathbb R^p \\right\\}.\\]\n\nWe refer to \\(\\beta\\) as the parameters of the model, as a given value of \\(\\beta\\) specifies a particular distribution within the set.\n\n\n\nOther Models\nThis linear regression model is just one statistical model we could use to describe our data. Almost any family of conditional distribution can be used instead. For example:\n\nThe more general linear regression model: we could consider the slightly more general case of conditional distributions defined by an expected value condition: \\[\\left\\{ P \\: : \\: \\mathbb E[Y \\mid X = x] \\: = \\: x^\\top\\beta, \\:\\: \\beta \\in \\mathbb R^p \\right.\\}\\]\n\nNote that \\(\\mathbb E[\\mathcal{N}(x^\\top\\beta, \\sigma^2)] = x^\\top \\beta\\), and so the linear regression model is a subset of this more general model.\nHowever, this model allows for the possibility of non-normal residuals.\n\nThe polynomial regression model: we could instead consider the slightly more general case where the expectation is a polynomial function of \\(X\\), rather than just a linear function: \\[\\left\\{ P \\: : \\: \\mathbb E[Y \\mid X = x] \\: = \\: p(x), \\:\\: p \\text{ is a polynomial of degree } d \\right\\}\\]\n\nNote again that our general linear model is a special case; however this model allows for more complex relationships between \\(X\\) and \\(Y\\).\nThis model also has parameters (the coefficients of the polynomial). though they are not explicit in this formulation.\n\nAll possible distributions: what if we used the most general set of distributions? \\[\\left\\{ P \\: : \\: P(Y \\mid X = x) \\text{ is any distribution} \\right\\}\\]\n\nWhile this is a valid set, it turns out that it is too general for the purposes of learning.\nSpecifically, the no free lunch theorem states that we will not be able to identify a “good” model from this set even if we were given infinite training data!\n(We need to have some “structure” or simplifying assumptions in our model to be able to learn from data.)"
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#estimation",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#estimation",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Estimation",
    "text": "Estimation\n\nOnce we have chosen a statistical model, the estimation step involves us choosing a specific distribution from the model that best fits our training data.\nAlternatively, we can think of it as estimating a set of parameters that define a distribution within our model that describe the training data.\nThere are many statistically valid estimation procedures. We’ll derive two that you’ve likely seen before: Maximum Likelihood Estimation (MLE) and Empirical Risk Minimization (ERM). (We’ll talk about others throughout this course.)\n\nWe will work through these two procedures using the linear regression model as our statistical model.\n\n1. Empirical Risk Minimization (ERM)\nGoal: Find the parameters that minimize the loss on our training data.\n\nFrom STAT 306, you may remember a procedure known as Ordinary Least Squares (OLS) to estimate the parameters of a linear regression model.\nThis procedures is a special case of a more general procedure Empirical Risk Minimization (ERM) for reasons that we will see in a few lectures.\n\nThe OLS/ERM Procedure:\n\nDefine a loss function\n\nWe need to quantify what we mean by “least bad.”\nA loss function measures how well a particular \\(P[Y \\mid X=X_i]\\) distribution fits the true label \\(Y_i\\).\nGiven a \\(\\beta\\) for our linear regression model, a common choice of loss function is the squared error loss: \\[L(Y_i, \\hat{Y_i}) = (Y_i - \\hat{Y_i})^2, \\qquad \\hat{Y_i} = X_i^\\top\\beta.\\]\n\nMinimize the average loss over the training data\n\nThe empirical risk is defined as the average loss over our training data:\n\\[\\hat{R}(\\beta) =\\frac{1}{n}\\sum_{i=1}^n L(Y_i, \\hat Y_i) = \\frac{1}{n}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2\\]\nThe OLS (ERM) estimator is the set of parameters \\(\\hat{\\beta}\\) (i.e. the distribution from our model) that minimizes this empirical risk:\n\\[\\hat{\\beta}_\\mathrm{OLS} = \\arg\\min_{\\beta} \\hat{R}(\\beta) = \\arg\\min_{\\beta} \\frac{1}{n}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2\\]\nTaking the derivative and setting to zero:\n\\[\\frac{\\partial}{\\partial \\beta}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2 = -2\\sum_{i=1}^n X_i(Y_i - X_i^\\top\\beta) = 0\\]\nBy arranging all of our training data into a matrix \\(\\boldsymbol X \\in \\mathbb{R}^{n \\times d}\\) and a vector \\(\\boldsymbol Y \\in \\mathbb{R}^n\\), with\n\\[\\boldsymbol X = \\begin{bmatrix} -X_1^\\top- \\\\ -X_2^\\top- \\\\ \\vdots \\\\ -X_n^\\top- \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}, \\qquad \\boldsymbol Y = \\begin{bmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{bmatrix},\\]\nwe get:\n\\[\\hat{\\beta}_\\mathrm{OLS} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y.\\]\n\n\n\n\n2. Maximum Likelihood Estimation (MLE)\nGoal: Find the parameters that maximize the likelihood of observing our data.\n\nThe likelihood function maps a distribution from our model (parameterized by \\(\\beta\\)) to the probability density of our observed training data.\n\nLikelihood function for linear regression.\n\nFor the linear regression model with normal errors, the likelihood function is: \\[\\mathcal{L} (\\beta, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(Y_i - X_i^\\top\\beta)^2}{2\\sigma^2}\\right)\\]\nThe term inside the product is the density of the normal distribution \\(\\mathcal{N}(X_i^\\top\\beta, \\sigma^2)\\) evaluated at \\(Y_i\\).\nWe take the product over all \\(n\\) training examples \\((X_i, Y_i)\\) because we assume that the data points are i.i.d.\nWe choose \\(\\beta\\) to maximize the likelihood function, i.e. the \\(\\beta\\) that make observed data most probable.\n\nLog-likelihoods are easier to work with.\n\nTaking the log of the likelihood function, log-likelihood: \\[\\ell(\\beta, \\sigma^2) = -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2\\]\nThe logarithm is a monotonic function, so maximizing the likelihood is equivalent to maximizing the log-likelihood.\nThe \\(-\\frac{n}{2}\\log(2\\pi\\sigma^2)\\) constant and the \\(\\frac{1}{2\\sigma^2}\\) scaling term also don’t affect the maximum, so we can ignore them.\nThus, the maximum likelihood estimate of \\(\\beta\\) is found by minimizing: \\[\\mathrm{argmin}_{\\beta} \\sum_{i=1}^n (Y_i - X_i^\\top\\beta)^2,\\]\nThis is exactly the same optimization problem we derived for ERM! Thus, \\[ \\hat{\\beta}_\\mathrm{MLE} = (\\boldsymbol X^\\top \\boldsymbol X)^{-1} \\boldsymbol X^\\top \\boldsymbol Y = \\hat{\\beta}_\\mathrm{OLS} \\]\n\nKey Insight: For linear regression with normal errors, MLE and OLS/ERM give identical results!\n\nIf we had used a different loss function (e.g. \\(L(Y_i, \\hat{Y_i}) = |Y_i - \\hat{Y_i}|\\)), the ERM estimator would not be the same as the OLS/MLE estimator.\nThere are many possible loss functions with different properties, and we will explore them in future lectures."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#prediction",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#prediction",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Prediction",
    "text": "Prediction\n\nOnce we have estimated \\(\\hat{\\beta}\\) and selected the distribution that “best” describes our data, we can make predictions on new data points \\(X_\\mathrm{new}\\).\nFor linear models, we often use the prediction rule: \\[\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top\\hat{\\beta}\\]\n\nDecision theory for predictions: We want a principled reason for using this prediction rule.\n\nDecision theory: choose a prediction \\(\\hat{Y}_\\mathrm{new}\\) that minimizes the expected loss:\n\\[\\hat{Y}_\\mathrm{new} = \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\mathbb E[L(Y, \\hat{y}_\\mathrm{new}) \\mid X_\\mathrm{new}, \\hat \\beta].\\]\nAccording to our estimated model, we have\n\\[P(Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta) = \\mathcal{N}(X_\\mathrm{new}^\\top\\hat{\\beta}, \\sigma^2).\\]\nPlugging in the squared error loss function into this formula, we get:\n\\[\\begin{align*}\n    \\hat{Y}_\\mathrm{new} &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\mathbb E[(Y_\\mathrm{new} - \\hat{y}_\\mathrm{new})^2 \\mid X_\\mathrm{new}, \\hat \\beta] \\\\\n    &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\left( \\underbrace{\\mathbb E[Y_\\mathrm{new}^2 \\mid X_\\mathrm{new}, \\hat \\beta]}_{\n       \\underbrace{\\mathrm{Var}[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta]}_{\\sigma^2} +\n       \\underbrace{(\\mathbb E[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta])^2}_{(X_\\mathrm{new}^\\top \\hat{\\beta})^2}\n    } - 2\\hat{y}_\\mathrm{new} \\underbrace{\\mathbb E[Y_\\mathrm{new} \\mid X_\\mathrm{new}, \\hat \\beta]}_{X_\\mathrm{new}^\\top \\beta}\n    + \\hat{y}_\\mathrm{new}^2 \\right) \\\\\n    &= \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} \\left(\n       \\underbrace{\\sigma^2}_\\mathrm{const.} + \\underbrace{\n          (X_\\mathrm{new}^\\top \\hat{\\beta})^2- 2\\hat{y}_\\mathrm{new} (X_\\mathrm{new}^\\top \\hat{\\beta}) + \\hat{y}_\\mathrm{new}^2\n       }_{(\\hat{y}_\\mathrm{new} - X_\\mathrm{new}^\\top \\hat{\\beta})^2}\n    \\right)\n\\end{align*}\\]\nWe can drop the \\(\\sigma^2\\) constant, since it doesn’t affect the minimum, and we are left with:\n\\[\\hat{Y}_\\mathrm{new} = \\mathrm{argmin}_{\\hat{y}_\\mathrm{new}} (\\hat{y}_\\mathrm{new} - X_\\mathrm{new}^\\top \\hat{\\beta})^2,\\]\nwhich, after taking the derivative and setting to zero, gives us: \\(\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\).\n\nImportant! If we had chosen a different loss function, we would have derived a different prediction rule."
  },
  {
    "objectID": "schedule/lectures/lecture_02_learning_procedure_regression.html#summary",
    "href": "schedule/lectures/lecture_02_learning_procedure_regression.html#summary",
    "title": "Lecture 2: Introduction to Learning, Regression",
    "section": "Summary",
    "text": "Summary\nThis lecture introduced the statistical framework for learning:\n\nStatistical models define the probabilistic structure of our data\nEstimation finds the best parameters using MLE or ERM\nPrediction uses fitted models to make predictions on new data\n\n\nThis framework will extend to more complex models and methods throughout the course.\nLinear regression serves as our foundational example, where MLE and ERM produce identical estimators under normal error assumptions.\nIn the next lecture, we will apply this statistical framework to classification problems."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#section",
    "href": "schedule/slides/00-version-control.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "00 Git and Github",
    "text": "00 Git and Github\nStat 406\nGeoff Pleiss\nLast modified – 01 September 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control",
    "href": "schedule/slides/00-version-control.html#why-version-control",
    "title": "UBC Stat406 2025 W1",
    "section": "Why version control?",
    "text": "Why version control?\n\nMuch of this lecture is based on material from Colin Rundel and Karl Broman"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#why-version-control-1",
    "href": "schedule/slides/00-version-control.html#why-version-control-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Why version control?",
    "text": "Why version control?\n\nSimple formal system for tracking all changes to a project\nTime machine for your projects\n\nTrack blame and/or praise\nRemove the fear of breaking things\n\nLearning curve is steep, but when you need it you REALLY need it\n\n\n\n\nWhen you get really good\n\n\nVersion control can act as a living lab notebook"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#overview",
    "href": "schedule/slides/00-version-control.html#overview",
    "title": "UBC Stat406 2025 W1",
    "section": "Overview",
    "text": "Overview\n\ngit is a command line program that lives on your machine\nIf you want to track changes in a directory, you type git init\nThis creates a (hidden) directory called .git\nThe .git directory contains a history of all changes made to “versioned” files\nThis top directory is referred to as a “repository” or “repo”\nhttp://github.com is a service that hosts a repo remotely and has other features: issues, project boards, pull requests, renders .ipynb & .md\nSome IDEs (pycharm, RStudio, VScode) have built in git\ngit/GitHub is broad and complicated. Here, just what you need"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "href": "schedule/slides/00-version-control.html#aside-on-built-in-command-line",
    "title": "UBC Stat406 2025 W1",
    "section": "Aside on “Built-in” & “Command line”",
    "text": "Aside on “Built-in” & “Command line”\n\n\n\n\n\n\nTip\n\n\nFirst things first, RStudio and the Terminal\n\n\n\n\nCommand line is the “old” type of computing. You type commands at a prompt and the computer “does stuff”.\nYou may not have seen where this is. RStudio has one built in called “Terminal”\nThe Mac System version is also called “Terminal”. If you have a Linux machine, this should all be familiar.\nWindows is not great at this.\nTo get the most out of Git, you have to use the command line."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#typical-workflow",
    "href": "schedule/slides/00-version-control.html#typical-workflow",
    "title": "UBC Stat406 2025 W1",
    "section": "Typical workflow",
    "text": "Typical workflow\n\nDownload a repo from Github\n\n```{bash}\ngit clone https://github.com/stat550-2021/lecture-slides.git\n```\n\nCreate a branch\n\n```{bash}\ngit branch &lt;branchname&gt;\n```\n\nMake changes to your files.\nAdd your changes to be tracked (“stage” them)\n\n```{bash}\ngit add &lt;name/of/tracked/file&gt;\n```\n\nCommit your changes\n\n```{bash}\ngit commit -m \"Some explanatory message\"\n```\nRepeat 3–5 as needed. Once you’re satisfied\n\nPush to GitHub\n\n```{bash}\ngit push\ngit push -u origin &lt;branchname&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "href": "schedule/slides/00-version-control.html#commit-messages-should-be-meaningful",
    "title": "UBC Stat406 2025 W1",
    "section": "Commit messages should be meaningful",
    "text": "Commit messages should be meaningful\n\n\n\n\n\nInstead, try “Update linear model in Question 1.2”"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nDefinitely\n\ncode, markdown documentation, tex files, bash scripts/makefiles, …\n\n\n\n\nPossibly\n\njupyter notebooks, images (that won’t change), …\n\n\n\n\nQuestionable\n\nprocessed data, static pdfs, …\n\n\n\n\nDefinitely not\n\nfull data, continually updated pdfs, other things compiled from source code, logs…"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\n\n\nTLDR\nAny file that YOU edit should be tracked\nAny file that’s computer generated should PROBABLY NOT be tracked\nHowever, in this course you will track rendered PDFs of your homeworks/labs. This makes it easier for the graders."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "href": "schedule/slides/00-version-control.html#what-should-be-tracked-2",
    "title": "UBC Stat406 2025 W1",
    "section": "What should be tracked?",
    "text": "What should be tracked?\nA file called .gitignore tells git files or types to never track\n```{bash}\n# History files\n.Rhistory\n.Rapp.history\n\n# Session Data files\n.RData\n\n# User-specific files\n.Ruserdata\n\n# Compiled junk\n*.o\n*.so\n*.DS_Store\n```\nShortcut to track everything (use carefully):\n```{bash}\ngit add .\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr",
    "href": "schedule/slides/00-version-control.html#whats-a-pr",
    "title": "UBC Stat406 2025 W1",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "href": "schedule/slides/00-version-control.html#whats-a-pr-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What’s a PR?",
    "text": "What’s a PR?\n\nThis exists on GitHub (not git)\nDemonstration"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "href": "schedule/slides/00-version-control.html#some-things-to-be-aware-of",
    "title": "UBC Stat406 2025 W1",
    "section": "Some things to be aware of",
    "text": "Some things to be aware of\n\nmaster vs main\nIf you think you did something wrong, stop and ask for help\nThere are guardrails in place. But those won’t stop a bulldozer.\nThe hardest part is the initial setup. Then, this should all be rinse-and-repeat.\nThis book is great: Happy Git with R\n\nSee Chapter 6 if you have install problems.\nSee Chapter 9 for credential caching (avoid typing a password all the time)\nSee Chapter 13 if RStudio can’t find git"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#protection",
    "href": "schedule/slides/00-version-control.html#protection",
    "title": "UBC Stat406 2025 W1",
    "section": "Protection",
    "text": "Protection\n\nTypical for your PR to trigger tests to make sure you don’t break things\nTypical for team members or supervisors to review your PR for compliance"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#guardrails",
    "href": "schedule/slides/00-version-control.html#guardrails",
    "title": "UBC Stat406 2025 W1",
    "section": "Guardrails",
    "text": "Guardrails\n\nIn this course, we protect main so that you can’t push there\n\n\n\n\n\n\nWarning\n\n\nIf you try to push to main, it will give an error like\n```{bash}\nremote: error: GH006: Protected branch update failed for refs/heads/main.\n```\nThe fix is: make a new branch, then push that."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "href": "schedule/slides/00-version-control.html#operations-in-rstudio",
    "title": "UBC Stat406 2025 W1",
    "section": "Operations in Rstudio",
    "text": "Operations in Rstudio\n\n\n\nStage\nCommit\nPush\nPull\nCreate a branch\n\nCovers:\n\nEverything to do your HW / Project if you’re careful\nPlus most other things you “want to do”\n\n\n\nCommand line versions (of the same)\n```{bash}\ngit add &lt;name/of/file&gt;\n\ngit commit -m \"some useful message\"\n\ngit push\n\ngit pull\n\ngit checkout -b &lt;name/of/branch&gt;\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "href": "schedule/slides/00-version-control.html#other-useful-stuff-but-command-line-only",
    "title": "UBC Stat406 2025 W1",
    "section": "Other useful stuff (but command line only)",
    "text": "Other useful stuff (but command line only)\n\n\nInitializing\n```{bash}\ngit config user.name --global \"Geoff Pleiss\"\ngit config user.email --global \"geoff.pleiss@stat.ubc.ca\"\ngit config core.editor --global nano\n# or emacs or ... (Geoff loves vim and you should too!)\n```\nStaging\n```{bash}\ngit add name/of/file # stage 1 file\ngit add . # stage all\n```\nCommitting\n```{bash}\n# stage/commit simultaneously\ngit commit -am \"message\"\n\n# open editor to write long commit message\ngit commit\n```\nPushing\n```{bash}\n# If branchname doesn't exist\n# on remote, create it and push\ngit push -u origin branchname\n```\n\n\nBranching\n```{bash}\n# switch to branchname, error if uncommitted changes\ngit checkout branchname\n# switch to a previous commit\ngit checkout aec356\n\n# create a new branch\ngit branch newbranchname\n# create a new branch and check it out\ngit checkout -b newbranchname\n\n# merge changes in branch2 onto branch1\ngit checkout branch1\ngit merge branch2\n\n# grab a file from branch2 and put it on current\ngit checkout branch2 -- name/of/file\n\ngit branch -v # list all branches\n```\nCheck the status\n```{bash}\ngit status\ngit remote -v # list remotes\ngit log # show recent commits, msgs\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#conflicts",
    "href": "schedule/slides/00-version-control.html#conflicts",
    "title": "UBC Stat406 2025 W1",
    "section": "Conflicts",
    "text": "Conflicts\n\nSometimes you merge things and “conflicts” happen.\nMeaning that changes on one branch would overwrite changes on a different branch.\n\n\n\n\nThey look like this:\n\nHere are lines that are either unchanged from\nthe common ancestor, or cleanly resolved\nbecause only one side changed.\n\nBut below we have some troubles\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; yours:sample.txt\nConflict resolution is hard;\nlet's go shopping.\n=======\nGit makes conflict resolution easy.\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; theirs:sample.txt\n\nAnd here is another line that is cleanly\nresolved or unmodified.\n\n\nYou get to decide, do you want to keep\n\nYour changes (above ======)\nTheir changes (below ======)\nBoth.\nNeither.\n\nBut always delete the &lt;&lt;&lt;&lt;&lt;, ======, and &gt;&gt;&gt;&gt;&gt; lines.\nOnce you’re satisfied, committing resolves the conflict."
  },
  {
    "objectID": "schedule/slides/00-version-control.html#some-other-pointers",
    "href": "schedule/slides/00-version-control.html#some-other-pointers",
    "title": "UBC Stat406 2025 W1",
    "section": "Some other pointers",
    "text": "Some other pointers\n\nCommits have long names: 32b252c854c45d2f8dfda1076078eae8d5d7c81f\n\nIf you want to use it, you need “enough to be unique”: 32b25\n\nOnline help uses directed graphs in ways different from statistics:\n\nIn stats, arrows point from cause to effect, forward in time\nIn git docs, it’s reversed, they point to the thing on which they depend\n\n\nCheat sheet\nhttps://training.github.com/downloads/github-git-cheat-sheet.pdf"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "href": "schedule/slides/00-version-control.html#how-to-undo-in-3-scenarios",
    "title": "UBC Stat406 2025 W1",
    "section": "How to undo in 3 scenarios",
    "text": "How to undo in 3 scenarios\n\nSuppose we’re concerned about a file named README.md\nOften, git status will give some of these as suggestions\n\n\n\n1. Saved but not staged\n\nIn RStudio, select the file and click   then select  Revert…\n\n```{bash}\n# grab the previously committed version\ngit checkout -- README.md\n```\n2. Staged but not committed\n\nIn RStudio, uncheck the box by the file, then use the method above.\n\n```{bash}\n# unstage\ngit reset HEAD README.md\ngit checkout -- README.md\n```\n\n\n3. Committed\n\nNot easy to do in RStudio…\n\n```{bash}\n# check the log to see where you made the chg,\ngit log\n# go one step before that (eg to 32b252)\n# and grab that earlier version\ngit checkout 32b252 -- README.md\n```\n\n```{bash}\n# alternatively\n# if it happens to also be on another branch\ngit checkout otherbranch -- README.md\n```"
  },
  {
    "objectID": "schedule/slides/00-version-control.html#recovering-from-things",
    "href": "schedule/slides/00-version-control.html#recovering-from-things",
    "title": "UBC Stat406 2025 W1",
    "section": "Recovering from things",
    "text": "Recovering from things\n\nAccidentally did work on main, Tried to Push but got refused\n\n```{bash}\n# make a new branch with everything, but stay on main\ngit branch newbranch\n# undo everything that hasn't been pushed to main\ngit fetch && git reset --hard origin/main\ngit checkout newbranch\n```\n\nMade a branch, did lots of work, realized it’s trash, and you want to burn it\n\n```{bash}\ngit checkout main\ngit branch -d badbranch\n```\n\nAnything more complicated, either post to Slack or LMGTFY"
  },
  {
    "objectID": "schedule/slides/00-intro.html#section",
    "href": "schedule/slides/00-intro.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "Methods for Statistical Learning",
    "text": "Methods for Statistical Learning\nStat 406\nGeoff Pleiss\nLast modified – 09 September 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-intro.html#who-am-i",
    "href": "schedule/slides/00-intro.html#who-am-i",
    "title": "UBC Stat406 2025 W1",
    "section": "Who am I?",
    "text": "Who am I?\n\n\nGeoff Pleiss\n\ngeoff.pleiss@stat.ubc.ca\nhttp://geoffpleiss.com/\nAssistant Professor, Department of Statistics\nResearch interests: machine learning\n\nUncertainty quantification\nSequential decision making\nDeep learning\nML for science"
  },
  {
    "objectID": "schedule/slides/00-intro.html#tas",
    "href": "schedule/slides/00-intro.html#tas",
    "title": "UBC Stat406 2025 W1",
    "section": "TAs",
    "text": "TAs\n\n\n\nAtabak Eghbal\n\n\nJunsong Tang\n\n\nParsa Delivary"
  },
  {
    "objectID": "schedule/slides/00-intro.html#who-are-you",
    "href": "schedule/slides/00-intro.html#who-are-you",
    "title": "UBC Stat406 2025 W1",
    "section": "Who are You?",
    "text": "Who are You?\n\nStats major?\nTook STAT 306?\nTook CPSC 340?\nFeel “knowledgable” about ML?\nNeed this course to graduate?"
  },
  {
    "objectID": "schedule/slides/00-intro.html#this-course",
    "href": "schedule/slides/00-intro.html#this-course",
    "title": "UBC Stat406 2025 W1",
    "section": "This Course",
    "text": "This Course\nGoal:\n\nDevelop deep statistical intuitions about prediction and learning\nDraw connections between modelling/learning paradigms\n\nAssumptions:\n\nYou have familiarity with linear models (STAT 306) or ML basics (CPSC 340)\nYou are willing to put in the work!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#differences-from-prior-courses",
    "href": "schedule/slides/00-intro.html#differences-from-prior-courses",
    "title": "UBC Stat406 2025 W1",
    "section": "Differences from Prior Courses",
    "text": "Differences from Prior Courses\n\n\n\nIf You’ve Taken STAT306\n\nRisks analyses\nHigh dimensional learning methods\nNon-linear learning methods\nNon-parametric learning methods\nUnsupervised learning methods\n“Modern” methods (deep learning/ensembles)\n\n\nIf You’ve Taken CPSC340\n\nSurface level: mostly same content\nUnder the hood: more depth/stats\n\nStatistical modelling/model selection\nBias-variance tradeoff\nCurse of dimensionality\nBlack-box computational methods\nGenerative versus discrminative modelling"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nA history lesson\n\n\nEarly AI, Summers, and Winters\n\n1950s: “intelligent machines”, Turing test\n1960s-80s: Early perceptrons, rule-based systems, hype cycles\n1970s-80s: AI winter(s)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-1",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-1",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\nMeanwhile, in Statistics Land…\nStatisticians are developing frameworks for reasoning, predicting, and making decision from data.\n\n1800s (Legendre and Gauss): predicting form data (least squares)\n1910s (Fisher): estimating unknown parameters (MLE)\n1940s (Shannon): quantifying information in data (information theory)\n1950s (Wald): making decisions from data (decision theory)\n1970s-80s: blackbox computer-based algorithms (bootstrap, MCMC)\n\n\n🧐 Aren’t these the same goals that the AI community has? 🧐"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-2",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-2",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\nStatistians Guide AI\n\n1980s (Vapnik, Chervonenkis): statistical learning theory\n\nStudy learning general knowledge from data\nMathematical formalism for when learning is possible\n\n1990s-2000s: new algorithms based on stats\n\n\nSupport vector machines\nBagging/random forests\nTopic modelling\n\nApplications in the new internet economy (search, ads, product recommendations)\nAI \\(\\rightarrow\\) machine learning"
  },
  {
    "objectID": "schedule/slides/00-intro.html#what-is-statistical-learning-3",
    "href": "schedule/slides/00-intro.html#what-is-statistical-learning-3",
    "title": "UBC Stat406 2025 W1",
    "section": "What is Statistical Learning?",
    "text": "What is Statistical Learning?\n\n\nStatisticians Play Catch-Up\n\n2010s: deep learning revolution\n\nStats: ideas as “unprincipled”\nCS: see lots of empirical success\n\n2020s: generative AI\n\nBreakthroughs come from scale (more data, more compute)\nBig gap between theory and practice\nCore ideas are stat. learning principles"
  },
  {
    "objectID": "schedule/slides/00-intro.html#course-learning-outcomes",
    "href": "schedule/slides/00-intro.html#course-learning-outcomes",
    "title": "UBC Stat406 2025 W1",
    "section": "Course Learning Outcomes",
    "text": "Course Learning Outcomes\n\nFormulate and analyze machine learning algorithms from a statistical perspective\nCategorize learning methods through multiple criteria\nDraw connections between any two learning algorithms and paradigms\nReason about which methods are most appropriate for a given situation\nApply these methods correctly, troubleshoot common pitfalls, and identify probable steps for improving\nImplement and utilize models in R using best coding practices"
  },
  {
    "objectID": "schedule/slides/00-intro.html#modules",
    "href": "schedule/slides/00-intro.html#modules",
    "title": "UBC Stat406 2025 W1",
    "section": "6 Modules",
    "text": "6 Modules\nEach module has a technical content theme (and a statistical principles theme)\n\nBasic regression/classification (statistical modelling/selection)\nLinear methods, regularization, featurization (bias-variance tradeoff)\nNonparametric methods (curse of dimensionality)\nUnsupervised learning (generative vs. discriminative modelling)\nEnsembles (black-box methods)\nDeep learning"
  },
  {
    "objectID": "schedule/slides/00-intro.html#course-components",
    "href": "schedule/slides/00-intro.html#course-components",
    "title": "UBC Stat406 2025 W1",
    "section": "Course Components",
    "text": "Course Components\n\nIn-Class (discussion, mathematical derivations)\nLabs (coding, tools, how-tos)\nHomework Assignments (synthesis, debugging, analysis)\nFinal Exam (all of the above)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#grading-structure",
    "href": "schedule/slides/00-intro.html#grading-structure",
    "title": "UBC Stat406 2025 W1",
    "section": "Grading Structure",
    "text": "Grading Structure\n\n\nKnowledge Based (40 points)\n\n\n\nComponent\nPoints\n\n\n\n\nMidterm\n10 points\n\n\nFinal\n30 points\n\n\nTotal\n40 points\n\n\n\n\nEffort Based (60 points)\n\n\n\nComponent\nPoints\n\n\n\n\nIn-Class\n15 points\n\n\nLabs\n20 points\n\n\nHomework\n40 points\n\n\nTotal\n60 points"
  },
  {
    "objectID": "schedule/slides/00-intro.html#section-1",
    "href": "schedule/slides/00-intro.html#section-1",
    "title": "UBC Stat406 2025 W1",
    "section": "🧐 15 + 20 + 40 > 60??? 🧐",
    "text": "🧐 15 + 20 + 40 &gt; 60??? 🧐\n\n\n\n\n\nComponent\nPoints\n\n\n\n\nIn-Class\n15 points\n\n\nLabs\n20 points\n\n\nHomework\n40 points\n\n\nTotal\n60 points\n\n\n\n\n\n\n\n\\[\\text{effort} = \\min\\left\\{60, \\text{class} + \\text{lab} + \\text{hw}\\right\\}\\]\nGet 60 of the 75 total points any way your want\n\nSkip class + perfect on labs/homework = 60 points\nMiss 1 homework + get in-class points = 60 points\n(More on this later)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#class-structure",
    "href": "schedule/slides/00-intro.html#class-structure",
    "title": "UBC Stat406 2025 W1",
    "section": "Class Structure",
    "text": "Class Structure\n\nLecture: I’ll go over content\nDerivations: You’ll do math in groups\nDiscussions: We’ll analyze content together\nClickers: You’ll apply your knowledge (in pairs)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n8am is hard.\nAttendance is optional."
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-skip-class",
    "href": "schedule/slides/00-intro.html#reasons-to-skip-class",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Skip Class",
    "text": "Reasons to Skip Class\n\nIf you’re sleep deprived\nGetting more sleep may be beneficial to your learning in the long run\nIf you’re not going to engage/participate\nLectures are recorded, watch on your own time\nIf you’re going to have your laptop open\nDo your homework/social media browsing/k-pop video watching in the comfort of your own home; then revisit the course material on your own time"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n1. Time to Think/Digest/Struggle with the Material\n\nDeveloping deep intuitions takes time/effort.\nClass gives you 3 hours/week to chew on the material.\nI’ll pace the material accordingly"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-1",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n2. In-Class is Where We Wrestle with Math\n\nThere is lots of math in the course\nWe will spend time in class working through derivations together\nWorking through derivations builds intuitions and deep knowledge\nFew opportunities on labs/homeworks for this kind of work"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-2",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-2",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n3. Questions and Discussions\n\nThere’s plenty of online materials that teach you this content (books, videos, etc)\nThere’s few opportunities to discuss content with peers/experts\nIf you’re graduating this year, this course may be one of your last opportunities for this type of learning!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-3",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-3",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n4. Effort-Based Points\n\nIf you skip class, you’ll need to get a perfect grade on labs/homeworks to get a maximum effort-based grade\nIf you attend class, you can skip a homework, mess up on all of them, and still get a perfect grade!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-4",
    "href": "schedule/slides/00-intro.html#reasons-to-attend-class-without-distractions-4",
    "title": "UBC Stat406 2025 W1",
    "section": "Reasons to Attend Class (without Distractions)",
    "text": "Reasons to Attend Class (without Distractions)\n5. Recommendation Letters\n\n\nWhat I Write for Students who Attend/Participate/Ask Questions \n\nWhat I Write for Students who Just “Show Up”\nI don’t get to know you, so all I can talk about is your grade."
  },
  {
    "objectID": "schedule/slides/00-intro.html#tldr",
    "href": "schedule/slides/00-intro.html#tldr",
    "title": "UBC Stat406 2025 W1",
    "section": "TLDR",
    "text": "TLDR\n\nWaking up early, avoiding distractions, and participating is hard\nIf you don’t have the energy or mental bandwidth on any given day, give yourself the gift of staying home.\nIf you come in, be prepared to engage and put in effort.\nYou’ll get a lot out of attending/engaging. I’ll make sure of it!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#earning-points",
    "href": "schedule/slides/00-intro.html#earning-points",
    "title": "UBC Stat406 2025 W1",
    "section": "Earning Points",
    "text": "Earning Points\n\n\nParticipation (5 points)\n\nWe will (try to) use Agora for virtual “hand-raising”\n1 point for “raising” your hand\n1 point if you are called on (assigned randomly by Agora)\nGrade: \\(\\min\\left\\{ 5, 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{classes\\_w/\\_agora}} \\right\\}\\)\n\n\nClickers (10 points)\n\nWe will use iClicker for questions that are similar to the midterm/final\n0 points for skipping, 2 points for trying, 4 points for correct\n\nAverage of 3 = 10 points (the max)\nAverage of 2 = 5 points\nAverage of 1 = 0 points\n\nGrade: \\(\\max\\left\\{ 0, \\min\\left\\{ 5 \\frac{n_\\mathrm{points}}{n_\\mathrm{questions}} - 5, 10 \\right\\} \\right\\}\\)\nBe sure to sync your device in Canvas."
  },
  {
    "objectID": "schedule/slides/00-intro.html#mechanics-and-grading",
    "href": "schedule/slides/00-intro.html#mechanics-and-grading",
    "title": "UBC Stat406 2025 W1",
    "section": "Mechanics and Grading",
    "text": "Mechanics and Grading\nThe goal is to “Do the work”\n\n\nLabs\n\nLabs should give you practice, allow for questions with the TAs.\nThey are due at 2300 on Friday, lightly graded.\nYou may do them at home, but you must submit individually (in lab, you may share submission)\nLabs are lightly graded\n\n\nAssignments\n\nNot easy, especially the first 2, especially if you are unfamiliar with R / Rmarkdown / ggplot\nYou may revise to raise your score to 7/10, see Syllabus. Only if you lose 3+ for content (penalties can’t be redeemed).\nDon’t leave these for the last minute"
  },
  {
    "objectID": "schedule/slides/00-intro.html#tools-r-and-github",
    "href": "schedule/slides/00-intro.html#tools-r-and-github",
    "title": "UBC Stat406 2025 W1",
    "section": "Tools: R and Github",
    "text": "Tools: R and Github\n\n\n\nLanguage/Libraries: R + Tidyverse\nSubmission: via Github\n\n\n\n\n\n\n\n\nImportant\n\n\nWe assume you’re familiar with these tools (R, Tidyverse, Git, Github)\nIf you’re not, it’s your responsibility to get up-to-speed with them.\nSee Canvas for tutorials / the website for resources.\n\n\n\n\n\nWorkflow\n\nYou each have your own repo\nYou make a branch\nDO NOT rename files\nMake enough commits (3 for labs, 5 for HW).\nPush your changes (at anytime) and make a PR against main when done.\nTAs review your work.\nIf you want to revise HWs, make changes in response to feedback and push to the same branch. Then “re-request review”."
  },
  {
    "objectID": "schedule/slides/00-intro.html#generative-ai-policy",
    "href": "schedule/slides/00-intro.html#generative-ai-policy",
    "title": "UBC Stat406 2025 W1",
    "section": "Generative AI Policy",
    "text": "Generative AI Policy\n\nAI tools are quite capable with this material (I use them frequently in my own work!)\nStrong recommendation: Use AI as little as possible to maximize learning\nIf you use AI, use it to supplement your thinking (code completion, rewriting) not replace your thinking (feeding entire assignments to chatbots)"
  },
  {
    "objectID": "schedule/slides/00-intro.html#why-minimize-ai-use",
    "href": "schedule/slides/00-intro.html#why-minimize-ai-use",
    "title": "UBC Stat406 2025 W1",
    "section": "Why Minimize AI Use?",
    "text": "Why Minimize AI Use?\n\nDeep intuitions require struggle\nThe only way to develop intuitions about challenging material is to wrestle with content\nStand out from the crowd\nAnyone can use Claude/Copilot for simple ML. Demonstrate thinking beyond these tools will make you more hireable/trusted\nCourse-specific subtleties\nEven with good prompting, chatbots likely won’t score above 7-8 on assignments\nNo AI on exams\nDon’t become too dependent on tools you can’t use during midterm/final"
  },
  {
    "objectID": "schedule/slides/00-intro.html#if-you-use-generative-ai",
    "href": "schedule/slides/00-intro.html#if-you-use-generative-ai",
    "title": "UBC Stat406 2025 W1",
    "section": "If you Use Generative AI…",
    "text": "If you Use Generative AI…\n\nSelf-reporting required:\n\nDescribe your AI usage on all assignments and labs.\nInclude all prompts you use.\nPlease be honest (it’ll help me improve the course/your learning experience)\nNo grade penalty"
  },
  {
    "objectID": "schedule/slides/00-intro.html#late-policy",
    "href": "schedule/slides/00-intro.html#late-policy",
    "title": "UBC Stat406 2025 W1",
    "section": "Late policy",
    "text": "Late policy\nIf you have not submitted your lab/assignment by the time grading starts, you will get a 0.\n\n\n\n\n\n\n\n\n\nWhen you submit\nLikelihood that your submission gets a 0\n\n\n\n\nBefore 11pm on due date (i.e. on time)\n0%\n\n\n11:01pm on due date\n0.01%\n\n\n9am after due date\n50%\n\n\n2 weeks after due date\n99.99999999%\n\n\n\n\n\n\nException: when you have grounds for academic consession. (See the UBC policy.)\n\n\n\n\n\n\nTip\n\n\nRemember: you can still get a “perfect” effort grade even if you get a 0 on one assignment."
  },
  {
    "objectID": "schedule/slides/00-intro.html#textbooks",
    "href": "schedule/slides/00-intro.html#textbooks",
    "title": "UBC Stat406 2025 W1",
    "section": "Textbooks",
    "text": "Textbooks\n\n\n\n\n\n\n\n\nAn Introduction to Statistical Learning\n\n\nJames, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: http://statlearning.com/\n\n\n\n\n\n\n\n\n\n\nThe Elements of Statistical Learning\n\n\nHastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\n\n\n\n\n\nEach class has a “required” reading from Introduction to Statistical Learning\nNo quizzes/knowledge demonstration/accountability for not reading, but please do them!\nReading before class will improve your preparation/ability to engage"
  },
  {
    "objectID": "schedule/slides/00-intro.html#exams",
    "href": "schedule/slides/00-intro.html#exams",
    "title": "UBC Stat406 2025 W1",
    "section": "Exams",
    "text": "Exams\n\n\nMidterm Exam\n\nIn-class, October 23\nIt is hard\nComputer-based\nT/F, multiple choice, etc.\n\n\nFinal Exam\n\nScheduled by the university.\nIt is hard\nThe median last year was 50% \\(\\Rightarrow\\) A-\nFormat: TBD\n\n\n\n\nPhilosophy\n\nIf you put in the effort, you’re guaranteed a C+.\nTo get an A+, you need to deeply understand the material.\nNo penalty for skipping the midterm/final."
  },
  {
    "objectID": "schedule/slides/00-intro.html#time-expectations-per-week",
    "href": "schedule/slides/00-intro.html#time-expectations-per-week",
    "title": "UBC Stat406 2025 W1",
    "section": "Time expectations per week:",
    "text": "Time expectations per week:\n\nComing to class – 3 hours\nReading the book – 1 hour\nLabs – 1 hour\nHomework – 4 hours\nStudy / thinking / playing – 1 hour"
  },
  {
    "objectID": "schedule/slides/00-intro.html#computer",
    "href": "schedule/slides/00-intro.html#computer",
    "title": "UBC Stat406 2025 W1",
    "section": "Computer",
    "text": "Computer\n\n\n\n\n\nWe will use R and we assume some background knowledge.\nSuggest you use RStudio IDE\nSee https://ubc-stat.github.io/stat-406/ for what you need to install for the whole term.\nLinks to useful supplementary resources are available on the website."
  },
  {
    "objectID": "schedule/slides/00-intro.html#other-resources",
    "href": "schedule/slides/00-intro.html#other-resources",
    "title": "UBC Stat406 2025 W1",
    "section": "Other Resources",
    "text": "Other Resources\n\nCourse website\nAll the material (slides, extra worksheets) https://ubc-stat.github.io/stat-406\nCanvas (minimal)\nQuiz 0, grades, course time/location info, links to videos from class\nSlack\nDiscussion board, questions\nGithub\nHomework / Lab submission"
  },
  {
    "objectID": "schedule/slides/00-intro.html#final-words-of-wisdom",
    "href": "schedule/slides/00-intro.html#final-words-of-wisdom",
    "title": "UBC Stat406 2025 W1",
    "section": "Final Words of Wisdom",
    "text": "Final Words of Wisdom\n\n\n8am is hard. But you can do it!\n\nI strongly urge you to get up at the same time everyday.\n\n\n\n\n\nIf you need help, please ask!\n\nI’m here to encourage you, get you un-stuck, and ponder ML mysteries with you\n\n\n\n\n\nThink of this course as preparation for a race/performance/etc.\n\nYou have to work out/practice, and there’s no shortcuts.\nTraining is hard, but you’ll be pleased with the outcome if you put in the work.\nWe’re here to help you stay excited and motivated on your journey!\n\n\n\n\n\nJoin me on a fun exploration of cool material!"
  },
  {
    "objectID": "schedule/slides/00-intro.html#todos",
    "href": "schedule/slides/00-intro.html#todos",
    "title": "UBC Stat406 2025 W1",
    "section": "TODOs",
    "text": "TODOs\n\n\nBy EOD Tomorrow:\n\nRead the syllabus\nInstall the R package, read docs, check your LaTeX installation\nBE SURE to follow the Computer Setup instructions on the website!\nTake Quiz 00 on Canvas\n\n\nBefore Next Tuesday:\n\nSign up for Slack (see Canvas)\nSign up for Agora (details on Canvas)\nLink your iClicker account with Canvas\n(If needed) watch Github and R tutorials\n\n\n\n\nAgora Signup"
  },
  {
    "objectID": "course-setup.html",
    "href": "course-setup.html",
    "title": "Guide for setting up the course infrastructure",
    "section": "",
    "text": "Version 2025\nThis guide (hopefully) gives enough instructions for recreating new iterations of Stat 406."
  },
  {
    "objectID": "course-setup.html#create-a-github.com-organization",
    "href": "course-setup.html#create-a-github.com-organization",
    "title": "Guide for setting up the course infrastructure",
    "section": "Create a GitHub.com organization",
    "text": "Create a GitHub.com organization\n\nThis is free for faculty with instructor credentials.\n\nNote make sure you upgrade the organization to a “Github Team.” If you have registered your instructor credentials with Github, you should be able to upgrade for free from the Github Global Campus page under “Upgrade your academic organizations.”\n\nAllows more comprehensive GitHub actions, PR templates and CODEOWNER behaviour than the UBC Enterprise version (last I checked)\nDownside is getting students added (though we include R scripts for this)\n\nOnce done, go to https://github.com/watching. Click the Red Down arrow “Unwatch all”. Then select this Org. The TAs should do the same.\n\nPermissions and structure\nSettings &gt; Member Privileges\nWe list only the important ones.\n\nBase Permissions: No Permission\nRepository creation: None\nRepo forking: None\nPages creation: None\nTeam creation rules: No\n\nBe sure to click save in each area after making changes.\nSettings &gt; Actions &gt; General\nAll repositories: Allow all actions and reusable workflows.\nWorkflow permissions: Read and write permissions.\n\n\nTeams\n\n2 teams, one for the TAs and one for the students\nYou must then manually add the teams to any repos they should access\n\nI generally give the TAs “Write” permission, and the students “Read” permission with some exceptions. See the Repos section below."
  },
  {
    "objectID": "course-setup.html#repos",
    "href": "course-setup.html#repos",
    "title": "Guide for setting up the course infrastructure",
    "section": "Repos",
    "text": "Repos\nThere are typically about 10 repositories. Homeworks and Labs each have 3 with very similar behaviours.\nBe careful copying directories. All of them have hidden files and folders, e.g. .git. Of particular importance are the .github directories which contain PR templates and GitHub Actions. Also relevant are the .Rprofile files which try to override Student Language settings and avoid unprintible markdown characters.\n\nHomeworks\n\nhomework-solutions\nThis is where most of the work happens. My practice is to create the homework solutions first. I edit these (before school starts) until I’m happy. I then duplicate the file and remove the answers. The result is hwxx-instructions.Rmd. The .gitignore file should ignore all of the solutions and commit only the instructions. Then, about 1 week after the deadline, I adjust the .gitignore and push the solution files.\n\nStudents have Read permission.\nTAs have Write permission.\nThe preamble.tex file is common to HWs and Labs. It creates a lavender box where the solution will go. This makes life easy for the TAs.\n\n\n\nhomework-template\nThis is a “template repo” used for creating student specific homework-studentgh repos (using the setup scripts).\nVery Important: copy the hwxx-instructions files over to a new directory. Do NOT copy the directory or you’ll end up with the solutions visible to the students.\nThen rename hwxx-instructions.Rmd to hwxx.Rmd. Now the students have a .pdf with instructions, and a template .Rmd to work on.\nOther important tasks:\n\nThe .gitignore is more elaborate in an attempt to avoid students pushing junk into these repos.\nThe .github directory contains 3 files:\n\nCODEOWNERS begins as an empty doc which will be populated with the assigned grader later;\npull_request_template.md is used for all HW submission PRs;\nworkflows contains a GH-action to comment on the PR with the date+time when the PR is opened.\n\nUnder Settings &gt; General, select “Template repository”. This makes it easier to duplicate to the student repos.\nSetup branch protection rules for the main branch. Create a new ruleset for default branches, and select the following:\n\nRequire a pull request before merging\nRequire review from Code Owners\nBlock force pushes\nI recommend adding the @TAs team to the bypass list.\n\n\n\n\n\nLabs\nThe three Labs repos operate exactly as the analogous homework repos.\n\nlabs-solutions\nDo any edits here before class begins.\n\n\nlabs-template\nSame as with the homeworks\n\n\n\nclicker-solutions\nThis contains the complete set of clicker questions.\nAnswers are hidden in comments on the presentation.\nI release them incrementally after each module (copying over from my clicker deck).\n\n\npractice-final\nContains a lengthy practice exam as well as solutions. I usually only post this during the last week of class.\n\n\nopen-pr-log\nThis contains a some GitHub actions to automatically keep track of open PRs for the TAs.\nIt’s still in testing phase, but should work properly. It will create two markdown docs, 1 for labs and 1 for homework. Each shows the assigned TA, the date the PR was opened, and a link to the PR. If everything is configured properly, it should run automatically at 3am every night.\n\nOnly the TAs should have access.\nUnder Settings &gt; Secrets and Variables &gt; Actions you must add a “Repository Secret”. This should be a GitHub Personal Access Token created in your account (Settings &gt; Developer settings &gt; Tokens (classic)). It needs Repo, Workflow, and Admin:Org permissions. I set it to expire at the end of the course. I use it only for this purpose (rather than my other tokens for typical logins).\n\n\n\n.github / .github-private\nThese contain README files that give some basic information about the available repos and the course.\nIt’s visible Publically, and appears on the Org homepage for all to see. The .github-private has the same function, but applies only to Org members.\n\n\nbakeoff-bakeoff\nThis is for the bonus for HW4. Both TAs and Students have access. I put the TA team as CODEOWNERS and protect the main branch (Settings &gt; Branches &gt; Branch Protection Rules). Here, we “Require approvals” and “Require Review from Code Owners”."
  },
  {
    "objectID": "computing/windows.html",
    "href": "computing/windows.html",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#installation-notes",
    "href": "computing/windows.html#installation-notes",
    "title": " Windows",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/windows.html#terminal",
    "href": "computing/windows.html#terminal",
    "title": " Windows",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works."
  },
  {
    "objectID": "computing/windows.html#github",
    "href": "computing/windows.html#github",
    "title": " Windows",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/windows.html#git-bash-and-windows-terminal",
    "href": "computing/windows.html#git-bash-and-windows-terminal",
    "title": " Windows",
    "section": "Git, Bash, and Windows Terminal",
    "text": "Git, Bash, and Windows Terminal\nAlthough these three are separate programs, we are including them in the same section here since they are packaged together in the same installer on Windows. Briefly, we will be using the Bash shell to interact with our computers via a command line interface, Git to keep a version history of our files and upload to/download from to GitHub, and Windows Terminal to run the both Bash and Git.\nGo to https://git-scm.com/download/win and download the windows version of git. After the download has finished, run the installer and accept the default configuration for all pages except for the following:\n\nOn the Select Components page, add a Git Bash profile to Windows Terminal.\n\n\nTo install windows terminal visit this link and click Get to open it in Windows Store. Inside the Store, click Get again and then click Install. After installation, click Launch to start Windows Terminal. In the top of the window, you will see the tab bar with one open tab, a plus sign, and a down arrow. Click the down arrow and select Settings (or type the shortcut Ctrl + ,). In the Startup section, click the dropdown menu under Default profile and select Git Bash.\n\nYou can now launch the Windows terminal from the start menu or pin it to the taskbar like any other program (you can read the rest of the article linked above for additional tips if you wish). To make sure everything worked, close down Windows Terminal, and open it again. Git Bash should open by default, the text should be green and purple, and the tab should read MINGW64:/c/Users/$USERNAME (you should also see /c/Users/$USERNAME if you type pwd into the terminal). This screenshot shows what it should look like:\n\n\n\n\n\n\n\nNote\n\n\n\nWhenever we refer to “the terminal” in these installation instructions, we want you to use the Windows Terminal that you just installed with the Git Bash profile. Do not use Windows PowerShell, CMD, or anything else unless explicitly instructed to do so.\n\n\nTo open a new tab you can click the plus sign or use Ctrl + Shift + t (you can close a tab with Ctrl + Shift + w). To copy text from the terminal, you can highlight it with the mouse and then click Ctrl + Shift + c. To paste text you use Ctrl + Shift + v, try it by pasting the following into the terminal to check which version of Bash you just installed:\nbash --version\nThe output should look similar to this:\nGNU bash, version 4.4.23(1)-release (x86_64-pc-sys)\nCopyright (C) 2019 Free Software Foundation, Inc.\nLicense GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;\nThis is free software; you are free to change and redistribute it.\nThere is NO WARRANTY, to the extent permitted by law.\n\n\n\n\n\n\nNote\n\n\n\nIf there is a newline (the enter character) in the clipboard when you are pasting into the terminal, you will be asked if you are sure you want to paste since this newline will act as if you pressed enter and run the command. As a guideline you can press Paste anyway unless you are sure you don’t want this to happen.\n\n\nLet’s also check which version of git was installed:\ngit --version\ngit version 2.32.0.windows.2\n\n\n\n\n\n\nNote\n\n\n\nSome of the Git commands we will use are only available since Git 2.23, so make sure your if your Git is at least this version.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/windows.html#latex",
    "href": "computing/windows.html#latex",
    "title": " Windows",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nNote that you might see two error messages regarding lua during the installation, you can safely ignore these, the installation will complete successfully after clicking “OK”.\nIf it still fails, proceed with the instructions\n\nIn RStudio, run the following commands to install the tinytex package and setup tinytex:\ninstall.packages('tinytex')\ntinytex::install_tinytex()\nIn order for Git Bash to be able to find the location of TinyTex, you will need to sign out of Windows and back in again. After doing that, you can check that the installation worked by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2021/W32TeX)\nkpathsea version 6.3.3\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/windows.html#github-pat",
    "href": "computing/windows.html#github-pat",
    "title": " Windows",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/windows.html#post-installation-notes",
    "href": "computing/windows.html#post-installation-notes",
    "title": " Windows",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/windows.html#attributions",
    "href": "computing/windows.html#attributions",
    "title": " Windows",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/mac_x86.html",
    "href": "computing/mac_x86.html",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#installation-notes",
    "href": "computing/mac_x86.html#installation-notes",
    "title": " MacOS x86",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_x86.html#terminal",
    "href": "computing/mac_x86.html#terminal",
    "title": " MacOS x86",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_x86.html#github",
    "href": "computing/mac_x86.html#github",
    "title": " MacOS x86",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_x86.html#git",
    "href": "computing/mac_x86.html#git",
    "title": " MacOS x86",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_x86.html#latex",
    "href": "computing/mac_x86.html#latex",
    "title": " MacOS x86",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_x86.html#github-pat",
    "href": "computing/mac_x86.html#github-pat",
    "title": " MacOS x86",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_x86.html#post-installation-notes",
    "href": "computing/mac_x86.html#post-installation-notes",
    "title": " MacOS x86",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_x86.html#attributions",
    "href": "computing/mac_x86.html#attributions",
    "title": " MacOS x86",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/index.html",
    "href": "computing/index.html",
    "title": " Computing",
    "section": "",
    "text": "In order to participate in this class, we will require the use of R, and encourage the use of RStudio. Both are free, and you likely already have both.\nYou also need Git, Github and Slack.\nBelow are instructions for installation. These are edited and simplified from the DSCI 310 Setup Instructions. If you took DSCI 310 last year, you may be good to go, with the exception of the R package."
  },
  {
    "objectID": "computing/index.html#laptop-requirements",
    "href": "computing/index.html#laptop-requirements",
    "title": " Computing",
    "section": "Laptop requirements",
    "text": "Laptop requirements\n\nRuns one of the following operating systems: Ubuntu 20.04, macOS (version 11.4.x or higher), Windows 10 (version 2004, 20H2, 21H1 or higher).\n\nWhen installing Ubuntu, checking the box “Install third party…” will (among other things) install proprietary drivers, which can be helpful for wifi and graphics cards.\n\nCan connect to networks via a wireless connection for on campus work\nHas at least 30 GB disk space available\nHas at least 4 GB of RAM\nUses a 64-bit CPU\nIs at most 6 years old (4 years old or newer is recommended)\nUses English as the default language. Using other languages is possible, but we have found that it often causes problems in the homework. We’ve done our best to fix them, but we may ask you to change it if you are having trouble.\nStudent user has full administrative access to the computer."
  },
  {
    "objectID": "computing/index.html#software-installation-instructions",
    "href": "computing/index.html#software-installation-instructions",
    "title": " Computing",
    "section": "Software installation instructions",
    "text": "Software installation instructions\nPlease click the appropriate link below to view the installation instructions for your operating system:\n\nmacOS x86 or macOS arm\nUbuntu\nWindows"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stat 406",
    "section": "",
    "text": "Jump to Schedule\n\n\nSyllabus"
  },
  {
    "objectID": "faq.html",
    "href": "faq.html",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Working with Git in RStudio involves several key steps:\n\nEnsure you are on the main branch. Pull in remote changes by clicking the down arrow .\nCreate a new branch by clicking the branch icon .\nWork on your documents and save frequently.\nStage your changes by checking the appropriate boxes.\nCommit your changes by clicking Commit.\nRepeat steps 3-5 as needed.\nPush to GitHub by clicking the up arrow .\nOpen a pull request (PR) on GitHub.\nUse the dropdown menu to return to main to avoid potential issues.\n\n\n\n\nIf you prefer using the command line, follow these steps:\n\nPull remote changes (optional but recommended): git pull\nCreate a new branch: git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes:\n\nFor specific documents: git add &lt;name-of-document1&gt;\nFor all changed documents: git add .\n\nCommit your changes with a meaningful message: git commit -m \"Descriptive commit message\"\nRepeat steps 3-5 as needed.\nPush to GitHub: git push (follow any suggested command variations)\nOpen a pull request on GitHub.\nSwitch back to main: git checkout main"
  },
  {
    "objectID": "faq.html#course-workflow-homework-and-labs",
    "href": "faq.html#course-workflow-homework-and-labs",
    "title": " Frequently asked questions",
    "section": "",
    "text": "Working with Git in RStudio involves several key steps:\n\nEnsure you are on the main branch. Pull in remote changes by clicking the down arrow .\nCreate a new branch by clicking the branch icon .\nWork on your documents and save frequently.\nStage your changes by checking the appropriate boxes.\nCommit your changes by clicking Commit.\nRepeat steps 3-5 as needed.\nPush to GitHub by clicking the up arrow .\nOpen a pull request (PR) on GitHub.\nUse the dropdown menu to return to main to avoid potential issues.\n\n\n\n\nIf you prefer using the command line, follow these steps:\n\nPull remote changes (optional but recommended): git pull\nCreate a new branch: git branch -b &lt;name-of-branch&gt;\nWork on your documents and save frequently.\nStage your changes:\n\nFor specific documents: git add &lt;name-of-document1&gt;\nFor all changed documents: git add .\n\nCommit your changes with a meaningful message: git commit -m \"Descriptive commit message\"\nRepeat steps 3-5 as needed.\nPush to GitHub: git push (follow any suggested command variations)\nOpen a pull request on GitHub.\nSwitch back to main: git checkout main"
  },
  {
    "objectID": "faq.html#homework-regrade-procedure",
    "href": "faq.html#homework-regrade-procedure",
    "title": " Frequently asked questions",
    "section": "Homework Regrade Procedure",
    "text": "Homework Regrade Procedure\n\n\n\n\n\n\nRegrade Eligibility\n\n\n\n\nDeductions must exceed 3 points and be related to content (not penalties).\nErrors can be corrected to potentially raise the grade to 7/10.\nRevisions and review requests must be submitted within one week of the initial review.\n\n\n\nRegrade process:\n\nLocate the local branch for the specific homework assignment.\n\nCheck the “Pull Requests” tab in your GitHub repository if you can’t recall the branch name.\n\nMake necessary corrections to the files.\nCommit and push changes, ensuring the PDF is re-rendered if needed.\nFind the original PR for the assignment on GitHub.\nAdd a concise, clear comment to the TA describing your changes.\nClick the recycling (🔁) button under “Reviewers” to request a review."
  },
  {
    "objectID": "faq.html#common-git-and-workflow-issues",
    "href": "faq.html#common-git-and-workflow-issues",
    "title": " Frequently asked questions",
    "section": "Common Git and Workflow Issues",
    "text": "Common Git and Workflow Issues\n\nmaster/main\n“master” has some pretty painful connotations. So as part of an effort to remove racist names from code, the default branch is now “main” on new versions of GitHub. But old versions (like the UBC version) still have “master”. Below, I’ll use “main”, but if you see “master” on what you’re doing, that’s the one to use.\n\n\nStart from main\nBranches should be created from the main branch, not the one you used for the last assignment.\ngit checkout main\nThis switches to main. Then pull and start the new assignment following the workflow above. (In Rstudio, use the dropdown menu.)\n\n\nYou forgot to work on a new branch\nUgh, you did some labs before realizing you forgot to create a new branch. Don’t stress. There are some things below to try. But if you’re confused ASK. We’ve had practice with this, and soon you will too!\n(1) If you started from main and haven’t made any commits (but you SAVED!!):\ngit branch -b &lt;new-branch-name&gt;\nThis keeps everything you have and puts you on a new branch. No problem. Commit and proceed as usual.\n(2) If you are on main and made some commits:\ngit branch &lt;new-branch-name&gt;\ngit log\nThe first line makes a new branch with all the stuff you’ve done. Then we look at the log. Locate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit reset ac2a8 --hard\nThis rolls main back to that commit. You don’t need the whole string, just the first few characters. Finally\ngit checkout &lt;new-branch-name&gt;\nand continue working.\n(3) If you started work on &lt;some-old-branch&gt; for work you already submitted: This one is harder, and I would suggest getting in touch with the TAs. Here’s the procedure.\ngit commit -am \"uhoh, I need to be on a different branch\"\ngit branch &lt;new-branch-name&gt;\nCommit your work with a dumb message, then create a new branch. It’s got all your stuff.\ngit log\nLocate the most recent commit before you started working. It’s a long string like ac2a8365ce0fa220c11e658c98212020fa2ba7d1. Then,\ngit rebase --onto main ac2a8 &lt;new-branch-name&gt;\ngit checkout &lt;new-branch-name&gt;\nThis makes the new branch look like main but without the differences from main that are on ac2a8 and WITH all the work you did after ac2a8. It’s pretty cool. And should work. Finally, we switch to our new branch."
  },
  {
    "objectID": "faq.html#improving-your-r-programming-skills",
    "href": "faq.html#improving-your-r-programming-skills",
    "title": " Frequently asked questions",
    "section": "Improving Your R Programming Skills",
    "text": "Improving Your R Programming Skills\n\nLearning Approach\nLearning to code is an active, immersive process. Simply reading books or watching videos is insufficient. To truly learn R:\n\nComplete tutorials multiple times\nExplore textbook code thoroughly\nQuestion the rationale behind function choices\nExperiment with different coding approaches\nBreak down and understand each line of code\n\n\n\nRecommended Learning Resources\n\nData Science: A First Introduction\nR for Data Science\nDSCI 310 Course Notes\nHappy Git with R\nModern Dive: Statistical Inference via Data Science\nStat545"
  },
  {
    "objectID": "faq.html#debugging-code",
    "href": "faq.html#debugging-code",
    "title": " Frequently asked questions",
    "section": "Debugging Code",
    "text": "Debugging Code\n\nGeneral Debugging Workflow\nWhen your code doesn’t run:\n\nIf the code runs but doesn’t produce expected results, see the code quality section.\nRead the Error Message\n\nError messages provide crucial debugging hints\nParsing them can be challenging but is a valuable skill\n\nExample:\n\nset.seed(12345)\ny &lt;- rnorm(10)\nx &lt;- matrix(rnorm(20), 2)\nlinmod &lt;- lm(y ~ x)\n## Error in model.frame.default(formula = y ~ x, drop.unused.levels = TRUE): variable lengths differ (found for 'x')\n\nNotice the error about variable lengths and matrix dimensions.\nConsult Documentation\n\nUse function-specific help (e.g., ?matrix)\n\nSearch Online\n\nCopy error messages into search engines\nRemove specific, identifying information\n\nSeek Peer Help\n\nUse class Slack channels\nPrepare a minimal working example (MWE)\n\nInstructor/TA Consultation\n\nBe prepared to show your code\nProvide a reproducible example\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen seeking help, always be ready to share your code or MWE.\n\n\nNote: If the error cannot be reproduced, it is unlikely that anyone can help you effectively."
  },
  {
    "objectID": "faq.html#crafting-minimal-working-examples-mwes",
    "href": "faq.html#crafting-minimal-working-examples-mwes",
    "title": " Frequently asked questions",
    "section": "Crafting Minimal Working Examples (MWEs)",
    "text": "Crafting Minimal Working Examples (MWEs)\nAn MWE is a compact code snippet that:\n\nReproduces an error on any machine\nIsolates the specific problem\nMinimizes external dependencies\n\nBenefits of creating MWEs:\n\nOften helps you solve the problem independently\nReveals the root cause of issues\nMakes it easier for others to help you\n\n\nMWE Tips\n\nSet random seeds for reproducibility\nUse minimal, generic data\nInclude only essential code\n\nPreparing an MWE is a valuable debugging skill. By stripping your problem down to its bare essence, you often uncover the root issue. For instance, the previous debugging example was an MWE: it used a fixed seed, ensured data reproducibility, and focused on a specific error.\nFor further guidance, consult:\n\nR Overview Slides\nStack Exchange Discussion on MWEs"
  },
  {
    "objectID": "faq.html#writing-high-quality-code",
    "href": "faq.html#writing-high-quality-code",
    "title": " Frequently asked questions",
    "section": "Writing High-Quality Code",
    "text": "Writing High-Quality Code\nThis is covered in much greater detail in the lectures. Here are key principles for writing clean, efficient R code:\n\nUse Script Files\n\nSave and source scripts\nAvoid console-only work\nTreat R as a scripting language, not a calculator\n\nAvoid Code Repetition\n\nNever copy and paste code\nDefine constants at the script’s beginning\nCreate reusable functions\n\nFunction Design\n\nFunctions are easily testable\nVerify inputs and outputs\nCatch potential errors through comprehensive testing\n\nError Types\n\nSyntax errors (detectable by R)\n\nMissing parentheses\nIncorrect arguments\n\nLogical errors (require thorough testing)\n\nRequire manual verification of results\n\n\nAvoid Magic Numbers\n\nAlways define constants\nMake numerical values meaningful and clear\n\nUse Meaningful Names\nBad example:\ndata(\"ChickWeight\")\nout &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\nGood example:\ndata(\"ChickWeight\")\nchick_weight_model &lt;- lm(weight ~ Time + Chick + Diet, data = ChickWeight)\nComment Strategically\n\nExplain code that isn’t immediately clear\nFocus on the “why”, not just the “what”\n\nExample of helpful commenting:\n# Calculate weighted average of chick weights, squared and adjusted\nchick_weight_summary &lt;- with(\n  ChickWeight,\n  by(weight, Chick, function(x) (x^2 + 23) / length(x))\n)\n\nRemember: Clear, readable code is as much about communication as it is about functionality."
  },
  {
    "objectID": "computing/mac_arm.html",
    "href": "computing/mac_arm.html",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#installation-notes",
    "href": "computing/mac_arm.html#installation-notes",
    "title": " MacOS ARM",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below.\nIn all the sections below, if you are presented with the choice to download either a 64-bit (also called x64) or a 32-bit (also called x86) version of the application always choose the 64-bit version."
  },
  {
    "objectID": "computing/mac_arm.html#terminal",
    "href": "computing/mac_arm.html#terminal",
    "title": " MacOS ARM",
    "section": "Terminal",
    "text": "Terminal\nBy “Terminal” below we mean the command line program called “Terminal”. Note that this is also available Inside RStudio. Either works. To easily pull up the Terminal (outside RStudio), Type Cmd + Space then begin typing “Terminal” and press Return."
  },
  {
    "objectID": "computing/mac_arm.html#github",
    "href": "computing/mac_arm.html#github",
    "title": " MacOS ARM",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/mac_arm.html#git",
    "href": "computing/mac_arm.html#git",
    "title": " MacOS ARM",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, we ask you to update it using the Xcode command line tools (not all of Xcode), which includes Git.\nOpen Terminal and type the following command to install Xcode command line tools:\nxcode-select --install\nAfter installation, in terminal type the following to ask for the version:\ngit --version\nyou should see something like this (does not have to be the exact same version) if you were successful:\ngit version 2.32.1 (Apple Git-133)\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Mac OS section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/mac_arm.html#latex",
    "href": "computing/mac_arm.html#latex",
    "title": " MacOS ARM",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it stall fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/mac_arm.html#github-pat",
    "href": "computing/mac_arm.html#github-pat",
    "title": " MacOS ARM",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/mac_arm.html#post-installation-notes",
    "href": "computing/mac_arm.html#post-installation-notes",
    "title": " MacOS ARM",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/mac_arm.html#attributions",
    "href": "computing/mac_arm.html#attributions",
    "title": " MacOS ARM",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "computing/ubuntu.html",
    "href": "computing/ubuntu.html",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#installation-notes",
    "href": "computing/ubuntu.html#installation-notes",
    "title": " Ubuntu",
    "section": "",
    "text": "If you have already installed Git, LaTeX, or any of the R packages, you should be OK. However, if you have difficulty with Homework or Labs, we may ask you to uninstall and try again.\nIn order to be able to support you effectively and minimize setup issues and software conflicts, we suggest you install the required software as specified below."
  },
  {
    "objectID": "computing/ubuntu.html#ubuntu-software-settings",
    "href": "computing/ubuntu.html#ubuntu-software-settings",
    "title": " Ubuntu",
    "section": "Ubuntu software settings",
    "text": "Ubuntu software settings\nTo ensure that you are installing the right version of the software in this guide, open “Software & Updates” and make sure that the boxes in the screenshot are checked (this is the default configuration)."
  },
  {
    "objectID": "computing/ubuntu.html#github",
    "href": "computing/ubuntu.html#github",
    "title": " Ubuntu",
    "section": "GitHub",
    "text": "GitHub\nIn Stat 406 we will use the publicly available GitHub.com. If you do not already have an account, please sign up for one at GitHub.com\nSign up for a free account at GitHub.com if you don’t have one already."
  },
  {
    "objectID": "computing/ubuntu.html#git",
    "href": "computing/ubuntu.html#git",
    "title": " Ubuntu",
    "section": "Git",
    "text": "Git\nWe will be using the command line version of Git as well as Git through RStudio. Some of the Git commands we will use are only available since Git 2.23, so if your Git is older than this version, so if your Git is older than this version, we ask you to update it using the following commands:\nsudo apt update\nsudo apt install git\nYou can check your git version with the following command:\ngit --version\n\n\n\n\n\n\nNote\n\n\n\nIf you run into trouble, please see the Install Git Linux section from Happy Git and GitHub for the useR for additional help or strategies for Git installation.\n\n\n\nConfiguring Git user info\nNext, we need to configure Git by telling it your name and email. To do this, type the following into the terminal (replacing Jane Doe and janedoe@example.com, with your name and email that you used to sign up for GitHub, respectively):\ngit config --global user.name \"Jane Doe\"\ngit config --global user.email janedoe@example.com\n\n\n\n\n\n\nNote\n\n\n\nTo ensure that you haven’t made a typo in any of the above, you can view your global Git configurations by either opening the configuration file in a text editor (e.g. via the command nano ~/.gitconfig) or by typing git config --list --global).\n\n\nIf you have never used Git before, we recommend also setting the default editor:\ngit config --global core.editor nano\nIf you prefer VScode (and know how to set it up) or something else, feel free."
  },
  {
    "objectID": "computing/ubuntu.html#latex",
    "href": "computing/ubuntu.html#latex",
    "title": " Ubuntu",
    "section": "LaTeX",
    "text": "LaTeX\nIt is possible you already have this installed.\nFirst try the following check in RStudio\nStat406::test_latex_installation()\nIf you see Green checkmarks, then you’re good.\nEven if it fails, follow the instructions, and try it again.\nIf it still fails, proceed with the instructions\n\nWe will install the lightest possible version of LaTeX and its necessary packages as possible so that we can render Jupyter notebooks and R Markdown documents to html and PDF. If you have previously installed LaTeX, please uninstall it before proceeding with these instructions.\nFirst, run the following command to make sure that /usr/local/bin is writable:\nsudo chown -R $(whoami):admin /usr/local/bin\n\n\n\n\n\n\nNote\n\n\n\nYou might be asked to enter your password during installation.\n\n\nNow open RStudio and run the following commands to install the tinytex package and setup tinytex:\ntinytex::install_tinytex()\nYou can check that the installation is working by opening a terminal and asking for the version of latex:\nlatex --version\nYou should see something like this if you were successful:\npdfTeX 3.141592653-2.6-1.40.23 (TeX Live 2022/dev)\nkpathsea version 6.3.4/dev\nCopyright 2021 Han The Thanh (pdfTeX) et al.\nThere is NO warranty.  Redistribution of this software is\ncovered by the terms of both the pdfTeX copyright and\nthe Lesser GNU General Public License.\nFor more information about these matters, see the file\nnamed COPYING and the pdfTeX source.\nPrimary author of pdfTeX: Han The Thanh (pdfTeX) et al.\nCompiled with libpng 1.6.37; using libpng 1.6.37\nCompiled with zlib 1.2.11; using zlib 1.2.11\nCompiled with xpdf version 4.03"
  },
  {
    "objectID": "computing/ubuntu.html#github-pat",
    "href": "computing/ubuntu.html#github-pat",
    "title": " Ubuntu",
    "section": "Github PAT",
    "text": "Github PAT\nYou’re probably familiar with 2-factor authentication for your UBC account or other accounts which is a very secure way to protect sensitive information (in case your password gets exposed). Github uses a Personal Access Token (PAT) for the Command Line Interface (CLI) and RStudio. This is different from the password you use to log in with a web browser. You will have to create one. There are some nice R functions that will help you along, and I find that easiest.\nComplete instructions are in Chapter 9 of Happy Git With R. Here’s the quick version (you need the usethis and gitcreds libraries, which you can install with install.packages(c(\"usethis\", \"gitcreds\"))):\n\nIn the RStudio Console, call usethis::create_github_token() This should open a webbrowser. In the Note field, write what you like, perhaps “Stat 406 token”. Then update the Expiration to any date after December 15. (“No expiration” is fine, though not very secure). Make sure that everything in repo is checked. Leave all other checks as is. Scroll to the bottom and click the green “Generate Token” button.\nThis should now give you a long string to Copy. It often looks like ghp_0asfjhlasdfhlkasjdfhlksajdhf9234u. Copy that. (You would use this instead of the browser password in RStudio when it asks for a password).\nTo store the PAT permanently in R (so you’ll never have to do this again, hopefully) call gitcreds::gitcreds_set() and paste the thing you copied there."
  },
  {
    "objectID": "computing/ubuntu.html#post-installation-notes",
    "href": "computing/ubuntu.html#post-installation-notes",
    "title": " Ubuntu",
    "section": "Post-installation notes",
    "text": "Post-installation notes\nYou have completed the installation instructions, well done 🙌!"
  },
  {
    "objectID": "computing/ubuntu.html#attributions",
    "href": "computing/ubuntu.html#attributions",
    "title": " Ubuntu",
    "section": "Attributions",
    "text": "Attributions\nThe DSCI 310 Teaching Team, notably, Anmol Jawandha, Tomas Beuzen, Rodolfo Lourenzutti, Joel Ostblom, Arman Seyed-Ahmadi, Florencia D’Andrea, and Tiffany Timbers."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus (2025 W1)",
    "section": "",
    "text": "Dates:\nTerm 2025 W1, 04 Sep - 04 Dec 2025\nInstructor:\nGeoff Pleiss\nWebsite: https://geoffpleiss.com/\nEmail: geoff.pleiss@stat.ubc.ca\nSlack: @geoff\nOffice hours:\nSee Canvas for times and locations.\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGitHub: https://github.com/stat-406-2025/\nCanvas: https://canvas.ubc.ca/courses/147492/\nLectures/Labs:\nSee Canvas for times and locations.\nTextbooks:\n[ISLR] [ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus (2025 W1)",
    "section": "",
    "text": "Dates:\nTerm 2025 W1, 04 Sep - 04 Dec 2025\nInstructor:\nGeoff Pleiss\nWebsite: https://geoffpleiss.com/\nEmail: geoff.pleiss@stat.ubc.ca\nSlack: @geoff\nOffice hours:\nSee Canvas for times and locations.\nCourse webpage:\nWWW: https://ubc-stat.github.io/stat-406/\nGitHub: https://github.com/stat-406-2025/\nCanvas: https://canvas.ubc.ca/courses/147492/\nLectures/Labs:\nSee Canvas for times and locations.\nTextbooks:\n[ISLR] [ESL]\nPrerequisite:\nSTAT 306 or CPSC 340"
  },
  {
    "objectID": "syllabus.html#course-objectives",
    "href": "syllabus.html#course-objectives",
    "title": "Syllabus (2025 W1)",
    "section": "Course Objectives",
    "text": "Course Objectives\nThis course introduces machine learning from a statistical perspective. Beginning with linear models and building beyond STAT 306 content, we will progress to an in-depth coverage of classic and modern learning methods with greater mathematical and statistical depth than CPSC 340.\n\nLearning Outcomes\nThe primary aim of this course is to develop deep statistical intuitions about different learning methods and the connections between these methods. If we are successful, by the end of the course you will be able to:\n\nFormulate and analyze machine learning algorithms from a statistical perspective\nCategorize learning methods through multiple criteria (supervised versus unsupervised, linear versus nonlinear, parametric versus nonparametric, high bias versus high variance)\nDraw connections between any two learning algorithms and paradigms covered in this course\nReason about which methods (regularization, nonlinearities, ensembles) are most appropriate for a given situation using mathematical tools like the bias/variance tradeoff and curse of dimensionality\nApply these methods correctly, troubleshoot common pitfalls, and identify probable steps for improving model performance\nImplement and utilize models in R using best coding practices\n\n\nWhat This Course Is Not\nThis course is not a practitioner’s introduction to machine learning. While you will gain practical skills in the course and you will be equipped to run and troubleshoot models, the emphasis is not on the latest and greatest models, software packages, using high performance compute infrastructure, or data wrangling. There is no project component where you get to build a model of your choice on your own dataset. If this is the content you are looking for, I suggest CPSC 330 or any number of tutorials that exist on the internet.\nThis course is also not a comprehensive introduction to all of the latest and greatest ML methods. Methods evolve constantly, and there are too many of them to introduce in one course. I suggest advanced topics courses in computer science or statistics if you want a flavour of new methods. The foundational methods introduced in this course are building blocks used by all machine learning algorithms. Mastering these concepts will dramatically improve your ability to understand new methods, and it will also help you cut through the BS components of modern ML/AI systems that don’t actually do anything too fancy.\nBuilding fluency with tools and processes is an important part of becoming a statistician or machine learning practitioner, as is being up to date with methods. It is equally important to be able to analyze and reason about the methods and tools that you’re using. The latter is the niche filled by this course."
  },
  {
    "objectID": "syllabus.html#textbooks",
    "href": "syllabus.html#textbooks",
    "title": "Syllabus (2025 W1)",
    "section": "Textbooks",
    "text": "Textbooks\nThere are short required readings before each lecture. You will not be quizzed or graded on the readings, but going over the material before seeing it in lecture will accelerate your learning of the material.\n\nRequired\nAn Introduction to Statistical Learning, James, Witten, Hastie, Tibshirani, 2013, Springer, New York. (denoted [ISLR])\nAvailable free online: https://www.statlearning.com\n\n\nOptional (But Excellent)\nThe Elements of Statistical Learning, Hastie, Tibshirani, Friedman, 2009, Second Edition, Springer, New York. (denoted [ESL])\nAlso available free online: https://web.stanford.edu/~hastie/ElemStatLearn/\nThis second book is a more advanced treatment of a superset of the topics we will cover. All readings from [ESL] are optional."
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus (2025 W1)",
    "section": "Lectures",
    "text": "Lectures\nThis will be taught primarily via lecture, with interactive activities, questions, and discussions interspersed throughout. Attendance is strongly encouraged.\nDeveloping intuitions for this material takes time and effort, and the lectures provide an excellent setting and opportunity for you to ponder, struggle, and engage with the material.\nWhen you attend lecture, I ask that you minimize the use of phones and computers other than for participation and note taking. I know that it is easy to be distracted by homework, social media, or k-pop videos.[^The lecture recordings capture the audience, so I get to see the fun things you’re looking at on your laptops!]\nIf you aren’t able to provide your full attention during class, especially during the participatory activities, I kindly ask that you stay home instead. Your grade won’t suffer (I don’t track attendance) and your health and academic performance will benefit from the extra sleep.\nHere are the benefits for being participatory during lecture:\n\nYou’ll put in the work to develop deep intuitions. We’ll work through derivations during lecture that you won’t see on labs or assignments. Participating in these derivations gives you a different mode for absorbing the material.\nYou get to ask questions. This material is my bread and butter, it’s my area of research, and I have lots of insights that I’m excited to share! I’ll also answer questions over slack and during office hours, but your best bet to get good answers from me is during class.\nYou’ll get participation points. See below for a way to boost your grade by participating during lecture.\nYou’ll get a better letter of recommendation. If you want to apply to graduate school and would consider me for a letter of recommendation, I should know something about you beyond your letter grade. Asking and answering questions during lecture and office hours gives me good material to write about."
  },
  {
    "objectID": "syllabus.html#course-assessment-opportunities",
    "href": "syllabus.html#course-assessment-opportunities",
    "title": "Syllabus (2025 W1)",
    "section": "Course Assessment Opportunities",
    "text": "Course Assessment Opportunities\n\nEffort-Based Component\nThe effort-based assessments are your main tools to develop the deep intuitions about ML that we’re trying to achieve in this course. They will ask you to implement and troubleshoot models, wrestle with complex mathematical ideas, and ponder strange phenomena. Think of these assessments like eating your vegetables, doing a workout, or practicing an instrument. The work is hard, but it builds a deeper understanding about statistics and ML methods that you can’t get through other means—helping you learn to formulate algorithms from a statistical perspective, connect different learning methods, and figure out which approaches work best in different situations.\n\n\n\nComponent\nPossible Points\n\n\n\n\nParticipation\n5\n\n\nClickers\n10\n\n\nLabs\n20\n\n\nHomeworks\n40\n\n\nTotal\nmax(60, Participation + Clickers + Labs + Homeworks)\n\n\n\n\nGenerative AI Policy\nChatbots and agentic tools are quite adept with the content that we’ll cover. I used them to help me develop materials for this course (mostly to flesh out/polish ideas rather than to generate content from scratch). I believe they have the potential to aid in your learning, but also the potential to be a tempting shortcut that inhibits your potential to grow from this course.\nTo get the most out of this course, I strongly recommend using generative AI as little as possible. If you feel the need to use it, use it to supplement and strengthen your thinking (e.g. code completion, rewriting bullet points into paragraphs, etc.) rather than to replace your thinking (e.g. feeding the entire assignment into a chatbot or agentic coding tool). Here’s why:\n\nThe goal of this course is developing deep intuitions about challenging material. Developing intuitions requires thinking, meditating, and struggling with content.\nAnyone can use Claude Code, Copilot, or Cursor to develop simple ML models and pipelines. If you want to stand out as a job or graduate student candidate, you need to demonstrate the ability to think beyond what these tools can do.\nThere are enough questions with course-specific subtleties where a chatbot will produce a subpar or wrong answer. Even with really good prompting, you likely won’t score above a 7 or 8 on assignments (which is the median effort grade anyways).\nYou won’t be able to use any AI tools during the midterm or final. There likely won’t be any coding questions, but you don’t want to be too dependent on these tools when it comes to proving your knowledge.\n\nIf you use any generative AI tool on homeworks or labs, I ask that you self report how you used the tools and how they contributed to your learning/ability to understand the assignment. (There will be a box to fill in at the top of every lab and assignment.) Include every prompt that you used. Please be as honest as possible, even if you did just get ChatGPT to answer every question for you. Your responses will help me structure the material better especially as these tools evolve.\n\n\n\nParticipation\nI believe that active participation during lecture is the best way to learn the material. To that end, I’ve crafted lectures to be entertaining (especially for 8am!), interactive, and engaging for all learning styles. You can help me (and yourself!) by asking questions, working through course exercises with your neighbours, and sharing insights.\nYou earn participation points by answering questions or contributing to discussions. I’ll use the Agora platform to monitor hand raises and select students. Every time you raise your hand in the Agora app, you earn 1 point. When you’re randomly called on from raised hands, you also gain a point. Your final participation score is calculated as min(5, 5 * num_points / num_classes_with_agora). In essence, if you raise your hand at least once every class, you’ll receive the maximum participation grade.\nNote on attendance. I will not be tracking attendance, so you do not need to alert me if you need to skip the occasional lecture. You can still get a perfect effort-based grade even if you do not participate (though I sincerely hope you choose to do so!)\n\n\n\nClickers\nThroughout lecture, I will ask short multiple choice and True/False questions, which you will answer using the iClicker app. For each question, correct answers are worth 4, incorrect answers are worth 2. You get 0 points for not answering.\nSuppose there are N total clicker questions, and you have x points. Your final score for the clicker component is max(0, min(5 * x / N - 5, 10)).\nNote that if your average is less than 1, you get 0 points in this component.\n\n\n\nLabs\nThese are intended to keep you on track. They are to be submitted via pull requests in your personal labs-&lt;username&gt; repo (see the computing tab for descriptions on how to do this).\nLabs typically have a few questions for you to answer or code to implement. These are designed to be done during lab periods, but you can do them on your own as well. These are worth 2 points each up to a maximum of 20 points. They are due at 2300 on the day of your assigned lab section.\nIf you attend lab, you may share a submission with another student (with acknowledgement on the PR). If you do not attend lab, you must work on your own (subject to the collaboration instructions for Assignments below).\n\nRules.\nYou must submit via PR by the deadline. Your PR must include at least 3 commits. After lab 2, failure to include at least 3 commits will result in a maximum score of 1.\n\n\n\n\n\n\nTip\n\n\n\nIf you attend your lab section, you may work in pairs, submitting a single document to one of your Repos. Be sure to put both names on the document, and mention the collaboration on your PR. You still have until 11pm to submit.\n\n\n\n\nMarking.\nThe overriding theme here is “if you put in the effort, you’ll get all the points.” Grading scheme:\n\n2 if basically all correct\n1 if complete but with some major errors, or mostly complete and mostly correct\n0 otherwise\n\nYou may submit as many labs as you wish up to 20 total points. There are no appeals on grades.\n\n\n\n\nHomeworks\nThere will be 4 homework assignments. These are submitted via pull request similar to the labs but to the homework-&lt;username&gt; repo. Each assignment is worth up to 10 points. They are due by 2300 on the deadline. You must make at least 5 commits. Failure to have at least 5 commits will result in a 25% deduction on HW1 and a 50% deduction thereafter. No exceptions.\nAssignments are typically lightly marked. The median last year was 8/10. But they are not easy. Nor are they short. They often involve a combination of coding, writing, description, and production of statistical graphics.\nAfter receiving a mark and feedback, if you score less than 7, you may make corrections to bring your total to 7. This means, if you fix everything that you did wrong, you get 7. Not 10. The revision must be submitted within 1 week of getting your mark. Only 1 revision per assignment. The TA decision is final. Note that the TAs will only regrade parts you missed, but if you somehow make it worse, they can deduct more points.\nThe revision allowance applies only if you got 3 or more points of “content” deductions. If you missed 3 points for content and 2 more for “penalties” (like insufficient commits, code that runs off the side of the page, etc), then you are ineligible.\n\nPolicy on collaboration on assignments\nDiscussing assignments with your classmates is allowed and encouraged, but it is important that every student get practice working on these problems. This means that all the work you turn in must be your own. The general policy on homework collaboration is:\n\nYou must first make a serious effort to solve the problem.\nIf you are stuck after doing so, you may ask for help from another student. You may discuss strategies to solve the problem, but you may not look at their code, nor may they spell out the solution to you step-by-step.\nOnce you have gotten help, you must write your own solution individually. You must disclose, in your GitHub pull request, the names of anyone from whom you got help.\nThis also applies in reverse: if someone approaches you for help, you must not provide it unless they have already attempted to solve the problem, and you may not share your code or spell out the solution step-by-step.\n\n\n\n\n\n\n\nWarning\n\n\n\nAdherence to the above policy means that identical answers, or nearly identical answers, cannot occur. Thus, such occurrences are violations of the Course’s Academic honesty policy.\n\n\nYou can always, of course, ask me for help on Slack. And public Slack questions are allowed and encouraged.\nYou may also use external sources (books, websites, papers, …) to\n\nLook up programming language documentation, find useful packages, find explanations for error messages, or remind yourself about the syntax for some feature. I do this all the time in the real world. Wikipedia is your friend.\nRead about general approaches to solving specific problems (e.g. a guide to dynamic programming or a tutorial on unit testing in your programming language), or\nClarify material from the course notes or assignments.\n\nIf you use code from online or other sources (including generative AI), you must include code comments identifying the source. It must be clear what code you wrote and what code is from other sources. This rule also applies to text, images, and any other material you submit.\nPlease talk to me if you have any questions about this policy. Any form of plagiarism or cheating will result in sanctions to be determined by me, including grade penalties (such as negative points for the assignment or reductions in letter grade) or course failure. I am obliged to report violations to the appropriate University authorities. See also the text below.\n\n\n\n\nYour score on HW, Labs, and Clickers\nThe total you can accumulate across these 3 components is 60 points. But you can get there however you want. The total available is 80 points. The rest is up to you. But with choice, comes responsibility.\nRules:\n\nNothing dropped.\nNo extensions.\nIf you miss a lab or a homework deadline, then you miss it.\nMake up for missed work somewhere else.\nIf you get sick, fine. You miss a few clickers and maybe a lab (though you can do it remotely).\nIf you have a job interview and can’t complete an assignment on time, then skip it.\n\nI’m not going to police this stuff. You don’t need to let me know if you miss an assignment. There is no reason that every single person enrolled in this course shouldn’t get &gt; 65 in this class.\nIllustrative scenarios:\n\nDoing 80% on 4 homeworks (32 points), getting 5 clicker points, completing 9 labs with perfect scores, and gaining 5 participation points gets you 60 points.\nDoing 90% on 4 homeworks (36 points), getting 7 clicker points, completing 6 labs with perfect scores, and gaining 5 participation points gets you 60 points.\nGetting full homeworks (40 points) and full labs (20 points), with 0 clicker and 0 participation points gets you 60 points.\n\nChoose your own adventure. Note that the biggest barrier to getting to 60 is skipping the assignments.\n\n\n\nLate policy\nLate lab/homework submissions will not be accepted.\nMore specifically, any submission that we receive after grading has commenced will receive a 0. We likely won’t start grading at 11:01pm on the due date, so don’t worry if you’re a few minutes late. On the other hand, don’t even bother submitting if you’ve missed the deadline by a few days. This policy may seem harsh, but remember that there are many paths to a full 65 on the effort-based grade. If you miss one assignment, focus on doing well on the other labs/assignments and you might still end up with an A in the course.\n\n\n\n\nSummative Assessment\n\nMidterm Exam\n10 points, in class, on Canvas.\n\nAll multiple choice, True/False, matching.\nThe clickers are the best preparation.\nQuestions may ask you to understand or find mistakes in code.\nNo writing code.\n\n\n\nFinal Exam\n30 points, hand written.\n\nThe midterm and final are very hard. It is intended to separate those who really understand the material from those who don’t. Last year, the median grade on the final was 50%.\nYou can still end up with a very good grade even if you don’t do well on the exams. If you put in the work (do all the effort points) and the median grade on the midterm and final (50%), you’ll get an 80. If you put in the work (do all the effort points) and skip the midterm and final, you get a 60. You do not have to pass the final to pass the course. You don’t even have to take the final.\nThe point of this scheme is for those who work hard to do well. But only those who really understand the material will get 90+."
  },
  {
    "objectID": "syllabus.html#health-issues-and-considerations",
    "href": "syllabus.html#health-issues-and-considerations",
    "title": "Syllabus (2025 W1)",
    "section": "Health Issues and Considerations",
    "text": "Health Issues and Considerations\n\n\n\n\n\n\nWarning\n\n\n\nIf you are sick, it’s important that you stay home – no matter what you think you may be sick with (e.g., cold, flu, covid, other).\n\n\n\nSleep is one of the most important factors for your health, and I realize that an 8am class makes good sleep challenging. Work on developing good habits that will enable you to attend class while staying well-rested. However, it may be better to sacrifice in-person attendance than your sleep, especially if you would be so sleep-deprived that you won’t be able to benefit from class. I hope you make responsible decisions.\nYour health precautions help reduce risk and keep everyone safer. In this class, the marking scheme provides flexibility so that you can prioritize your health and still succeed. All work can be completed outside of class with reasonable time allowances.\nIf you do miss class because of illness:\n\nMake connections early in the term with other students in the class. You can help each other by sharing notes. If you don’t yet know anyone in the class, post on the discussion forum to connect with other students.\nConsult the class resources here and on Canvas. We will post all slides, readings, and recordings for each class.\nUse Slack for help.\nCome to virtual office hours.\nSee the marking scheme for reassurance about your flexibility options. No part of your final grade will be directly impacted by missing class.\n\nIf you are sick on midterm day or final exam day, do not attend the exam. You must follow up with your home faculty’s advising office to apply for deferred standing. Students who are granted deferred standing write the final exam at a later date. If you’re a Science student, you must apply for deferred standing (an academic concession) through Science Advising no later than 48 hours after the missed final exam/assignment. Learn more and find the application online. For additional information about academic concessions, see the UBC policy here.\n\n\n\n\n\n\n\nNote\n\n\n\nPlease talk with me if you have any concerns or ask me if you are worried about falling behind."
  },
  {
    "objectID": "syllabus.html#university-policies",
    "href": "syllabus.html#university-policies",
    "title": "Syllabus (2025 W1)",
    "section": "University policies",
    "text": "University policies\nUBC provides resources to support student learning and to maintain healthy lifestyles but recognizes that sometimes crises arise and so there are additional resources to access including those for survivors of sexual violence. UBC values respect for the person and ideas of all members of the academic community. Harassment and discrimination are not tolerated nor is suppression of academic freedom. UBC provides appropriate accommodation for students with disabilities and for religious, spiritual and cultural observances. UBC values academic honesty and students are expected to acknowledge the ideas generated by others and to uphold the highest academic standards in all of their actions. Details of the policies and how to access support are available here.\n\nAcademic honesty and standards\nUBC Vancouver Statement\nAcademic honesty is essential to the continued functioning of the University of British Columbia as an institution of higher learning and research. All UBC students are expected to behave as honest and responsible members of an academic community. Breach of those expectations or failure to follow the appropriate policies, principles, rules, and guidelines of the University with respect to academic honesty may result in disciplinary action.\nFor the full statement, please see the 2022/23 Vancouver Academic Calendar\nCourse specific\nWhile course materials are freely available online, selling or distributing homework solutions, lab answers, or exam questions to other students or commercial services is strictly prohibited and violates UBC’s academic integrity policies. Violations will be reported to the Dean of Science and may result in serious academic consequences, including course failure.\nI have caught students cheating on exams in previous years. In my experience, cheating typically stems from students not understanding the material, which usually results in a failing grade even before any penalties are imposed and the incident is reported to the Dean’s office. Please do your own work and utilize the TAs and me as resources. We are here to help if you are struggling.\n\n\n\n\n\n\nCaution\n\n\n\nIf I suspect cheating, your case will be forwarded to the Dean’s office. No questions asked.\n\n\n\n\nAcademic Concessions\nThese are handled according to UBC policy. Please see\n\nUBC student services\nUBC Vancouver Academic Calendar\nFaculty of Science Concessions\n\n\n\nMissed final exam\nStudents who miss the final exam must report to their Faculty advising office within 72 hours of the missed exam, and must supply supporting documentation. Only your Faculty Advising office can grant deferred standing in a course. You must also notify your instructor prior to (if possible) or immediately after the exam. Your instructor will let you know when you are expected to write your deferred exam. Deferred exams will ONLY be provided to students who have applied for and received deferred standing from their Faculty.\n\n\nTake care of yourself\nCourse work at this level can be intense, and I encourage you to take care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, avoiding drugs and alcohol, getting enough sleep and taking some time to relax. This will help you achieve your goals and cope with stress. I struggle with these issues too, and I try hard to set aside time for things that make me happy (cooking, playing/listening to music, exercise, going for walks).\nAll of us benefit from support during times of struggle. If you are having any problems or concerns, do not hesitate to speak with me. There are also many resources available on campus that can provide help and support. Asking for support sooner rather than later is almost always a good idea.\nIf you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, I strongly encourage you to seek support. UBC Science has resources on their website. UBC Counseling Services is here to help: call 604 822 3811 or visit their website. Consider also reaching out to a friend, faculty member, or family member you trust to help get you the support you need."
  },
  {
    "objectID": "schedule/index.html",
    "href": "schedule/index.html",
    "title": " Schedule",
    "section": "",
    "text": "Required readings are listed below for each module. Readings from ISLR are required, while those from ESL (in parentheses) are optional and supplemental."
  },
  {
    "objectID": "schedule/index.html#the-learning-procedure---models-fitting-model-selection",
    "href": "schedule/index.html#the-learning-procedure---models-fitting-model-selection",
    "title": " Schedule",
    "section": "1 The Learning Procedure - Models, Fitting, Model Selection",
    "text": "1 The Learning Procedure - Models, Fitting, Model Selection\nTopics: Learning through statistician and algorithmic lenses, model selection; cross validation\nLearning Objectives:\n\nFormulate learning problems in terms of statistical models, estimators, and model selection\nIdentify criteria for good statistical models, estimators, and model selection metrics\n\nHandouts and Resources:\n\nProgramming in R .Rmd, .pdf\nUsing RMarkdown .Rmd, .pdf\nOverview of R and Tidyverse (Slides from last term)\nOverview of git and version control (Slides from last term)\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nSep 2\n(no class, Imagine UBC)\n\n\n\n\nSep 4\nClass Overview (slides)Probability Review (notes)\n\n\n\n\nSep 9\nIntroduction to Learning, Regression(notes)\nISLR 2.1(ESL 2.4, 2.6)\n\n\n\nSep 11\nLearning (cont.), Classification(notes)\nISLR 4.3(ESL 4.4)\n\n\n\n\n\n\nLab 00 (Sep 12)\n\n\nSep 16\nModel Selection, Cross Validation\nISLR 5.1(ESL 2.9, 7.10)"
  },
  {
    "objectID": "schedule/index.html#bias-variance-tradeoff-linear-methods",
    "href": "schedule/index.html#bias-variance-tradeoff-linear-methods",
    "title": " Schedule",
    "section": "2 Bias-Variance Tradeoff, Linear Methods",
    "text": "2 Bias-Variance Tradeoff, Linear Methods\nTopics: bias/variance tradeoff; regularized regression (ridge and lasso); non-linearities via basis functions; advanced model selection and analysis\nLearning Objectives:\n\nDecompose prediction error into bias and variance components\nImplement regularized versions of linear regression (ridge, lasso) and understand their impact on bias and variance\nImplement basis expansions for linear regression and understand their impact on bias and variance\nApply closed-form selection techniques to linear methods, and identify factors in the formula that affect bias and variance\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nSep 18\nBias-Variance Tradeoff\nISLR 2.2(ESL 7.1-7.3)\n\n\n\n\n\n\nLab 01 (Sep 19)\n\n\nSep 23\nRidge Regression\nISLR 6.2.1(ESL 3.4.0-3.4.1)\nHW 1 due\n\n\nSep 25\nLasso Regression, Optimization\nISLR 6.2.2-6.2.3(ESL 3.4.2-3.4.3)\n\n\n\n\n\n\nLab 02 (Sep 26)\n\n\nSep 30\n(no class, Truth and Reconciliation)\n\n\n\n\nOct 2\nBasis Functions\nISLR 7.1, 7.4(ESL 5.1-5.3)\n\n\n\n\n\n\nLab 03 (Oct 3)\n\n\nOct 7\nModel Selection for Linear Methods\n(ESL 7.6-7.7)"
  },
  {
    "objectID": "schedule/index.html#nonparametric-methods-curse-of-dimensionality",
    "href": "schedule/index.html#nonparametric-methods-curse-of-dimensionality",
    "title": " Schedule",
    "section": "3 Nonparametric Methods, Curse of Dimensionality",
    "text": "3 Nonparametric Methods, Curse of Dimensionality\nTopics: kNN; trees; kernel machines; curse of dimensionality\nLearning Objectives:\n\nAnalyze how dimensionality affects the performance of parametric vs nonparametric methods\nImplement nonparametric methods (kNN, kernel smoothing, kernel machines) and analyze their properties\nWrite the parametric version of nonparametric methods (e.g. kernel ridge regression) and vice versa\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nOct 9\nkNN, parametric vs non-parametric\nISLR 3.5(ESL 2.3.2, 5.4.1)\nHW 2 due\n\n\n\n\n\nLab 04 (Oct 10)\n\n\nOct 14\nApproximate kNN, trees\nISLR 8.1(ESL 9.2)\n\n\n\nOct 16\nKernel Machines\n\n\n\n\n\n\n\nLab 05 (Oct 17)\n\n\nOct 21\nCurse of Dimensionality, Review\nISLR 8.1(ESL 9.2)"
  },
  {
    "objectID": "schedule/index.html#midterm-exam",
    "href": "schedule/index.html#midterm-exam",
    "title": " Schedule",
    "section": "Midterm Exam",
    "text": "Midterm Exam\n\n\n\n\n\n\n\nDate\nTopic\n\n\n\n\nOct 23\nMIDTERM EXAM (In Class)\n\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou must bring your computer as the exam will be given through Canvas\nPlease arrange to borrow one from the library if you do not have your own. Let me know ASAP if this may pose a problem.\nYou may bring 2 sheets of front/back 8.5 × 11 inch paper with handwritten notes you want to use. No other materials will be allowed.\nThere will be no required coding, but I may show code or output and ask questions about it.\nIt will be entirely multiple choice / True-False / matching, etc. Delivered on Canvas."
  },
  {
    "objectID": "schedule/index.html#unsupervised-learning-generative-modelling",
    "href": "schedule/index.html#unsupervised-learning-generative-modelling",
    "title": " Schedule",
    "section": "4 Unsupervised Learning, Generative Modelling",
    "text": "4 Unsupervised Learning, Generative Modelling\nTopics: dimension reduction and clustering; generative vs discriminative modelling\nLearning Objectives:\n\nDifferentiate between generative and discriminative modelling approaches and identify when each is most appropriate\nImplement dimensionality reduction techniques (PCA, kernel PCA) and analyze their impact on data representation\nApply clustering algorithms (k-means, Gaussian mixture models) and evaluate their performance using appropriate metrics\nConnect unsupervised learning methods to their generative/discriminative modelling framework\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nOct 28\nGenerative vs Discriminative Modelling\nISLR 4.2.0, 12.1\n\n\n\nOct 30\nDimensionality Reduction\nISLR 12.2(ESL 14.5.1, 14.5.4)\n\n\n\n\n\n\nLab 06 (Oct 31)\n\n\nNov 04\nClustering 1\nISLR 12.4.1(ESL 14.3)\n\n\n\nNov 6\nClustering 2\n\nHW 3 due\n\n\n\n\n\nLab 07 (Nov 7)"
  },
  {
    "objectID": "schedule/index.html#ensembles-black-box-methods",
    "href": "schedule/index.html#ensembles-black-box-methods",
    "title": " Schedule",
    "section": "5 Ensembles, Black-Box Methods",
    "text": "5 Ensembles, Black-Box Methods\nTopics: ensembles; bootstrap; bagging; boosting; random forests\nLearning Objectives:\n\nImplement bootstrap and ensembling methods, reason through computational tradeoffs\nDifferentiate ensemble methods that reduce bias or variance\nUtilize “hidden advantages” of ensembles around feature importance, uncertainty quantification, etc.\nIdentify assumptions in black-box methods of uncertainty quantification, variance reduction, and bias reduction\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nNov 11\n(no class, Midterm Break)\n\n\n\n\nNov 13\nThe Bootstrap\nISLR 5.2(ESL 7.11, 8.2)\n\n\n\nNov 18\nBagging and Random Forests\nISLR 8.2.0-8.2.2(ESL 8.7, 15.1-15.3)\n\n\n\nNov 20\nBoosting\nISLR 8.2.3(ESL 10.1-10.5, 10.9)\n\n\n\n\n\n\nLab 08 (Nov 21)"
  },
  {
    "objectID": "schedule/index.html#deep-learning",
    "href": "schedule/index.html#deep-learning",
    "title": " Schedule",
    "section": "6 Deep Learning",
    "text": "6 Deep Learning\nTopics: neural networks; deep learning architectures; generative AI\nLearning Objectives:\n\nConstruct a basic neural network architecture from simple mathematical building blocks\nArticulate the effects of depth and width on the representational capacity and generalization of neural networks\nConnect neural networks to other methods covered in the course (basis functions, kernel methods, boosting methods)\nDerive the backpropagation algorithm\nEvaluate modern neural network architectures for different problem types\n\n\n\n\n\n\n\n\n\n\nDate\nTopic\nReadings\nDeadlines\n\n\n\n\nNov 25\nIntroduction to Neural Networks\nISLR 10.1-10.2(ESL 11.1, 11.3)\n\n\n\nNov 27\nNeural Network OptimizationGeneralization\nISLR 10.7-10.8(ESL 11.4)\nHW 4 due\n\n\n\n\n\nLab 09 (Nov 28)\n\n\nDec 2\nNeural Net ArchitecturesGenerative AI\n\n\n\n\nDec 4\nReview"
  },
  {
    "objectID": "schedule/index.html#final-exam",
    "href": "schedule/index.html#final-exam",
    "title": " Schedule",
    "section": "Final Exam",
    "text": "Final Exam\n\n\n\n\n\n\nImportant\n\n\n\nDo not make any plans to leave Vancouver before the final exam date is announced.\n\n\n\nIn person attendance is required (per Faculty of Science guidelines)\nYou may bring 2 sheets of front/back 8.5 × 11 inch paper with handwritten notes you want to use. No other materials will be allowed."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section",
    "href": "schedule/slides/00-r-review.html#section",
    "title": "UBC Stat406 2025 W1",
    "section": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour",
    "text": "00 R, Rmarkdown, code, and {tidyverse}:  A whirlwind tour\nStat 406\nGeoff Pleiss\nLast modified – 25 August 2025\n\\[\n\\DeclareMathOperator*{\\argmin}{argmin}\n\\DeclareMathOperator*{\\argmax}{argmax}\n\\DeclareMathOperator*{\\minimize}{minimize}\n\\DeclareMathOperator*{\\maximize}{maximize}\n\\DeclareMathOperator*{\\find}{find}\n\\DeclareMathOperator{\\st}{subject\\,\\,to}\n\\newcommand{\\E}{E}\n\\newcommand{\\Expect}[1]{\\E\\left[ #1 \\right]}\n\\newcommand{\\Var}[1]{\\mathrm{Var}\\left[ #1 \\right]}\n\\newcommand{\\Cov}[2]{\\mathrm{Cov}\\left[#1,\\ #2\\right]}\n\\newcommand{\\given}{\\ \\vert\\ }\n\\newcommand{\\X}{\\mathbf{X}}\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\\newcommand{\\P}{\\mathcal{P}}\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\norm}[1]{\\left\\lVert #1 \\right\\rVert}\n\\newcommand{\\snorm}[1]{\\lVert #1 \\rVert}\n\\newcommand{\\tr}[1]{\\mbox{tr}(#1)}\n\\newcommand{\\brt}{\\widehat{\\beta}^R_{s}}\n\\newcommand{\\brl}{\\widehat{\\beta}^R_{\\lambda}}\n\\newcommand{\\bls}{\\widehat{\\beta}_{ols}}\n\\newcommand{\\blt}{\\widehat{\\beta}^L_{s}}\n\\newcommand{\\bll}{\\widehat{\\beta}^L_{\\lambda}}\n\\newcommand{\\U}{\\mathbf{U}}\n\\newcommand{\\D}{\\mathbf{D}}\n\\newcommand{\\V}{\\mathbf{V}}\n\\]"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "href": "schedule/slides/00-r-review.html#tour-of-rstudio",
    "title": "UBC Stat406 2025 W1",
    "section": "Tour of Rstudio",
    "text": "Tour of Rstudio\nThings to note\n\nConsole\nTerminal\nScripts, .Rmd, Knit\nFiles, Projects\nGetting help\nEnvironment, Git"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "href": "schedule/slides/00-r-review.html#r-and-the-tidyverse",
    "title": "UBC Stat406 2025 W1",
    "section": "R and the {tidyverse}",
    "text": "R and the {tidyverse}\n\n\n\n\nToday is going to be a whirlwind tour of R.\nIf you are new to R: read the first 4 chapters of Data Science: A First Introduction.\nIt’s available for free at https://datasciencebook.ca. It covers:\n\nData loading from .csv, Excel, database, and web sources\nData saving to .csv files\nData wrangling with tidyverse functions\nPlotting with ggplot"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures",
    "href": "schedule/slides/00-r-review.html#basic-data-structures",
    "title": "UBC Stat406 2025 W1",
    "section": "Basic data structures",
    "text": "Basic data structures\n\n\nVectors:\n\nx &lt;- c(1, 3, 4)\nx[1]\n\n[1] 1\n\nx[-1]\n\n[1] 3 4\n\nrev(x)\n\n[1] 4 3 1\n\nc(x, x)\n\n[1] 1 3 4 1 3 4\n\n\n\n\n\nMatrices:\n\nx &lt;- matrix(1:25, nrow = 5, ncol = 5)\nx[1,]\n\n[1]  1  6 11 16 21\n\nx[,-1]\n\n     [,1] [,2] [,3] [,4]\n[1,]    6   11   16   21\n[2,]    7   12   17   22\n[3,]    8   13   18   23\n[4,]    9   14   19   24\n[5,]   10   15   20   25\n\nx[c(1,3),  2:3]\n\n     [,1] [,2]\n[1,]    6   11\n[2,]    8   13\n\n\n\nAll elements of a vector/matrix must be of the same type"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#basic-data-structures-cont.",
    "href": "schedule/slides/00-r-review.html#basic-data-structures-cont.",
    "title": "UBC Stat406 2025 W1",
    "section": "Basic data structures (cont.)",
    "text": "Basic data structures (cont.)\n\n\nLists\n\n(l &lt;- list(\n  a = letters[1:2],\n  b = 1:4,\n  c = list(a = 1)))\n\n$a\n[1] \"a\" \"b\"\n\n$b\n[1] 1 2 3 4\n\n$c\n$c$a\n[1] 1\n\nl$a\n\n[1] \"a\" \"b\"\n\nl$c$a\n\n[1] 1\n\nl[\"b\"] # compare to l[[\"b\"]] == l$b\n\n$b\n[1] 1 2 3 4\n\n\n\n\nData frames\n\n(dat &lt;- data.frame(\n  z = 1:5,\n  b = 6:10,\n  c = letters[1:5]))\n\n  z  b c\n1 1  6 a\n2 2  7 b\n3 3  8 c\n4 4  9 d\n5 5 10 e\n\nclass(dat)\n\n[1] \"data.frame\"\n\ndat$b\n\n[1]  6  7  8  9 10\n\ndat[1,]\n\n  z b c\n1 1 6 a\n\n\n\n\nLists can have multiple element types; data frames are lists of vectors"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#functions-in-r",
    "href": "schedule/slides/00-r-review.html#functions-in-r",
    "title": "UBC Stat406 2025 W1",
    "section": "Functions in R",
    "text": "Functions in R\nA function is a mapping from inputs to outputs, and is defined with the function keyword.\nThe function’s body is wrapped in curly braces, and its output is given by the return keyword (or the last evaluated statement)\n\nf &lt;- function(x, y){\n  x+y\n}\n\nf(3,5)\n\n[1] 8\n\n\n\nf &lt;- function(x, y){\n  return(x+y)\n}\n\nf(3,5)\n\n[1] 8"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#function-signatures",
    "href": "schedule/slides/00-r-review.html#function-signatures",
    "title": "UBC Stat406 2025 W1",
    "section": "Function Signatures",
    "text": "Function Signatures\n\n\nCode\nsig &lt;- sig::sig\n\n\n\nsig(lm)\n\nfn &lt;- function(formula, data, subset, weights, na.action, method = \"qr\", model\n  = TRUE, x = FALSE, y = FALSE, qr = TRUE, singular.ok = TRUE, contrasts =\n  NULL, offset, ...)\n\nsig(`+`)\n\nfn &lt;- function(e1, e2)\n\nsig(dplyr::filter)\n\nfn &lt;- function(.data, ..., .by = NULL, .preserve = FALSE)\n\nsig(stats::filter)\n\nfn &lt;- function(x, filter, method = c(\"convolution\", \"recursive\"), sides = 2,\n  circular = FALSE, init = NULL)\n\nsig(rnorm)\n\nfn &lt;- function(n, mean = 0, sd = 1)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "href": "schedule/slides/00-r-review.html#these-are-all-the-same",
    "title": "UBC Stat406 2025 W1",
    "section": "These are all the same",
    "text": "These are all the same\n\nset.seed(12345)\nrnorm(3)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(3, 0, 1)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\nset.seed(12345)\nrnorm(sd = 1, n = 3, mean = 0)\n\n[1]  0.5855288  0.7094660 -0.1093033\n\n\n\nFunctions can have default values.\nYou may, but don’t have to, name the arguments\nIf you name them, you can pass them out of order (but you shouldn’t)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects",
    "title": "UBC Stat406 2025 W1",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nf &lt;- function(arg1, arg2, arg3 = 12, ...) {\n  stuff &lt;- arg1 * arg3\n  stuff2 &lt;- stuff + arg2\n  plot(arg1, stuff2, ...)\n  return(stuff2)\n}\nx &lt;- rnorm(100)\n\n\n\n\ny1 &lt;- f(x, 3, 15, col = 4, pch = 19)\n\n\n\n\n\n\n\nstr(y1)\n\n num [1:100] -3.8 12.09 -24.27 12.45 -1.14 ..."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "href": "schedule/slides/00-r-review.html#outputs-vs.-side-effects-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Outputs vs. Side Effects",
    "text": "Outputs vs. Side Effects\n\n\n\nSide effects are things a function changes in global scope\nOutputs can be assigned to variables\nA good example is the hist function\nYou have probably only seen the side effect which is to plot the histogram\n\n\nmy_histogram &lt;- hist(rnorm(1000))\n\n\n\n\n\n\n\n\n\n\n\nstr(my_histogram)\n\nList of 6\n $ breaks  : num [1:14] -3 -2.5 -2 -1.5 -1 -0.5 0 0.5 1 1.5 ...\n $ counts  : int [1:13] 4 21 41 89 142 200 193 170 74 38 ...\n $ density : num [1:13] 0.008 0.042 0.082 0.178 0.284 0.4 0.386 0.34 0.148 0.076 ...\n $ mids    : num [1:13] -2.75 -2.25 -1.75 -1.25 -0.75 -0.25 0.25 0.75 1.25 1.75 ...\n $ xname   : chr \"rnorm(1000)\"\n $ equidist: logi TRUE\n - attr(*, \"class\")= chr \"histogram\"\n\nclass(my_histogram)\n\n[1] \"histogram\""
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "href": "schedule/slides/00-r-review.html#when-writing-functions-program-defensively-ensure-behaviour",
    "title": "UBC Stat406 2025 W1",
    "section": "When writing functions, program defensively, ensure behaviour",
    "text": "When writing functions, program defensively, ensure behaviour\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  x + 1\n}\n\nincrementer(2)\n\n[1] 3\n\nincrementer(1:4)\n\n[1] 2 3 4 5\n\nincrementer(\"a\")\n\nError in x + 1: non-numeric argument to binary operator\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  stopifnot(is.numeric(x))\n  return(x + 1)\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): is.numeric(x) is not TRUE\n\n\n\n\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + 1\n}\nincrementer(\"a\")\n\nError in incrementer(\"a\"): `x` must be numeric\n\nincrementer(2, -3) ## oops!\n\n[1] 3\n\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  x + inc_by\n}\nincrementer(2, -3)\n\n[1] -1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#unit-testing",
    "href": "schedule/slides/00-r-review.html#unit-testing",
    "title": "UBC Stat406 2025 W1",
    "section": "Unit Testing",
    "text": "Unit Testing\nWhen you write functions, test them!\nUse testthat: check a few usual values and corner cases\n\n\n\nlibrary(testthat)\nincrementer &lt;- function(x, inc_by = 1) {\n  if (!is.numeric(x)) {\n    stop(\"`x` must be numeric\")\n  }\n  if (!is.numeric(inc_by)) {\n    stop(\"`inc_by` must be numeric\")\n  }\n  x + inc_by\n}\nexpect_error(incrementer(\"a\"))\nexpect_equal(incrementer(1:3), 2:4)\nexpect_equal(incrementer(2, -3), -1)\nexpect_error(incrementer(1, \"b\"))\nexpect_identical(incrementer(1:3), 2:4)\n\nError: incrementer(1:3) not identical to 2:4.\nObjects equal but not identical\n\n\n\n\n\nis.integer(2:4)\n\n[1] TRUE\n\nis.integer(incrementer(1:3))\n\n[1] FALSE\n\nexpect_identical(incrementer(1:3, 1L), 2:4)\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nDon’t copy code; write a function. Validate your arguments. Write tests to check if inputs result in predicted outputs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#classes",
    "href": "schedule/slides/00-r-review.html#classes",
    "title": "UBC Stat406 2025 W1",
    "section": "Classes",
    "text": "Classes\n\n\nWe saw some of these earlier:\n\ntib &lt;- tibble(\n  x1 = rnorm(100),\n  x2 = rnorm(100),\n  y = x1 + 2 * x2 + rnorm(100)\n)\nmdl &lt;- lm(y ~ ., data = tib )\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nclass(mdl)\n\n[1] \"lm\"\n\n\nThe class allows for the use of “methods”\n\nprint(mdl)\n\n\nCall:\nlm(formula = y ~ ., data = tib)\n\nCoefficients:\n(Intercept)           x1           x2  \n    -0.1742       1.0454       2.0470  \n\n\n\n\n\nR “knows what to do” when you print() an object of class \"lm\".\nprint() is called a “generic” function.\nYou can create “methods” that get dispatched.\nFor any generic, R looks for a method for the class.\nIf available, it calls that function."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "href": "schedule/slides/00-r-review.html#viewing-the-dispatch-chain",
    "title": "UBC Stat406 2025 W1",
    "section": "Viewing the dispatch chain",
    "text": "Viewing the dispatch chain\n\nsloop::s3_dispatch(print(incrementer))\n\n=&gt; print.function\n * print.default\n\nsloop::s3_dispatch(print(tib))\n\n   print.tbl_df\n=&gt; print.tbl\n * print.data.frame\n * print.default\n\nsloop::s3_dispatch(print(mdl))\n\n=&gt; print.lm\n * print.default"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#generic-methods",
    "href": "schedule/slides/00-r-review.html#generic-methods",
    "title": "UBC Stat406 2025 W1",
    "section": "Generic Methods",
    "text": "Generic Methods\nThere are lots of generic functions in R\nCommon ones are print(), summary(), and plot().\nAlso, lots of important statistical modelling concepts: residuals() coef()\n(In python, these work the opposite way: obj.residuals. The dot after the object accesses methods defined for that type of object. But the dispatch behaviour is less robust.)\n\nThe convention is that the specialized function is named method.class(), e.g., summary.lm().\nIf no specialized function is defined, R will try to use method.default().\n\nFor this reason, R programmers try to avoid . in names of functions or objects."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#wherefore-methods",
    "href": "schedule/slides/00-r-review.html#wherefore-methods",
    "title": "UBC Stat406 2025 W1",
    "section": "Wherefore methods?",
    "text": "Wherefore methods?\n\nThe advantage is that you don’t have to learn a totally new syntax to grab residuals or plot things\nYou just use residuals(mdl) whether mdl has class lm or any other class you expect to have residuals\nThe one draw-back is the help pages for the generic methods tend to be pretty vague\nCompare ?summary with ?summary.lm."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#different-environments",
    "href": "schedule/slides/00-r-review.html#different-environments",
    "title": "UBC Stat406 2025 W1",
    "section": "Different environments",
    "text": "Different environments\n(known as scope in other languages)\n\nThese are often tricky, but are very common.\nMost programming languages have this concept in one way or another.\nIn R code run in the Console produces objects in the “Global environment”\nYou can see what you create in the “Environment” tab.\nBut there’s lots of other stuff.\nMany packages are automatically loaded at startup, so you have access to the functions and data inside\n\nFor example mean(), lm(), plot(), iris (technically iris is lazy-loaded, meaning it’s not in memory until you call it, but it is available)"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-1",
    "href": "schedule/slides/00-r-review.html#section-1",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "Other packages require you to load them with library(pkg) before their functions are available.\nBut, you can call those functions by prefixing the package name ggplot2::ggplot().\nYou can also access functions that the package developer didn’t “export” for use with ::: like dplyr:::as_across_fn_call()\n\n\nThat is all about accessing “objects in package environments”"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments",
    "title": "UBC Stat406 2025 W1",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nAs one might expect, functions create an environment inside the function.\n\nz &lt;- 1\nfun &lt;- function(x) {\n  z &lt;- x\n  print(z)\n  invisible(z)\n}\nfun(14)\n\n[1] 14\n\n\n\nNon-trivial cases are data-masking environments.\n\ntib &lt;- tibble(x1 = rnorm(100),  x2 = rnorm(100),  y = x1 + 2 * x2)\nmdl &lt;- lm(y ~ x2, data = tib)\nx2\n\nError: object 'x2' not found\n\n\n\nlm() looks “inside” the tib to find y and x2\nThe data variables are added to the lm() environment"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "href": "schedule/slides/00-r-review.html#other-issues-with-environments-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Other issues with environments",
    "text": "Other issues with environments\nWhen Knit, .Rmd files run in their OWN environment.\nThey are run from top to bottom, with code chunks depending on previous\nThis makes them reproducible.\n\nObjects in your local environment are not available in the .Rmd\nObjects in the .Rmd are not available locally.\n\n\n\n\n\n\nTip\n\n\nThe most frequent error I see is:\n\nrunning chunks individually, 1-by-1, and it works\nKnitting, and it fails\n\nThe reason is almost always that the chunks refer to objects in the Environment that don’t exist in the .Rmd"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-2",
    "href": "schedule/slides/00-r-review.html#section-2",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "This error also happens because:\n\nlibrary() calls were made globally but not in the .Rmd\n\nso the packages aren’t loaded\n\npaths to data or other objects are not relative to the .Rmd in your file system\n\nthey must be\n\nCarefully keeping Labs / Assignments in their current location will help to avoid some of these.\n\n\n\n\n\n\n\nTip\n\n\nKnit frequently throughout your homework / lab so that you encounter environment errors early and often!"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#how-to-fix-code",
    "href": "schedule/slides/00-r-review.html#how-to-fix-code",
    "title": "UBC Stat406 2025 W1",
    "section": "How to fix code",
    "text": "How to fix code\n\nIf you’re using a function in a package, start with ?function to see the help\n\nMake sure you’re calling the function correctly.\nTry running the examples.\npaste the error into Google (if you share the error on Slack, I often do this first)\nGo to the package website if it exists, and browse around\n\nIf your .Rmd won’t Knit\n\nDid you make the mistake on the last slide?\nDid it Knit before? Then the bug is in whatever you added.\nDid you never Knit it? Why not?\nCall rstudioapi::restartSession(), then run the Chunks 1-by-1"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#adding-browser",
    "href": "schedule/slides/00-r-review.html#adding-browser",
    "title": "UBC Stat406 2025 W1",
    "section": "Adding browser()",
    "text": "Adding browser()\n(known as a breakpoint in any other language)\n\nOnly useful with your own functions.\nOpen the script with the function, and add browser() to the code somewhere\nThen call your function.\nThe execution will Stop where you added browser() and you’ll have access to the local environment to play around"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#reproducible-examples",
    "href": "schedule/slides/00-r-review.html#reproducible-examples",
    "title": "UBC Stat406 2025 W1",
    "section": "Reproducible examples",
    "text": "Reproducible examples\n\n\n\n\n\n\nQuestion I frequently get:\n\n\n“I ran this code, but it didn’t work.”\n\n\n\n\nIf you want to ask me why the code doesn’t work, you need to show me what’s wrong.\n\n\n\n\n\n\n\nDon’t just paste a screenshot!\n\n\nUnless you get lucky, I won’t be able to figure it out from that. And we’ll both get frustrated.\n\n\n\nWhat you need is a Reproducible Example or reprex.\n\nThis is a small chunk of code that\n\nruns in it’s own environment\nand produces the error."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#the-reprex-package",
    "href": "schedule/slides/00-r-review.html#the-reprex-package",
    "title": "UBC Stat406 2025 W1",
    "section": "The {reprex} package",
    "text": "The {reprex} package\n\nOpen a new .R script.\nPaste your buggy code in the file (no need to save)\nEdit your code to make sure it’s “enough to produce the error” and nothing more. (By rerunning the code a few times.)\nCopy your code (so that it’s on the clipboard)\nCall reprex::reprex(venue = \"r\") from the console. This will run your code in a new environment and show the result in the Viewer tab. Does it create the error you expect?\nIf it creates other errors, that may be the problem. You may fix the bug on your own!\nIf it doesn’t have errors, then your global environment is Farblunget.\nThe Output is now on your clipboard. Go to Slack and paste it in a message. Then press Cmd+Shift+Enter (on Mac) or Ctrl+Shift+Enter (Windows/Linux). Under Type, select R.\nSend the message, perhaps with more description and an SOS emoji.\n\n\n\n\n\n\n\nNote\n\n\nBecause Reprex runs in it’s own environment, it doesn’t have access to any of the libraries you loaded or the stuff in your global environment. You’ll have to load these things in the script."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#r-pitfalls",
    "href": "schedule/slides/00-r-review.html#r-pitfalls",
    "title": "UBC Stat406 2025 W1",
    "section": "R Pitfalls",
    "text": "R Pitfalls\n\nR is very permissive, and this leads to frequent silent errors\n\nnonstandard evaluation of arguments, data masking\nallows dots in names (even though they mean something syntactically!)\nallows accessing attributes that don’t exist\npromotion of ints to floats, floats to strings 😱\n\nLots of unusual design decisions\n\nmany assignment operators (-&gt;, &lt;-, -&gt;&gt;, &lt;&lt;-, =)\nmany accessors (a$b is a[[\"b\"]] but not a[\"b\"])\nlacking basic data types (e.g., hash maps)\ninformal classes (class(x) &lt;- \"a weird new class!\")\ntonnes of functions/data/objects in the global namespace\n3 == \"3\" (evaluates to TRUE?!!?!)\n\nRscript executable treats code differently than the R REPL"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "href": "schedule/slides/00-r-review.html#tidyverse-is-huge",
    "title": "UBC Stat406 2025 W1",
    "section": "{tidyverse} is huge",
    "text": "{tidyverse} is huge\nCore tidyverse is ~30 different packages, but we’re going to just talk about a few.\nLoad all of them by calling library(tidyverse)\nPackages fall roughly into a few categories:\n\nConvenience functions: {magrittr} and many many others.\nData processing: {dplyr} and many others.\nGraphing: {ggplot2} and some others like {scales}.\nUtilities\n\n\n\nWe’re going to talk quickly about some of it, but ignore much of 2.\nThere’s a lot that’s great about these packages, especially ease of data processing.\nBut it doesn’t always jive with base R (it’s almost a separate proglang at this point)."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#when-in-doubt",
    "href": "schedule/slides/00-r-review.html#when-in-doubt",
    "title": "UBC Stat406 2025 W1",
    "section": "When in doubt…",
    "text": "When in doubt…\n\n\n\n\nRead the first 4 chapters (especially 3 and 4!)\nhttps://datasciencebook.ca"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "href": "schedule/slides/00-r-review.html#piping-with-magrittr",
    "title": "UBC Stat406 2025 W1",
    "section": "Piping with {magrittr}",
    "text": "Piping with {magrittr}\nThis was introduced by {magrittr} as %&gt;%,\nbut is now in base R (&gt;=4.1.0) as |&gt;.\nNote: there are other pipes in {magrittr} (e.g. %$% and %T%) but I’ve never used them.\nThe point of the pipe is to logically sequence nested operations\nThe pipe passes the left hand side as the first argument of the right hand side"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#example",
    "href": "schedule/slides/00-r-review.html#example",
    "title": "UBC Stat406 2025 W1",
    "section": "Example",
    "text": "Example\n\n\n\nselect(filter(mtcars, cyl == 6), mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse1 &lt;- print(\n  sum(\n    residuals(\n      lm(y~., data = mutate(\n        tib,\n        x3 = x1^2,\n        x4 = log(x2 + abs(min(x2)) + 1)\n      )\n      )\n    )^2\n  )\n)\n\n[1] 1.371438e-29\n\n\n\n\nmtcars |&gt; filter(cyl == 6) |&gt; select(mpg)\n\n                mpg\nMazda RX4      21.0\nMazda RX4 Wag  21.0\nHornet 4 Drive 21.4\nValiant        18.1\nMerc 280       19.2\nMerc 280C      17.8\nFerrari Dino   19.7\n\n\n\nmse2 &lt;- tib |&gt;\n  mutate(\n    x3 = x1^2,\n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() |&gt;\n  print()\n\n[1] 1.371438e-29"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#section-3",
    "href": "schedule/slides/00-r-review.html#section-3",
    "title": "UBC Stat406 2025 W1",
    "section": "",
    "text": "It may seem like we should push this all the way\n\ntib |&gt;\n  mutate(\n    x3 = x1^2,\n    x4 = log(x2 + abs(min(x2)) + 1)\n  ) %&gt;% # base pipe only goes to first arg\n  lm(y ~ ., data = .) |&gt; # note the use of `.`\n  residuals() |&gt;\n  magrittr::raise_to_power(2) |&gt; # same as `^`(2)\n  sum() -&gt;\n  mse3\n\nThis technically works…but at a minimum it makes it hard to extend pipe sequences.\n\n\n\n\n\n\n\nNote\n\n\nOpinion zone: It’s also just weird. Don’t encourage the R devs."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "href": "schedule/slides/00-r-review.html#data-processing-in-dplyr",
    "title": "UBC Stat406 2025 W1",
    "section": "Data processing in {dplyr}",
    "text": "Data processing in {dplyr}\nThis package has all sorts of things. And it interacts with {tibble} generally.\nThe basic idea is “tibble in, tibble out”.\nSatisfies data masking which means you can refer to columns by name or use helpers like ends_with(\"_rate\")\nMajorly useful operations:\n\nselect() (chooses columns to keep)\nmutate() (showed this already)\ngroup_by()\npivot_longer() and pivot_wider()\nleft_join() and full_join()\nsummarise()\n\n\n\n\n\n\n\nNote\n\n\nfilter() and select() are functions in Base R.\nSometimes you get 🐞 because it called the wrong version.\nTo be sure, prefix it like dplyr::select()."
  },
  {
    "objectID": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "href": "schedule/slides/00-r-review.html#a-useful-data-frame",
    "title": "UBC Stat406 2025 W1",
    "section": "A useful data frame",
    "text": "A useful data frame\n\n7-day rolling avg COVID case/death counts for CA and WA from Aug 1-21, 2022 from Johns Hopkins\n\nlibrary(tidyverse)\ncovid &lt;- read_csv(\"data/covid.csv\") |&gt;\n  select(geo_value, time_value, signal, value)\n\ncovid\n\n# A tibble: 84 × 4\n   geo_value time_value signal                        value\n   &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;                         &lt;dbl&gt;\n 1 ca        2022-08-01 confirmed_7dav_incidence_prop  45.4\n 2 wa        2022-08-01 confirmed_7dav_incidence_prop  27.7\n 3 ca        2022-08-02 confirmed_7dav_incidence_prop  44.9\n 4 wa        2022-08-02 confirmed_7dav_incidence_prop  27.7\n 5 ca        2022-08-03 confirmed_7dav_incidence_prop  44.5\n 6 wa        2022-08-03 confirmed_7dav_incidence_prop  26.6\n 7 ca        2022-08-04 confirmed_7dav_incidence_prop  42.3\n 8 wa        2022-08-04 confirmed_7dav_incidence_prop  26.6\n 9 ca        2022-08-05 confirmed_7dav_incidence_prop  40.7\n10 wa        2022-08-05 confirmed_7dav_incidence_prop  34.6\n# ℹ 74 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples",
    "href": "schedule/slides/00-r-review.html#examples",
    "title": "UBC Stat406 2025 W1",
    "section": "Examples",
    "text": "Examples\nRename the signal to something short.\n\ncovid &lt;- covid |&gt;\n  mutate(signal = case_when(\n    str_starts(signal, \"confirmed\") ~ \"case_rate\",\n    TRUE ~ \"death_rate\"\n  ))\n\nSort by time_value then geo_value\n\ncovid &lt;- covid |&gt; arrange(time_value, geo_value)\n\nCalculate grouped medians\n\ncovid |&gt;\n  group_by(geo_value, signal) |&gt;\n  summarise(med = median(value), .groups = \"drop\")\n\n# A tibble: 4 × 3\n  geo_value signal        med\n  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;\n1 ca        case_rate  33.2  \n2 ca        death_rate  0.112\n3 wa        case_rate  23.2  \n4 wa        death_rate  0.178"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#examples-1",
    "href": "schedule/slides/00-r-review.html#examples-1",
    "title": "UBC Stat406 2025 W1",
    "section": "Examples",
    "text": "Examples\nSplit the data into two tibbles by signal\n\ncases &lt;- covid |&gt;\n  filter(signal == \"case_rate\") |&gt;\n  rename(case_rate = value) |&gt; select(-signal)\ndeaths &lt;- covid |&gt;\n  filter(signal == \"death_rate\") |&gt;\n  rename(death_rate = value) |&gt; select(-signal)\n\nJoin them together\n\njoined &lt;- full_join(cases, deaths, by = c(\"geo_value\", \"time_value\"))\n\nDo the same thing by pivoting\n\ncovid |&gt; pivot_wider(names_from = signal, values_from = value)\n\n# A tibble: 42 × 4\n   geo_value time_value case_rate death_rate\n   &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 ca        2022-08-01      45.4      0.105\n 2 wa        2022-08-01      27.7      0.169\n 3 ca        2022-08-02      44.9      0.106\n 4 wa        2022-08-02      27.7      0.169\n 5 ca        2022-08-03      44.5      0.107\n 6 wa        2022-08-03      26.6      0.173\n 7 ca        2022-08-04      42.3      0.112\n 8 wa        2022-08-04      26.6      0.173\n 9 ca        2022-08-05      40.7      0.116\n10 wa        2022-08-05      34.6      0.225\n# ℹ 32 more rows"
  },
  {
    "objectID": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "href": "schedule/slides/00-r-review.html#plotting-with-ggplot2",
    "title": "UBC Stat406 2025 W1",
    "section": "Plotting with {ggplot2}",
    "text": "Plotting with {ggplot2}\n\nEverything you can do with ggplot(), you can do with plot(). But the defaults are much prettier.\nIt’s also much easier to adjust by aesthetics / panels by factors.\nIt also uses “data masking”: data goes into ggplot(data = mydata), then the columns are available to the rest.\nIt (sort of) pipes, but by adding layers with +\nIt strongly prefers “long” data frames over “wide” data frames.\n\n\nI’ll give a very fast overview of some confusing bits."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html",
    "href": "schedule/lectures/lecture_01_probability.html",
    "title": "Lecture 1: Probability Review",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine a random variable and understand when to use them to model quantities\nDerive probability rules through the product and sum rules\nApply linearity of expectation and the law of total expectation to simplify calculations"
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#learning-objectives",
    "href": "schedule/lectures/lecture_01_probability.html#learning-objectives",
    "title": "Lecture 1: Probability Review",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine a random variable and understand when to use them to model quantities\nDerive probability rules through the product and sum rules\nApply linearity of expectation and the law of total expectation to simplify calculations"
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#random-variables",
    "href": "schedule/lectures/lecture_01_probability.html#random-variables",
    "title": "Lecture 1: Probability Review",
    "section": "Random Variables",
    "text": "Random Variables\n\nMotivation\n\nExample: Let’s say you want to grab a coffee at Loafe and you want to know how long you’ll have to wait in line.\nDenote this time by the variable \\(A\\)\n\\(Y\\) depends on a multitude of factors:\n\nHow hot it is outside\nWhat day of the week it is\nHow late Josh and his friends were up playing video games, thus leading them to take up spots in line\n\nWhile we could try to model all of these factors, it would be infeasible to do so.\nInstead, we can treat \\(Y\\) as a random variable: a variable whose value is randomly sampled from some distribution.\n\n\n\nNotation\n\nWe (almost) always denote random variables with uppercase letters (e.g., \\(A\\))\nWe (almost) always denote their realizations (i.e., specific values they can take) with lowercase letters (e.g., \\(a\\)).\n\n\n\nJoint Random Variables\n\nThroughout this class, most of the probability we will encounter will be concerned with relationships between two or more random variables.\nExample: maybe we want to understand how the temperature outside, denoted by \\(B\\), affects the Loafe line length.\nAgain, \\(B\\) depends on many factors:\n\nWhat time of year it is\nWhether or not it’s sunny outside\nHow many flights Sarah took last year, thus leading to an increase in greenhouse gases\n\nWe can treat \\(B\\) as a random variable as well.\n\\(A\\) and \\(B\\) are related to one another, potentially in a causal manner. If we treat them as joint random variables, we can derive many useful probabilistic representations about their relationship."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#distributions-and-the-two-rules-of-probability",
    "href": "schedule/lectures/lecture_01_probability.html#distributions-and-the-two-rules-of-probability",
    "title": "Lecture 1: Probability Review",
    "section": "Distributions and The Two Rules of Probability",
    "text": "Distributions and The Two Rules of Probability\n\nGiven two random variables (A) and (B), we can describe this relationship through a joint probability distribution\n\\[\n  \\begin{cases}\n      P(A=a, B=b) & \\text{for discrete random variables} \\\\\n      f_{A,B}(a, b) & \\text{for continuous random variables}\n  \\end{cases}\n  \\]\nWithout loss of generality, we will use the discrete random variable notation throughout the rest of this lecture (and throughout most of the course).\nWe can also describe (A) and (B) through:\n\nConditional distributions, i.e., (P(A=a | B=b)) or (P(B=b | A=a))\nMarginal distributions, i.e., (P(A=a)) or (P(B=b))\n\nWhile there are many fundamental rules of probability to manipulate these distributions, most of them can be derived from two basic rules: the product rule and the sum rule."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#the-product-rule",
    "href": "schedule/lectures/lecture_01_probability.html#the-product-rule",
    "title": "Lecture 1: Probability Review",
    "section": "The Product Rule",
    "text": "The Product Rule\nThe product rule allows us to decompose a joint distribution into the product of a conditional and marginal probability:\n\\[\\begin{align*}\nP(A=a, B=b) &= P(A=a|B=b)P(B=b) \\\\\n&= P(B=b|A=a)P(A=a)\n\\end{align*}\\]\n\nThis rule can be applied recursively in the case of more than two random variables.\nThis rule gives rise to lots of useful facts from probability theory.\n\n\nIndependence\n\nWe say that \\(A\\) and \\(B\\) are independent if \\(P(A=a, B=b) = P(A=a) P(B=b)\\); that is, their joint probability (density) is the product of their marginal probability (densities).\nBy the product rule, for independent random variables we have that\n\\[ P(A=a) P(B=b) = P(A=a, B=b) = P(A=a | B=b) P(B=b), \\]\nand, through some algebra, that \\(P(A=a) = P(A=a | B=b)\\). (Similarly, \\(P(B=b) = P(B=b | A=a)\\).)\nIn other words, when \\(A\\) and \\(B\\) are independent, the occurrence of \\(B\\) does not affect the probability of \\(A\\), and vice versa.\n\n\n\nBayes’ Rule\nWe can derive Bayes’ formula\n\\[ P(B=b | A=a) = \\frac{P(A=a | B=b) P(B=b)}{P(A=a)} \\]\nusing the product rule by starting from the identity \\(P(B=b, A=a) = P(A=a, B=b)\\) and simplifying."
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#the-sum-rule",
    "href": "schedule/lectures/lecture_01_probability.html#the-sum-rule",
    "title": "Lecture 1: Probability Review",
    "section": "The Sum Rule",
    "text": "The Sum Rule\nThe sum rule allows us to obtain a marginal probability for \\(A\\) (or \\(B\\)) from a joint probability over \\(A\\) and \\(B\\):\n\\[ P(A=a) = \\int_{b} P(A=a, B=b) \\: \\mathrm{d}b. \\]\n\nHere, we are again assuming \\(A\\) and \\(B\\) are continuous and \\(P\\) represents a density. The integral becomes a summation in the case of discrete random variables.\nAgain, this rule can be extended to three or more variables recursively.\nThis rule is instrumental in establishing properties about expectations:\n\n\nLinearity of Expectation\n\nWe define the expected value of \\(A\\) as:\n\\[ \\mathbb{E}[A] := \\int_{a} a \\: P(A=a) \\: \\mathrm{d}a. \\]\nSimilarly, the expected value of some function of \\(A\\) and \\(B\\) is defined as:\n\\[ \\mathbb{E}[f(A, B)] := \\int_a \\int_b f(a, b) P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a. \\]\nWe can use the sum rule in conjunction with Fubini’s theorem to derive one of the most important formulas in all of probability:\n\\[\\begin{align*}\n  \\mathbb{E}[A + B]\n  &= \\int_a \\int_b (a + b) P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a \\int_b a P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\ &\\quad + \\int_a \\int_b b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a a \\int_b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\ &\\quad + \\int_b b \\int_a P(A=a, B=b) \\: \\mathrm{d}a \\: \\mathrm{d}b\n  \\\\\n  &= \\int_a a P(A=a) \\: \\mathrm{d}a + \\int_b b P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\mathbb{E}[A] + \\mathbb{E}[B]\n  \\end{align*}\\]\nThis formula, known as linearity of expectation, holds even when \\(A\\) and \\(B\\) are not independent! We will use this fact constantly throughout the course.\nAs a fun challenge problem, try using this formula to derive the famous inclusion-exclusion principle:\n\\[ P(A \\cup B) = P(A) + P(B) - P(A \\cap B). \\]\nHint: note that \\(P(A) = \\mathbb{E}[\\mathbf{1}_A]\\), where \\(\\mathbf{1}_A\\) is the indicator random variable for event \\(A\\). Similarly, \\(P(A \\cap B) = \\mathbb{E}[\\mathbf{1}_A \\mathbf{1}_B]\\) and \\(P(A \\cup B) = 1 - P(\\overline{A} \\cap \\overline{B})\\).\n\n\n\nThe Tower Rule\n\nWe define the expected value of \\(A\\) as:\n\\[ \\mathbb{E}[A] := \\int_{a} a \\: P(A=a) \\: \\mathrm{d}a. \\]\nUsing the sum rule and product rule gives us:\n\\[\\begin{align*}\n  \\mathbb{E}[A] &= \\int_a a \\int_b P(A=a, B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_a a \\int_b P(A=a \\mid B=b) P(B=b) \\: \\mathrm{d}b \\: \\mathrm{d}a\n  \\\\\n  &= \\int_b \\left( \\int_a a P(A=a \\mid B=b) \\: \\mathrm{d}a \\right) P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\int_b \\mathbb{E}[A \\mid B=b] P(B=b) \\: \\mathrm{d}b\n  \\\\\n  &= \\mathbb{E} \\left[ \\mathbb{E}[ A \\mid B ] \\right]\n  \\end{align*}\\]\nThis rule is known as the Tower Rule. It allows us to express marginal expectations as recursive applications of conditional expectations.\nThis notation is often confusing and scary the first few times you encounter it. Try translating it back into probabilities via the sum and product rules, and you’ll fluently understand it in no time!"
  },
  {
    "objectID": "schedule/lectures/lecture_01_probability.html#conclusion",
    "href": "schedule/lectures/lecture_01_probability.html#conclusion",
    "title": "Lecture 1: Probability Review",
    "section": "Conclusion",
    "text": "Conclusion\n\nWe will use (jointly-distributed) random variables to model data, models that depend on data, and predictions that depend on models that depend on data.\nYou will need to manipulate marginal, joint, and conditional probabilities and expectations of these random variables throughout the course.\nThis review has covered most of the probability rules that we’ll use, but just remember that you can always derive any of them through the product and sum rules!"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine the log-odds statistical model for binary classification, and justify its advantages\nDerive logistic regression through MLE and ERM\nConstruct predictions for classifiers based on different notions of risk"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#learning-objectives",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#learning-objectives",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "",
    "text": "By the end of this lecture, you will be able to:\n\nDefine the log-odds statistical model for binary classification, and justify its advantages\nDerive logistic regression through MLE and ERM\nConstruct predictions for classifiers based on different notions of risk"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#supervised-learning-for-classification-statistical-perspective",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#supervised-learning-for-classification-statistical-perspective",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Supervised Learning for Classification: Statistical Perspective",
    "text": "Supervised Learning for Classification: Statistical Perspective\nIn the previous lecture, we developed a statistical perspective of the supervised learning procedure, and we worked through the steps of the learning procedure in a regression context.\n\n\n\n\n\n\n\n\n\nStep\nCS Perspective\nStatistical Perspective\nExample: Linear Regression\n\n\n\n\n2\nHypothesis Class\nStatistical Model\n\\(\\mathbb{E}[Y \\mid X = x] = x^\\top \\beta\\)\n\n\n3\nTraining\nEstimation\n\\(\\hat{\\beta}_\\mathrm{MLE/OLS} = (\\boldsymbol{X}^\\top \\boldsymbol{X})^{-1} \\boldsymbol{X}^\\top \\boldsymbol{Y}\\)\n\n\n6\nTesting (Inference)\nPrediction\n\\(\\hat{Y}_\\mathrm{new} = X_\\mathrm{new}^\\top \\hat{\\beta}\\)\n\n\n\nIn this lecture, we will work through the same steps, but this time for classification problems.\nClassification versus regression:\n\nIn regression, the response variable \\(Y\\) is continuous (i.e. \\(\\mathcal Y = \\mathbb{R}\\)).\nIn binary classification, the response variable \\(Y\\) is boolean (i.e. \\(\\mathcal Y = \\{0, 1\\}\\)), where\n\n\\(Y = 1\\) represents the “positive” class\n\\(Y = 0\\) represents the “negative” class\n(You might see \\(Y = -1\\) used to denote the negative class. It doesn’t make a huge difference, but we’ll stick with \\(Y=0\\) for mathematical simplicity.)\n\nThe goal remains the same: learn a function \\(\\hat{f}: \\mathbb{R}^d \\to \\{0, 1\\}\\) that accurately predicts the class label for new observations.\n\nLet’s now derive a statistical model estimation procedure, and prediction rule for binary classification!\nSneak peak: logistic regression\n\nWe will ultimately derive logistic regression,"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#statistical-model-linear-log-odds",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#statistical-model-linear-log-odds",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Statistical Model: Linear Log-Odds",
    "text": "Statistical Model: Linear Log-Odds\n\nRecall that a statistical model is a set of probability distributions.\nIn classification, we are interested in families of \\(P(Y=1 \\mid X = x)\\) distributions\n\nThese distributions implicitly define the distribution of \\(P(Y=0 \\mid X = x)\\) by the law of total probability, i.e. \\(P(Y=0 \\mid X = x) = 1 - P(Y=1 \\mid X = x)\\).\n\nNotation: For simplicity, let’s define:\n\n\\(\\pi_1(x) := P(Y = 1 \\mid X = x)\\) (probability of positive class)\n\\(\\pi_0(x) := P(Y = 0 \\mid X = x)\\) (probability of negative class)\nNote that \\(\\pi_0(x) = 1 - \\pi_1(x)\\).\n\n\nSneak peak: the logistic regression model\n\nYou may remember from STAT 406 or CPSC 340 a set of distributions that look like: \\[P(Y = 1 \\mid X = x) = \\pi_1(x) = \\frac{\\exp(x^\\top \\beta)}{1 + \\exp(x^\\top \\beta)}\\]\n\n\\(\\beta \\in \\mathbb{R}^p\\) is a vector of parameters.\n\nThis statistical model is known as logistic regression, and it is a good starting point for binary classification.\nWe will now derive this model from first principles.\n\nWhat type of distribution should we model?\n\nIt’s hard to directly model \\(\\pi_1(x)\\) or \\(\\pi_0(x)\\) distributions.\nConsider \\(\\pi_1(x) = x^\\top \\beta\\). What’s the problem with this?\n\nIf \\(x^\\top \\beta &gt; 1\\), then \\(\\pi_1(x) = P(Y=1|X=x) &gt; 1\\), which is not a valid probability.\nIf \\(x^\\top \\beta &lt; 0\\), then \\(\\pi_1(x) = P(Y=1|X=x) &lt; 0\\), which is also not a valid probability.\n\n\nModelling the log-odds\n\nInstead of defining distributions through \\(P(Y = 1 \\mid X = x)\\) directly, we will instead define distributions through the log-odds ratio\nThe log-odds ratio is then: \\[r(x) := \\log\\left(\\frac{\\pi_1(x)}{\\pi_0(x)}\\right) = \\log\\left(\\frac{\\pi_1(x)}{1 - \\pi_1(x)}\\right)\\]\nI claim that this ratio can take any real value, i.e. \\(r(x) \\in \\mathbb{R}\\).\n\n\n\n\\(\\pi_1(x)\\)\nOdds Ratio\n\\(r(x)\\) (Log Odds Ratio)\n\n\n\n\n\\(\\approx 1\\)\n\\(\\pi_1(x) / \\pi_0(x) \\to \\infty\\)\n\\(r(x) \\to \\infty\\)\n\n\n\\(\\approx 0\\)\n\\(\\pi_1(x) / \\pi_0(x) \\to 0\\)\n\\(r(x) \\to -\\infty\\)\n\n\n\n\nThe linear log-odds model\nWe’re now ready to define a statistical model for binary classification.\n\nWe will consider\n\\[\\log\\left(\\frac{\\pi_1(x)}{\\pi_0(x)}\\right) = x^\\top\\beta\\]\nBy substituting \\(\\pi_0(x) = 1 - \\pi_1(x)\\) and then solving for \\(\\pi_1(x)\\), we get:\n\\[\\pi_1(x) = \\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)}\\]\nThus our statistical model is the following set of distributions: \\[\\left\\{ P(Y=1 \\mid X) \\: : \\: P(Y=1 \\mid X=x) = \\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)}, \\quad \\beta \\in \\mathbb{R}^d \\right\\}\\]\n\nThe logistic function:\n\nThe function in the above equation is known as the logistic function or sigmoid function:\n\\[\\frac{\\exp(x^\\top\\beta)}{1 + \\exp(x^\\top\\beta)} = \\frac{1}{1 + \\exp(-x^\\top\\beta)} := \\sigma(x^\\top \\beta)\\]\nIt has many useful properties\n\nRange: \\(\\sigma(z) \\in (0, 1)\\) for all \\(z \\in \\mathbb{R}\\)\nSymmetric: \\(\\sigma(-z) = 1 - \\sigma(z)\\)\nConvenient derivative: \\(\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))\\)\n\nIt can be seen as a “smooth approximation” to the 0-1 step function:\n\n\n\n\nLogistic Function"
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#prediction-and-different-notions-of-risk",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#prediction-and-different-notions-of-risk",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Prediction and Different Notions of Risk",
    "text": "Prediction and Different Notions of Risk\nGoing a bit out of order, let’s discuss how we can make predictions using the logistic regression model.\nMaking predictions:\nGiven\n\na \\(\\hat \\beta\\) estimate\na new observation \\(X_{\\text{new}}\\)\na loss function \\(L(Y, \\hat{Y})\\) that quantifies how “bad” a prediction is\n\nrecall that we make predictions by minimizing the expected loss:\n\\[\\hat{Y}_{\\text{new}} = \\mathrm{argmin}_{\\hat{y}} \\mathbb{E}[L(Y, \\hat{y}) \\mid X_{\\text{new}}, \\hat \\beta]\\]\n0/1 Loss:\n\nThe most natural loss function for classification is the 0/1 loss:\n\n\\[L_{0/1}(Y, \\hat{Y}) = \\mathbb{I}(Y \\neq \\hat{Y}) = \\begin{cases} 0 & \\text{if } Y = \\hat{Y} \\\\ 1 & \\text{if } Y \\neq \\hat{Y} \\end{cases}\\]\n\nUnder our logistic model, we have:\n\n\\[\\begin{align*}\n\\hat{Y}_{\\text{new}} &= \\mathrm{argmin}_{\\hat{y}} \\mathbb{E}[ \\mathbb{I}(Y \\neq \\hat{y}) \\mid X_{\\text{new}}, \\hat \\beta] \\\\\n&= \\mathrm{argmin}_{\\hat{y}} P(Y \\neq \\hat{y} \\mid X_{\\text{new}}, \\hat \\beta) \\\\\n&= \\mathrm{argmin}_{\\hat{y}} \\left[\n   \\underbrace{P(Y = 1 \\mid X_{\\text{new}}, \\hat \\beta)}_{\\sigma(X_\\mathrm{new}^\\top \\hat \\beta)} \\mathbb{I}(\\hat{y} = 0) +\n   \\underbrace{P(Y = 0 \\mid X_{\\text{new}}, \\hat \\beta)}_{1 - \\sigma(X_\\mathrm{new}^\\top \\hat \\beta)} \\mathbb{I}(\\hat{y} = 1) \\right] \\\\\n&= \\begin{cases}\n1 & \\text{if } \\sigma(X_\\mathrm{new}^\\top \\hat \\beta) &gt; 0.5 \\\\\n0 & \\text{if } \\sigma(X_\\mathrm{new}^\\top \\hat \\beta) \\leq 0.5\n\\end{cases}\n\\end{align*}\\].\nDecision boundary:\n\nNote that\n\n\\(\\sigma(X_\\mathrm{new}^\\top \\hat \\beta) &gt; 0.5\\) when \\(X_\\mathrm{new}^\\top \\hat \\beta &gt; 0\\)\n\\(\\sigma(X_\\mathrm{new}^\\top \\hat \\beta) \\leq 0.5\\) when \\(X_\\mathrm{new}^\\top \\hat \\beta \\leq 0\\).\n\nThe decision boundary, defined by:\n\\[x^\\top\\beta = 0\\]\nis the hyperplane that separates the positively-classified \\(x\\) from the negatively classified \\(x\\).\nThis decision boundary for logistic regression is linear in \\(x\\).\n\n\n\n\nDecision Boundary\n\n\nOther losses:\nThere are other losses that we could use to generated different prediction rules:\n\nProbabilistic loss: \\(L_\\mathrm{prob}(Y, \\hat{Y}) = -\\log P(Y = \\hat Y)\\)\n\nThis loss produces “soft” predictions (e.g. \\(\\hat{Y}_{\\text{new}} = 0.273\\)) that give a probability estimate of the positive class.\n\nAsymmetric losses: \\(L_\\alpha(Y, \\hat{Y}) = \\alpha \\mathbb{I}(Y = 1, \\hat{Y} = 0) + (1-\\alpha) \\mathbb{I}(Y = 0, \\hat{Y} = 1)\\) for some \\(\\alpha \\in (0, 1)\\)\n\nThis loss allows us to penalize false positives and false negatives differently, which can be useful in imbalanced datasets.\n\n\nWe will explore these losses, as well as metrics derived from these losses, in a homework assignment."
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#estimation",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#estimation",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Estimation",
    "text": "Estimation\nAs with regression, we will derive estimators for the logistic regression parameters \\(\\beta\\) through the principles of maximum likelihood estimation (MLE) and empirical risk minimization (ERM).\n\nMaximum Likelihood Estimation (MLE)\nRecall the MLE estimator is given by\n\\[\\hat{\\beta}_\\mathrm{MLE} = \\mathrm{argmax}_{\\beta} \\mathcal L(\\beta) = \\mathrm{argmax}_{\\beta} \\ell(\\beta)\\]\nwhere \\(\\mathcal{L}(\\beta)\\) is the likelihood function and \\(\\ell(\\beta)\\) is the log-likelihood function.\nLog likelihood of the linear log-odds model:\n\nGiven training data \\(\\mathcal{D} = \\{(X_1, Y_1), \\ldots, (X_n, Y_n)\\}\\), recall that the likelihood/log-likelihood function is:\n\\[\\begin{gather}\n\\mathcal{L}(\\beta) = \\prod_{i=1}^n P(Y_i \\mid X_i; \\beta) \\\\\n\\ell(\\beta) = \\sum_{i=1}^n \\log P(Y_i \\mid X_i; \\beta)\n\\end{gather}\\]\nPlugging in our model, we have that:\n\\[\nP(Y_i \\mid X_i; \\beta) = \\begin{cases}\n   \\sigma(X_i^\\top \\beta) & \\text{if } Y_i = 1 \\\\\n   1 - \\sigma(X_i^\\top \\beta) & \\text{if } Y_i = 0\n\\end{cases}\n\\]\nWe can write this equation more compactly as:\n\\[P(Y_i \\mid X_i, \\beta) = \\sigma(X_i^\\top \\beta)^{Y_i} (1 - \\sigma(X_i^\\top \\beta))^{1-Y_i}.\\]\nThus the log likelihood function is:\n\\[\n\\ell(\\beta) = \\sum_{i=1}^n \\left[ Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right]\n\\]\nwhere \\(\\pi_1(X_i) = \\sigma(\\beta_0 + X_i^\\top\\beta)\\) and \\(\\pi_0(X_i) = 1 - \\pi_1(X_i)\\).\n\nComputing the MLE estimator:\n\\[\n\\hat \\beta_\\mathrm{MLE} = \\mathrm{argmax}_{\\beta} \\sum_{i=1}^n \\left[ Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1 - Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right]\n\\]\n\nUnlike linear regression, we cannot compute this maximum of this log-likelihood in closed form.\nWe can numerically solve for the optimization using a technique called gradient descent, which we will cover in a future lecture.\n\n\n\nEmpirical Risk Minimization (ERM)\nRecall the ERM estimator is given by\n\\[\\hat{\\beta}_\\mathrm{MLE} = \\mathrm{argmin}_{\\beta} \\sum_{i=1}^n L(Y_i, \\hat Y_i)\\]\nwhere \\(L\\) is a loss function of our choosing.\nLoss function for classification: - Using the 0/1 loss , as well as its corresponding prediction rule that we derived above, we have \\[L(Y, \\hat Y) = \\mathbb{I}(Y \\neq \\hat Y), \\qquad \\hat Y = \\mathbb{I}(X^\\top \\beta &gt; 0)\\]\n\nHowever, this loss leads to a hard minimization problem:\n\n\\[\\mathrm{argmin}_{\\beta} \\sum_{i=1}^n \\mathbb{I}(Y_i \\neq \\mathbb{I}(X_i^\\top \\beta &gt; 0))\\]\n\nThis optimization problem not only has no closed-form solution, but numerical solutions to it are intractible to compute.\nComputationally, it is NP-hard, meaning it will take \\(O(2^n)\\) time to solve in the worse case!\n\nLogistic Loss as a Surrogate\n\nInstead, we can substitute \\(L(Y, \\hat Y)\\) with the following approximation:\n\n\\[L_{\\text{logistic}}(Y, \\mathbb{I}(X_i^\\top \\beta &gt; 0)) \\approx \\left[ (1-Y) \\sigma(X^\\top \\beta) + Y (1 - \\sigma(X^\\top \\beta)) \\right]\\]\n\nThis approximation will be close to the 0/1 loss when \\(X_i^\\top \\beta\\) is very positive or very negative:\n\n\n\n\n\n\n\n\n\n\\(X_i^\\top \\beta\\)\n\\(\\sigma(X_i^\\top \\beta)\\)\n\\((1 - \\sigma(X_i^\\top \\beta)\\)\n\n\n\n\n\\(\\gg 0\\)\n\\(\\approx 1\\)\n\\(\\approx 0\\)\n\n\n\\(\\ll 0\\)\n\\(\\approx 0\\)\n\\(\\approx 1\\)\n\n\n\n\nSubstituting this loss into the ERM estimator yields a convex optimization problem, which are (relatively) easy to solve numerically:\n\n\\[\\hat{\\beta}_\\mathrm{ERM} = \\mathrm{argmin}_{\\beta} \\sum_{i=1}^n \\left[ (1-Y_i) \\sigma(X_i^\\top \\beta) + Y_i (1 - \\sigma(X_i^\\top \\beta)) \\right]\\]\n\nA little manipulation will show that this is equalivalent to the MLE optimization problem!\n\n\\[\\begin{align*}\n\\hat{\\beta}_\\mathrm{ERM} &= \\mathrm{argmin}_{\\beta} \\sum_{i=1}^n \\left[ (1-Y_i) \\sigma(X_i^\\top \\beta) + Y_i (1 - \\sigma(X_i^\\top \\beta)) \\right] \\\\\n&= \\mathrm{argmax}_{\\beta} \\sum_{i=1}^n \\left[ (Y_i - 1) \\log(\\sigma(X_i^\\top \\beta)) + (-Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right] \\\\\n&= \\mathrm{argmax}_{\\beta} \\sum_{i=1}^n \\left[ Y_i \\log(\\sigma(X_i^\\top \\beta)) + (1-Y_i) \\log(1 - \\sigma(X_i^\\top \\beta)) \\right]\n= \\hat{\\beta}_\\mathrm{MLE}.\n\\end{align*}\\]\nKey insight: For logistic regression, MLE and ERM (with the logistic loss approximation) are equivalent."
  },
  {
    "objectID": "schedule/lectures/lecture_03_learning_procedure_classification.html#summary",
    "href": "schedule/lectures/lecture_03_learning_procedure_classification.html#summary",
    "title": "Lecture 3: Introduction to Learning (Cont.), Classification, Logistic Regression",
    "section": "Summary",
    "text": "Summary\nThis lecture extended the statistical framework from regression to classification:\n\nStatistical Model: The log-odds model provides a principled way to model binary responses while ensuring probabilities stay in \\([0, 1]\\).\nPrediction: The optimal prediction rule depends on the loss function.\nLinear Decision Boundary: Under the \\(0/1 loss\\), the \\(Y=0\\) and \\(Y=1\\) predictions are separated by a hyperplane defined by the decision boundary \\(X^\\top \\beta = 0\\).\nEstimation: Both MLE and ERM (with logistic loss) lead to the same optimization problem, requiring numerical methods.\n\nIn the next lecture, we will explore the last remaining steps of the learning procedure: model selection and evaluation."
  }
]