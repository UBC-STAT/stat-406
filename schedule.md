---
title: Schedule
layout: page
icon: "far fa-calendar-alt"
---

Required readings and lecture videos are listed below for each module.
Readings from [\[ISLR\]](https://www.statlearning.com) are always required while those from [\[ESL\]](https://web.stanford.edu/~hastie/ElemStatLearn/) are optional and supplemental. The exception is the coverage of Neural Networks in Module 4 (this material does not appear in \[ISLR\]). Required readings and videos must be completed before the mini quiz at the beginning of each module.


<div class="text-center">
<div class="btn-group" role="group">
  <a role="button" class="btn btn-secondary" color="white" href="#0-introduction-and-review">0 Intro</a>
  <a role="button" class="btn btn-secondary" color="white" href="#1-model-accuracy">1 Model Checks</a>
  <a role="button" class="btn btn-secondary" color="white" href="#2-regularization-smoothing-and-trees">2 Regression</a>
  <a role="button" class="btn btn-secondary" color="white" href="#3-classification">3 Classification</a>
  <a role="button" class="btn btn-secondary" color="white" href="#4-modern-techniques">4 Modern</a>
  <a role="button" class="btn btn-secondary" color="white" href="#5-unsupervised-learning">5 Unsupervised</a>
  <a role="button" class="btn btn-secondary" color="white" href="#f-final-exam">F Final</a>
</div>
</div>



## 0 Introduction and Review

Required reading below is meant to reengage brain cells which have no doubt forgotten all
the material that was covered in STAT 306 or CPSC 340. We don't presume that you remember all these details, but that, upon rereading, they at least sound familiar. If this all strikes you as completely foreign, this class may not be for you. 

Required reading  
: \[ISLR\] 2.1, 2.2, and Chapter 3 (this material is review)

Video lectures from last year
: Lectures 1 and 2

Optional reading
: \[ESL\] 2.4 and 2.6

|Date      |Slides |Deadlines    |
|:---------|:-----------|:-----------|
|7 Sep 21  | |(no class, Imagine UBC) |
|9 Sep 21  |[Intro to class](00-intro-to-class.html), [Git](00-version-control.html) |Syllabus, Git, Github | |
|14 Sep 21 | [LM review](01-lm-review.html), [LM Example](02-lm-example.html)|   |
{: .table .table-striped}




## 1 Model Accuracy

Topics  
: Model selection; cross validation; information criteria; stepwise regression

Required reading  
: \[ISLR\] Ch 2.2 (not 2.2.3), 5.1 (not 5.1.5), 6.1, 6.4

Video lectures from last year
: Lectures 3 to 7 

Optional reading
: \[ESL\] 7.1-7.5, 7.10

|Date      |Slides |Deadlines    |
|:---------|:-----------|:-----------|
|16 Sep 21 |[Regression function](03-regression-function.html), [Bias and Variance](04-bias-variance.html)
|21 Sep 21 |[Risk estimation](05-estimating-test-mse.html), [Info Criteria](06-information-criteria.html)    | |
|23 Sep 21 | [Greedy selection](07-greedy-selection.html) | PC 1 due |
|28 Sep 21 |         | HW 1 due  |
{: .table .table-striped}




## 2 Regularization, smoothing, and trees

Topics  
: Ridge regression, lasso, and related; linear smoothers (splines, kernels); kNN

Required reading  
: \[ISLR\] Ch 6.2, 7.1-7.7.1, 8.1, 8.1.1, 8.1.3, 8.1.4

Video lectures from last year  
: Lectures 8-13

Optional reading
: \[ESL\] 3.4, 3.8, 5.4, 6.3

|Date      |Slides |Deadlines    |
|:---------|:---------|:-----|
|30 Sep 21 |(no class, National Day for Truth and Reconciliation) |
|5 Oct 21  |[Ridge](08-ridge-regression.html), [Lasso](09-l1-penalties.html)  |  |
|7 Oct 21  |[NP 1](10-basis-expansions.html), [NP 2](11-kernel-smoothers.html) |  |
|12 Oct 21 |[Why smoothing?](12-why-smooth.html)  |  |
|14 Oct 21 |[Other](13-gams-trees.html)          |  |
|19 Oct 21 |          | HW 2 due |
{: .table .table-striped}



## 3 Classification

Topics  
: logistic regression; LDA/QDA; naive bayes; trees

Required reading  
: \[ISLR\] Ch 2.2.3, 5.1.5, 4-4.5, 8.1.2

Video lectures from last year
: Lectures 14-17 and Gradient descent (best after lecture 15)

Optional reading
: \[ESL\] 4-4.4, 9.2, 13.3

|Date      |Slides |Deadlines    |
|:---------|:---------|:-----|
|21 Oct 21 |[Classification](14-classification-intro.html), [LDA and QDA](15-LDA-and-QDA.html) | PC 2 |
|26 Oct 21 |[Gradient descent](00-gradient-descent.html), [Logistic regression](16-logistic-regression.html) |
|28 Oct 21 |[Nonlinear](17-nonlinear-classifiers.html)  |
|2 Nov 21  |          |
{: .table .table-striped}



## 4 Modern techniques

Topics  
: bagging; boosting; random forests; neural networks

Required reading  
: \[ISLR\] 5.2, 8.2, \[ESL\] 11.1, 11.3, 11.4, 11.7

Video lectures  
: Lecture 18-23

Optional reading
: \[ESL\] 10.1-10.10 (skip 10.7), 11.2, 11.5, 11.6

|Date      |Slides |Deadlines    |
|:---------|:---------|:-----|
| 4 Nov 21   | [The bootstrap](18-the-bootstrap.html) |HW 3 due |
| 9 Nov 21   |[Bagging and random forests](19-bagging-and-rf.html), [Boosting](20-boosting.html)|
| 11 Nov 21  | (no class, Midterm Break) |
| 16 Nov 21  |[Intro to neural nets](21-nnets-intro.html), [Estimating neural nets](22-nnets-estimation.html)  |
| 18 Nov 21  |[Neural nets wrapup](23-nnets-other.html) | HW 4 due |
| 23 Nov 21  |  |
{: .table .table-striped}



## 5 Unsupervised learning

Topics  
: dimension reduction and clustering

Required reading  
: \[ISLR\] 10

Video lectures  
: Lectures 24-29 (Lecture 29 will be a asynchronous)

Optional reading
: \[ESL\] 8.5, 13.2, 14.3, 14.5.1, 14.8, 14.9


|Date      |Slides |Deadlines    |
|:---------|:-----------|:-----------|
|25 Nov 20 |[Intro to PCA](24-pca-intro.html), [Issues with PCA](25-pca-issues.html)  |
|30 Nov 20 | [PCA v KPCA](00-pca-v-kpca.html), [Manifold learning](26-manifolds.html) |
|2 Dec 20  |[K means clustering](27-kmeans.html), [Hierarchical clustering](28-hclust.html) |HW 5 due |
|7 Dec 20  | |PC 3 due |
{: .table .table-striped}




## F Final exam

TBA â€“ During University Scheduled Final Exam Period  
