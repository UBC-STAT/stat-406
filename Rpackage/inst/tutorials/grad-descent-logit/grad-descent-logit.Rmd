---
title: "Gradient Descent: Logistic Regression"
author: DJM
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  learnr::tutorial:
    progressive: true
    theme: journal
    highlight: kate
    ace_theme: solarized_dark
runtime: shiny_prerendered
---

<!--
Derived from IU Stat 432 ic9.Rmd
css: "/css/tutorials.css"
-->

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
library(learnr)
library(knitr)
library(tidyverse)
library(cowplot)
```

## Logistic Regression

###

Suppose $Y=1$ with probability $p(x)$ and $Y=0$ with probability $1-p(x)$. 

I want to model $P(Y=1| X=x)$. 

I'll assume that $\log\left(p(x)/(1-p(x))\right) = ax$ for some scalar $a$. This means that
$p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}$

We're going to estimate $a$ given data.

### Generate Data


```{r you-generate-data, exercise=TRUE}
set.seed(20200405)
n = 100
a = 2
x = runif(n)*10-5
logit <- function(x) 1/(1+exp(-x))
p = logit(a*x)
y = rbinom(n, 1, p)
df = tibble(x=x, y=y)
head(df)
```

```{r generate-data}
set.seed(20200405)
n = 100
a = 2
x = runif(n)*10-5
logit <- function(x) 1/(1+exp(-x))
p = logit(a*x)
y = rbinom(n, 1, p)
df = tibble(x=x, y=y)
```


Now that the data is generated, let's go ahead and take a look at it.

```{r plotdata, exercise=TRUE, exercise.setup="generate-data"}
ggplot(df, aes(x,y)) + geom_point(color="red") +
  stat_function(fun=function(x) logit(a*x))
```

Tasks:

1. $a$ is the "slope". What does it mean?
2. Take a look at the `stat_function()` portion of the code. What happens if you replace `a` with 3? 0.3? What if you change `a*x` to `2*x+1`? Think through what this means.
3. Still examining `stat_function()`, what if you replace `logit(a*x)` with `a*x`? Try one more. Replace `logit(a*x)` with `as.numeric(a*x)`. Explain what this means.

### 

The likelihood is given by

\[
L(y | a, x) = \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i}
\]

The log likelihood is therefore

\[
\begin{aligned}
\ell(y | a, x) &= \log \prod_{i=1}^n p(x_i)^{y_i}(1-p(x_i))^{1-y_i} \\
&= \sum_{i=1}^n y_i\log p(x_i) + (1-y_i)\log(1-p(x_i))\\
&= \sum_{i=1}^n\log(1-p(x_i)) + y_i\log\left(\frac{p(x_i)}{1-p(x_i)}\right)\\
&=\sum_{i=1}^n ax_i y_i + \log\left(1-p(x_i)\right)\\
&=\sum_{i=1}^n ax_i y_i + \log\left(\frac{1}{1+\exp(ax_i)}\right)
\end{aligned}
\]

Remember, we defined $p(x) = \frac{\exp(ax)}{1+\exp(ax)} = \frac{1}{1+\exp(-ax)}$.

Now, we want the negative of this. Why? 

We would maximize the likelihood/log-likelihood, so we minimize the negative likelihood/log-likelihood.

\[
-\ell(y | a, x) = \sum_{i=1}^n -ax_i y_i - \log\left(\frac{1}{1+\exp(ax_i)}\right)
\]

This is, in the notation of our slides $f(a)$. 

We want to minimize it in $a$ by "gradient descent". 

So we need the derivative with respect to $a$: $f'(a)$. 

Now, conveniently, this simplifies a lot. 

\[
\frac{d}{d a} f(a) = \sum_{i=1}^n -x_i y_i - \left(-\frac{x_i \exp(ax_i)}{1+\exp(ax_i)}\right) = 
\sum_{i=1}^n -x_i y_i + p(x_i)x_i = \sum_{i=1}^n -x_i(y_i-p(x_i)).
\]



(Simple) gradient descent (to minimize $-\ell(a)$) is:

1. Input $a_0, \gamma>0, j_\max, \epsilon>0, d/da -L(a)$.
2. For $j=1,2,\ldots,j_\max$,
\[
a_j = a_{j-1} - \gamma \frac{d}{da} (-\ell(a_{j-1}))
\]
3. Stop if $\epsilon > |a_j - a_{j-1}|$.


### Write a function to find $\hat{a}$

Write a function that will keep and store the entire chain of iterates $a_1,a_2,...$.


```{r amlefun, exercise=TRUE}
amle <- function(x, y, a0, gam=0.5, jmax=50, eps=1e-6){
  err = 1e8
  j = 1
  a = double(jmax)
  a[1] = a0
  while(j < jmax && err>eps){
    j    = ____________
    p    = ____________
    grad  = ____________
    a[j] = ____________
    err = abs(a[j] - a[j-1])
  }
  a[1:j]
}

```

###

If done correctly, your code should look something like:

```{r amlecorrect, echo=TRUE, exercise=TRUE, exercise.setup="generate-data"}
amle <- function(x, y, a0, gam=0.5, jmax=50, eps=1e-6){
  ## don't mess with me, or it'll all mess up
  err = 1e8
  j = 1
  a = double(jmax)
  a[1] = a0
  while(j < jmax && err>eps){
    j = j+1
    p = logit(a[j-1]*x)
    grad = sum(-x*(y-p))
    a[j] = a[j-1] - gam * grad
    err = abs(a[j] - a[j-1])
  }
  a[1:j]
}
```

### Run your function and report the results


Let's start by running the function with the default intputs and an initialization of $a_0$=5.

```{r ourmle1, exercise=TRUE, exercise.setup="amlecorrect"}
amle(x, y, 5)
```

What happened? Did it converge to something?



* Try changing the initialization to `a_0=0.1`.
* Try changing the initialization back to `a_0=5` but set `gam=0.1`.
* Try leaving the initialization at `a_0=5` but set `gam=1`.

###

We can compute the actual value of $a_{mle}$ by using the `glm()` function.  What value does this return? Make sure you don't use an intercept.

```{r do-it-with-glm, exercise=TRUE, exercise.setup="amlecorrect", exercise.lines=3}


```

###

```{r gen1-quiz, echo=FALSE}
quiz(caption="Quiz",
  question("Based on the output from `glm()`, which of the inputs returned the correct estimate of $a_{mle}$?",
    answer("`amle(x, y, 5)`"),
    answer("`amle(x, y, .1)`"),
    answer("`amle(x, y, 5, .1)`",correct=TRUE),
    answer("`amle(x, y, 5, 1)`"),
    allow_retry = TRUE
  )
)
```

##

![](https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png)