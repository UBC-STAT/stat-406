---
title: "Correlated predictors"
author: DJM
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  learnr::tutorial:
    progressive: true
    theme: journal
    highlight: kate
    ace_theme: solarized_dark
runtime: shiny_prerendered
---

<!--
Derived from IU Stat 432 ic4.Rmd
css: "/css/tutorials.css"
-->

```{r setup, include=FALSE}
library(learnr)
library(gradethis)

tutorial_options(exercise.timelimit = 5, exercise.checker = gradethis::grade_learnr)

knitr::opts_chunk$set(echo = FALSE)
```


## Data Generation Function

Examine the function below called `generate.data()`.

```{r whynotprint, echo=TRUE}
generate.data <- function(n, cor){
  epsilon = rnorm(n)
  X = matrix(rnorm(2*n),nrow=n) %*% 
    chol(matrix(c(1, cor, cor,1), 2, 2))
  beta = c(3, 2, 1)
  y = cbind(1, X) %*% beta + epsilon
  df = data.frame(y, X)
  return(df)
}
```

```{r generatedata}
generate.data <- function(n, cor){
  epsilon = rnorm(n)
  X = matrix(rnorm(2*n),nrow=n) %*% 
    chol(matrix(c(1, cor, cor,1), 2, 2))
  beta = c(3, 2, 1)
  y = cbind(1, X) %*% beta + epsilon
  df = data.frame(y, X)
  return(df)
}
```



The function above generates a data set of $y$, $x_1$, $x_2$ with $n$ observations from the model $y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \epsilon_i$.

```{r gen1-quiz}
quiz(caption="Question 1",
  question("What is the population slope coefficient of $x_2$ generated from this function?",
    answer("4"),
    answer("3"),
    answer("2"),
    answer("1",correct=TRUE),
    allow_retry = TRUE
  )
)
```

Notice that $X = (\mathbf{x}_1, \mathbf{x}_2)$ is created by generating two columns from standard normal distributions.  This matrix is then multiplied by `chol(matrix(c(1, cor, cor,1), 2, 2))`.

```{r gen2-quiz}
quiz(caption="Question 2",
  question("What is the purpose of this multiplication?",
    answer("This allows $x_1$ to be correlated with $y$."),
    answer("This allows $x_2$ to be correlated with $Y$."),
    answer("This allows $x_1$ to be correlated with $x_2$.", correct = TRUE),
    answer("This allows $x_1$ to be independent of $x_2$."),
    allow_retry = TRUE,
    correct = paste0("Good")
  )
)
```

### Explanation

The goal here is to create correlated predictors (covariates) `X1` and `X2`.
Just as I can generate $y\sim \mathrm{N}(0,\sigma^2)$ with code like

```{r explanation, eval=FALSE, echo=TRUE}
y = rnorm(1,0,1)
y = sigma*y
```

that is, multiplying a standard normal by the sd,
I can get correlated variables by multiplying standard normals by the square root
of the variance matrix that I want. In this case, I'm using the Cholesky decomposition to
get the square root.

## Confidence Interval Widths

Next, we will examine the confidence intervals associated with $\beta_1$ and $\beta_2$.  The function `intervals()` below is supposed to calculate the average width of these two intervals, but first you need to fill in the missing bits so that it works.

After filling in the missing bits, test your function with n=250 and cor=0.5.

```{r confint, exercise=TRUE, exercise.setup="generatedata", exercise.lines = 18, eval=FALSE, include=FALSE}
## This function finds confidence interval widths 
## for the Linear model
## Inputs: n, the number of data points
##         cor, the correlation between X1 and X2
## Outputs: avg, the average width of the confidence 
##               intervals for the regression
intervals <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(y~X1+X2, data=df)
  itvals = ___  ## Get the confidences intervals for the bhats (95%)
  widths = ___   ## Find the width of each interval
  avg = ___   ## get the average width, ignore intercept
  return(avg)
}

set.seed(07042020)
intervals(n= ___, cor= ___)
```

```{r confint-check}
generate.data <- function(n, cor){
  epsilon = rnorm(n)
  X = matrix(rnorm(2*n),nrow=n) %*% 
    chol(matrix(c(1, cor, cor,1), 2, 2))
  beta = c(3, 2, 1)
  y = cbind(1, X) %*% beta + epsilon
  df = data.frame(y, X)
  return(df)
}
intervals <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(y~X1+X2, data=df)
  itvals = confint(mdl) ## Get the confidences intervals for the bhats (95%)
  widths = itvals %*% c(-1,1) ## Find the width of each interval
  avg = mean(widths[-1]) ## get the average width, ignore intercept
  return(avg)
}
intervalsintercept <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(y~X1+X2, data=df)
  itvals = confint(mdl) ## Get the confidences intervals for the bhats (95%)
  widths = itvals %*% c(-1,1) ## Find the width of each interval
  avg = mean(widths) ## get the average width, ignore intercept
  return(avg)
}

set.seed(07042020)
sol = intervals(n=250,cor=0.5)

set.seed(07042020)
wrong = intervalsintercept(n=250,cor=0.5)

grade_result(
  pass_if(~ identical(.result, sol), "Correct!"),
  fail_if(~ identical(.result, wrong ), "Incorrect.  Did you include the intercept in your average?"),
  fail_if(~ TRUE, "Incorrect.")
)
```

###

### The solution should look something like:

```{r intervalfunc, echo=TRUE}
intervals <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(y~X1+X2, data=df)
  itvals = confint(mdl) ## Get the confidences intervals for the bhats (95%)
  widths = itvals %*% c(-1,1) ## Find the width of each interval
  avg = mean(widths[-1]) ## get the average width, ignore intercept
  return(avg)
}
```

## Simulating Interval Widths
This last part will have you examine and visualize the interval widths for 25 different correlations.

As you should be hopefully aware, the correlation is a value that must run between -1 and 1.  Setting `ncors` to 25, the following code generates a sequence of logarithmically-spaced correlation values.

```{r simcor, exercise=TRUE, exercise.lines = 3, eval=FALSE, include=FALSE}
ncors = ___
cors = 1 / (1 + exp(-seq(-5,5,length.out = ncors)))*2 - 1
cors
```

###

Now, let's use the `intervals()` function to calculate the interval width for each of these correlations.  Let's perform the experiment with 50 simulations each.  While this could be performed with a couple of loops.  It is faster (and easier?) to use `replicate()` and `sapply()`.

```{r simcor2-setup}
generate.data <- function(n, cor){
  epsilon = rnorm(n)
  X = matrix(rnorm(2*n),nrow=n) %*% 
    chol(matrix(c(1, cor, cor,1), 2, 2))
  beta = c(3, 2, 1)
  Y = cbind(1, X) %*% beta + epsilon
  df = data.frame(Y, X)
  return(df)
}
intervals <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(Y~X1+X2, data=df)
  itvals = confint(mdl) ## Get the confidences intervals for the bhats (95%)
  widths = itvals %*% c(-1,1) ## Find the width of each interval
  avg = mean(widths[-1]) ## get the average width, ignore intercept
  return(avg)
}
ncors = 25
cors = 1 / (1 + exp(-seq(-5,5,length.out = ncors)))*2 - 1
```


```{r simcor2, exercise=TRUE, exercise.lines = 7}
## calculate the average width for each value of cors
## repeat the experiment 50 times
avg.widths = replicate(50, sapply(cors, intervals, n=250))
avg.widths = rowMeans(avg.widths)
avg.widths
```

###

Finally, let's plot these values using ggplot.  Run the following code:
```{r plotcors-setup}
generate.data <- function(n, cor){
  epsilon = rnorm(n)
  X = matrix(rnorm(2*n),nrow=n) %*% 
    chol(matrix(c(1, cor, cor,1), 2, 2))
  beta = c(3, 2, 1)
  Y = cbind(1, X) %*% beta + epsilon
  df = data.frame(Y, X)
  return(df)
}
intervals <- function(n, cor){
  df = generate.data(n, cor) 
  mdl = lm(Y~X1+X2, data=df)
  itvals = confint(mdl) ## Get the confidences intervals for the bhats (95%)
  widths = itvals %*% c(-1,1) ## Find the width of each interval
  avg = mean(widths[-1]) ## get the average width, ignore intercept
  return(avg)
}
ncors = 25
cors = 1 / (1 + exp(-seq(-5,5,length.out = ncors)))*2 - 1
avg.widths = replicate(50, sapply(cors, intervals, n=250))
avg.widths = rowMeans(avg.widths)
```

```{r plotcors, exercise=TRUE}
dat = data.frame(correlation=cors, avg.widths=avg.widths)
library(ggplot2)
ggplot(dat, aes(correlation, avg.widths)) + 
  geom_path(color="orange",size=2) +
  geom_point(color="purple", size=2) + 
  scale_y_log10() + theme_minimal()
# If you don't like ggplot, try
# plot(cors, avg.widths, las=1, log='y', xlab='correlation')
```

```{r cor3-quiz}
quiz(caption="Question 3",
  question("From the plot above, what can you say about how correlation affects confidence intervals?",
    answer("The larger the magnitude of the correlation, the wider the confidence interval.", correct = TRUE),
    answer("Postively correlated predictors tend to have wider confidence intervals than negatively correlated predictors."),
    answer("Negatively correlated predictors tend to have wider confidence intervals than positively correlated predictors."),
    answer("There does not appear to be a relationship between the magnitude of the correlation and the width of the interval."),
    allow_retry = TRUE,
    correct = paste0("Good")
  )
)
```

###

This is the end of the lab.

Well done!